10.1016/j.scitotenv.2010.12.039  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   In this paper we propose a methodology consisting of specific computational intelligence methods, i.e. principal component analysis and artificial neural networks, in order to inter-compare air quality and meteorological data, and to forecast the concentration levels for environmental parameters of interest (air pollutants). We demonstrate these methods to data monitored in the urban areas of Thessaloniki and Helsinki in Greece and Finland, respectively. For this purpose, we applied the principal component analysis method in order to inter-compare the patterns of air pollution in the two selected cities. Then, we proceeded with the development of air quality forecasting models for both studied areas. On this basis, we formulated and employed a novel hybrid scheme in the selection process of input variables for the forecasting models, involving a combination of linear regression and artificial neural networks (multi-layer perceptron) models. The latter ones were used for the forecasting of the daily mean concentrations of PM₁₀ and PM₂.₅ for the next day. Results demonstrated an index of agreement between measured and modelled daily averaged PM₁₀ concentrations, between 0.80 and 0.85, while the kappa index for the forecasting of the daily averaged PM₁₀ concentrations reached 60% for both cities. Compared with previous corresponding studies, these statistical parameters indicate an improved performance of air quality parameters forecasting. It was also found that the performance of the models for the forecasting of the daily mean concentrations of PM₁₀ was not substantially different for both cities, despite the major differences of the two urban environments under consideration. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(11)00005-2  |  
------------------------------------------- 
10.1109/TNN.2011.2175748  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   A fuzzy min-max neural network based on data core (DCFMN) is proposed for pattern classification. A new membership function for classifying the neuron of DCFMN is defined in which the noise, the geometric center of the hyperbox, and the data core are considered. Instead of using the contraction process of the FMNN described by Simpson, a kind of overlapped neuron with new membership function based on the data core is proposed and added to neural network to represent the overlapping area of hyperboxes belonging to different classes. Furthermore, some algorithms of online learning and classification are presented according to the structure of DCFMN. DCFMN has strong robustness and high accuracy in classification taking onto account the effect of data core and noise. The performance of DCFMN is checked by some benchmark datasets and compared with some traditional fuzzy neural networks, such as the fuzzy min-max neural network (FMNN), the general FMNN, and the FMNN with compensatory neuron. Finally the pattern classification of a pipeline is evaluated using DCFMN and other classifiers. All the results indicate that the performance of DCFMN is excellent. 
  |  https://dx.doi.org/10.1109/TNN.2011.2175748  |  
------------------------------------------- 
10.1109/TCBB.2011.30  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Current feature selection methods for supervised classification of tissue samples from microarray data generally fail to exploit complementary discriminatory power that can be found in sets of features. Using a feature selection method with the computational architecture of the cross-entropy method, including an additional preliminary step ensuring a lower bound on the number of times any feature is considered, we show when testing on a human lymph node data set that there are a significant number of genes that perform well when their complementary power is assessed, but “pass under the radar” of popular feature selection methods that only assess genes individually on a given classification tool. We also show that this phenomenon becomes more apparent as diagnostic specificity of the tissue samples analysed increases. 
  |  https://dx.doi.org/10.1109/TCBB.2011.30  |  
------------------------------------------- 
10.1007/978-3-642-22092-0_25  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   We present a novel probabilistic framework to learn across several subjects a mapping from brain anatomical connectivity to functional connectivity, i.e. the covariance structure of brain activity. This prediction problem must be formulated as a structured-output learning task, as the predicted parameters are strongly correlated. We introduce a model selection framework based on cross-validation with a parametrization-independent loss function suitable to the manifold of covariance matrices. Our model is based on constraining the conditional independence structure of functional activity by the anatomical connectivity. Subsequently, we learn a linear predictor of a stationary multivariate autoregressive model. This natural parameterization of functional connectivity also enforces the positive-definiteness of the predicted covariance and thus matches the structure of the output space. Our results show that functional connectivity can be explained by anatomical connectivity on a rigorous statistical basis, and that a proper model of functional connectivity is essential to assess this link. 
  |  None  |  
------------------------------------------- 
10.1016/j.neuroimage.2011.08.031  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Functional MRI studies have uncovered a number of brain areas that demonstrate highly specific functional patterns. In the case of visual object recognition, small, focal regions have been characterized with selectivity for visual categories such as human faces. In this paper, we develop an algorithm that automatically learns patterns of functional specificity from fMRI data in a group of subjects. The method does not require spatial alignment of functional images from different subjects. The algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to learn the patterns of functional specificity shared across the group, which we call functional systems, and estimate the number of these systems. Inference based on our model enables automatic discovery and characterization of dominant and consistent functional systems. We apply the method to data from a visual fMRI study comprised of 69 distinct stimulus images. The discovered system activation profiles correspond to selectivity for a number of image categories such as faces, bodies, and scenes. Among systems found by our method, we identify new areas that are deactivated by face stimuli. In empirical comparisons with previously proposed exploratory methods, our results appear superior in capturing the structure in the space of visual categories of stimuli. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(11)00924-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21884803/  |  
------------------------------------------- 
10.1016/j.jtbi.2010.11.026  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   It is supposed that humans are genetically predisposed to be able to recognize sequences of context-free grammars with centre-embedded recursion while other primates are restricted to the recognition of finite state grammars with tail-recursion. Our aim was to construct a minimalist neural network that is able to parse artificial sentences of both grammars in an efficient way without using the biologically unrealistic backpropagation algorithm. The core of this network is a neural stack-like memory where the push and pop operations are regulated by synaptic gating on the connections between the layers of the stack. The network correctly categorizes novel sentences of both grammars after training. We suggest that the introduction of the neural stack memory will turn out to be substantial for any biological 'hierarchical processor' and the minimalist design of the model suggests a quest for similar, realistic neural architectures. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-5193(10)00621-1  |  
------------------------------------------- 
10.1016/j.biosystems.2011.03.006  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The emergence of a unified cognitive behaviour relies on the coordination of specialized components that distribute across a 'brain', body and environment. Although a general dynamical mechanism involved in agent-environment integration is still largely unknown for behavioural robustness, discussions here are focussed on one of the most plausible candidate: the formation of distributed mechanisms working in transient during agent-environment coupling. This article provides discussions on this sort of coordination based on a mobile object-tracking task with situated, embodied and minimal agents, and tests for robust yet adaptive behaviour. The proposed scenario provides examples of behavioural mechanisms that counterbalance the functional organization of internal control activity and agents' situatedness to enable the evolution of a two-agent interaction task. Discussions in this article suggest that future studies of distributed cognition should take into account that there are at least two possible modes of interpreting distributed mechanisms and that these have a qualitatively different effect on behavioural robustness. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0303-2647(11)00059-1  |  
------------------------------------------- 
10.1016/j.neuroimage.2011.10.015  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Different imaging modalities provide essential complementary information that can be used to enhance our understanding of brain disorders. This study focuses on integrating multiple imaging modalities to identify individuals at risk for mild cognitive impairment (MCI). MCI, often an early stage of Alzheimer's disease (AD), is difficult to diagnose due to its very mild or insignificant symptoms of cognitive impairment. Recent emergence of brain network analysis has made characterization of neurological disorders at a whole-brain connectivity level possible, thus providing new avenues for brain diseases classification. Employing multiple-kernel Support Vector Machines (SVMs), we attempt to integrate information from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (rs-fMRI) for improving classification performance. Our results indicate that the multimodality classification approach yields statistically significant improvement in accuracy over using each modality independently. The classification accuracy obtained by the proposed method is 96.3%, which is an increase of at least 7.4% from the single modality-based methods and the direct data fusion method. A cross-validation estimation of the generalization performance gives an area of 0.953 under the receiver operating characteristic (ROC) curve, indicating excellent diagnostic power. The multimodality classification approach hence allows more accurate early detection of brain abnormalities with greater sensitivity. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(11)01176-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22019883/  |  
------------------------------------------- 
10.3390/s110908626  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   One of the fundamental requirements for an artificial hand to successfully grasp and manipulate an object is to be able to distinguish different objects' shapes and, more specifically, the objects' surface curvatures. In this study, we investigate the possibility of enhancing the curvature detection of embedded tactile sensors by proposing a ridged fingertip structure, simulating human fingerprints. In addition, a curvature detection approach based on machine learning methods is proposed to provide the embedded sensors with the ability to discriminate the surface curvature of different objects. For this purpose, a set of experiments were carried out to collect tactile signals from a 2 × 2 tactile sensor array, then the signals were processed and used for learning algorithms. To achieve the best possible performance for our machine learning approach, three different learning algorithms of Naïve Bayes (NB), Artificial Neural Networks (ANN), and Support Vector Machines (SVM) were implemented and compared for various parameters. Finally, the most accurate method was selected to evaluate the proposed skin structure in recognition of three different curvatures. The results showed an accuracy rate of 97.5% in surface curvature discrimination. 
  |  http://www.mdpi.com/resolver?pii=s110908626  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22164095/  |  
------------------------------------------- 
10.1109/TSMCB.2010.2082525  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   In a clinical setting, pain is reported either through patient self-report or via an observer. Such measures are problematic as they are: 1) subjective, and 2) give no specific timing information. Coding pain as a series of facial action units (AUs) can avoid these issues as it can be used to gain an objective measure of pain on a frame-by-frame basis. Using video data from patients with shoulder injuries, in this paper, we describe an active appearance model (AAM)-based system that can automatically detect the frames in video in which a patient is in pain. This pain data set highlights the many challenges associated with spontaneous emotion detection, particularly that of expression and head movement due to the patient's reaction to pain. In this paper, we show that the AAM can deal with these movements and can achieve significant improvements in both the AU and pain detection performance compared to the current-state-of-the-art approaches which utilize similarity-normalized appearance features only. 
  |  https://dx.doi.org/10.1109/TSMCB.2010.2082525  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21097382/  |  
------------------------------------------- 
10.1039/c1mb05102d  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Dynamical modeling is an accurate tool for describing the dynamic regulation of one-carbon metabolism (1CM) with emphasis on the alteration of DNA methylation and/or dUMP methylation into dTMP. Using logic programming we present a comprehensive and adaptative mathematical model to study the impact of folate deficiency, including folate transport and enzymes activities. 5-Methyltetrahydrofolate (5mTHF) uptake and DNA and dUMP methylation were studied by simulating nutritional 5mTHF deficiency and methylenetetrahydrofolate reductase (MTHFR) gene defects. Both conditions had distinct effects on 1CM metabolite synthesis. Simulating severe 5mTHF deficiency (25% of normal levels) modulated 11 metabolites. However, simulating a severe decrease in MTHFR activity (25% of normal activity) modulated another set of metabolites. Two oscillations of varying amplitude were observed at the steady state for DNA methylation with severe 5mTHF deficiency, and the dUMP/dTMP ratio reached a steady state after 2 h, compared to 2.5 h for 100% 5mTHF. MTHFR activity with 25% of V(max) resulted in an increased methylated DNA pool after half an hour. We observed a deviation earlier in the profile compared to 50% and 100% V(max). For dUMP methylation, the highest level was observed with 25%, suggesting a low rate of dUMP methylation into dTMP with 25% of MTHFR activity. In conclusion, using logic programming we were able to construct the 1CM for analyzing the dynamic system behavior. This model may be used to refine biological interpretations of data or as a tool that can provide new hypotheses for pathogenesis. 
  |  https://doi.org/10.1039/c1mb05102d  |  
------------------------------------------- 
10.1016/j.artmed.2011.09.003  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objective:  Predicting or prioritizing the human genes that cause disease, or "disease genes", is one of the emerging tasks in biomedicine informatics. Research on network-based approach to this problem is carried out upon the key assumption of "the network-neighbour of a disease gene is likely to cause the same or a similar disease", and mostly employs data regarding well-known disease genes, using supervised learning methods. This work aims to find an effective method to exploit the disease gene neighbourhood and the integration of several useful omics data sources, which potentially enhance disease gene predictions. 
  Methods:  We have presented a novel method to effectively predict disease genes by exploiting, in the semi-supervised learning (SSL) scheme, data regarding both disease genes and disease gene neighbours via protein-protein interaction network. Multiple proteomic and genomic data were integrated from six biological databases, including Universal Protein Resource, Interologous Interaction Database, Reactome, Gene Ontology, Pfam, and InterDom, and a gene expression dataset. 
  Results:  By employing a 10 times stratified 10-fold cross validation, the SSL method performs better than the k-nearest neighbour method and the support vector machines method in terms of sensitivity of 85%, specificity of 79%, precision of 81%, accuracy of 82%, and a balanced F-function of 83%. The other comparative experimental evaluations demonstrate advantages of the proposed method given a small amount of labeled data with accuracy of 78%. We have applied the proposed method to detect 572 putative disease genes, which are biologically validated by some indirect ways. 
  Conclusion:  Semi-supervised learning improved ability to study disease genes, especially a specific disease when the known disease genes (as labeled data) are very often limited. In addition to the computational improvement, the analysis of predicted disease proteins indicates that the findings are beneficial in deciphering the pathogenic mechanisms. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(11)00123-0  |  
------------------------------------------- 
10.1074/mcp.M111.012161  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The goal of many shotgun proteomics experiments is to determine the protein complement of a complex biological mixture. For many mixtures, most methodological approaches fall significantly short of this goal. Existing solutions to this problem typically subdivide the task into two stages: first identifying a collection of peptides with a low false discovery rate and then inferring from the peptides a corresponding set of proteins. In contrast, we formulate the protein identification problem as a single optimization problem, which we solve using machine learning methods. This approach is motivated by the observation that the peptide and protein level tasks are cooperative, and the solution to each can be improved by using information about the solution to the other. The resulting algorithm directly controls the relevant error rate, can incorporate a wide variety of evidence and, for complex samples, provides 18-34% more protein identifications than the current state of the art approaches. 
  |  http://www.mcponline.org/cgi/pmidlookup?view=long&pmid=22052992  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22052992/  |  
------------------------------------------- 
10.1016/j.aap.2011.04.011  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The UK Intelligent Speed Adaptation (ISA) project produced a rich database with high-resolution data on driver behaviour covering a comprehensive range of road environment. The field trials provided vital information on driver behaviour in the presence of ISA. The purpose of this paper is to exploit the information gathered in the field trials to predict the impacts of various forms of ISA and to assess whether ISA is viable in terms of benefit-to-cost ratio. ISA is predicted to save up to 33% of accidents on urban roads, and to reduce CO(2) emissions by up to 5.8% on 70 mph roads. In order to investigate the long-term impacts of ISA, two hypothetical deployment scenarios were envisaged covering a 60-year appraisal period. The results indicate that ISA could deliver a very healthy benefit-to-cost ratio, ranging from 3.4 to 7.4, depending on the deployment scenarios. Under both deployment scenarios, ISA has recovered its implementation costs in less than 15 years. It can be concluded that implementation of ISA is clearly justified from a social cost and benefit perspective. Of the two deployment scenarios, the Market Driven one is substantially outperformed by the Authority Driven one. The benefits of ISA on fuel saving and emission reduction are real but not substantial, in comparison with the benefits on accident reduction; up to 98% of benefits are attributable to accident savings. Indeed, ISA is predicted to lead to a savings of 30% in fatal crashes and 25% in serious crashes over the 60-year period modelled. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0001-4575(11)00092-3  |  
------------------------------------------- 
10.1109/ICORR.2011.5975381  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Successful results have been booked with using robotics in therapy interventions for autism spectrum disorders (ASD). However, to make the best use of robots, the behavior of the robot needs to be tailored to the learning objectives and personal characteristics of each unique individual with ASD. Currently training practices include adaptation of the training programs to the condition of each individual client, based on the particular learning goals or the mood of the client. To include robots in such training will imply that the trainers are enabled to control a robot through an intuitive interface. For this purpose we use a visual programming environment called TiViPE as an interface between robot and trainer, where scenarios for specific learning objectives can easily be put together as if they were graphical LEGO-like building blocks. This programming platform is linked to the NAO robot from Aldebaran Robotics. A process flow for converting trainers' scenarios was developed to make sure the gist of the original scenarios was kept intact. We give an example of how a scenario is processed, and implemented into the clinical setting, and how detailed parts of a scenario can be developed. 
  |  https://dx.doi.org/10.1109/ICORR.2011.5975381  |  
------------------------------------------- 
10.1517/17425255.2011.611501  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Introduction:  Drug development is a time-consuming and cost-intensive process. On average, it takes around 12 - 15 years and approximately €800 billion to bring a new drug to the market. Despite introduction of combinatorial chemistry and establishment of high-throughput screening (HTS), the number of new drug entities is limited. In fact, a number of established drug entities have been withdrawn from the market because of drug-drug interactions (DDIs) and adverse drug reactions (ADRs). 
  Areas covered:  This review covers the advancements in cytochrome P450 (CYP450) modeling using different computational/machine learning (ML) tools over the past decade. A computational model for identifying non-toxic drug molecule from the pool of small chemical molecules is always welcome in the drug industry. Any computational tool that identifies the toxic molecule at early stage reduces the economic burden by slashing the number of molecules to be screened. This review covers all issues related to CYP-mediated toxicity such as specificity, inhibition, induction and regioselectivity. 
  Expert opinion:  Several computational methods for CYP-mediated toxicity are available, which are popular in computer-aided drug designing (CADD). These models may become helpful in toxicity prediction during early stages and can reduce high failure rates in preclinical and clinical trials. There is an urgent need to improve the accuracy, interpretability and confidence of the computation models used in drug discovery pathways. 
  |  http://www.tandfonline.com/doi/full/10.1517/17425255.2011.611501  |  
------------------------------------------- 
10.1016/j.neuroimage.2010.09.074  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   In this paper, we present a semi-supervised clustering-based framework for discovering coherent subpopulations in heterogeneous image sets. Our approach involves limited supervision in the form of labeled instances from two distributions that reflect a rough guess about subspace of features that are relevant for cluster analysis. By assuming that images are defined in a common space via registration to a common template, we propose a segmentation-based method for detecting locations that signify local regional differences in the two labeled sets. A PCA model of local image appearance is then estimated at each location of interest, and ranked with respect to its relevance for clustering. We develop an incremental k-means-like algorithm that discovers novel meaningful categories in a test image set. The application of our approach in this paper is in analysis of populations of healthy older adults. We validate our approach on a synthetic dataset, as well as on a dataset of brain images of older adults. We assess our method's performance on the problem of discovering clusters of MR images of human brain, and present a cluster-based measure of pathology that reflects the deviation of a subject's MR image from normal (i.e. cognitively stable) state. We analyze the clusters' structure, and show that clustering results obtained using our approach correlate well with clinical data. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(10)01276-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/20933091/  |  
------------------------------------------- 
10.1186/1752-0509-5-122  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Synthetic biology is used to develop cell factories for production of chemicals by constructively importing heterologous pathways into industrial microorganisms. In this work we present a retrosynthetic approach to the production of therapeutics with the goal of developing an in situ drug delivery device in host cells. Retrosynthesis, a concept originally proposed for synthetic chemistry, iteratively applies reversed chemical transformations (reversed enzyme-catalyzed reactions in the metabolic space) starting from a target product to reach precursors that are endogenous to the chassis. So far, a wider adoption of retrosynthesis into the manufacturing pipeline has been hindered by the complexity of enumerating all feasible biosynthetic pathways for a given compound. 
  Results:  In our method, we efficiently address the complexity problem by coding substrates, products and reactions into molecular signatures. Metabolic maps are represented using hypergraphs and the complexity is controlled by varying the specificity of the molecular signature. Furthermore, our method enables candidate pathways to be ranked to determine which ones are best to engineer. The proposed ranking function can integrate data from different sources such as host compatibility for inserted genes, the estimation of steady-state fluxes from the genome-wide reconstruction of the organism's metabolism, or the estimation of metabolite toxicity from experimental assays. We use several machine-learning tools in order to estimate enzyme activity and reaction efficiency at each step of the identified pathways. Examples of production in bacteria and yeast for two antibiotics and for one antitumor agent, as well as for several essential metabolites are outlined. 
  Conclusions:  We present here a unified framework that integrates diverse techniques involved in the design of heterologous biosynthetic pathways through a retrosynthetic approach in the reaction signature space. Our engineering methodology enables the flexible design of industrial microorganisms for the efficient on-demand production of chemical compounds with therapeutic applications. 
  |  https://bmcsystbiol.biomedcentral.com/articles/10.1186/1752-0509-5-122  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21819595/  |  
------------------------------------------- 
10.1089/dia.2010.0104  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Continuous glucose monitoring (CGM) technologies report measurements of interstitial glucose concentration every 5 min. CGM technologies have the potential to be utilized for prediction of prospective glucose concentrations with subsequent optimization of glycemic control. This article outlines a feed-forward neural network model (NNM) utilized for real-time prediction of glucose. 
  Methods:  A feed-forward NNM was designed for real-time prediction of glucose in patients with diabetes implementing a prediction horizon of 75 min. Inputs to the NNM included CGM values, insulin dosages, metered glucose values, nutritional intake, lifestyle, and emotional factors. Performance of the NNM was assessed in 10 patients not included in the model training set. 
  Results:  The NNM had a root mean squared error of 43.9 mg/dL and a mean absolute difference percentage of 22.1. The NNM routinely overestimates hypoglycemic extremes, which can be attributed to the limited number of hypoglycemic reactions in the model training set. The model predicts 88.6% of normal glucose concentrations (&gt; 70 and &lt; 180 mg/dL), 72.6% of hyperglycemia (≥ 180 mg/dL), and 2.1% of hypoglycemia (≤ 70 mg/dL). Clarke Error Grid Analysis of model predictions indicated that 92.3% of predictions could be regarded as clinically acceptable and not leading to adverse therapeutic direction. Of these predicted values, 62.3% and 30.0% were located within Zones A and B, respectively, of the error grid. 
  Conclusions:  Real-time prediction of glucose via the proposed NNM may provide a means of intelligent therapeutic guidance and direction. 
  |  https://www.liebertpub.com/doi/full/10.1089/dia.2010.0104?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1109/IEMBS.2011.6091764  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   We discuss computer-based image analysis algorithms of multi-parametric MRI of brain tumors, aiming to assist in early diagnosis of infiltrating brain tumors, and to construct statistical atlases summarizing population-based characteristics of brain tumors. These methods combine machine learning, deformable registration, multi-parametric segmentation, and biophysical modeling of brain tumors. 
  |  https://dx.doi.org/10.1109/IEMBS.2011.6091764  |  
------------------------------------------- 
10.1109/IEMBS.2011.6091553  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   This study examined the feasibility of decoding semantic information from human cortical activity. Four human subjects undergoing presurgical brain mapping and seizure foci localization participated in this study. Electrocorticographic (ECoG) signals were recorded while the subjects performed simple language tasks involving semantic information processing, such as a picture naming task where subjects named pictures of objects belonging to different semantic categories. Robust high-gamma band (60-120 Hz) activation was observed at the left inferior frontal gyrus (LIFG) and the posterior portion of the superior temporal gyrus (pSTG) with a temporal sequence corresponding to speech production and perception. Furthermore, Gaussian Naïve Bayes and Support Vector Machine classifiers, two commonly used machine learning algorithms for pattern recognition, were able to predict the semantic category of an object using cortical activity captured by ECoG electrodes covering the frontal, temporal and parietal cortices. These findings have implications for both basic neuroscience research and development of semantic-based brain-computer interface systems (BCI) that can help individuals with severe motor or communication disorders to express their intention and thoughts. 
  |  https://dx.doi.org/10.1109/IEMBS.2011.6091553  |  
------------------------------------------- 
10.1109/TNSRE.2010.2091429  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The task of discriminating the neural pathways responsible for the activity recorded using a multi-contact nerve cuff electrode has recently been approached as an inverse problem of source localization, similar to EEG source localization. A major drawback of this method is that it requires a model of the nerve, and that the localization performance is highly dependent on the accuracy of this model. Using recordings from a 56-contact "matrix" cuff electrode placed on a rat sciatic nerve, we investigated a method that eliminates the need for a model, and uses instead an "experimental" leadfield constructed from a training set of experimental recordings. The resulting pathway-identification task is solved using an inverse problem framework. The experimental leadfield approach was able to identify the correct branch in cases in which a single fascicle was active with a success rate of 94.2%, but was not able to reliably identify combinations of fascicles. Nevertheless, the proposed methodology provides a framework for the study of multi-pathway discrimination, within which methods to improve performance can be investigated. Specifically, the influence of nerve anatomy and electrode design should be examined, and regularization approaches better suited to this novel inverse problem should be sought. 
  |  https://dx.doi.org/10.1109/TNSRE.2010.2091429  |  
------------------------------------------- 
10.1117/1.3642010  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Glaucoma is a chronic neurodegenerative disease characterized by apoptosis of retinal ganglion cells and subsequent loss of visual function. Early detection of glaucoma is critical for the prevention of permanent structural damage and irreversible vision loss. Raman spectroscopy is a technique that provides rapid biochemical characterization of tissues in a nondestructive and noninvasive fashion. In this study, we explored the potential of using Raman spectroscopy for detection of glaucomatous changes in vitro. Raman spectroscopic imaging was conducted on retinal tissues of dogs with hereditary glaucoma and healthy control dogs. The Raman spectra were subjected to multivariate discriminant analysis with a support vector machine algorithm, and a classification model was developed to differentiate disease tissues versus healthy tissues. Spectroscopic analysis of 105 retinal ganglion cells (RGCs) from glaucomatous dogs and 267 RGCs from healthy dogs revealed spectroscopic markers that differentiated glaucomatous specimens from healthy controls. Furthermore, the multivariate discriminant model differentiated healthy samples and glaucomatous samples with good accuracy [healthy 89.5% and glaucomatous 97.6% for the same breed (Basset Hounds); and healthy 85.0% and glaucomatous 85.5% for different breeds (Beagles versus Basset Hounds)]. Raman spectroscopic screening can be used for in vitro detection of glaucomatous changes in retinal tissue with a high specificity. 
  |  https://doi.org/10.1117/1.3642010  |  
------------------------------------------- 
10.1109/TNSRE.2010.2069104  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   This paper proposes an advanced robust multivariable control strategy for a powered wheelchair system. The new control strategy is based on a combination of the systematic triangularization technique and the robust neuro-sliding mode control approach. This strategy effectively copes with parameter uncertainties and external disturbances in real-time in order to achieve robustness and optimal performance of a multivariable system. This novel strategy reduces coupling effects on a multivariable system, eliminates chattering phenomena, and avoids the plant Jacobian calculation problem. Furthermore, the strategy can also achieve fast and global convergence using less computation. The effectiveness of the new multivariable control strategy is verified in real-time implementation on a powered wheelchair system. The obtained results confirm that robustness and desired performance of the overall system are guaranteed, even under parameter uncertainty and external disturbance effects. 
  |  https://dx.doi.org/10.1109/TNSRE.2010.2069104  |  
------------------------------------------- 
10.1523/JNEUROSCI.2091-11.2011  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Currently, there are two opposing models for how voice and face information is integrated in the human brain to recognize person identity. The conventional model assumes that voice and face information is only combined at a supramodal stage (Bruce and Young, 1986; Burton et al., 1990; Ellis et al., 1997). An alternative model posits that areas encoding voice and face information also interact directly and that this direct interaction is behaviorally relevant for optimizing person recognition (von Kriegstein et al., 2005; von Kriegstein and Giraud, 2006). To disambiguate between the two different models, we tested for evidence of direct structural connections between voice- and face-processing cortical areas by combining functional and diffusion magnetic resonance imaging. We localized, at the individual subject level, three voice-sensitive areas in anterior, middle, and posterior superior temporal sulcus (STS) and face-sensitive areas in the fusiform gyrus [fusiform face area (FFA)]. Using probabilistic tractography, we show evidence that the FFA is structurally connected with voice-sensitive areas in STS. In particular, our results suggest that the FFA is more strongly connected to middle and anterior than to posterior areas of the voice-sensitive STS. This specific structural connectivity pattern indicates that direct links between face- and voice-recognition areas could be used to optimize human person recognition. 
  |  http://www.jneurosci.org/cgi/pmidlookup?view=long&pmid=21900569  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21900569/  |  
------------------------------------------- 
10.1109/TITB.2010.2095463  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The coordination between locomotion and respiration of Parkinson's disease (PD) patients is reduced or even absent. The degree of this disturbance is assumed to be associated with the disease severity [S. Schiermeier, D. Schäfer, T. Schäfer, W. Greulich, and M. E. Schläfke, "Breathing and locomotion in patients with Parkinson's disease," Eur. J. Physiol., vol. 443, No. 1, pp. 67-71, Jul. 2001]. To enable a long-term and online analysis of the locomotion-respiration coordination for scientific purpose, we have developed a distributed wireless communicating network. We aim to integrate biofeedback protocols with the real-time analysis of the locomotion-respiration coordination in the system to aid rehabilitation of PD patients. The network of sensor nodes is composed of intelligent network operating devices (iNODEs). The miniaturized iNODE contains a continuous data acquisition system based on microcontroller, local data storage, capability of on-sensor digital signal processing in real time, and wireless communication based on IEEE 802.15.4. Force sensing resistors and respiratory inductive plethysmography are applied for motion and respiration sensing, respectively. A number of experiments have been undertaken in clinic and laboratory to test the system. It shall facilitate identification of therapeutic effects on PD, allowing to measure the patients' health status, and to aid in the rehabilitation of PD patients. 
  |  https://dx.doi.org/10.1109/TITB.2010.2095463  |  
------------------------------------------- 
10.1007/978-3-642-23623-5_28  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Minimal invasive procedures such as transcatheter valve interventions are substituting conventional surgical techniques. Thus, novel operating rooms have been designed to augment traditional surgical equipment with advanced imaging systems to guide the procedures. We propose a novel method to fuse pre-operative and intra-operative information by jointly estimating anatomical models from multiple image modalities. Thereby high-quality patient-specific models are integrated into the imaging environment of operating rooms to guide cardiac interventions. Robust and fast machine learning techniques are utilized to guide the estimation process. Our method integrates both the redundant and complementary multimodal information to achieve a comprehensive modeling and simultaneously reduce the estimation uncertainty. Experiments performed on 28 patients with pairs of multimodal volumetric data are used to demonstrate high quality intra-operative patient-specific modeling of the aortic valve with a precision of 1.09mm in TEE and 1.73mm in 3D C-arm CT. Within a processing time of 10 seconds we additionally obtain model sensitive mapping between the pre- and intraoperative images. 
  |  None  |  
------------------------------------------- 
10.1186/1471-2105-12-424  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Supervised classifiers for digital pathology can improve the ability of physicians to detect and diagnose diseases such as cancer. Generating training data for classifiers is problematic, since only domain experts (e.g. pathologists) can correctly label ground truth data. Additionally, digital pathology datasets suffer from the "minority class problem", an issue where the number of exemplars from the non-target class outnumber target class exemplars which can bias the classifier and reduce accuracy. In this paper, we develop a training strategy combining active learning (AL) with class-balancing. AL identifies unlabeled samples that are "informative" (i.e. likely to increase classifier performance) for annotation, avoiding non-informative samples. This yields high accuracy with a smaller training set size compared with random learning (RL). Previous AL methods have not explicitly accounted for the minority class problem in biomedical images. Pre-specifying a target class ratio mitigates the problem of training bias. Finally, we develop a mathematical model to predict the number of annotations (cost) required to achieve balanced training classes. In addition to predicting training cost, the model reveals the theoretical properties of AL in the context of the minority class problem. 
  Results:  Using this class-balanced AL training strategy (CBAL), we build a classifier to distinguish cancer from non-cancer regions on digitized prostate histopathology. Our dataset consists of 12,000 image regions sampled from 100 biopsies (58 prostate cancer patients). We compare CBAL against: (1) unbalanced AL (UBAL), which uses AL but ignores class ratio; (2) class-balanced RL (CBRL), which uses RL with a specific class ratio; and (3) unbalanced RL (UBRL). The CBAL-trained classifier yields 2% greater accuracy and 3% higher area under the receiver operating characteristic curve (AUC) than alternatively-trained classifiers. Our cost model accurately predicts the number of annotations necessary to obtain balanced classes. The accuracy of our prediction is verified by empirically-observed costs. Finally, we find that over-sampling the minority class yields a marginal improvement in classifier accuracy but the improved performance comes at the expense of greater annotation cost. 
  Conclusions:  We have combined AL with class balancing to yield a general training strategy applicable to most supervised classification problems where the dataset is expensive to obtain and which suffers from the minority class problem. An intelligent training strategy is a critical component of supervised classification, but the integration of AL and intelligent choice of class ratios, as well as the application of a general cost model, will help researchers to plan the training process more quickly and effectively. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-424  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22034914/  |  
------------------------------------------- 
10.1093/rpd/ncq530  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Irradiation of individual cultured mammalian cells with a pre-selected number of ions down to one ion per single cell is a useful experimental approach to investigating the low-dose ionising radiation exposure effects and thus contributing to a more realistic human cancer risk assessment. One of the crucial tasks of all the microbeam apparatuses is the visualisation, recognition and positioning of every individual cell of the cell culture to be irradiated. Before irradiations, mammalian cells (specifically, Chinese hamster V79 cells) are seeded and grown as a monolayer on a mylar surface used as the bottom of a specially designed holder. Manual recognition of unstained cells in a bright-field microscope is a time-consuming procedure; therefore, a parallel algorithm has been conceived and developed in order to speed up this irradiation protocol step. Many technical problems have been faced to overcome the complexity of the images to be analysed: cell discrimination in an inhomogeneous background, among many disturbing bodies mainly due to the mylar surface roughness and culture medium bodies; cell shapes, depending on how they attach to the surface, which phase of the cell cycle they are in and on cell density. Preliminary results of the recognition and classification based on a method of wavelet kernels for the support vector machine classifier will be presented. 
  |  https://academic.oup.com/rpd/article-lookup/doi/10.1093/rpd/ncq530  |  
------------------------------------------- 
10.1109/TIP.2011.2172800  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   An approach to the problem of estimating the size of inhomogeneous crowds, which are composed of pedestrians that travel in different directions, without using explicit object segmentation or tracking is proposed. Instead, the crowd is segmented into components of homogeneous motion, using the mixture of dynamic-texture motion model. A set of holistic low-level features is extracted from each segmented region, and a function that maps features into estimates of the number of people per segment is learned with Bayesian regression. Two Bayesian regression models are examined. The first is a combination of Gaussian process regression with a compound kernel, which accounts for both the global and local trends of the count mapping but is limited by the real-valued outputs that do not match the discrete counts. We address this limitation with a second model, which is based on a Bayesian treatment of Poisson regression that introduces a prior distribution on the linear weights of the model. Since exact inference is analytically intractable, a closed-form approximation is derived that is computationally efficient and kernelizable, enabling the representation of nonlinear functions. An approximate marginal likelihood is also derived for kernel hyperparameter learning. The two regression-based crowd counting methods are evaluated on a large pedestrian data set, containing very distinct camera views, pedestrian traffic, and outliers, such as bikes or skateboarders. Experimental results show that regression-based counts are accurate regardless of the crowd size, outperforming the count estimates produced by state-of-the-art pedestrian detectors. Results on 2 h of video demonstrate the efficiency and robustness of the regression-based crowd size estimation over long periods of time. 
  |  https://dx.doi.org/10.1109/TIP.2011.2172800  |  
------------------------------------------- 
10.1016/j.compbiomed.2010.10.010  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   This work develops a decision support system based on machine learning and scoring measures to determine the type of urinary incontinence in women with low urinary tract symptoms. This system has two main branches. The former consists of selecting the feature set which best defines the UI type from the set of features (age, weight, etc.) characterizing a patient. This feature set is computed from several scoring measures. The patients characterized by the optimum feature set are then classified according to C4.5 and SVM classifiers. The results are evaluated according to Sensitivity and Specificity evaluation measures. The management of the final system is simple and its performance is high, getting Sensitivities over 80% and Specificities near 100% for some configurations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(10)00149-6  |  
------------------------------------------- 
10.1016/j.cmpb.2011.06.007  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Diabetic retinopathy (DR) is one of the most important complications of diabetes mellitus, which causes serious damages in the retina, consequently visual loss and sometimes blindness if necessary medical treatment is not applied on time. One of the difficulties in this illness is that the patient with diabetes mellitus requires a continuous screening for early detection. So far, numerous methods have been proposed by researchers to automate the detection process of DR in retinal fundus images. In this paper, we developed an alternative simple approach to detect DR. This method was built on the inverse segmentation method, which we suggested before to detect Age Related Macular Degeneration (ARMDs). Background image approach along with inverse segmentation is employed to measure and follow up the degenerations in retinal fundus images. Direct segmentation techniques generate unsatisfactory results in some cases. This is because of the fact that the texture of unhealthy areas such as DR is not homogenous. The inverse method is proposed to exploit the homogeneity of healthy areas rather than dealing with varying structure of unhealthy areas for segmenting bright lesions (hard exudates and cotton wool spots). On the other hand, the background image, dividing the retinal image into high and low intensity areas, is exploited in segmentation of hard exudates and cotton wool spots, and microaneurysms (MAs) and hemorrhages (HEMs), separately. Therefore, a complete segmentation system is developed for segmenting DR, including hard exudates, cotton wool spots, MAs, and HEMs. This application is able to measure total changes across the whole retinal image. Hence, retinal images that belong to the same patients are examined in order to monitor the trend of the illness. To make a comparison with other methods, a Naïve Bayes method is applied for segmentation of DR. The performance of the system, tested on different data sets including various qualities of retinal fundus images, is over 95% in detection of the optic disc (OD), and 90% in segmentation of the DR. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(11)00172-6  |  
------------------------------------------- 
10.1093/schbul/sbr145  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Reliable prognostic biomarkers are needed for the early recognition of psychosis. Recently, multivariate machine learning methods have demonstrated the feasibility to predict illness onset in clinically defined at-risk individuals using structural magnetic resonance imaging (MRI) data. However, it remains unclear whether these findings could be replicated in independent populations. 
  Methods:  We evaluated the performance of an MRI-based classification system in predicting disease conversion in at-risk individuals recruited within the prospective FePsy (Früherkennung von Psychosen) study at the University of Basel, Switzerland. Pairwise and multigroup biomarkers were constructed using the MRI data of 22 healthy volunteers, 16/21 at-risk subjects with/without a subsequent disease conversion. Diagnostic performance was measured in unseen test cases using repeated nested cross-validation. 
  Results:  The classification accuracies in the "healthy controls (HCs) vs converters," "HCs vs nonconverters," and "converters vs nonconverters" analyses were 92.3%, 66.9%, and 84.2%, respectively. A positive likelihood ratio of 6.5 in the converters vs nonconverters analysis indicated a 40% increase in diagnostic certainty by applying the biomarker to an at-risk population with a transition rate of 43%. The neuroanatomical decision functions underlying these results particularly involved the prefrontal perisylvian and subcortical brain structures. 
  Conclusions:  Our findings suggest that the early prediction of psychosis may be reliably enhanced using neuroanatomical pattern recognition operating at the single-subject level. These MRI-based biomarkers may have the potential to identify individuals at the highest risk of developing psychosis, and thus may promote informed clinical strategies aiming at preventing the full manifestation of the disease. 
  |  https://academic.oup.com/schizophreniabulletin/article-lookup/doi/10.1093/schbul/sbr145  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22080496/  |  
------------------------------------------- 
10.1111/j.1469-8986.2010.01155.x  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Most midlife women have hot flashes. The conventional criterion (≥2 μmho rise/30 s) for classifying hot flashes physiologically has shown poor performance. We improved this performance in the laboratory with Support Vector Machines (SVMs), a pattern classification method. We aimed to compare conventional to SVM methods to classify hot flashes in the ambulatory setting. Thirty-one women with hot flashes underwent 24 h of ambulatory sternal skin conductance monitoring. Hot flashes were quantified with conventional (≥2 μmho/30 s) and SVM methods. Conventional methods had low sensitivity (sensitivity=.57, specificity=.98, positive predictive value (PPV)=.91, negative predictive value (NPV)=.90, F1=.60), with performance lower with higher body mass index (BMI). SVMs improved this performance (sensitivity=.87, specificity=.97, PPV=.90, NPV=.96, F1=.88) and reduced BMI variation. SVMs can improve ambulatory physiologic hot flash measures. 
  |  https://doi.org/10.1111/j.1469-8986.2010.01155.x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21143609/  |  
------------------------------------------- 
10.1109/TIP.2011.2179054  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   We present an energy-minimization-based framework for locating the centerline and estimating the width of tubelike objects from their structural network with a nonparametric model. The nonparametric representation promotes simple modeling of nested branches and n -way furcations, i.e., structures that abound in an arterial network, e.g., a cerebrovascular circulation. Our method is capable of extracting the entire vascular tree from an angiogram in a single execution with a proper initialization. A succinct initial model from the user with arterial network inlets, outlets, and branching points is sufficient for complex vasculature. The novel method is based upon the theory of principal curves. In this paper, theoretical extension to grayscale angiography is discussed, and an algorithm to find an arterial network as principal curves is also described. Quantitative validation on a number of simulated data sets, synthetic volumes of 19 BrainWeb vascular models, and 32 Rotterdam Coronary Artery volumes was conducted. We compared the algorithm to a state-of-the-art method and further tested it on two clinical data sets. Our algorithmic outputs-lumen centers and flow channel widths-are important to various medical and clinical applications, e.g., vasculature segmentation, registration and visualization, virtual angioscopy, and vascular atlas formation and population study. 
  |  https://dx.doi.org/10.1109/TIP.2011.2179054  |  
------------------------------------------- 
10.1162/EVCO_a_00048  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Evolutionary robotics (ER) aims at automatically designing robots or controllers of robots without having to describe their inner workings. To reach this goal, ER researchers primarily employ phenotypes that can lead to an infinite number of robot behaviors and fitness functions that only reward the achievement of the task-and not how to achieve it. These choices make ER particularly prone to premature convergence. To tackle this problem, several papers recently proposed to explicitly encourage the diversity of the robot behaviors, rather than the diversity of the genotypes as in classic evolutionary optimization. Such an approach avoids the need to compute distances between structures and the pitfalls of the noninjectivity of the phenotype/behavior relation; however, it also introduces new questions: how to compare behavior? should this comparison be task specific? and what is the best way to encourage diversity in this context? In this paper, we review the main published approaches to behavioral diversity and benchmark them in a common framework. We compare each approach on three different tasks and two different genotypes. The results show that fostering behavioral diversity substantially improves the evolutionary process in the investigated experiments, regardless of genotype or task. Among the benchmarked approaches, multi-objective methods were the most efficient and the generic, Hamming-based, behavioral distance was at least as efficient as task specific behavioral metrics. 
  |  http://www.mitpressjournals.org/doi/full/10.1162/EVCO_a_00048?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1111/j.1468-3083.2010.03834.x  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  In vivo reflectance confocal microscopy (RCM) has been shown to be a valuable imaging tool in the diagnosis of melanocytic skin tumours. However, diagnostic image analysis performed by automated systems is to date quite rare. 
  Objectives:  In this study, we investigated the applicability of an automated image analysis system using a machine learning algorithm on diagnostic discrimination of benign and malignant melanocytic skin tumours in RCM. 
  Methods:  Overall, 16,269 RCM tumour images were evaluated. Image analysis was based on features of the wavelet transform. A learning set of 6147 images was used to establish a classification tree algorithm and an independent test set of 10, 122 images was applied to validate the tree model (grouping method 1). Additionally, randomly generated 'new' learning and test sets, tumour images only and different skin layers were evaluated (grouping method 2, 3 and 4). 
  Results:  The classification tree analysis correctly classified 93.60% of the melanoma and 90.40% of the nevi images of the learning set. When the classification tree was applied to the independent test set 46.71 ± 19.97% (range 7.81-83.87%) of the tumour images in benign melanocytic skin lesions were classified as 'malignant', in contrast to 55.68 ± 14.58% (range 30.65-83.59%; t-test: P &lt; 0.036) in malignant melanocytic skin lesions (grouping method 1). Further investigations could not improve the results significantly (grouping method 2, 3 and 4). 
  Conclusions:  The automated RCM image analysis procedure holds promise for further investigations. However, to date our system cannot be applied to routine skin tumour screening. 
  |  https://doi.org/10.1111/j.1468-3083.2010.03834.x  |  
------------------------------------------- 
10.1136/amiajnl-2011-000415  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objective:  Current techniques for knowledge-based Word Sense Disambiguation (WSD) of ambiguous biomedical terms rely on relations in the Unified Medical Language System Metathesaurus but do not take into account the domain of the target documents. The authors' goal is to improve these methods by using information about the topic of the document in which the ambiguous term appears. 
  Design:  The authors proposed and implemented several methods to extract lists of key terms associated with Medical Subject Heading terms. These key terms are used to represent the document topic in a knowledge-based WSD system. They are applied both alone and in combination with local context. 
  Measurements:  A standard measure of accuracy was calculated over the set of target words in the widely used National Library of Medicine WSD dataset. 
  Results and discussion:  The authors report a significant improvement when combining those key terms with local context, showing that domain information improves the results of a WSD system based on the Unified Medical Language System Metathesaurus alone. The best results were obtained using key terms obtained by relevance feedback and weighted by inverse document frequency. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1136/amiajnl-2011-000415  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21900701/  |  
------------------------------------------- 
10.1109/TBME.2010.2093576  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   In this paper, we propose a Markov random field-based method that uses saliency and gradient information for elastic registration of dynamic contrast enhanced (DCE) magnetic resonance (MR) images of the heart. DCE-MR images are characterized by rapid intensity changes over time, thus posing challenges for conventional intensity-based registration methods. Saliency information contributes to a contrast invariant metric to identify similar regions in spite of contrast enhancement. Its robustness and accuracy are attributed to a close adherence to a neurobiological model of the human visual system (HVS). The HVS has a remarkable ability to match images in the face of intensity changes and noise. This ability motivated us to explore the efficacy of such a model for registering DCE-MR images. The data penalty is a combination of saliency and gradient information. The smoothness cost depends upon the relative displacement and saliency difference of neighboring pixels. Saliency is also used in a modified narrow band graph cut framework to identify relevant pixels for registration, thus reducing the number of graph nodes and computation time. Experimental results on real patient images demonstrate superior registration accuracy for a combination of saliency and gradient information over other similarity metrics. 
  |  https://dx.doi.org/10.1109/TBME.2010.2093576  |  
------------------------------------------- 
10.1109/TBME.2011.2119484  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Whole knee joint MR image datasets were used to compare the performance of geometric trabecular bone features and advanced machine learning techniques in predicting biomechanical strength properties measured on the corresponding ex vivo specimens. Changes of trabecular bone structure throughout the proximal tibia are indicative of several musculoskeletal disorders involving changes in the bone quality and the surrounding soft tissue. Recent studies have shown that MR imaging also allows non-invasive 3-D characterization of bone microstructure. Sophisticated features like the scaling index method (SIM) can estimate local structural and geometric properties of the trabecular bone and may improve the ability of MR imaging to determine local bone quality in vivo. A set of 67 bone cubes was extracted from knee specimens and their biomechanical strength estimated by the yield stress (YS) [in MPa] was determined through mechanical testing. The regional apparent bone volume fraction (BVF) and SIM derived features were calculated for each bone cube. A linear multiregression analysis (MultiReg) and a optimized support vector regression (SVR) algorithm were used to predict the YS from the image features. The prediction accuracy was measured by the root mean square error (RMSE) for each image feature on independent test sets. The best prediction result with the lowest prediction error of RMSE = 1.021 MPa was obtained with a combination of BVF and SIM features and by using SVR. The prediction accuracy with only SIM features and SVR (RMSE = 1.023 MPa) was still significantly better than BVF alone and MultiReg (RMSE = 1.073 MPa). The current study demonstrates that the combination of sophisticated bone structure features and supervised learning techniques can improve MR-based determination of trabecular bone quality. 
  |  https://dx.doi.org/10.1109/TBME.2011.2119484  |  
------------------------------------------- 
10.1371/journal.pbio.1000615  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The evolution of altruism is a fundamental and enduring puzzle in biology. In a seminal paper Hamilton showed that altruism can be selected for when rb - c &gt; 0, where c is the fitness cost to the altruist, b is the fitness benefit to the beneficiary, and r is their genetic relatedness. While many studies have provided qualitative support for Hamilton's rule, quantitative tests have not yet been possible due to the difficulty of quantifying the costs and benefits of helping acts. Here we use a simulated system of foraging robots to experimentally manipulate the costs and benefits of helping and determine the conditions under which altruism evolves. By conducting experimental evolution over hundreds of generations of selection in populations with different c/b ratios, we show that Hamilton's rule always accurately predicts the minimum relatedness necessary for altruism to evolve. This high accuracy is remarkable given the presence of pleiotropic and epistatic effects as well as mutations with strong effects on behavior and fitness (effects not directly taken into account in Hamilton's original 1964 rule). In addition to providing the first quantitative test of Hamilton's rule in a system with a complex mapping between genotype and phenotype, these experiments demonstrate the wide applicability of kin selection theory. 
  |  http://dx.plos.org/10.1371/journal.pbio.1000615  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21559320/  |  
------------------------------------------- 
10.1016/j.ridd.2011.07.004  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Three-dimensional gait analysis (3DGA) generates a wealth of highly variable data. Gait classifications help to reduce, simplify and interpret this vast amount of 3DGA data and thereby assist and facilitate clinical decision making in the treatment of CP. CP gait is often a mix of several clinically accepted distinct gait patterns. Therefore, there is a need for a classification which characterizes each CP gait by different degrees of membership for several gait patterns, which are considered by clinical experts to be highly relevant. In this respect, this paper introduces Bayesian networks (BN) as a new approach for classification of 3DGA data of the ankle and knee in children with CP. A BN is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph. Furthermore, they provide an explicit way of introducing clinical expertise as prior knowledge to guide the BN in its analysis of the data and the underlying clinically relevant relationships. BNs also enable to classify gait on a continuum of patterns, as their outcome consists of a set of probabilistic membership values for different clinically accepted patterns. A group of 139 patients with CP was recruited and divided into a training- (n=80% of all patients) and a validation-dataset (n=20% of all patients). An average classification accuracy of 88.4% was reached. The BN of this study achieved promising accuracy rates and was found to be successful for classifying ankle and knee joint motion on a continuum of different clinically relevant gait patterns. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0891-4222(11)00260-5  |  
------------------------------------------- 
10.1002/mrm.22584  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Among recent parallel MR imaging reconstruction advances, a Bayesian method called Edge-preserving Parallel Imaging reconstructions with GRAph cuts Minimization (EPIGRAM) has been demonstrated to significantly improve signal-to-noise ratio when compared with conventional regularized sensitivity encoding method. However, EPIGRAM requires a large number of iterations in proportion to the number of intensity labels in the image, making it computationally expensive for high dynamic range images. The objective of this study is to develop a Fast EPIGRAM reconstruction based on the efficient binary jump move algorithm that provides a logarithmic reduction in reconstruction time while maintaining image quality. Preliminary in vivo validation of the proposed algorithm is presented for two-dimensional cardiac cine MR imaging and three-dimensional coronary MR angiography at acceleration factors of 2-4. Fast EPIGRAM was found to provide similar image quality to EPIGRAM and maintain the previously reported signal-to-noise ratio improvement over regularized sensitivity encoding method, while reducing EPIGRAM reconstruction time by 25-50 times. 
  |  https://doi.org/10.1002/mrm.22584  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/20939095/  |  
------------------------------------------- 
10.1109/TMI.2010.2091513  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   A general approach to the first-order analysis of error in rigid point registration is presented that accommodates fiducial localization error (FLE) that may be inhomogeneous (varying from point to point) and anisotropic (varying with direction) and also accommodates arbitrary weighting that may also be inhomogeneous and anisotropic. Covariances are derived for target registration error (TRE) and for weighted fiducial registration error (FRE) in terms of covariances of FLE, culminating in a simple implementation that encompasses all combinations of weightings and anisotropy. Furthermore, it is shown that for ideal weighting, in which the weighting matrix for each fiducial equals the inverse of the square root of the cross covariance of its two-space FLE, fluctuations of FRE and TRE are mutually independent. These results are validated by comparison with previously published expressions and by simulation. Furthermore, simulations for randomly generated fiducial positions and FLEs are presented that show that correlation is negligible (correlation coefficient &lt; 0.1) in the exact case for both ideal and uniform weighting (i.e., no weighting), the latter of which is employed in commercial surgical guidance systems. From these results we conclude that for these weighting schemes, while valid expressions exist relating the covariance of FRE to the covariance of TRE, there are no measures of the goodness of fit of the fiducials for a given registration that give to first order any information about the fluctuation of TRE from its expected value and none that give useful information in the exact case. Therefore, as estimators of registration accuracy, such measures should be approached with extreme caution both by the purveyors of guidance systems and by the practitioners who use them. 
  |  https://dx.doi.org/10.1109/TMI.2010.2091513  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21075718/  |  
------------------------------------------- 
10.1007/s11548-011-0645-6  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Purpose:  Accurate and robust estimates of camera position and orientation in a bronchoscope are required for navigation. Fusion of pre-interventional information (e.g., CT, MRI, or US) and intra-interventional information (e.g., bronchoscopic video) were incorporated into a navigation system to provide physicians with an augmented reality environment for bronchoscopic interventions. 
  Methods:  Two approaches were used to predict bronchoscope movements by incorporating sequential Monte Carlo (SMC) simulation including (1) image-based tracking techniques and (2) electromagnetic tracking (EMT) methods. SMC simulation was introduced to model ambiguities or uncertainties that occurred in image- and EMT-based bronchoscope tracking. Scale invariant feature transform (SIFT) features were employed to overcome the limitations of image-based motion tracking methods. Validation was performed on five phantom and ten human case datasets acquired in the supine position. 
  Results:  For dynamic phantom validation, the EMT-SMC simulation method improved the tracking performance of the successfully registered bronchoscopic video frames by 12.7% compared with a hybrid-based method. In comparisons between tracking results and ground truth, the accuracy of the EMT-SMC simulation method was 1.51 mm (positional error) and 5.44° (orientation error). During patient assessment, the SIFT-SMC simulation scheme was more stable or robust than a previous image-based approach for bronchoscope motion estimation, showing 23.6% improvement of successfully tracked frames. Comparing the estimates of our method to ground truth, the position and orientation errors are 3.72 mm and 10.2°, while those of our previous image-based method were at least 7.77 mm and 19.3°. The computational times of our EMT- and SIFT-SMC simulation methods were 0.9 and 1.2 s per frame, respectively. 
  Conclusion:  The SMC simulation method was developed to model ambiguities that occur in bronchoscope tracking. This method more stably and accurately predicts the bronchoscope camera position and orientation parameters, reducing uncertainties due to problematic bronchoscopic video frames and airway deformation during intra-bronchoscopy navigation. 
  |  https://dx.doi.org/10.1007/s11548-011-0645-6  |  
------------------------------------------- 
10.1016/j.jbi.2011.01.004  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objective:  Clinical questions are often long and complex and take many forms. We have built a clinical question answering system named AskHERMES to perform robust semantic analysis on complex clinical questions and output question-focused extractive summaries as answers. 
  Design:  This paper describes the system architecture and a preliminary evaluation of AskHERMES, which implements innovative approaches in question analysis, summarization, and answer presentation. Five types of resources were indexed in this system: MEDLINE abstracts, PubMed Central full-text articles, eMedicine documents, clinical guidelines and Wikipedia articles. 
  Measurement:  We compared the AskHERMES system with Google (Google and Google Scholar) and UpToDate and asked physicians to score the three systems by ease of use, quality of answer, time spent, and overall performance. 
  Results:  AskHERMES allows physicians to enter a question in a natural way with minimal query formulation and allows physicians to efficiently navigate among all the answer sentences to quickly meet their information needs. In contrast, physicians need to formulate queries to search for information in Google and UpToDate. The development of the AskHERMES system is still at an early stage, and the knowledge resource is limited compared with Google or UpToDate. Nevertheless, the evaluation results show that AskHERMES' performance is comparable to the other systems. In particular, when answering complex clinical questions, it demonstrates the potential to outperform both Google and UpToDate systems. 
  Conclusions:  AskHERMES, available at http://www.AskHERMES.org, has the potential to help physicians practice evidence-based medicine and improve the quality of patient care. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1532-0464(11)00006-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21256977/  |  
------------------------------------------- 
10.1016/j.ijmedinf.2011.02.008  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  As the volume of biomedical text increases exponentially, automatic indexing becomes increasingly important. However, existing approaches do not distinguish central (or core) concepts from concepts that were mentioned in passing. We focus on the problem of indexing MEDLINE records, a process that is currently performed by highly trained humans at the National Library of Medicine (NLM). NLM indexers are assisted by a system called the Medical Text Indexer (MTI) that suggests candidate indexing terms. 
  Objective:  To improve the ability of MTI to select the core terms in MEDLINE abstracts. These core concepts are deemed to be most important and are designated as "major headings" by MEDLINE indexers. We introduce and evaluate a graph-based indexing methodology called MEDRank that generates concept graphs from biomedical text and then ranks the concepts within these graphs to identify the most important ones. 
  Methods:  We insert a MEDRank step into the MTI and compare MTI's output with and without MEDRank to the MEDLINE indexers' selected terms for a sample of 11,803 PubMed Central articles. We also tested whether human raters prefer terms generated by the MEDLINE indexers, MTI without MEDRank, and MTI with MEDRank for a sample of 36 PubMed Central articles. 
  Results:  MEDRank improved recall of major headings designated by 30% over MTI without MEDRank (0.489 vs. 0.376). Overall recall was only slightly (6.5%) higher (0.490 vs. 0.460) as was F(2) (3%, 0.408 vs. 0.396). However, overall precision was 3.9% lower (0.268 vs. 0.279). Human raters preferred terms generated by MTI with MEDRank over terms generated by MTI without MEDRank (by an average of 1.00 more term per article), and preferred terms generated by MTI with MEDRank and the MEDLINE indexers at the same rate. 
  Conclusions:  The addition of MEDRank to MTI significantly improved the retrieval of core concepts in MEDLINE abstracts and more closely matched human expectations compared to MTI without MEDRank. In addition, MEDRank slightly improved overall recall and F(2). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1386-5056(11)00054-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21439897/  |  
------------------------------------------- 
10.1016/j.plaphy.2011.01.002  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Identification of regulatory relationships between transcription factors (TFs) and their targets is a central problem in post-genomic biology. In this paper, we apply an approach based on the support vector machine (SVM) and gene-expression data to predict the regulatory interactions in Arabidopsis. A set of 125 experimentally validated TF-target interactions and 750 negative regulatory gene pairs are collected as the training data. Their expression profiles data at 79 experimental conditions are fed to the SVM to perform the prediction. Through the jackknife cross-validation test, we find that the overall prediction accuracy of our approach achieves 88.68%. Our approach could help to widen the understanding of Arabidopsis gene regulatory scheme and may offer a cost-effective alternative to construct the gene regulatory network. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0981-9428(11)00003-9  |  
------------------------------------------- 
10.1109/TCBB.2009.48  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Recent experimental advances facilitate the collection of time series data that indicate which genes in a cell are expressed. This information can be used to understand the genetic regulatory network that generates the data. Typically, Bayesian analysis approaches are applied which neglect the time series nature of the experimental data, have difficulty in determining the direction of causality, and do not perform well on networks with tight feedback. To address these problems, this paper presents a method to learn genetic network connectivity which exploits the time series nature of experimental data to achieve better causal predictions. This method first breaks up the data into bins. Next, it determines an initial set of potential influence vectors for each gene based upon the probability of the gene's expression increasing in the next time step. These vectors are then combined to form new vectors with better scores. Finally, these influence vectors are competed against each other to determine the final influence vector for each gene. The result is a directed graph representation of the genetic network's repression and activation connections. Results are reported for several synthetic networks with tight feedback showing significant improvements in recall and runtime over Yu's dynamic Bayesian approach. Promising preliminary results are also reported for an analysis of experimental data for genes involved in the yeast cell cycle. 
  |  https://dx.doi.org/10.1109/TCBB.2009.48  |  
------------------------------------------- 
10.1109/TITB.2011.2107916  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Posture analysis in quiet standing is a key component of the clinical evaluation of Parkinson's disease (PD), postural instability being one of PD's major symptoms. The aim of this study was to assess the feasibility of using accelerometers to characterize the postural behavior of early mild PD subjects. Twenty PD and 20 control subjects, wearing an accelerometer on the lower back, were tested in five conditions characterized by sensory and attentional perturbation. A total of 175 measures were computed from the signals to quantify tremor, acceleration, and displacement of body sway. Feature selection was implemented to identify the subsets of measures that better characterize the distinctive behavior of PD and control subjects. It was based on different classifiers and on a nested cross validation, to maximize robustness of selection with respect to changes in the training set. Several subsets of three features achieved misclassification rates as low as 5%. Many of them included a tremor-related measure, a postural measure in the frequency domain, and a postural displacement measure. Results suggest that quantitative posture analysis using a single accelerometer and a simple test protocol may provide useful information to characterize early PD subjects. This protocol is potentially usable to monitor the disease's progression. 
  |  https://dx.doi.org/10.1109/TITB.2011.2107916  |  
------------------------------------------- 
10.1016/j.neuroimage.2011.03.027  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Characterization and quantification of magnetic resonance perfusion images is important for clinical interpretation, though this calls for a reproducible and accurate method of analysis and a robust healthy reference. The few studies which have examined the perfusion of the healthy brain using dynamic susceptibility contrast (DSC) imaging were largely limited to manual definition of the regions of interest (ROI) and results were dependent on the location of the ROI. The current study aimed to develop a methodology for DSC data analysis and to obtain reference values of healthy subjects. Twenty three healthy volunteers underwent DSC. An unsupervised multiparametric clustering method was applied to four perfusion parameters. Three clusters were defined and identified as: dura-blood-vessels, gray matter and white matter and their vascular characteristics were obtained. Additionally, regional perfusion differences were studied and revealed a prolonged mean transient time and a trend for higher vascularity in the posterior compared with the anterior and middle cerebral vascular territories. While additional studies are required to confirm our findings, this result may have important clinical implications. The proposed unsupervised multiparametric method enabled accurate tissue differentiation, is easy replicable and has a wide range of applications in both pathological and healthy brains. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(11)00303-X  |  
------------------------------------------- 
10.1109/TIP.2010.2076820  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Human pose estimation via motion tracking systems can be considered as a regression problem within a discriminative framework. It is always a challenging task to model the mapping from observation space to state space because of the high-dimensional characteristic in the multimodal conditional distribution. In order to build the mapping, existing techniques usually involve a large set of training samples in the learning process which are limited in their capability to deal with multimodality. We propose, in this work, a novel online sparse Gaussian Process (GP) regression model to recover 3-D human motion in monocular videos. Particularly, we investigate the fact that for a given test input, its output is mainly determined by the training samples potentially residing in its local neighborhood and defined in the unified input-output space. This leads to a local mixture GP experts system composed of different local GP experts, each of which dominates a mapping behavior with the specific covariance function adapting to a local region. To handle the multimodality, we combine both temporal and spatial information therefore to obtain two categories of local experts. The temporal and spatial experts are integrated into a seamless hybrid system, which is automatically self-initialized and robust for visual tracking of nonlinear human motion. Learning and inference are extremely efficient as all the local experts are defined online within very small neighborhoods. Extensive experiments on two real-world databases, HumanEva and PEAR, demonstrate the effectiveness of our proposed model, which significantly improve the performance of existing models. 
  |  https://dx.doi.org/10.1109/TIP.2010.2076820  |  
------------------------------------------- 
10.1016/j.media.2011.11.008  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Respiratory motion is a challenging factor for image acquisition and image-guided procedures in the abdominal and thoracic region. In order to address the issues arising from respiratory motion, it is often necessary to detect the respiratory signal. In this article, we propose a novel, purely image-based retrospective respiratory gating method for ultrasound and MRI. Further, we apply this technique to acquire breathing-affected 4D ultrasound with a wobbler probe and, similarly, to create 4D MR with a slice stacking approach. We achieve the gating with Laplacian eigenmaps, a manifold learning technique, to determine the low-dimensional manifold embedded in the high-dimensional image space. Since Laplacian eigenmaps assign to each image frame a coordinate in low-dimensional space by respecting the neighborhood relationship, they are well suited for analyzing the breathing cycle. We perform the image-based gating on several 2D and 3D ultrasound datasets over time, and quantify its very good performance by comparing it to measurements from an external gating system. For MRI, we perform the manifold learning on several datasets for various orientations and positions. We achieve very high correlations by a comparison to an alternative gating with diaphragm tracking. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(11)00170-8  |  
------------------------------------------- 
10.1016/j.aap.2011.03.014  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The Intelligent Speed Adaptation (ISA) project we describe in this article is based on Pay as You Drive principles. These principles assume that the ISA equipment informs a driver of the speed limit, warns the driver when speeding and calculates penalty points. Each penalty point entails the reduction of a 30% discount on the driver's car insurance premium, which therefore produced the name, Pay as You Speed. The ISA equipment consists of a GPS-based On Board Unit with a mobile phone connection to a web server. The project was planned for a three-year test period with 300 young car drivers, but it never succeeded in recruiting that number of drivers. After several design changes, the project eventually went forward with 153 test drivers of all ages. This number represents approximately one thousandth of all car owners in the proving ground of North Jutland in Denmark. Furthermore the project was terminated before its scheduled closing date. This article describes the project with an emphasis on recruitment efforts and the project's progress. We include a discussion of possible explanations for the failure to recruit volunteers for the project and reflect upon the general barriers to using ISA with ordinary drivers. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0001-4575(11)00065-0  |  
------------------------------------------- 
10.1186/1471-2105-12-329  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  BRENDA (BRaunschweig ENzyme DAtabase, http://www.brenda-enzymes.org) is a major resource for enzyme related information. First and foremost, it provides data which are manually curated from the primary literature. DRENDA (Disease RElated ENzyme information DAtabase) complements BRENDA with a focus on the automatic search and categorization of enzyme and disease related information from title and abstracts of primary publications. In a two-step procedure DRENDA makes use of text mining and machine learning methods. 
  Results:  Currently enzyme and disease related references are biannually updated as part of the standard BRENDA update. 910,897 relations of EC-numbers and diseases were extracted from titles or abstracts and are included in the second release in 2010. The enzyme and disease entity recognition has been successfully enhanced by a further relation classification via machine learning. The classification step has been evaluated by a 5-fold cross validation and achieves an F1 score between 0.802 ± 0.032 and 0.738 ± 0.033 depending on the categories and pre-processing procedures. In the eventual DRENDA content every category reaches a classification specificity of at least 96.7% and a precision that ranges from 86-98% in the highest confidence level, and 64-83% for the smallest confidence level associated with higher recall. 
  Conclusions:  The DRENDA processing chain analyses PubMed, locates references with disease-related information on enzymes and categorises their focus according to the categories causal interaction, therapeutic application, diagnostic usage and ongoing research. The categorisation gives an impression on the focus of the located references. Thus, the relation categorisation can facilitate orientation within the rapidly growing number of references with impact on diseases and enzymes. The DRENDA information is available as additional information in BRENDA. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-329  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21827651/  |  
------------------------------------------- 
10.1093/bioinformatics/btr204  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Motivation:  Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification. 
  Results:  In this article, we present a Support Vector Machine classifier that can correct the prediction for observed confounding factors. This is achieved by minimizing the statistical dependence between the classifier and the confounding factors. We prove that this formulation can be transformed into a standard Support Vector Machine with rescaled input data. In our experiments, our confounder correcting SVM (ccSVM) improves tumor diagnosis based on samples from different labs, tuberculosis diagnosis in patients of varying age, ethnicity and gender, and phenotype prediction in the presence of population structure and outperforms state-of-the-art methods in terms of prediction accuracy. 
  Availability:  A ccSVM-implementation in MATLAB is available from http://webdav.tuebingen.mpg.de/u/karsten/Forschung/ISMB11_ccSVM/. 
  Contact:  limin.li@tuebingen.mpg.de; karsten.borgwardt@tuebingen.mpg.de. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr204  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21685091/  |  
------------------------------------------- 
10.1142/9789814335058_0007  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   RNA virus phenotypic changes often result from multiple alternative molecular mechanisms, where each mechanism involves changes to a small number of key residues. Accordingly, we propose to learn genotype-phenotype functions, using Disjunctive Normal Form (DNF) as the assumed functional form. In this study we develop DNF learning algorithms that attempt to construct predictors as Boolean combinations of covariates. We demonstrate the learning algorithm's consistency and efficiency on simulated sequences, and establish their biological relevance using a variety of real RNA virus datasets representing different viral phenotypes, including drug resistance, antigenicity, and pathogenicity. We compare our algorithms with previously published machine learning algorithms in terms of prediction quality: leave-one-out performance shows superior accuracy to other machine learning algorithms on the HIV drug resistance dataset and the UCIs promoter gene dataset. The algorithms are powerful in inferring the genotype-phenotype mapping from a moderate number of labeled sequences, as are typically produced in mutagenesis experiments. They can also greedily learn DNFs from large datasets. The Java implementation of our algorithms will be made publicly available. 
  |  https://doi.org/10.1142/9789814335058_0007  |  
------------------------------------------- 
10.1118/1.3539749  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Purpose:  To ensure plan quality for adaptive IMRT of the prostate, we developed a quantitative evaluation tool using a machine learning approach. This tool generates dose volume histograms (DVHs) of organs-at-risk (OARs) based on prior plans as a reference, to be compared with the adaptive plan derived from fluence map deformation. 
  Methods:  Under the same configuration using seven-field 15 MV photon beams, DVHs of OARs (bladder and rectum) were estimated based on anatomical information of the patient and a model learned from a database of high quality prior plans. In this study, the anatomical information was characterized by the organ volumes and distance-to-target histogram (DTH). The database consists of 198 high quality prostate plans and was validated with 14 cases outside the training pool. Principal component analysis (PCA) was applied to DVHs and DTHs to quantify their salient features. Then, support vector regression (SVR) was implemented to establish the correlation between the features of the DVH and the anatomical information. 
  Results:  DVH/DTH curves could be characterized sufficiently just using only two or three truncated principal components, thus, patient anatomical information was quantified with reduced numbers of variables. The evaluation of the model using the test data set demonstrated its accuracy approximately 80% in prediction and effectiveness in improving ART planning quality. 
  Conclusions:  An adaptive IMRT plan quality evaluation tool based on machine learning has been developed, which estimates OAR sparing and provides reference in evaluating ART. 
  |  https://doi.org/10.1118/1.3539749  |  
------------------------------------------- 
10.1016/j.mri.2010.07.004  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Magnetic resonance diffusion tensor imaging with fiber tracking is used for 3-dimensional visualization of the nervous system. Peripheral nerves and all cranial nerves, except for the olfactory tract, have previously been visualized. The olfactory tracts are difficult to depict with diffusion-weighted imaging due to the high sensitivity to susceptibility artifacts at the base of the skull. Here we report an optimized single-shot diffusion-weighted echo planar imaging sequence that can visualize the olfactory tracts with fiber tracking. Five healthy individuals were examined, and the olfactory tracts could be fiber tracked with the diffusion-weighted sequence. For comparison and as a negative control, an anosmic patient was examined. No olfactory tracts were visualized on T2-weighted nor diffusion-weighted fiber tracking images. Measuring diffusion in the olfactory tracts promise to facilitate the identification of different hyposmic and anosmic conditions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0730-725X(10)00205-5  |  
------------------------------------------- 
10.2217/pgs.11.76  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Pharmacogenetics aims to elucidate the genetic factors underlying the individual's response to pharmacotherapy. Coupled with the recent (and ongoing) progress in high-throughput genotyping, sequencing and other genomic technologies, pharmacogenetics is rapidly transforming into pharmacogenomics, while pursuing the primary goals of identifying and studying the genetic contribution to drug therapy response and adverse effects, and existing drug characterization and new drug discovery. Accomplishment of both of these goals hinges on gaining a better understanding of the underlying biological systems; however, reverse-engineering biological system models from the massive datasets generated by the large-scale genetic epidemiology studies presents a formidable data analysis challenge. In this article, we review the recent progress made in developing such data analysis methodology within the paradigm of systems biology research that broadly aims to gain a 'holistic', or 'mechanistic' understanding of biological systems by attempting to capture the entirety of interactions between the components (genetic and otherwise) of the system. 
  |  http://www.futuremedicine.com/doi/full/10.2217/pgs.11.76?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21919609/  |  
------------------------------------------- 
10.1016/j.jaci.2011.02.039  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Common variable immunodeficiency (CVID) is a heterogeneous immune defect characterized by hypogammaglobulinemia, failure of specific antibody production, susceptibility to infections, and an array of comorbidities. 
  Objective:  To address the underlying immunopathogenesis of CVID and comorbidities, we conducted the first genome-wide association and gene copy number variation (CNV) study in patients with CVID. 
  Methods:  Three hundred sixty-three patients with CVID from 4 study sites were genotyped with 610,000 single nucleotide polymorphisms (SNPs). Patients were divided into a discovery cohort of 179 cases in comparison with 1,917 control subjects and a replication cohort of 109 cases and 1,114 control subjects. 
  Results:  Our analyses detected strong association with the MHC region and association with a disintegrin and metalloproteinase (ADAM) genes (P combined = 1.96 × 10(-7)) replicated in the independent cohort. CNV analysis defined 16 disease-associated deletions and duplications, including duplication of origin recognition complex 4L (ORC4L) that was unique to 15 cases (P = 8.66 × 10(-16)), as well as numerous unique rare intraexonic deletions and duplications suggesting multiple novel genetic causes of CVID. Furthermore, the 1,000 most significant SNPs were strongly predictive of the CVID phenotype by using a Support Vector Machine algorithm with positive and negative predictive values of 1.0 and 0.957, respectively. 
  Conclusion:  Our integrative genome-wide analysis of SNP genotypes and CNVs has uncovered multiple novel susceptibility loci for CVID, both common and rare, which is consistent with the highly heterogeneous nature of CVID. These results provide new mechanistic insights into immunopathogenesis based on these unique genetic variations and might allow for improved diagnosis of CVID based on accurate prediction of the CVID clinical phenotypes by using our Support Vector Machine model. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0091-6749(11)00367-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21497890/  |  
------------------------------------------- 
10.1016/j.jtbi.2010.10.037  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Some creatures living in extremely low temperatures can produce some special materials called "antifreeze proteins" (AFPs), which can prevent the cell and body fluids from freezing. AFPs are present in vertebrates, invertebrates, plants, bacteria, fungi, etc. Although AFPs have a common function, they show a high degree of diversity in sequences and structures. Therefore, sequence similarity based search methods often fails to predict AFPs from sequence databases. In this work, we report a random forest approach "AFP-Pred" for the prediction of antifreeze proteins from protein sequence. AFP-Pred was trained on the dataset containing 300 AFPs and 300 non-AFPs and tested on the dataset containing 181 AFPs and 9193 non-AFPs. AFP-Pred achieved 81.33% accuracy from training and 83.38% from testing. The performance of AFP-Pred was compared with BLAST and HMM. High prediction accuracy and successful of prediction of hypothetical proteins suggests that AFP-Pred can be a useful approach to identify antifreeze proteins from sequence information, irrespective of their sequence similarity. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-5193(10)00584-9  |  
------------------------------------------- 
10.1109/TMI.2010.2094200  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The histopathological examination of tissue specimens is essential for cancer diagnosis and grading. However, this examination is subject to a considerable amount of observer variability as it mainly relies on visual interpretation of pathologists. To alleviate this problem, it is very important to develop computational quantitative tools, for which image segmentation constitutes the core step. In this paper, we introduce an effective and robust algorithm for the segmentation of histopathological tissue images. This algorithm incorporates the background knowledge of the tissue organization into segmentation. For this purpose, it quantifies spatial relations of cytological tissue components by constructing a graph and uses this graph to define new texture features for image segmentation. This new texture definition makes use of the idea of gray-level run-length matrices. However, it considers the runs of cytological components on a graph to form a matrix, instead of considering the runs of pixel intensities. Working with colon tissue images, our experiments demonstrate that the texture features extracted from "graph run-length matrices" lead to high segmentation accuracies, also providing a reasonable number of segmented regions. Compared with four other segmentation algorithms, the results show that the proposed algorithm is more effective in histopathological image segmentation. 
  |  https://dx.doi.org/10.1109/TMI.2010.2094200  |  
------------------------------------------- 
10.1016/j.zemedi.2010.12.002  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0939-3889(10)00144-3  |  
------------------------------------------- 
10.1109/IEMBS.2011.6091962  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   There is a need for objective tools to help clinicians to diagnose Alzheimer's Disease (AD) early and accurately and to conduct Clinical Trials (CTs) with fewer patients. Magnetic Resonance Imaging (MRI) is a promising AD biomarker but no single MRI feature is optimal for all disease stages. Machine Learning classification can address these challenges. In this study, we have investigated the classification of MRI features from AD, Mild Cognitive Impairment (MCI), and control subjects from ADNI with four techniques. The highest accuracy rates for the classification of controls against ADs and MCIs were 89.2% and 72.7%, respectively. Moreover, we used the classifiers to select AD and MCI subjects who are most likely to decline for inclusion in hypothetical CTs. Using the hippocampal volume as an outcome measure, we found that the required group sizes for the CTs were reduced from 197 to 117 AD patients and from 366 to 215 MCI subjects. 
  |  https://dx.doi.org/10.1109/IEMBS.2011.6091962  |  
------------------------------------------- 
10.1109/IEMBS.2011.6091480  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Accurate segmentation of cell nuclei in microscope images of tissue sections is a key step in a number of biological and clinical applications. Often such applications require analysis of large image datasets for which manual segmentation becomes subjective and time consuming. Hence automation of the segmentation steps using fast, robust and accurate image analysis and pattern classification techniques is necessary for high throughput processing of such datasets. We describe a supervised learning framework, based on artificial neural networks (ANNs), to identify well-segmented nuclei in tissue sections from a multistage watershed segmentation algorithm. The successful automation was demonstrated by screening over 1400 well segmented nuclei from 9 datasets of human breast tissue section images and comparing the results to a previously used stacked classifier based analysis framework. 
  |  https://dx.doi.org/10.1109/IEMBS.2011.6091480  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22255704/  |  
------------------------------------------- 
10.1093/bioinformatics/btr263  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Motivation:  Many ChIP-Seq experiments are aimed at developing gold standards for determining the locations of various genomic features such as transcription start or transcription factor binding sites on the whole genome. Many such pioneering experiments lack rigorous testing methods and adequate 'gold standard' annotations to compare against as they themselves are the most reliable source of empirical data available. To overcome this problem, we propose a self-consistency test whereby a dataset is tested against itself. It relies on a supervised machine learning style protocol for in silico annotation of a genome and accuracy estimation to guarantee, at least, self-consistency. 
  Results:  The main results use a novel performance metric (a calibrated precision) in order to assess and compare the robustness of the proposed supervised learning method across different test sets. As a proof of principle, we applied the whole protocol to two recent ChIP-Seq ENCODE datasets of STAT1 and Pol-II binding sites. STAT1 is benchmarked against in silicodetection of binding sites using available position weight matrices. Pol-II, the main focus of this paper, is benchmarked against 17 algorithms for the closely related and well-studied problem of in silico transcription start site (TSS) prediction. Our results also demonstrate the feasibility of in silico genome annotation extension with encouraging results from a small portion of annotated genome to the remainder. 
  Availability:  Available fromhttp://www.genomics.csse.unimelb.edu.au/gat. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr263  |  
------------------------------------------- 
10.1016/j.ygeno.2011.04.011  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   MicroRNAs (miRNAs) are non-coding RNAs that play important roles in post-transcriptional regulation. Identification of miRNAs is crucial to understanding their biological mechanism. Recently, machine-learning approaches have been employed to predict miRNA precursors (pre-miRNAs). However, features used are divergent and consequently induce different performance. Thus, feature selection is critical for pre-miRNA prediction. We generated an optimized feature subset including 13 features using a hybrid of genetic algorithm and support vector machine (GA-SVM). Based on SVM, the classification performance of the optimized feature subset is much higher than that of the two feature sets used in microPred and miPred by five-fold cross-validation. Finally, we constructed the classifier miR-SF to predict the most recently identified human pre-miRNAs in miRBase (version 16). Compared with microPred and miPred, miR-SF achieved much higher classification performance. Accuracies were 93.97%, 86.21% and 64.66% for miR-SF, microPred and miPred, respectively. Thus, miR-SF is effective for identifying pre-miRNAs. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0888-7543(11)00111-X  |  
------------------------------------------- 
10.1118/1.3562898  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Purpose:  A massive-training artificial neural network (MTANN) has been developed for the reduction of false positives (FPs) in computer-aided detection (CADe) of polyps in CT colonography (CTC). A major limitation of the MTANN is the long training time. To address this issue, the authors investigated the feasibility of two state-of-the-art regression models, namely, support vector regression (SVR) and Gaussian process regression (GPR) models, in the massive-training framework and developed massive-training SVR (MTSVR) and massive-training GPR (MTGPR) for the reduction of FPs in CADe of polyps. 
  Methods:  The authors applied SVR and GPR as volume-processing techniques in the distinction of polyps from FP detections in a CTC CADe scheme. Unlike artificial neural networks (ANNs), both SVR and GPR are memory-based methods that store a part of or the entire training data for testing. Therefore, their training is generally fast and they are able to improve the efficiency of the massive-training methodology. Rooted in a maximum margin property, SVR offers excellent generalization ability and robustness to outliers. On the other hand, GPR approaches nonlinear regression from a Bayesian perspective, which produces both the optimal estimated function and the covariance associated with the estimation. Therefore, both SVR and GPR, as the state-of-the-art nonlinear regression models, are able to offer a performance comparable or potentially superior to that of ANN, with highly efficient training. Both MTSVR and MTGPR were trained directly with voxel values from CTC images. A 3D scoring method based on a 3D Gaussian weighting function was applied to the outputs of MTSVR and MTGPR for distinction between polyps and nonpolyps. To test the performance of the proposed models, the authors compared them to the original MTANN in the distinction between actual polyps and various types of FPs in terms of training time reduction and FP reduction performance. The authors' CTC database consisted of 240 CTC data sets obtained from 120 patients in the supine and prone positions. The training set consisted of 27 patients, 10 of which had 10 polyps. The authors selected 10 nonpolyps (i.e., FP sources) from the training set. These ten polyps and ten nonpolyps were used for training the proposed models. The testing set consisted of 93 patients, including 19 polyps in 7 patients and 86 negative patients with 474 FPs produced by an original CADe scheme. 
  Results:  With the MTSVR, the training time was reduced by a factor of 190, while a FP reduction performance [by-polyp sensitivity of 94.7% (18/19) with 2.5 (230/93) FPs/patient] comparable to that of the original MTANN [the same sensitivity with 2.6 (244/93) FPs/patient] was achieved. The classification performance in terms of the area under the receiver-operating-characteristic curve value of the MTGPR (0.82) was statistically significantly higher than that of the original MTANN (0.77), with a two-sided p-value of 0.03. The MTGPR yielded a 94.7% (18/19) by-polyp sensitivity at a FP rate of 2.5 (235/93) per patient and reduced the training time by a factor of 1.3. 
  Conclusions:  Both MTSVR and MTGPR improve the efficiency of the training in the massive-training framework while maintaining a comparable performance. 
  |  https://doi.org/10.1118/1.3562898  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21626922/  |  
------------------------------------------- 
10.1002/bip.21589  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   One of the major challenges in genomics is to understand the function of gene products from their 3D structures. Computational methods are needed for the high-throughput prediction of the function of proteins from their 3D structure. Methods that identify active sites are important for understanding and annotating the function of proteins. Traditional methods exploiting either sequence similarity or structural similarity can be unreliable and cannot be applied to proteins with novel folds or low homology with other proteins. Here, we present a machine-learning application that combines computed electrostatic, evolutionary, and pocket geometric information for high-performance prediction of catalytic residues. Input features consist of our structure-based theoretical microscopic anomalous titration curve shapes (THEMATICS) electrostatics data, enhanced with sequence-based phylogenetic information from INTREPID and topological pocket information from ConCavity. Our THEMATICS-based input features are augmented with an additional metric, the theoretical buffer range. With the integration of the three different types of input, each of which performs admirably on its own, significantly better performance is achieved than that of any of these methods by itself. This combined method achieves 86.7%, 92.5%, and 93.8% recall of annotated functional residues at 5, 8, and 10% false-positive rates, respectively. 
  |  https://doi.org/10.1002/bip.21589  |  
------------------------------------------- 
10.1371/journal.pone.0028507  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Temperature-sensitive (TS) mutants are powerful tools to study gene function in vivo. These mutants exhibit wild-type activity at permissive temperatures and reduced activity at restrictive temperatures. Although random mutagenesis can be used to generate TS mutants, the procedure is laborious and unfeasible in multicellular organisms. Further, the underlying molecular mechanisms of the TS phenotype are poorly understood. To elucidate TS mechanisms, we used a machine learning method-logistic regression-to investigate a large number of sequence and structure features. We developed and tested 133 features, describing properties of either the mutation site or the mutation site neighborhood. We defined three types of neighborhood using sequence distance, Euclidean distance, and topological distance. We discovered that neighborhood features outperformed mutation site features in predicting TS mutations. The most predictive features suggest that TS mutations tend to occur at buried and rigid residues, and are located at conserved protein domains. The environment of a buried residue often determines the overall structural stability of a protein, thus may lead to reversible activity change upon temperature switch. We developed TS prediction models based on logistic regression and the Lasso regularized procedure. Through a ten-fold cross-validation, we obtained the area under the curve of 0.91 for the model using both sequence and structure features. Testing on independent datasets suggested that the model predicted TS mutations with a 50% precision. In summary, our study elucidated the molecular basis of TS mutants and suggested the importance of neighborhood properties in determining TS mutations. We further developed models to predict TS mutations derived from single amino acid substitutions. In this way, TS mutants can be efficiently obtained through experimentally introducing the predicted mutations. 
  |  http://dx.plos.org/10.1371/journal.pone.0028507  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22164302/  |  
------------------------------------------- 
10.1007/978-3-642-22092-0_40  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Junction structures, as the natural anatomical markers, are useful to study the organ or tumor motion. However, detection and tracking of the junctions in four-dimensional (4D) images are challenging. The paper presents a novel framework to automate this task. Detection of their centers and sizes is first achieved by an analysis of local shape profiles on one segmented reference image. Junctions are then separately tracked by simultaneously using neighboring intensity features from all images. Defined by a closed B-spline space curve, the individual trajectory is assumed to be cyclic and obtained by maximizing the metric of combined correlation coefficients. Local extrema are suppressed by improving the initial conditions using random walks from pair-wise optimizations. Our approach has been applied to analyze the vessel junctions in five real 4D respiration-gated computed tomography (CT) image datasets with promising results. More than 500 junctions in the lung are detected with an average accuracy of greater than 85% and the mean error between the automated and the manual tracking is sub-voxel. 
  |  None  |  
------------------------------------------- 
10.1016/j.media.2011.01.002  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   A stochastic deformable model is proposed for the segmentation of the myocardium in Magnetic Resonance Imaging. The segmentation is posed as a probabilistic optimization problem in which the optimal time-dependent surface is obtained for the myocardium of the heart in a discrete space of locations built upon simple geometric assumptions. For this purpose, first, the left ventricle is detected by a set of image analysis tools gathered from the literature. Then, the segmentation solution is obtained by the Maximization of the Posterior Marginals for the myocardium location in a Markov Random Field framework which optimally integrates temporal-spatial smoothness with intensity and gradient related features in an unsupervised way by the Maximum Likelihood estimation of the parameters of the field. This scheme provides a flexible and robust segmentation method which has been able to generate results comparable to manually segmented images for some derived cardiac function parameters in a set of 43 patients affected in different degrees by an Acute Myocardial Infarction. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(11)00003-X  |  
------------------------------------------- 
10.1007/978-3-642-22092-0_3  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   This paper presents a new supervised learning framework for the efficient recognition and segmentation of anatomical structures in 3D computed tomography (CT), with as little training data as possible. Training supervised classifiers to recognize organs within CT scans requires a large number of manually delineated exemplar 3D images, which are very expensive to obtain. In this study, we borrow ideas from the field of active learning to optimally select a minimum subset of such images that yields accurate anatomy segmentation. The main contribution of this work is in designing a combined generative-discriminative model which: i) drives optimal selection of training data; and ii) increases segmentation accuracy. The optimal training set is constructed by finding unlabeled scans which maximize the disagreement between our two complementary probabilistic models, as measured by a modified version of the Jensen-Shannon divergence. Our algorithm is assessed on a database of 196 labeled clinical CT scans with high variability in resolution, anatomy, pathologies, etc. Quantitative evaluation shows that, compared with randomly selecting the scans to annotate, our method decreases the number of training images by up to 45%. Moreover, our generative model of body shape substantially increases segmentation accuracy when compared to either using the discriminative model alone or a generic smoothness prior (e.g. via a Markov Random Field). 
  |  None  |  
------------------------------------------- 
10.1016/j.isatra.2010.10.005  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   An optimal PID and an optimal fuzzy PID have been tuned by minimizing the Integral of Time multiplied Absolute Error (ITAE) and squared controller output for a networked control system (NCS). The tuning is attempted for a higher order and a time delay system using two stochastic algorithms viz. the Genetic Algorithm (GA) and two variants of Particle Swarm Optimization (PSO) and the closed loop performances are compared. The paper shows that random variation in network delay can be handled efficiently with fuzzy logic based PID controllers over conventional PID controllers. 
  |  None  |  
------------------------------------------- 
10.1016/j.jtcvs.2011.10.046  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objectives:  Non-small cell lung cancer (NSCLC) is the leading cause of cancer-related mortality. Development of an early diagnosis method may improve survivals. We aimed to develop a new diagnostic model for NSCLC using serum biomarkers. 
  Methods:  We set up a patient group diagnosed with NSCLC (n = 122) and a healthy control group (n = 225). Thirty serum analytes were selected on the basis of previous studies and a literature search. An antibody-bead array of 30 markers was constructed using the Luminex bead array platform (Luminex Inc, Austin, Tex) and was analyzed. Each marker was ranked by importance using the random forest method and then selected. Using selected markers, multivariate classification algorithms were constructed and were validated by application to independent validation cohort of 21 NSCLC and 28 control subjects. 
  Results:  There was no difference in demographics between patients and the control population except for age (64.8 ± 10.0 for patients vs 53.0 ± 7.6 years for the control group). Among the 30 serum proteins, 23 showed a difference between the 2 groups (12 increased and 11 decreased in the patient group). We found the highest accuracy of multivariate classification algorithms when using the 5 highest-ranked biomarkers (A1AT, CYFRA 21-1, IGF-1, RANTES, AFP). When we applied the algorithms on a validation cohort, each method recognized the patients from the controls with high accuracy (89.8% with random forest, 91.8% with support vector machine, 88.2% with linear discriminant analysis, and 90.5% with logistic regression). 
  Conclusions:  We confirmed that a new diagnostic method using 5 serum biomarkers profiling constructed by multivariate classification algorithms could distinguish NSCLC from healthy controls with high accuracy. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-5223(11)01165-2  |  
------------------------------------------- 
10.1016/j.jtcvs.2011.06.023  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objective:  The vast majority of reports describing beating heart robotic myocardial revascularization (total endoscopic coronary artery bypass) contain very small numbers of patients undergoing single-vessel bypass. We present a large series of patients undergoing multivessel total endoscopic coronary artery bypass. 
  Methods:  We performed a retrospective clinical review of 106 patients undergoing total endoscopic coronary artery bypass (72% multivessel) at 1 institution by 1 experienced cardiac surgeon/physician assistant team. These results were compared with the expected clinical outcomes from conventional coronary artery bypass grafting calculated using the Society of Thoracic Surgeons risk calculator. 
  Results:  Of the 106 patients, 1% underwent quadruple total endoscopic coronary artery bypass, 8% triple, 63% double, and 28% single. The emergent conversion rate for hemodynamic instability was 6.6%. The postoperative renal failure rate (doubling of baseline serum creatinine or dialysis required) was 7.5%. Overall, 23 patients (21.7%) exhibited at least 1 major morbidity/mortality (4 deaths). The number of vessels bypassed (single/double/triple/quadruple) correlated positively with the surgical/operating room time, the lung separation time, vasoactive medication use, blood use, a postoperative ventilation time longer than 24 hours, intensive care unit length of stay, and hospital length of stay. An increased surgical time was significantly associated with major morbidity (P = .011) and mortality (P = .043). A comparison with the Society for Thoracic Surgeons expected outcomes revealed a similar hospital length of stay but an increased incidence of prolonged ventilation (P = .003), renal failure (P &lt; .001), morbidity (P = .045), and mortality (P = .049). 
  Conclusions:  Our results suggest that addressing multivessel coronary artery disease using total endoscopic coronary artery bypass offers no obvious clinical benefits and might increase the morbidity and mortality. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-5223(11)00687-8  |  
------------------------------------------- 
10.1371/journal.pone.0022513  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Esophageal adenocarcinoma (EAC) has become a major concern in Western countries due to rapid rises in incidence coupled with very poor survival rates. One of the key risk factors for the development of this cancer is the presence of Barrett's esophagus (BE), which is believed to form in response to repeated gastro-esophageal reflux. In this study we performed comparative, genome-wide expression profiling (using Illumina whole-genome Beadarrays) on total RNA extracted from esophageal biopsy tissues from individuals with EAC, BE (in the absence of EAC) and those with normal squamous epithelium. We combined these data with publically accessible raw data from three similar studies to investigate key gene and ontology differences between these three tissue states. The results support the deduction that BE is a tissue with enhanced glycoprotein synthesis machinery (DPP4, ATP2A3, AGR2) designed to provide strong mucosal defenses aimed at resisting gastro-esophageal reflux. EAC exhibits the enhanced extracellular matrix remodeling (collagens, IGFBP7, PLAU) effects expected in an aggressive form of cancer, as well as evidence of reduced expression of genes associated with mucosal (MUC6, CA2, TFF1) and xenobiotic (AKR1C2, AKR1B10) defenses. When our results are compared to previous whole-genome expression profiling studies keratin, mucin, annexin and trefoil factor gene groups are the most frequently represented differentially expressed gene families. Eleven genes identified here are also represented in at least 3 other profiling studies. We used these genes to discriminate between squamous epithelium, BE and EAC within the two largest cohorts using a support vector machine leave one out cross validation (LOOCV) analysis. While this method was satisfactory for discriminating squamous epithelium and BE, it demonstrates the need for more detailed investigations into profiling changes between BE and EAC. 
  |  http://dx.plos.org/10.1371/journal.pone.0022513  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21829465/  |  
------------------------------------------- 
10.1016/j.eururo.2011.08.040  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Robot-assisted and laparoscopic partial nephrectomies (PNs) for medial tumors are technically challenging even with the hilum clamped and, until now, were impossible to perform with the hilum unclamped. 
  Objective:  Evaluate whether targeted vascular microdissection (VMD) of renal artery branches allows zero-ischemia PN to be performed even for challenging medial tumors. 
  Design, setting, and participants:  A prospective cohort evaluation of 44 patients with renal masses who underwent robot-assisted or laparoscopic zero-ischemia PN either with anatomic VMD (group 1; n=22) or without anatomic VMD (group 2; n=22) performed by a single surgeon from April 2010 to January 2011. 
  Intervention:  Zero-ischemia PN with VMD incorporates four maneuvers: (1) preoperative computed tomographic reconstruction of renal arterial branch anatomy, (2) anatomic dissection of targeted, tumor-specific tertiary or higher-order renal arterial branches, (3) neurosurgical aneurysm microsurgical bulldog clamp(s) for superselective tumor devascularization, and (4) transient, controlled reduction of blood pressure, if necessary. 
  Measurements:  Baseline, perioperative, and postoperative data were collected prospectively. 
  Results and limitations:  Group 1 tumors were larger (4.3 vs 2.6 cm; p=0.011), were more often hilar (41% vs 9%; p=0.09), were medial (59% and 23%; p=0.017), were closer to the hilum (1.46 vs 3.26 cm; p=0.0002), and had a lower C index score (2.1 vs 3.9; p=0.004) and higher RENAL nephrometry scores (7.7 vs 6.2; p=0.013). Despite greater complexity, no group 1 tumor required hilar clamping, and perioperative outcomes were similar to those of group 2: operating room time (4.7 and 4.1h), median blood loss (200 and 100ml), surgical margins for cancer (all negative), major complications (0% and 9%), and minor complications (18% and 14%). The median serum creatinine level was similar 2 mo postoperatively (1.2 and 1.3mg/dl). The study was limited by the relatively small sample size. 
  Conclusions:  Anatomic targeted dissection and superselective control of tumor-specific renal arterial branches facilitate zero-ischemia PN. Even challenging medial and hilar tumors can be excised without hilar clamping. Global surgical renal ischemia has been eliminated for most patients undergoing PN at our institution. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0302-2838(11)00908-0  |  
------------------------------------------- 
10.1016/j.eururo.2011.12.027  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Prior to the introduction and dissemination of robot-assisted radical prostatectomy (RARP), population-based studies comparing open radical prostatectomy (ORP) and minimally invasive radical prostatectomy (MIRP) found no clinically significant difference in perioperative complication rates. 
  Objective:  Assess the rate of RARP utilization and reexamine the difference in perioperative complication rates between RARP and ORP in light of RARP's supplanting laparoscopic radical prostatectomy (LRP) as the most common MIRP technique. 
  Design, setting, and participants:  As of October 2008, a robot-assisted modifier was introduced to denote robot-assisted procedures. Relying on the Nationwide Inpatient Sample between October 2008 and December 2009, patients treated with radical prostatectomy (RP) were identified. The robot-assisted modifier (17.4x) was used to identify RARP (n=11 889). Patients with the minimally invasive modifier code (54.21) without the robot-assisted modifier were classified as having undergone LRP and were removed from further analyses. The remainder were classified as ORP patients (n=7389). 
  Intervention:  All patients underwent RARP or ORP. 
  Measurements:  We compared the rates of blood transfusions, intraoperative and postoperative complications, prolonged length of stay (pLOS), and in-hospital mortality. Multivariable logistic regression analyses of propensity score-matched populations, fitted with general estimation equations for clustering among hospitals, further adjusted for confounding factors. 
  Results and limitations:  Of 19 462 RPs, 61.1% were RARPs, 38.0% were ORPs, and 0.9% were LRPs. In multivariable analyses of propensity score-matched populations, patients undergoing RARP were less likely to receive a blood transfusion (odds ratio [OR]: 0.34; 95% confidence interval [CI], 0.28-0.40), to experience an intraoperative complication (OR: 0.47; 95% CI, 0.31-0.71) or a postoperative complication (OR: 0.86; 95% CI, 0.77-0.96), and to experience a pLOS (OR: 0.28; 95% CI, 0.26-0.30). Limitations of this study include lack of adjustment for tumor characteristics, surgeon volume, learning curve effect, and longitudinal follow-up. 
  Conclusions:  RARP has supplanted ORP as the most common surgical approach for RP. Moreover, we demonstrate superior adjusted perioperative outcomes after RARP in virtually all examined outcomes. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0302-2838(11)01411-4  |  
------------------------------------------- 
10.1016/j.urology.2011.08.045  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Objective:  To address the long-term biochemical recurrence (BCR)-free survival rates of patients treated with robotic-assisted laparoscopic prostatectomy (RALP) with a minimum follow-up of 5 years. 
  Materials and methods:  Prospectively collected data of 184 patients treated with RALP at a single institution were analyzed. Kaplan-Meier and life tables analyses targeted the rates of BCR according to pathologic parameters. Cox regression analyses addressed predictors of BCR. 
  Results:  Median follow-up was 67.5 months. One and 10 patients died of prostate cancer (PCa) and other causes, respectively. Mean time to BCR was 83.8 months. The 3-, 5-, and 7-year BCR-free survival rates were 94%, 86%, and 81%, respectively. These rates were 97%, 93%, and 85% for pT2 disease; 94%, 84%, and 84% for pT3a; and 69%, 43%, and 43% for pT3b (P&lt;.001). The same figures were 97%, 90%, and 88% for Gleason sum 6 or lower; 90%, 86%, and 75% for Gleason sum 7; and 85%, 65%, and 65% for Gleason sum 8-10 (P=.01). At univariable analyses, prostate-specific antigen, pathologic Gleason score, and presence of extracapsular extension, seminal vesicle invasion, and adjuvant radiotherapy were significantly associated with BCR. At multivariable analysis, the presence of seminal vesicle invasion and the presence of Gleason sum 8-10 represented independent predictors of BCR (HR=5.14; P=.004 and HR=3.04; P=.04, respectively). 
  Conclusion:  We report the longest available follow-up in RALP patients. RALP represents an oncologically effective procedure. Our oncological results support the increasing diffusion of RALP for the treatment of organ-confined PCa. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0090-4295(11)02321-1  |  
------------------------------------------- 
10.1111/j.1742-4658.2011.08177.x  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   AraL from Bacillus subtilis is a member of the ubiquitous haloalkanoate dehalogenase superfamily. The araL gene has been cloned, over-expressed in Escherichia coli and its product purified to homogeneity. The enzyme displays phosphatase activity, which is optimal at neutral pH (7.0) and 65 °C. Substrate screening and kinetic analysis showed AraL to have low specificity and catalytic activity towards several sugar phosphates, which are metabolic intermediates of the glycolytic and pentose phosphate pathways. On the basis of substrate specificity and gene context within the arabinose metabolic operon, a putative physiological role of AraL in the detoxification of accidental accumulation of phosphorylated metabolites has been proposed. The ability of AraL to catabolize several related secondary metabolites requires regulation at the genetic level. In the present study, using site-directed mutagenesis, we show that the production of AraL is regulated by a structure in the translation initiation region of the mRNA, which most probably blocks access to the ribosome-binding site, preventing protein synthesis. Members of haloalkanoate dehalogenase subfamily IIA and IIB are characterized by a broad-range and overlapping specificity anticipating the need for regulation at the genetic level. We provide evidence for the existence of a genetic regulatory mechanism controlling the production of AraL. 
  |  https://dx.doi.org/10.1111/j.1742-4658.2011.08177.x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21575135/  |  
------------------------------------------- 
10.1007/s00405-011-1838-x  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   We present a series of patients treated by transoral robotic surgery (TORS) using a new CO(2) laser wave guide (CO(2) LWG) (Lumenis, Santa Clara, CA). Patients older than 18 years, with malignant pharyngo-laryngeal tumors were enrolled in this prospective study after signing an informed consent. Four patients were enrolled in the study. The mean age was 56 years. One patient had a T1 base of tongue tumor, two patients had supraglottic tumors (T1, T2), and one had a T1 palatine tonsil tumor. All the procedures could be performed using a Maryland forceps, a 0° endoscope and a CO(2) LWG introduced via the robotic arm introducer. The laser parameters were: superpulse or continuous mode, 7-15 W, continuous delivery. The average set-up time was 30 min. The average surgical time was 94 min. No complications were noted due to the intraoperative use of the robot or the CO(2) LWG. One laser fiber was used for each of the surgeries. The mean coagulation depth was 200 μm (range 100-300). The mean hospital stay was 6 days. The CO(2) LWG is a reliable tool for TORS. It allowed more than 1 h of work without any trouble. 
  |  https://dx.doi.org/10.1007/s00405-011-1838-x  |  
------------------------------------------- 
10.1007/s10661-010-1763-2  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Soilmicrobial ecology plays a significant role in global ecosystems. Nevertheless, methods of model prediction and mapping have yet to be established for soil microbial ecology. The present study was undertaken to develop an artificial-intelligence- and geographical information system (GIS)-integrated framework for predicting and mapping soil bacterial diversity using pre-existing environmental geospatial database information, and to further evaluate the applicability of soil bacterial diversity mapping for planning construction of eco-friendly roads. Using a stratified random sampling, soil bacterial diversity was measured in 196 soil samples in a forest area where construction of an eco-friendly road was planned. Model accuracy, coherence analyses, and tree analysis were systematically performed, and four-class discretized decision tree (DT) with ordinary pair-wise partitioning (OPP) was selected as the optimal model among tested five DT model variants. GIS-based simulations of the optimal DT model with varying weights assigned to soil ecological quality showed that the inclusion of soil ecology in environmental components, which are considered in environmental impact assessment, significantly affects the spatial distributions of overall environmental quality values as well as the determination of an environmentally optimized road route. This work suggests a guideline to use systematic accuracy, coherence, and tree analyses in selecting an optimal DT model from multiple candidate model variants, and demonstrates the applicability of the OPP-improved DT integrated with GIS in rule induction for mapping bacterial diversity. These findings also provide implication on the significance of soil microbial ecology in environmental impact assessment and eco-friendly construction planning. 
  |  https://doi.org/10.1007/s10661-010-1763-2  |  
------------------------------------------- 
10.3414/ME11-01-0060  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  In the past decade, Medical Informatics (MI) and Bioinformatics (BI) have converged towards a new discipline, called Biomedical Informatics (BMI) bridging informatics methods across the spectrum from genomic research to personalized medicine and global healthcare. This convergence still raises challenging research questions which are being addressed by researchers internationally, which in turn raises the question of how biomedical informatics publications reflect the contributions from around the world in documenting the research. 
  Objectives:  To analyse the worldwide participation of biomedical informatics researchers from professional groups and societies in the best-known scientific conferences in the field. The analysis is focused on their geographical affiliation, but also includes other features, such as the impact and recognition of the conferences. 
  Methods:  We manually collected data about authors of papers presented at three major MI conferences: Medinfo, MIE and the AMIA symposium. In addition, we collected data from a BI conference, ISMB, as a comparison. Finally, we analyzed the impact and recognition of these conferences within their scientific contexts. 
  Results:  Data indicate a predominance of local authors at the regional conferences (AMIA and MIE), whereas other conferences with a world-wide scope (Medinfo and ISMB) had broader participation. Our analysis shows that the influence of these conferences beyond the discipline remains somewhat limited. 
  Conclusions:  Our results suggest that for BMI to be recognized as a broad discipline, both in the geographical and scientific sense, it will need to extend the scope of collaborations and their interdisciplinary impacts worldwide. 
  |  http://www.thieme-connect.com/DOI/DOI?10.3414/ME11-01-0060  |  
------------------------------------------- 
10.1109/TIP.2011.2160955  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   In this paper, an iterative narrow-band-based graph cuts (INBBGC) method is proposed to optimize the geodesic active contours with region forces (GACWRF) model for interactive object segmentation. Based on cut metric on graphs proposed by Boykov and Kolmogorov, an NBBGC method is devised to compute the local minimization of GAC. An extension to an iterative manner, namely, INBBGC, is developed for less sensitivity to the initial curve. The INBBGC method is similar to graph-cuts-based active contour (GCBAC) presented by Xu , and their differences have been analyzed and discussed. We then integrate the region force into GAC. An improved INBBGC (IINBBGC) method is proposed to optimize the GACWRF model, thus can effectively deal with the concave region and complicated real-world images segmentation. Two region force models such as mean and probability models are studied. Therefore, the GCBAC method can be regarded as the special case of our proposed IINBBGC method without region force. Our proposed algorithm has been also analyzed to be similar to the Grabcut method when the Gaussian mixture model region force is adopted, and the band region is extended to the whole image. Thus, our proposed IINBBGC method can be regarded as narrow-band-based Grabcut method or GCBAC with region force method. We apply our proposed IINBBGC algorithm on synthetic and real-world images to emphasize its performance, compared with other segmentation methods, such as GCBAC and Grabcut methods. 
  |  https://dx.doi.org/10.1109/TIP.2011.2160955  |  
------------------------------------------- 
10.1093/protein/gzr036  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Structural water molecules are found in many protein-ligand complexes. They are known to be vital in mediating hydrogen-bonding interactions and, in some cases, key for facilitating tight binding. It is thus very important to consider water molecules when attempting to model protein-ligand interactions for cognate ligand identification, virtual screening and drug design. While the rigid treatment of water molecules present in structures is feasible, the more relevant task of treating all possible positions and orientations of water molecules with each possible ligand pose is computationally daunting. Current methods in molecular docking provide partial treatment for such water molecules, with modest success. Here we describe a new method employing dead-end elimination to place water molecules within a binding site, bridging interactions between protein and ligand. Dead-end elimination permits a thorough, though still incomplete, treatment of water placement. The results show that this method is able to place water molecules correctly within known complexes and to create physically reasonable hydrogen bonds. The approach has also been incorporated within an inverse molecular design approach, to model a variety of compounds in the process of de novo ligand design. The inclusion of structural water molecules, combined with ranking based on the electrostatic contribution to binding affinity, improves a number of otherwise poor energetic predictions. 
  |  https://academic.oup.com/peds/article-lookup/doi/10.1093/protein/gzr036  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21771870/  |  
------------------------------------------- 
10.1186/1746-1596-6-S1-S19  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Digital pathology, i.e., applications of digital information technologies to pathology practice, has been expanding in the recent decades and the mode of pathology diagnostic practice is changing with enhanced precision. In the present study the changing processes of digital pathology in Japan were investigated and trends to future were discussed. 
  Methods:  The changing status of digital pathology was investigated through reviewing the records of annual meetings of the Japanese Research Society of Telepathology and Pathology Informatics (JRST-PI) and of the Japanese pathology related medical and informatics journals. The results of the Japanese questionnaire survey conducted in 2008-2009 on telepathology and virtual slide were also reviewed. In addition effectiveness of an experimental automatic pathology diagnostic aid system using computer artificial intelligence was investigated by checking its rate of correct diagnosis for given prostate carcinoma digital images. 
  Results:  Telepathology played a central role in the development of digital pathology in Japan. Both macroscopic and microscopic pathology digital images were routinely generated and used for diagnostic purposes in major hospitals. Virtual slide (VS) digital images were used first for education then for conference, consultation and also gradually for routine diagnosis and telepathology. The experimental automatic diagnostic aid system achieved the rate of correct diagnosis around 95% for prostate carcinoma and its use for automatic mapping of cancerous areas in a given tissue image was successful. 
  Conclusions:  Advance in the digital information technologies gave revolutionary impacts on pathology education, conference, consultation, diagnosis, telepathology and also on pathology diagnostic procedures in Japan. The future will be bright for pathologists by the advanced digital pathology but we should pay attention to make the technologies and their effects under our control. 
  |  https://diagnosticpathology.biomedcentral.com/articles/10.1186/1746-1596-6-S1-S19  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21489189/  |  
------------------------------------------- 
10.1155/2012/769702  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   This article summarizes some methods from biological intelligence for modeling and optimization of supply chain management (SCM) systems, including genetic algorithms, evolutionary programming, differential evolution, swarm intelligence, artificial immune, and other biological intelligence related methods. An SCM system is adaptive, dynamic, open self-organizing, which is maintained by flows of information, materials, goods, funds, and energy. Traditional methods for modeling and optimizing complex SCM systems require huge amounts of computing resources, and biological intelligence-based solutions can often provide valuable alternatives for efficiently solving problems. The paper summarizes the recent related methods for the design and optimization of SCM systems, which covers the most widely used genetic algorithms and other evolutionary algorithms. 
  |  https://doi.org/10.1155/2012/769702  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22162724/  |  
------------------------------------------- 
10.3233/CBM-2012-0229  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Computed tomography (CT) scanning has emerged as an effective means of early detection for lung cancer. Despite marked improvement over earlier methodologies, the low level of specificity demonstrated by CT scanning has limited its clinical implementation as a screening tool. A minimally-invasive biomarker-based test that could further characterize CT-positive patients based on risk of malignancy would greatly enhance its clinical efficacy. 
  Methods:  We performed an analysis of 81 serum proteins in 92 patients diagnosed with lung cancer and 172 CT-screened control individuals. We utilize a series of bioinformatics algorithms including Metropolis-Monte Carlo, artificial neural networks, Naïve Bayes, and additive logistic regression to identify multimarker panels capable of discriminating cases from controls with high levels of sensitivity and specificity in distinct training and independent validation sets. 
  Results:  A three-biomarker panel comprised of MIF, prolactin, and thrombospondin identified using the Metropolis-Monte Carlo algorithm provided the best classification with a %Sensitivity/Specificity/Accuracy of 74/90/86 in the training set and 70/93/82 in the validation set. This panel was effective in the classification of control individuals demonstrating suspicious pulmonary nodules and stage I lung cancer patients. 
  Conclusions:  The selected serum biomarker panel demonstrated a high diagnostic utility in the current study and performance characteristics which compare favorably with previous reports. Further advancements may lead to the development of a diagnostic tool useful as an adjunct to CT-scanning. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/CBM-2012-0229  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22297547/  |  
------------------------------------------- 
10.1007/s00427-011-0378-0  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Driesch's statement, made around 1900, that the physics and chemistry of his day were unable to explain self-regulation during embryogenesis was correct and could be extended until the year 1972. The emergence of theories of self-organisation required progress in several areas including chemistry, physics, computing and cybernetics. Two parallel lines of development can be distinguished which both culminated in the early 1970s. Firstly, physicochemical theories of self-organisation arose from theoretical (Lotka 1910-1920) and experimental work (Bray 1920; Belousov 1951) on chemical oscillations. However, this research area gained broader acceptance only after thermodynamics was extended to systems far from equilibrium (1922-1967) and the mechanism of the prime example for a chemical oscillator, the Belousov-Zhabotinski reaction, was deciphered in the early 1970s. Secondly, biological theories of self-organisation were rooted in the intellectual environment of artificial intelligence and cybernetics. Turing wrote his The chemical basis of morphogenesis (1952) after working on the construction of one of the first electronic computers. Likewise, Gierer and Meinhardt's theory of local activation and lateral inhibition (1972) was influenced by ideas from cybernetics. The Gierer-Meinhardt theory provided an explanation for the first time of both spontaneous formation of spatial order and of self-regulation that proved to be extremely successful in elucidating a wide range of patterning processes. With the advent of developmental genetics in the 1980s, detailed molecular and functional data became available for complex developmental processes, allowing a new generation of data-driven theoretical approaches. Three examples of such approaches will be discussed. The successes and limitations of mathematical pattern formation theory throughout its history suggest a picture of the organism, which has structural similarity to views of the organic world held by the philosopher Immanuel Kant at the end of the eighteenth century. 
  |  https://dx.doi.org/10.1007/s00427-011-0378-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/22086125/  |  
------------------------------------------- 
PMID:22320970  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The incidence of oral cancer is high for those of Indian ethnic origin in Malaysia. Various clinical and pathological data are usually used in oral cancer prognosis. However, due to time, cost and tissue limitations, the number of prognosis variables need to be reduced. In this research, we demonstrated the use of feature selection methods to select a subset of variables that is highly predictive of oral cancer prognosis. The objective is to reduce the number of input variables, thus to identify the key clinicopathologic (input) variables of oral cancer prognosis based on the data collected in the Malaysian scenario. Two feature selection methods, genetic algorithm (wrapper approach) and Pearson's correlation coefficient (filter approach) were implemented and compared with single-input models and a full-input model. The results showed that the reduced models with feature selection method are able to produce more accurate prognosis results than the full-input model and single-input model, with the Pearson's correlation coefficient achieving the most promising results. 
  |  http://journal.waocp.org/?sid=Entrez:PubMed&id=pmid:22320970&key=2011.12.10.2659  |  
------------------------------------------- 
10.1093/molbev/msr222  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   Although the possibility of gene evolution by domain rearrangements has long been appreciated, current methods for reconstructing and systematically analyzing gene family evolution are limited to events such as duplication, loss, and sometimes, horizontal transfer. However, within the Drosophila clade, we find domain rearrangements occur in 35.9% of gene families, and thus, any comprehensive study of gene evolution in these species will need to account for such events. Here, we present a new computational model and algorithm for reconstructing gene evolution at the domain level. We develop a method for detecting homologous domains between genes and present a phylogenetic algorithm for reconstructing maximum parsimony evolutionary histories that include domain generation, duplication, loss, merge (fusion), and split (fission) events. Using this method, we find that genes involved in fusion and fission are enriched in signaling and development, suggesting that domain rearrangements and reuse may be crucial in these processes. We also find that fusion is more abundant than fission, and that fusion and fission events occur predominantly alongside duplication, with 92.5% and 34.3% of fusion and fission events retaining ancestral architectures in the duplicated copies. We provide a catalog of ∼9,000 genes that undergo domain rearrangement across nine sequenced species, along with possible mechanisms for their formation. These results dramatically expand on evolution at the subgene level and offer several insights into how new genes and functions arise between species. 
  |  https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msr222  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/21900599/  |  
------------------------------------------- 
10.1089/dia.2011.0093  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Prediction of glycemic profile is an important task for both early recognition of hypoglycemia and enhancement of the control algorithms for optimization of insulin infusion rate. Adaptive models for glucose prediction and recognition of hypoglycemia based on statistical and artificial intelligence techniques are presented. 
  Methods:  We compared an autoregressive (AR) model using only glucose information, an AR model with external insulin input (ARX), and an artificial neural network (ANN) using both glucose and insulin information. Online adaptive models were used to account for the intra- and inter-subject variability of the population with diabetes. The evaluation of the predictive ability included prediction horizons (PHs) of 30 min and 45 min. 
  Results:  The AR model presented root mean square error (RMSE) values of 14.0-21.6 mg/dL and correlation coefficients (CCs) of 0.92-0.95 for PH=30 min and 23.2-35.9 mg/dL and 0.79-0.87, respectively, for PH=45 min. The respective values for the ARX models were slightly better (PH=30 min, 13.3-18.8 mg/dL and 0.94-0.96; PH=45 min, 22.8-29.4 mg/dL and 0.83-0.88). For the ANN, the RMSE values ranged from 2.8 to 6.3 mg/dL, and the CC was 0.99 for all cases and PHs. The sensitivity of hypoglycemia prediction was 78% for AR, 81% for ARX, and 96% for ANN for PH=30 min and 65%, 67%, and 95%, respectively, for PH=45 min. The corresponding specificities were 96%, 96%, and 99% for PH=30 min and 93%, 93%, and 99% for PH=45 min. 
  Conclusions:  The ANN appears to be more appropriate for the prediction of glucose profile based on glucose and insulin data. 
  |  https://www.liebertpub.com/doi/full/10.1089/dia.2011.0093?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.jplph.2011.01.013  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   ADP-glucose pyrophosphorylase (AGPase) catalyses the synthesis of ADP-glucose, and is a highly regulated enzyme in the pathway of starch synthesis. In Arabidopsis thaliana, the enzyme is a heterotetramer, containing two small subunits encoded by the APS1 gene and two large subunits encoded by the APL1-4 genes. TILLING (Targeting Induced Local Lesions IN Genomes) of a chemically mutagenised population of A. thaliana plants identified 33 novel mutations in the APS1 gene, including 21 missense mutations in the protein coding region. High throughput measurements using a robotised cycling assay showed that maximal AGPase activity in the aps1 mutants varied from &lt;15 to 117% of wild type (WT), and that the kinetic properties of the enzyme were altered in several lines, indicating a role for the substituted amino acid residues in catalysis or substrate binding. These results validate the concept of using such a platform for efficient high-throughput screening of very large populations of mutants, natural accessions or introgression lines. AGPase was estimated to have a flux control coefficient of 0.20, indicating that the enzyme exerted only modest control over the rate of starch synthesis in plants grown under short day conditions (8 h light/16 h dark) with an irradiance of 150 μmol quanta m(-2)s(-1). Redox activation of the enzyme, via reduction of the intermolecular disulphide bridge between the two small subunits, was increased in several lines. This was sometimes, but not always, associated with a decrease in the abundance of the APS1 protein. In conclusion, the TILLING technique was used to generate an allelic series of aps1 mutants in A. thaliana that revealed new insights into the multi-layered regulation of AGPase. These mutants offer some advantages over the available loss-of-function mutants, e.g. adg1, for investigating the effects of subtle changes in the enzyme's activity on the rate of starch synthesis. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0176-1617(11)00073-3  |  
------------------------------------------- 
10.1016/j.juro.2011.02.2586  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    |  https://www.auajournals.org/doi/10.1016/j.juro.2011.02.2586  |  
------------------------------------------- 
10.1016/j.eururo.2011.05.011  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Background:  Early studies reported comparative results of functional outcomes between robot-assisted (RARP) and retropubic radical prostatectomy (RRP). However, well-controlled single-surgeon prospective studies comparing the outcomes are rare. 
  Objective:  To compare functional outcomes after RARP and RRP performed by a single surgeon, and to identify factors predictive of early return of continence and potency. 
  Design, setting, and participants:  A total of 763 consecutive patients undergoing RP between 2007 and 2010 were prospectively included and serially followed postoperatively for comparative analysis. 
  Intervention:  RARP was performed in 528 patients, and 235 underwent RRP. 
  Measurements:  Continence was defined as being completely pad free. Potency was defined as having erection sufficient for intercourse with or without a phosphodiesterase type 5 inhibitor. Continence and potency recovery were checked serially by interview and questionnaire at 1, 3, 6, 9, 12, 18, and 24 mo postoperatively. Cox proportional hazards method analyses was performed to determine predictive factors for early recovery. 
  Results and limitations:  After the initial 132 cases, patients who underwent RARP demonstrated faster recovery of urinary continence compared to RRP patients. Potency recovery was more rapid in the RARP group at all evaluation time points, beginning from the initial cases. In multivariate analysis, younger age and longer preoperative membranous urethral length seen by prostate magnetic resonance imaging (MRI) demonstrated statistical significance as independent prognostic factors for continence recovery; younger age, surgical method (RARP vs RRP), and higher preoperative serum testosterone were independent prognostic factors for potency recovery. The limitations of the present study were that it was nonrandomized and used interview to evaluate potency recovery. 
  Conclusions:  Patients after RARP demonstrated superior functional recovery. Moreover, membranous urethral length on preoperative MRI and patient age were factors independently predictive of continence recovery, while patient age and higher preoperative serum testosterone were independent prognostic factors for potency recovery. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0302-2838(11)00500-8  |  
------------------------------------------- 
10.1007/s12011-011-9306-4  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The present study was designed to evaluate the levels of eight elements including lithium, zinc, chromium, copper, iron, manganese, nickel and vanadium in whole blood of type-2 diabetes patients, to compare them with age-matched healthy controls and to investigate the feasibility of combining them with an ensemble model for diagnosing purpose. A dataset involving 158 samples, among which 105 were taken from healthy adults and the remaining 53 from patients with type-2 diabetes, was collected. All samples were split into the training set and the test set with the equal size. Based on a simple variable selection, two elements, i.e., chromium and iron, are also picked out as the most important elements. Three kinds of algorithms, i.e., fisher linear discriminate analysis (FLDA), support vector machine (SVM) and decision tree (DT), were used for constructing member models. The best ensemble classifiers constructed on the training set were validated on the independent test set, and the prediction results were compared with those from clinical diagnostics on the same subjects. The results reveal that almost all ensemble classifiers exhibit similar performance, implying that these elements coupled with an appropriate ensemble classifier can serve as a valuable tool of diagnosing diabetes type-2. 
  |  https://dx.doi.org/10.1007/s12011-011-9306-4  |  
------------------------------------------- 
10.1007/s00423-011-0852-1  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |    Introduction:  There are no randomised studies comparing open and laparoscopic approaches foradrenalectomy in patients with adrenal cortical carcinoma. 
  Methods:  There is evidence of postoperative benefit for the patients undergoing laparoscopic adrenalectomy compared to open adrenalectomy (level B). 
  Results:  Results from comparison of oncological outcomes in ACC between open and laparoscopic approaches are equivocal: increasedrisk of local recurrence and peritoneal carcinomatosis by the laparoscopic route (level D), and identical results between the two approaches in terms of survival, recurrence and peritoneal carcinomatosis (level C). 
  Conclusion:  An open approach is recommended in case of local invasion, with a view to achieving an R0 resection (level D). Laparoscopic resection of ACC/potentially malignant tumours, which includes removal of surrounding periadrenal fat and results in an R0 resection without tumour capsule rupture, may be performed for preoperative and intraoperative stage 1-2 ACC and tumours with a diameter &lt; 10 cm (level C). 
  |  https://dx.doi.org/10.1007/s00423-011-0852-1  |  
------------------------------------------- 
10.1016/j.procs.2011.04.219  |  SubfieldM  |  SubfieldA  |  ResearchTypeM  |  ResearchTypeA  |   The assessment of chemical similarity between molecules is a basic operation in chemoinformatics, a computational area concerning with the manipulation of chemical structural information. Comparing molecules is the basis for a wide range of applications such as searching in chemical databases, training prediction models for virtual screening or aggregating clusters of similar compounds. However, currently available multimillion databases represent a challenge for conventional chemoinformatics algorithms raising the necessity for faster similarity methods. In this paper, we extensively analyze the advantages of using many-core architectures for calculating some commonly-used chemical similarity coefficients such as Tanimoto, Dice or Cosine. Our aim is to provide a wide-breath <i>proof-of-concept</i> regarding the usefulness of GPU architectures to chemoinformatics, a class of computing problems still uncovered. In our work, we present a general GPU algorithm for all-to-all chemical comparisons considering both binary fingerprints and floating point descriptors as molecule representation. Subsequently, we adopt optimization techniques to minimize global memory accesses and to further improve efficiency. We test the proposed algorithm on different experimental setups, a laptop with a low-end GPU and a desktop with a more performant GPU. In the former case, we obtain a 4-to-6-fold speed-up over a single-core implementation for fingerprints and a 4-to-7-fold speed-up for descriptors. In the latter case, we respectively obtain a 195-to-206-fold speed-up and a 100-to-328-fold speed-up. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/27774113/  |  
-------------------------------------------