{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anytree\n",
      "  Using cached anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/maida/.conda/envs/py38/lib/python3.8/site-packages (from anytree) (1.14.0)\n",
      "Installing collected packages: anytree\n",
      "Successfully installed anytree-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree, PreOrderIter\n",
    "from anytree.exporter import JsonExporter, DictExporter\n",
    "from anytree.importer import DictImporter\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_methods(text, nodes):\n",
    "    methods = set()\n",
    "    last_node = nodes[-1]\n",
    "    for node in reversed(nodes):\n",
    "        for keyword in node.keywords:\n",
    "            if text.__contains__(keyword):\n",
    "                node.count = node.count+1\n",
    "                if not (node in last_node.ancestors):\n",
    "                    methods.add(node)\n",
    "                    last_node = node\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-59182483117b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" | \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"acm_hierarchy_restructure.txt\", \"r\", encoding=\"utf8\") as f_in:\n",
    "    \n",
    "    hierarchy = f_in.read().split(\"\\n\")\n",
    "    nodes = list()\n",
    "    lvl = 0\n",
    "    \n",
    "    for term in hierarchy:\n",
    "        values = term.split(\" | \")\n",
    "        lvl = values[-5].count(\"\\t\")\n",
    "        values[0] = values[0].replace(\"\\t\",\"\")\n",
    "        values[-5] = values[1].replace(\"\\r\",\"\")\n",
    "        #print(str(lvl) + \" \" + str(values[0]))\n",
    "        kws = set()\n",
    "        if(values[1] == \"\"):\n",
    "            keyword = values[0].replace(\"-\",\" \").lower()\n",
    "            if(keyword[-1] == \"s\"):\n",
    "                keyword = keyword[0:-1]\n",
    "            kws.add(keyword)\n",
    "        else:\n",
    "            kws_list = values[1].split(\",\")\n",
    "            for kw in kws_list:\n",
    "                kws.add(kw)\n",
    "        node = Node(values[0], keywords = kws, level = lvl, count = 0) \n",
    "        nodes.append(node)\n",
    "    \n",
    "    index1 = 0\n",
    "    for node in nodes:\n",
    "        children = list()\n",
    "        index2 = index1\n",
    "        while(index2 < len(nodes)-1):\n",
    "            index2 += 1\n",
    "            if(nodes[index2].level == node.level+1):\n",
    "                children.append(nodes[index2])\n",
    "            elif(nodes[index2].level < node.level):\n",
    "                break\n",
    "        if(children):\n",
    "            nodes[index1].children = children\n",
    "        index1 += 1\n",
    "f_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "for pre, _, node in RenderTree(nodes[0]):\n",
    "    print(\"%s%s\" % (pre, node.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extractUntil2020.json\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Methods\"] = \"None\"\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged: 1 untagged: 78\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tagged = 0\n",
    "untagged = 0\n",
    "while(i < df.Methods.size):\n",
    "    text_raw = df.iat[i, 0] + \" \" + df.iat[i, 4] + \" \" + df.iat[i, 7]\n",
    "    text = re.sub(\"\\W\", \" \", text_raw.lower()) \n",
    "    methods = find_methods(text, nodes)\n",
    "    if(methods):\n",
    "        tagged+=1\n",
    "        paths = []\n",
    "        for method in methods:\n",
    "            paths.append(get_path(method))\n",
    "        df.iat[i, 9] = \"\\n\".join(paths)\n",
    "    else:\n",
    "        untagged+=1\n",
    "    i+=1\n",
    "print(\"tagged: \" + str(tagged) + \" untagged: \" + str(untagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"extractUntil2020.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(node):\n",
    "    out = \"\"\n",
    "    for ancestor in node.ancestors:\n",
    "        out = out + str(ancestor.name) + \"/\"\n",
    "    out = out + str(node.name)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development of a point of care system for automated coma prognosis: a prospective cohort study protocol\n",
      "\n",
      "  Introduction:  Coma is a deep state of unconsciousness that can be caused by a variety of clinical conditions. Traditional tests for coma outcome prediction are based mainly on a set of clinical observations. Recently, certain event-related potentials (ERPs), which are transient electroencephalogram (EEG) responses to auditory, visual or tactile stimuli, have been introduced as useful predictors of a positive coma outcome (ie, emergence). However, such tests require the skills of clinical neurophysiologists, who are not commonly available in many clinical settings. Additionally, none of the current standard clinical approaches have sufficient predictive accuracies to provide definitive prognoses.   Objective:  The objective of this study is to develop improved machine learning procedures based on EEG/ERP for determining emergence from coma.   Methods and analysis:  Data will be collected from 50 participants in coma. EEG/ERP data will be recorded for 24 consecutive hours at a maximum of five time points spanning 30 days from the date of recruitment to track participants' progression. The study employs paradigms designed to elicit brainstem potentials, middle-latency responses, N100, mismatch negativity, P300 and N400. In the case of patient emergence, data are recorded on that occasion to form an additional basis for comparison. A relevant data set will be developed from the testing of 20 healthy controls, each spanning a 15-hour recording period in order to formulate a baseline. Collected data will be used to develop an automated procedure for analysis and detection of various ERP components that are salient to prognosis. Salient features extracted from the ERP and resting-state EEG will be identified and combined to give an accurate indicator of prognosis.   Ethics and dissemination:  This study is approved by the Hamilton Integrated Research Ethics Board (project number 4840). Results will be disseminated through peer-reviewed journal articles and presentations at scientific conferences.   Trial registration number:  <a href=\"http://clinicaltrials.gov/show/NCT03826407\" title=\"See in ClinicalTrials.gov\">NCT03826407</a>. \n",
      "\n",
      "Observational Study;  Research Support, Non-U.S. Gov't;  Brain / physiopathology*;  Coma / diagnosis*;  Coma / pathology;  Electroencephalography;  Evoked Potentials;  Humans;  Machine Learning;  Point-of-Care Systems*;  Prognosis;  Prospective Studies;  Research Design;  \n",
      "\n",
      "31320356\n"
     ]
    }
   ],
   "source": [
    "i = 16\n",
    "print(str(df.iat[i, 0]) + \"\\n\")\n",
    "print(str(df.iat[i, 7]) + \"\\n\")\n",
    "print(str(df.iat[i, 9]) + \"\\n\")\n",
    "print(df.iat[i, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Doi</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords_Abstract</th>\n",
       "      <th>Keywords_MeSH</th>\n",
       "      <th>Keywords_Merged</th>\n",
       "      <th>FullTextLinks</th>\n",
       "      <th>Methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 19‑miRNA Support Vector Machine Classifier a...</td>\n",
       "      <td>Jingwei Dong; Mingjun Xu;</td>\n",
       "      <td>10.3892/or.2019.7108</td>\n",
       "      <td>PMID:31002358</td>\n",
       "      <td>Ovarian cancer (OC) is the most common gyneco...</td>\n",
       "      <td></td>\n",
       "      <td>Computational Biology;  Disease-Free Survival;...</td>\n",
       "      <td>Computational Biology;  Disease-Free Surviva...</td>\n",
       "      <td>http://www.spandidos-publications.com/or/41/6/...</td>\n",
       "      <td>Artificial intelligence/Healthcare and medicin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Silico Identification of microRNAs as Candi...</td>\n",
       "      <td>Adewale Oluwaseun Fadaka; Ashwil Klein; Ashley...</td>\n",
       "      <td>10.1177/1010428319883721</td>\n",
       "      <td>PMID:31718480</td>\n",
       "      <td>The involvement of microRNA in cancers plays ...</td>\n",
       "      <td>BLAST; CD-HIT-EST-2D; Colorectal cancer; bio...</td>\n",
       "      <td>Biomarkers, Tumor / genetics*;  Colorectal Neo...</td>\n",
       "      <td>BLAST; CD-HIT-EST-2D; Colorectal cancer; bio...</td>\n",
       "      <td>https://journals.sagepub.com/doi/10.1177/10104...</td>\n",
       "      <td>Artificial intelligence/Modeling and simulatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Combining Information From Multiple Bone Turno...</td>\n",
       "      <td>Tianxiao Zhang; Ping Liu; Yunzhi Zhang; Weiwei...</td>\n",
       "      <td>10.1080/1354750X.2018.1539767</td>\n",
       "      <td>PMID:30442069</td>\n",
       "      <td>Context:  Osteoporosis (OP) is a progressive...</td>\n",
       "      <td>Osteoporosis bone turnover markers; bone min...</td>\n",
       "      <td>Absorptiometry, Photon;  Aged;  Biomarkers / b...</td>\n",
       "      <td>Osteoporosis bone turnover markers; bone min...</td>\n",
       "      <td>http://www.tandfonline.com/doi/full/10.1080/13...</td>\n",
       "      <td>Artificial intelligence/Machine learning/Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investigating a Downstream Gene of</td>\n",
       "      <td>Ye Lu; Diana Zhou; Hong Lu; Fuyi Xu; Junming Y...</td>\n",
       "      <td></td>\n",
       "      <td>PMID:31057322</td>\n",
       "      <td>Purpose:  Glaucoma is characterized by optic...</td>\n",
       "      <td></td>\n",
       "      <td>Research Support, N.I.H., Extramural;  Researc...</td>\n",
       "      <td>Research Support, N.I.H., Extramural;  Resea...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/pmid...</td>\n",
       "      <td>Artificial intelligence/Healthcare and medicin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning to Anticipate Delivery Room A...</td>\n",
       "      <td>Albane de l'Espinay; Catherine Sauvage; Anne B...</td>\n",
       "      <td>10.1016/j.jogoh.2018.11.012</td>\n",
       "      <td>PMID:30513355</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Editorial;  Delivery Rooms / organization &amp; ad...</td>\n",
       "      <td>Editorial;  Delivery Rooms / organization &amp; ...</td>\n",
       "      <td>https://linkinghub.elsevier.com/retrieve/pii/S...</td>\n",
       "      <td>Artificial intelligence/Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Artificial Intelligence and Optical Coherence ...</td>\n",
       "      <td>Rahul Kapoor; Benjamin T Whigham; Lama A Al-As...</td>\n",
       "      <td>10.22608/APO.201904</td>\n",
       "      <td>PMID:30997756</td>\n",
       "      <td>This review article aimed to highlight the ap...</td>\n",
       "      <td>age-related macular degeneration; artificial...</td>\n",
       "      <td>Review;  Artificial Intelligence*;  Eye Diseas...</td>\n",
       "      <td>age-related macular degeneration; artificial...</td>\n",
       "      <td>https://doi.org/10.22608/APO.201904;</td>\n",
       "      <td>Artificial intelligence/Logic, knowledge repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>The Current State of Artificial Intelligence i...</td>\n",
       "      <td>Rahul Kapoor; Stephen P Walters; Lama A Al-Asw...</td>\n",
       "      <td>10.1016/j.survophthal.2018.09.002</td>\n",
       "      <td>PMID:30248307</td>\n",
       "      <td>Artificial intelligence (AI) is a branch of c...</td>\n",
       "      <td>artificial intelligence; convolutional neura...</td>\n",
       "      <td>Review;  Artificial Intelligence / trends*;  H...</td>\n",
       "      <td>artificial intelligence; convolutional neura...</td>\n",
       "      <td>https://linkinghub.elsevier.com/retrieve/pii/S...</td>\n",
       "      <td>Artificial intelligence/Natural language proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>High-performance Medicine: The Convergence of ...</td>\n",
       "      <td>Eric J Topol;</td>\n",
       "      <td>10.1038/s41591-018-0300-7</td>\n",
       "      <td>PMID:30617339</td>\n",
       "      <td>The use of artificial intelligence, and the d...</td>\n",
       "      <td></td>\n",
       "      <td>Research Support, N.I.H., Extramural;  Review;...</td>\n",
       "      <td>Research Support, N.I.H., Extramural;  Revie...</td>\n",
       "      <td>http://dx.doi.org/10.1038/s41591-018-0300-7;</td>\n",
       "      <td>Artificial intelligence/Information retrieval ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Artificial Intelligence in Nephrology: Core Co...</td>\n",
       "      <td>Olivier Niel; Paul Bastard;</td>\n",
       "      <td>10.1053/j.ajkd.2019.05.020</td>\n",
       "      <td>PMID:31451330</td>\n",
       "      <td>Artificial intelligence is playing an increas...</td>\n",
       "      <td>Deep learning; IgA nephropathy (IgAN); algor...</td>\n",
       "      <td>Review;  Algorithms*;  Artificial Intelligence...</td>\n",
       "      <td>Deep learning; IgA nephropathy (IgAN); algor...</td>\n",
       "      <td>https://linkinghub.elsevier.com/retrieve/pii/S...</td>\n",
       "      <td>Artificial intelligence/Machine learning/Learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Artificial Intelligence in Medical Imaging of ...</td>\n",
       "      <td>Li-Qiang Zhou; Jia-Yu Wang; Song-Yuan Yu; Ge-G...</td>\n",
       "      <td>10.3748/wjg.v25.i6.672</td>\n",
       "      <td>PMID:30783371</td>\n",
       "      <td>Artificial intelligence (AI), particularly de...</td>\n",
       "      <td>Artificial intelligence; Deep learning; Imag...</td>\n",
       "      <td>Review;  Algorithms;  Artificial Intelligence*...</td>\n",
       "      <td>Artificial intelligence; Deep learning; Imag...</td>\n",
       "      <td>http://www.wjgnet.com/1007-9327/full/v25/i6/67...</td>\n",
       "      <td>Artificial intelligence/Machine learning/Learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9996 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     A 19‑miRNA Support Vector Machine Classifier a...   \n",
       "1     In Silico Identification of microRNAs as Candi...   \n",
       "2     Combining Information From Multiple Bone Turno...   \n",
       "3                   Investigating a Downstream Gene of    \n",
       "4     Machine Learning to Anticipate Delivery Room A...   \n",
       "...                                                 ...   \n",
       "9991  Artificial Intelligence and Optical Coherence ...   \n",
       "9992  The Current State of Artificial Intelligence i...   \n",
       "9993  High-performance Medicine: The Convergence of ...   \n",
       "9994  Artificial Intelligence in Nephrology: Core Co...   \n",
       "9995  Artificial Intelligence in Medical Imaging of ...   \n",
       "\n",
       "                                                Authors  \\\n",
       "0                            Jingwei Dong; Mingjun Xu;    \n",
       "1     Adewale Oluwaseun Fadaka; Ashwil Klein; Ashley...   \n",
       "2     Tianxiao Zhang; Ping Liu; Yunzhi Zhang; Weiwei...   \n",
       "3     Ye Lu; Diana Zhou; Hong Lu; Fuyi Xu; Junming Y...   \n",
       "4     Albane de l'Espinay; Catherine Sauvage; Anne B...   \n",
       "...                                                 ...   \n",
       "9991  Rahul Kapoor; Benjamin T Whigham; Lama A Al-As...   \n",
       "9992  Rahul Kapoor; Stephen P Walters; Lama A Al-Asw...   \n",
       "9993                                     Eric J Topol;    \n",
       "9994                       Olivier Niel; Paul Bastard;    \n",
       "9995  Li-Qiang Zhou; Jia-Yu Wang; Song-Yuan Yu; Ge-G...   \n",
       "\n",
       "                                    Doi           PMID  \\\n",
       "0                  10.3892/or.2019.7108  PMID:31002358   \n",
       "1              10.1177/1010428319883721  PMID:31718480   \n",
       "2         10.1080/1354750X.2018.1539767  PMID:30442069   \n",
       "3                                        PMID:31057322   \n",
       "4           10.1016/j.jogoh.2018.11.012  PMID:30513355   \n",
       "...                                 ...            ...   \n",
       "9991                10.22608/APO.201904  PMID:30997756   \n",
       "9992  10.1016/j.survophthal.2018.09.002  PMID:30248307   \n",
       "9993          10.1038/s41591-018-0300-7  PMID:30617339   \n",
       "9994         10.1053/j.ajkd.2019.05.020  PMID:31451330   \n",
       "9995             10.3748/wjg.v25.i6.672  PMID:30783371   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0      Ovarian cancer (OC) is the most common gyneco...   \n",
       "1      The involvement of microRNA in cancers plays ...   \n",
       "2       Context:  Osteoporosis (OP) is a progressive...   \n",
       "3       Purpose:  Glaucoma is characterized by optic...   \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "9991   This review article aimed to highlight the ap...   \n",
       "9992   Artificial intelligence (AI) is a branch of c...   \n",
       "9993   The use of artificial intelligence, and the d...   \n",
       "9994   Artificial intelligence is playing an increas...   \n",
       "9995   Artificial intelligence (AI), particularly de...   \n",
       "\n",
       "                                      Keywords_Abstract  \\\n",
       "0                                                         \n",
       "1       BLAST; CD-HIT-EST-2D; Colorectal cancer; bio...   \n",
       "2       Osteoporosis bone turnover markers; bone min...   \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "9991    age-related macular degeneration; artificial...   \n",
       "9992    artificial intelligence; convolutional neura...   \n",
       "9993                                                      \n",
       "9994    Deep learning; IgA nephropathy (IgAN); algor...   \n",
       "9995    Artificial intelligence; Deep learning; Imag...   \n",
       "\n",
       "                                          Keywords_MeSH  \\\n",
       "0     Computational Biology;  Disease-Free Survival;...   \n",
       "1     Biomarkers, Tumor / genetics*;  Colorectal Neo...   \n",
       "2     Absorptiometry, Photon;  Aged;  Biomarkers / b...   \n",
       "3     Research Support, N.I.H., Extramural;  Researc...   \n",
       "4     Editorial;  Delivery Rooms / organization & ad...   \n",
       "...                                                 ...   \n",
       "9991  Review;  Artificial Intelligence*;  Eye Diseas...   \n",
       "9992  Review;  Artificial Intelligence / trends*;  H...   \n",
       "9993  Research Support, N.I.H., Extramural;  Review;...   \n",
       "9994  Review;  Algorithms*;  Artificial Intelligence...   \n",
       "9995  Review;  Algorithms;  Artificial Intelligence*...   \n",
       "\n",
       "                                        Keywords_Merged  \\\n",
       "0       Computational Biology;  Disease-Free Surviva...   \n",
       "1       BLAST; CD-HIT-EST-2D; Colorectal cancer; bio...   \n",
       "2       Osteoporosis bone turnover markers; bone min...   \n",
       "3       Research Support, N.I.H., Extramural;  Resea...   \n",
       "4       Editorial;  Delivery Rooms / organization & ...   \n",
       "...                                                 ...   \n",
       "9991    age-related macular degeneration; artificial...   \n",
       "9992    artificial intelligence; convolutional neura...   \n",
       "9993    Research Support, N.I.H., Extramural;  Revie...   \n",
       "9994    Deep learning; IgA nephropathy (IgAN); algor...   \n",
       "9995    Artificial intelligence; Deep learning; Imag...   \n",
       "\n",
       "                                          FullTextLinks  \\\n",
       "0     http://www.spandidos-publications.com/or/41/6/...   \n",
       "1     https://journals.sagepub.com/doi/10.1177/10104...   \n",
       "2     http://www.tandfonline.com/doi/full/10.1080/13...   \n",
       "3     https://www.ncbi.nlm.nih.gov/pmc/articles/pmid...   \n",
       "4     https://linkinghub.elsevier.com/retrieve/pii/S...   \n",
       "...                                                 ...   \n",
       "9991              https://doi.org/10.22608/APO.201904;    \n",
       "9992  https://linkinghub.elsevier.com/retrieve/pii/S...   \n",
       "9993      http://dx.doi.org/10.1038/s41591-018-0300-7;    \n",
       "9994  https://linkinghub.elsevier.com/retrieve/pii/S...   \n",
       "9995  http://www.wjgnet.com/1007-9327/full/v25/i6/67...   \n",
       "\n",
       "                                                Methods  \n",
       "0     Artificial intelligence/Healthcare and medicin...  \n",
       "1     Artificial intelligence/Modeling and simulatio...  \n",
       "2     Artificial intelligence/Machine learning/Machi...  \n",
       "3     Artificial intelligence/Healthcare and medicin...  \n",
       "4              Artificial intelligence/Machine learning  \n",
       "...                                                 ...  \n",
       "9991  Artificial intelligence/Logic, knowledge repre...  \n",
       "9992  Artificial intelligence/Natural language proce...  \n",
       "9993  Artificial intelligence/Information retrieval ...  \n",
       "9994  Artificial intelligence/Machine learning/Learn...  \n",
       "9995  Artificial intelligence/Machine learning/Learn...  \n",
       "\n",
       "[9996 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Doi</th>\n",
       "      <th>Link</th>\n",
       "      <th>Result</th>\n",
       "      <th>Title</th>\n",
       "      <th>PMID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1371/journal.pbio.1000615</td>\n",
       "      <td>https://doi.org/10.1371/journal.pbio.1000615</td>\n",
       "      <td>\\nResults\\nWe conducted 500 generations of sel...</td>\n",
       "      <td>A Quantitative Test of Hamilton's Rule for the...</td>\n",
       "      <td>21559320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1371/journal.pbio.1002110</td>\n",
       "      <td>https://doi.org/10.1371/journal.pbio.1002110</td>\n",
       "      <td></td>\n",
       "      <td>A Biotic Game Design Project for Integrated Li...</td>\n",
       "      <td>25807212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1371/journal.pbio.1001561</td>\n",
       "      <td>https://doi.org/10.1371/journal.pbio.1001561</td>\n",
       "      <td>\\nConclusion\\nAchieving skillful control of a ...</td>\n",
       "      <td>Advances in Neuroprosthetic Learning and Control</td>\n",
       "      <td>23700383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.1371/journal.pbio.0040120</td>\n",
       "      <td>https://doi.org/10.1371/journal.pbio.0040120</td>\n",
       "      <td>\\nResults\\nWe investigate the adaptation and s...</td>\n",
       "      <td>A Model of the Ventral Visual System Based on ...</td>\n",
       "      <td>16605306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.1371/journal.pbio.2000663</td>\n",
       "      <td>https://doi.org/10.1371/journal.pbio.2000663</td>\n",
       "      <td>\\nResults\\nThe results of the simulations are ...</td>\n",
       "      <td>A Mechanism for the Cortical Computation of Hi...</td>\n",
       "      <td>28253256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7902</th>\n",
       "      <td>36</td>\n",
       "      <td>10.1186/1752-0509-9-S1-S10</td>\n",
       "      <td>https://www.biomedcentral.com/1752-0509/9/S1/S10</td>\n",
       "      <td>\\nResults and discussion\\n### The selection of...</td>\n",
       "      <td>Identifying DNA-binding Proteins by Combining ...</td>\n",
       "      <td>25708928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7903</th>\n",
       "      <td>37</td>\n",
       "      <td>10.1186/1752-0509-9-S6-S3</td>\n",
       "      <td>https://www.biomedcentral.com/1752-0509/9/S6/S3</td>\n",
       "      <td>\\nResults and discussion\\nWe produced a set of...</td>\n",
       "      <td>Disentangling the Multigenic and Pleiotropic N...</td>\n",
       "      <td>26678917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>38</td>\n",
       "      <td>10.1186/1752-0509-9-S3-S5</td>\n",
       "      <td>https://www.biomedcentral.com/1752-0509/9/S3/S5</td>\n",
       "      <td>\\nConclusions\\nBIOWINE is a web resource for t...</td>\n",
       "      <td>A Knowledge Base for Vitis Vinifera Functional...</td>\n",
       "      <td>26050794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7905</th>\n",
       "      <td>39</td>\n",
       "      <td>10.1186/1752-0509-9-S1-S3</td>\n",
       "      <td>https://www.biomedcentral.com/1752-0509/9/S1/S3</td>\n",
       "      <td>\\nResults and discussion\\n### Yeast, human, mo...</td>\n",
       "      <td>Integrating Multiple Networks for Protein Func...</td>\n",
       "      <td>25707434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7906</th>\n",
       "      <td>40</td>\n",
       "      <td>10.1186/1752-0509-9-S5-S1</td>\n",
       "      <td>https://www.biomedcentral.com/1752-0509/9/S5/S1</td>\n",
       "      <td>\\nResults and discussion\\nOur experimental res...</td>\n",
       "      <td>An Empirical Study of Ensemble-Based Semi-Supe...</td>\n",
       "      <td>26356316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7907 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                           Doi  \\\n",
       "0         0  10.1371/journal.pbio.1000615   \n",
       "1         1  10.1371/journal.pbio.1002110   \n",
       "2         2  10.1371/journal.pbio.1001561   \n",
       "3         3  10.1371/journal.pbio.0040120   \n",
       "4         4  10.1371/journal.pbio.2000663   \n",
       "...     ...                           ...   \n",
       "7902     36    10.1186/1752-0509-9-S1-S10   \n",
       "7903     37     10.1186/1752-0509-9-S6-S3   \n",
       "7904     38     10.1186/1752-0509-9-S3-S5   \n",
       "7905     39     10.1186/1752-0509-9-S1-S3   \n",
       "7906     40     10.1186/1752-0509-9-S5-S1   \n",
       "\n",
       "                                                  Link  \\\n",
       "0         https://doi.org/10.1371/journal.pbio.1000615   \n",
       "1         https://doi.org/10.1371/journal.pbio.1002110   \n",
       "2         https://doi.org/10.1371/journal.pbio.1001561   \n",
       "3         https://doi.org/10.1371/journal.pbio.0040120   \n",
       "4         https://doi.org/10.1371/journal.pbio.2000663   \n",
       "...                                                ...   \n",
       "7902  https://www.biomedcentral.com/1752-0509/9/S1/S10   \n",
       "7903   https://www.biomedcentral.com/1752-0509/9/S6/S3   \n",
       "7904   https://www.biomedcentral.com/1752-0509/9/S3/S5   \n",
       "7905   https://www.biomedcentral.com/1752-0509/9/S1/S3   \n",
       "7906   https://www.biomedcentral.com/1752-0509/9/S5/S1   \n",
       "\n",
       "                                                 Result  \\\n",
       "0     \\nResults\\nWe conducted 500 generations of sel...   \n",
       "1                                                         \n",
       "2     \\nConclusion\\nAchieving skillful control of a ...   \n",
       "3     \\nResults\\nWe investigate the adaptation and s...   \n",
       "4     \\nResults\\nThe results of the simulations are ...   \n",
       "...                                                 ...   \n",
       "7902  \\nResults and discussion\\n### The selection of...   \n",
       "7903  \\nResults and discussion\\nWe produced a set of...   \n",
       "7904  \\nConclusions\\nBIOWINE is a web resource for t...   \n",
       "7905  \\nResults and discussion\\n### Yeast, human, mo...   \n",
       "7906  \\nResults and discussion\\nOur experimental res...   \n",
       "\n",
       "                                                  Title      PMID  \n",
       "0     A Quantitative Test of Hamilton's Rule for the...  21559320  \n",
       "1     A Biotic Game Design Project for Integrated Li...  25807212  \n",
       "2      Advances in Neuroprosthetic Learning and Control  23700383  \n",
       "3     A Model of the Ventral Visual System Based on ...  16605306  \n",
       "4     A Mechanism for the Cortical Computation of Hi...  28253256  \n",
       "...                                                 ...       ...  \n",
       "7902  Identifying DNA-binding Proteins by Combining ...  25708928  \n",
       "7903  Disentangling the Multigenic and Pleiotropic N...  26678917  \n",
       "7904  A Knowledge Base for Vitis Vinifera Functional...  26050794  \n",
       "7905  Integrating Multiple Networks for Protein Func...  25707434  \n",
       "7906  An Empirical Study of Ensemble-Based Semi-Supe...  26356316  \n",
       "\n",
       "[7907 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"final_Data.json\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "    df = pd.DataFrame(data)\n",
    "data_file.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results\n",
      "We investigate the adaptation and specialization of areas in a hierarchically organized visual processing stream using both a real-world robot, as well as a simulated virtual approximation (see \n",
      "\t\t\t\t). The agents are embedded in a complex environment, and a camera mounted on the robot provides continuous input to the neural network (A). The model of the visual system consists of five areas each comprising units with both intra-area and feed-forward inter-area connections (B). The convergence of the feed-forward connectivity increases while moving up the hierarchy, similar to that observed in the visual pathway []. These feed-forward connections are subject to online unsupervised learning optimizing a temporal stability objective while the intra-area connections serve the decorrelation of the states of one area. In addition, all units are leaky integrators providing them with a local transient memory trace. The data analysis focuses on the learning and network dynamics and a comparison of the response properties of neurons at different levels of the hierarchy with their respective counterparts in the real brain.\n",
      "\t\t\t(A) A camera mounted on top of the cylindrical body provides the visual input that is processed by our model of the ventral visual system. The infra-red (IR) sensors are used for obstacle avoidance during exploration of a real-world office environment within an arena of approximately 31 × 22 cm.\n",
      "\t\t\t\t\t(B) Diagram showing the hierarchical network comprising five levels of identical computational units. Units are arranged uniformly within a two-dimensional square lattice, and their number per level decreases with a constant factor of 0.5 moving up the hierarchy. Each efferent unit receives input from a topographically aligned square region within the afferent level (red connectivity) and connects laterally to all the units in the same level with which it shares feed-forward input (blue connectivity). The average relative size of a unit's feed-forward arbor within the afferent level (as given in percentages), and consequently also the lateral degree of connectivity, increases with the hierarchical level and reaches 100% for the units at the highest level. The input to the network has a resolution of 16 × 16 pixels.Exposing the network to the visual input provided by the mobile robot, we observe that after approximately 6 ·10 time steps (66 h of real-time, with a frame rate of 25 Hz) the stability of all levels has converged (). In addition, the model shows that the reorganization of the levels, in terms of their stability, follows the hierarchical order of the system, i.e., higher levels enhance their stability only after their afferent levels have reached a certain level of stable representations. After convergence, units at different processing levels show characteristic differences in their response properties. In particular, cells at the first level show orientation selectivity confirming previous results [,\n",
      "\t\t\t\t] (unpublished data). In the following, we analyze the response properties of the cells at different levels with respect to the orientation and position of the robot within the environment (A). Both the view dependence and the size of the region where activity can be elicited varies significantly with respect to the hierarchical level (one-way ANOVA, F(4,491) = 30.6, 128.3, respectively, \n",
      "\t\t\t\t ≪ .001). The view dependence of the units increases from the first to the third level on average by 16% and subsequently decreases and reaches its minimum at the last level, 32% below the first level (B). In contrast, the size of the regions in which individual units are activated decreases monotonically (C). On average, units at the first level are responsive within 52% of the environment. In contrast, units at the highest level only cover 24% of the environment. In order to control for an increasing fragmentation of the representations, we also measure the compactness of the responsive region, which does not change significantly across the hierarchical levels (D, F(4,491) = 1.28, p > 0.2). In summary, these results show that our model captures pertinent properties of the ventral visual stream. The response properties of units at early stages are selective to low-level features. Such features are visible from many different positions within the environment and the responsive regions tend to be large and selective for the orientation of the robot. At intermediate stages, each unit responds specifically to a particular view from a region of limited size, similar to landmarks, leading to a high orientation selectivity. Higher levels learn to associate neighboring “landmark” views, rendering small, compact responsive regions. At the highest level, these “landmark” representations are combined into an allocentric representation of space: a place field that is highly selectively for the robot being at a certain position within the environment irrespective of its orientation [,\n",
      "\t\t\t\t].\n",
      "\t\t\tAfter an initial phase, within which the transients due to initial conditions have decayed, learning is initiated after 10,000 time steps.(A) Responses of two-example units per hierarchical level with respect to the robot's position (left column) and its orientation (right column). The response maps show responses of cells averaged across the robot's orientation where the black contour (responsive region) identifies the 50% level of the maximal response. The polar plot shows the mean ±SD of the unit's activity within the black region with respect to 16 equally spaced orientations covering 360°. Both response maps and polar plots are normalized to the maximal response of the units across space or orientation, respectively. (B and C) Boxplots of the view, dependence, size, and compactness of the responsive regions versus the hierarchical level of the units. The blue box represents the upper and lower quartiles. The median is indicated by the red horizontal line whereas the extent of the remaining data is given by the vertical whiskers. The view dependence is measured as the CV of the response of a unit across orientations for a fixed position, averaged across the responsive region. The size of the responsive region is normalized to the size of the environment. The compactness is given by the ratio between the true perimeter of the 50% contour and the perimeter of a disc with equal area.The receptive field (RF) sizes of the feed-forward projections are bounded by the synaptic arbors of the cells at the different levels of the hierarchy. Optimizing the weights of these synapses, however, may result in different effective RF sizes. Therefore, we subsequently compare the optimized hierarchy to a reference network where all the weights are fixed to one. For this purpose, we approximate the two-subunit energy detectors by single linear units whereas the pair of weights associated to each pre-synaptic cell is replaced by a single weight \n",
      "\t\t\t\t. Using this linearization, we can project unit activations from higher levels back down to the input level, yielding an approximation of their effective RF with respect to the input. Interpreting the resulting activation as a two-dimensional distribution of mass, we compute its normalized inertial tensor \n",
      "\t\t\t\t. The two eigenvalues of \n",
      "\t\t\t\t correspond to the two principal axes of the distribution, and therefore yield a measure for the effective RF size. Comparing the optimized to the reference network, we find that the relative difference in the effective RF sizes amounts on average to −23%, −4%, +4%, +7%, and +8% for the five levels, respectively. Thus, while the optimization leads to smaller RF sizes than expected in the lower two levels, the higher levels can increase their effective RFs by converging to nonhomogeneous weight distributions.\n",
      "\t\t\tWhile early visual areas have been found to preferentially respond to simple oriented gratings, various studies have also reported a selectivity for increasingly complex shapes in subsequent levels of processing []. In the following we attempt a qualitative comparison with these results, exposing the first two levels of our model hierarchy to five simple stimuli, each composed of two bars of equal length in different spatial arrangements (see \n",
      "\t\t\t\t). For the first four stimuli, the bars are catenated to form an angle of 45°, 90°, 135°, and 180°, respectively. The fifth stimulus consists of two parallel bars. The stimuli are presented at all possible positions and 12 different orientations within the input space. For each unit, the preferred stimulus, for which it responds maximally, is determined. While the units in the first level of the hierarchy show a preference for grating-like parallel bars (68%, \n",
      "\t\t\t\t, left), the selectivity of the units at the second level is more distributed (, right). In particular, the majority of units do prefer stimuli 1–4, in which the two bars are catenated at different angles (74%). This is in accordance with experimental results, which report that cells in higher visual areas such as V2 or V4 do show an increased selectivity for curvature and corner-like shapes [,\n",
      "\t\t\t\t].\n",
      "\t\t\tThe first two levels of the hierarchy are exposed to five different stimuli, each composed of two bars of equal length in different spatial arrangements (see text). Each stimulus is presented at all possible positions and 12 different orientations within the input space (16 × 16 pixels, the width of the bars is one pixel). All units are assigned to one of the five stimuli for which they respond maximally. The particular position/orientation for which this maximal response is achieved is not considered. The individual distributions of response preferences for the first level units (\n",
      "\t\t\t\t\t\t = 256) and second level units (\n",
      "\t\t\t\t\t\t = 128) is shown in the two histograms, respectively.\n",
      "\t\t\t\t\tLearning-feature preferences based on a principle of optimal stability run counter to the intuition that biological systems are geared for fast reaction. However, we have to differentiate the behavioral timescale, the stability of visual features, and how fast these are processed by the sensory system. To further elucidate this distinction we investigated the dynamics of cells at the highest level with respect to a modified input stream. Visual stimuli recorded by the moving robot were related to its trajectory and place fields of the cells investigated. We cut the video stream, deleting sections where the robot was moving from an area of low average activity of a considered cell (< 25% of maximum) toward an area of high average activity (> 75% of maximum). The resulting video thus contains sudden jumps from low to high average activity of a particular cell. In \n",
      "\t\t\t\t we compare the resulting dynamics to the processing of unmodified videos. The scatter plot demonstrates that processing of rapidly changing stimuli by the network is fast, most often a few time steps only. The dynamics of the complete system is dominated by the behavioral timescale, slower by a factor of two. Thus, the system rapidly processes learned optimally stable features.\n",
      "\t\t\tFor each unit at the highest level of the hierarchy, the input frames for which the average response of the unit lies between 25% and 75% of the maximum has been removed from the input stream. Subsequently, the resulting rise time, which is the number of time steps required for a unit to traverse the interval between its 25% and 75% level of maximal response, is plotted versus the rise time under normal input conditions. The ratio “rise time” : “rise time with gap” is 2.1 ± 1.9 (mean ± SD, \n",
      "\t\t\t\t\t\t = 226).\n",
      "\t\t\t\t\tTo evaluate our hypothesis that the highest level of our model forms place fields, we assessed whether these allow an accurate reconstruction of the position of the robot. We use a standard Bayesian framework for position reconstruction []. Half of the responses recorded over 10 time steps from units at the last hierarchical level serve to acquire the distribution of posterior probabilities \n",
      "\t\t\t\t(\n",
      "\t\t\t\t|\n",
      "\t\t\t\t) where \n",
      "\t\t\t\t is the position of the robot within the environment, and \n",
      "\t\t\t\t a vector containing the responses \n",
      "\t\t\t\t of the individual cells. The other half is used for testing the quality of reconstruction. According to Bayes rule, \n",
      "\t\t\t\t(\n",
      "\t\t\t\t|\n",
      "\t\t\t\t) ∝ \n",
      "\t\t\t\t(\n",
      "\t\t\t\t|\n",
      "\t\t\t\t)\n",
      "\t\t\t\t(\n",
      "\t\t\t\t), where \n",
      "\t\t\t\t(\n",
      "\t\t\t\t) is the probability of the robot to be at a certain position within the environment. The most likely position of the robot is then given by ◯ = argmax\n",
      "\t\t\t\t(\n",
      "\t\t\t\t|\n",
      "\t\t\t\t). Applying this procedure to the responses of the different levels in the processing hierarchy yields a monotonically decreasing reconstruction error when moving from lower to higher levels (). In particular we find that responses of the units at the fifth level allow a highly accurate position reconstruction with an average error of 0.08 ± 0.08 (mean ± SD, in units of the length of the long side of the environment). In addition, we analyzed the spatial distribution of reconstruction errors, which shows that reconstruction is good for the central part and becomes poorer around the border of the environment (\n",
      "\t\t\t\t). Thus, these allocentric representations at the highest level allow a reconstruction of the position of the behaving system with an accuracy equivalent to that observed in reconstructions based on the responses of hippocampal place cells [].\n",
      "\t\t\tThe position of the robot is reconstructed using the responses of the \n",
      "\t\t\t\t\t\t = 2 units in the different levels \n",
      "\t\t\t\t\t\t = 1…5 of the processing hierarchy using a standard Bayesian framework (see text). The reconstruction error, defined as the Euclidean distance between the true and the reconstructed position, is shown as a function of the hierarchical level. The error bars indicate the standard deviation from the mean.\n",
      "\t\t\t\t\tAn important property of place fields in the hippocampus is their response to changes in the environment [,\n",
      "\t\t\t\t]. For instance, it was shown by stretching a rectangular arena along its principle axes that localization and shape of place fields in rat hippocampus are controlled by the distance to the walls and surrounding landmarks []. As a comparison we perform the same manipulations using a virtual environment (A). After learning in the small square environment, the network connectivity is frozen and exposed to three test environments (B–D). We observe that the place fields depend on the robot's distance from one or more of the four surrounding walls in close analogy with the experimental data. Furthermore, three main effects with respect to the stretching of the environment can be distinguished. The place cells either keep a fixed distance to one wall, split into two subfields, or stretch along the direction the environment is stretched. The units shown in \n",
      "\t\t\t\tB and \n",
      "\t\t\t\tC both stretch vertically while maintaining a fixed shape and distance to the right wall. The unit shown in \n",
      "\t\t\t\tC is selective for a certain distance from both top and bottom walls as well as from the left wall, such that the place field is split in the vertical and stretched in the horizontal direction. These results match the properties of neurons observed in the hippocampus and suggest that optimally stable representations capture important aspects of representations in entorhinal cortex and hippocampus.\n",
      "\t\t\t(A) The virtual robot environment consists of a square arena of comparable relative size to the real-world setup and surrounding objects, e.g., a large wall along one side of the arena and a cylinder next to one of the opposite corners. This environment is stretched along either or both directions indicated by the red arrows by a factor of 1.5.(B–D) Response map of three example units that have been acquired in the original environment—small square, lower left map in (B–D)—and subsequently tested in three variants of the original environment, i.e., stretched along the vertical and/or horizontal directions. For each unit, the intensity scale of all four response maps is normalized to the maximal response of all environments.To assess the properties of cortical representations at higher stages of the visual hierarchy, such as the IT, image scrambling has been successfully used in both experimental [,\n",
      "\t\t\t\t] as well as theoretical [] studies. Here we apply this scrambling method to perform a similar analysis on our intermediate level of the hierarchy, i.e., the “landmark” cells at the third level. Those cells do qualify best for IT-like cells, not only due to their relative position within our visual hierarchy, but also because they show maximal view selectivity (B). We freeze the weights in the network after learning the real-world environment and subsequently perform four different tests with input streams spatially scrambled at four different scales (A). We observe that the average activity decreases monotonically relative to control (B), suggesting that these units are selective for complex visual features characterized at multiple spatial scales. This result is compatible with recent experimental studies [,\n",
      "\t\t\t\t] that have shown a similar characteristic relationship between the spatial scale of scrambling and the degradation of the responses in IT.\n",
      "\t\t\t(A) The original image shows the environment as it is perceived by the camera mounted on top of the robot. The normal image is the downscaled 16 × 16 pixel version that serves as the input to the proposed network model. The four subsequent images show the same input image scrambled by increasing degrees, i.e., by randomly permuting 2 × 2, 4 × 4, 8 × 8, or 16 × 16 blocks of equal size.(B) Average response of the units at the third level of the hierarchy for a normal visual input as well as the four different scales of spatial scrambling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iat[3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
