10.1093/jnen/nlz122  |   The neuropathology associated with cognitive decline in military personnel exposed to traumatic brain injury (TBI) and chronic stress is incompletely understood. Few studies have examined clinicopathologic correlations between phosphorylated-tau neurofibrillary tangles, β-amyloid neuritic plaques, neuroinflammation, or white matter (WM) lesions, and neuropsychiatric disorders in veterans. We describe clinicopathologic findings in 4 military veterans with early-onset dementia (EOD) who had varying histories of blunt- and blast-TBI, cognitive decline, behavioral abnormalities, post-traumatic stress disorder, suicidal ideation, and suicide. We found that pathologic lesions in these military-EOD cases could not be categorized as classic Alzheimer's disease (AD), chronic traumatic encephalopathy, traumatic axonal injury, or other well-characterized clinicopathologic entities. Rather, we observed a mixture of polypathology with unusual patterns compared with pathologies found in AD or other dementias. Also, ultrahigh resolution ex vivo MRI in 2 of these 4 brains revealed unusual patterns of periventricular WM injury. These findings suggest that military-EOD cases are associated with atypical combinations of brain lesions and distribution rarely seen in nonmilitary populations. Future prospective studies that acquire neuropsychiatric data before and after deployments, as well as genetic and environmental exposure data, are needed to further elucidate clinicopathologic correlations in military-EOD. 
  |  https://academic.oup.com/jnen/article-lookup/doi/10.1093/jnen/nlz122  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31851313/  |  
------------------------------------------- 
10.1148/radiol.2020200905  |   Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value&lt;0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020200905?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.mri.2019.09.006  |   One major thrust in radiology today is image standardization with a focus on rapidly acquired quantitative multi-contrast information. This is critical for multi-center trials, for the collection of big data and for the use of artificial intelligence in evaluating the data. Strategically acquired gradient echo (STAGE) imaging is one such method that can provide 8 qualitative and 7 quantitative pieces of information in 5 min or less at 3 T. STAGE provides qualitative images in the form of proton density weighted images, T1 weighted images, T2* weighted images and simulated double inversion recovery (DIR) images. STAGE also provides quantitative data in the form of proton spin density, T1, T2* and susceptibility maps as well as segmentation of white matter, gray matter and cerebrospinal fluid. STAGE uses vendors' product gradient echo sequences. It can be applied from 0.35 T to 7 T across all manufacturers producing similar results in contrast and quantification of the data. In this paper, we discuss the strengths and weaknesses of STAGE, demonstrate its contrast-to-noise (CNR) behavior relative to a large clinical data set and introduce a few new image contrasts derived from STAGE, including DIR images and a new concept referred to as true susceptibility weighted imaging (tSWI) linked to fluid attenuated inversion recovery (FLAIR) or tSWI-FLAIR for the evaluation of multiple sclerosis lesions. The robustness of STAGE T1 mapping was tested using the NIST/NIH phantom, while the reproducibility was tested by scanning a given individual ten times in one session and the same subject scanned once a week over a 12-week period. Assessment of the CNR for the enhanced T1W image (T1WE) showed a significantly better contrast between gray matter and white matter than conventional T1W images in both patients with Parkinson's disease and healthy controls. We also present some clinical cases using STAGE imaging in patients with stroke, metastasis, multiple sclerosis and a fetus with ventriculomegaly. Overall, STAGE is a comprehensive protocol that provides the clinician with numerous qualitative and quantitative images. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0730-725X(19)30382-0  |  
------------------------------------------- 
10.1007/s12149-020-01460-z  |    Objective:  The aim of the study was to investigate the outcomes and prognostic factors of high-dose <sup>131</sup>I-metaiodobenzylguanidine (<sup>131</sup>I-MIBG) therapy in patients with refractory or relapsed neuroblastoma (NBL) in Japan. 
  Methods:  We retrospectively analyzed 20 patients with refractory or relapsed high-risk NBL who underwent <sup>131</sup>I-MIBG therapy with an administration dose ranging from 444 to 666 MBq/kg at Kanazawa University Hospital, Japan, between September 2008 and September 2013. We focused on measurements regarding their initial responses, prognostic factors, survivals, and toxicities following <sup>131</sup>I-MIBG therapy using our hospital data and questionnaires from the hospitals that these patients were initially referred from. Furthermore, we performed Kaplan-Meier survival analysis to evaluate event-free survival (EFS) and overall survival (OS). 
  Results:  In 19 patients with complete follow-up data, the median age at first <sup>131</sup>I-MIBG treatment was 7.9 years (range 2.5-17.7 years). Following <sup>131</sup>I-MIBG therapy, 17 of the 19 patients underwent stem-cell transplantations, and their treatment response was either complete (CR) or partial (PR) in three and two cases, respectively. The EFS and OS rates at 1 year following <sup>131</sup>I-MIBG therapy were 42% and 58%, respectively, and those at 5 years following <sup>131</sup>I-MIBG therapy were 16% and 42%, respectively. Using the two-sample log-rank test, the OS time following <sup>131</sup>I-MIBG therapy was significantly longer for &lt; 3-year time interval between the initial diagnosis and <sup>131</sup>I-MIBG therapy (p = 0.017), Curie score &lt; 16 just before <sup>131</sup>I-MIBG therapy (p = 0.002), without pain (p = 0.002), without both vanillylmandelic acid (VMA) and homovanillic acid (HVA) elevation (p = 0.037) at <sup>131</sup>I-MIBG therapy, and with CR or PR following <sup>131</sup>I-MIBG therapy (p = 0.015). Although severe hematological toxicities were identified in all 19 patients, severe nonhematological toxicity was not recorded in any patient, except for one patient with grade 3 anorexia and nausea. 
  Conclusions:  High-dose <sup>131</sup>I-MIBG therapy in patients with refractory or relapsed high-risk NBL can provide a favorable prognosis without severe nonhematological toxicities. Better prognosis may be anticipated in patients with the initial good response, no pain at <sup>131</sup>I-MIBG therapy, no VMA and HVA elevation at <sup>131</sup>I-MIBG therapy, low Curie score (&lt; 16) just before <sup>131</sup>I-MIBG therapy, and short time interval (&lt; 3 years) between the initial diagnosis and <sup>131</sup>I-MIBG therapy. 
  |  https://dx.doi.org/10.1007/s12149-020-01460-z  |  
------------------------------------------- 
10.1007/s12350-018-1317-5  |    Background:  Coronary PET shows promise in the detection of high-risk atherosclerosis, but there remains a need to optimize imaging and reconstruction techniques. We investigated the impact of reconstruction parameters and cardiac motion-correction in <sup>18</sup>F Sodium Fluoride (<sup>18</sup>F-NaF) PET. 
  Methods:  Twenty-two patients underwent <sup>18</sup>F-NaF PET within 22 days of an acute coronary syndrome. Optimal reconstruction parameters were determined in a subgroup of six patients. Motion-correction was performed on ECG-gated data of all patients with optimal reconstruction. Tracer uptake was quantified in culprit and reference lesions by computing signal-to-noise ratio (SNR) in diastolic, summed, and motion-corrected images. 
  Results:  Reconstruction using 24 subsets, 4 iterations, point-spread-function modelling, time of flight, and 5-mm post-filtering provided the highest median SNR (31.5) compared to 4 iterations 0-mm (22.5), 8 iterations 0-mm (21.1), and 8 iterations 5-mm (25.6; all P &lt; .05). Motion-correction improved SNR of culprit lesions (n = 33) (24.5[19.9-31.5]) compared to diastolic (15.7[12.4-18.1]; P &lt; .001) and summed data (22.1[18.9-29.2]; P &lt; .001). Motion-correction increased the SNR difference between culprit and reference lesions (10.9[6.3-12.6]) compared to diastolic (6.2[3.6-10.3]; P = .001) and summed data (7.1 [4.8-11.6]; P = .001). 
  Conclusions:  The number of iterations and extent of post-filtering has marked effects on coronary <sup>18</sup>F-NaF PET quantification. Cardiac motion-correction improves discrimination between culprit and reference lesions. 
  |  https://dx.doi.org/10.1007/s12350-018-1317-5  |  
------------------------------------------- 
10.3322/caac.21608  |   Patient-generated health data (PGHD), or health-related data gathered from patients to help address a health concern, are used increasingly in oncology to make regulatory decisions and evaluate quality of care. PGHD include self-reported health and treatment histories, patient-reported outcomes (PROs), and biometric sensor data. Advances in wireless technology, smartphones, and the Internet of Things have facilitated new ways to collect PGHD during clinic visits and in daily life. The goal of the current review was to provide an overview of the current clinical, regulatory, technological, and analytic landscape as it relates to PGHD in oncology research and care. The review begins with a rationale for PGHD as described by the US Food and Drug Administration, the Institute of Medicine, and other regulatory and scientific organizations. The evidence base for clinic-based and remote symptom monitoring using PGHD is described, with an emphasis on PROs. An overview is presented of current approaches to digital phenotyping or device-based, real-time assessment of biometric, behavioral, self-report, and performance data. Analytic opportunities regarding PGHD are envisioned in the context of big data and artificial intelligence in medicine. Finally, challenges and solutions for the integration of PGHD into clinical care are presented. The challenges include electronic medical record integration of PROs and biometric data, analysis of large and complex biometric data sets, and potential clinic workflow redesign. In addition, there is currently more limited evidence for the use of biometric data relative to PROs. Despite these challenges, the potential benefits of PGHD make them increasingly likely to be integrated into oncology research and clinical care. 
  |  https://doi.org/10.3322/caac.21608  |  
------------------------------------------- 
10.1192/bjp.2019.127  |    Background:  Schizophrenia is a complex mental disorder with high heritability and polygenic inheritance. Multimodal neuroimaging studies have also indicated that abnormalities of brain structure and function are a plausible neurobiological characterisation of schizophrenia. However, the polygenic effects of schizophrenia on these imaging endophenotypes have not yet been fully elucidated. 
  Aims:  To investigate the effects of polygenic risk for schizophrenia on the brain grey matter volume and functional connectivity, which are disrupted in schizophrenia. 
  Method:  Genomic and neuroimaging data from a large sample of Han Chinese patients with schizophrenia (N = 509) and healthy controls (N = 502) were included in this study. We examined grey matter volume and functional connectivity via structural and functional magnetic resonance imaging, respectively. Using the data from a recent meta-analysis of a genome-wide association study that comprised a large number of Chinese people, we calculated a polygenic risk score (PGRS) for each participant. 
  Results:  The imaging genetic analysis revealed that the individual PGRS showed a significantly negative correlation with the hippocampal grey matter volume and hippocampus-medial prefrontal cortex functional connectivity, both of which were lower in the people with schizophrenia than in the controls. We also found that the observed neuroimaging measures showed weak but similar changes in unaffected first-degree relatives of patients with schizophrenia. 
  Conclusions:  These findings suggested that genetically influenced brain grey matter volume and functional connectivity may provide important clues for understanding the pathological mechanisms of schizophrenia and for the early diagnosis of schizophrenia. 
  |  https://www.cambridge.org/core/product/identifier/S0007125019001272/type/journal_article  |  
------------------------------------------- 
10.1002/humu.23948  |   Primary microcephaly (PM) is characterized by a small head since birth and is vastly heterogeneous both genetically and phenotypically. While most cases are monogenic, genetic interactions between Aspm and Wdr62 have recently been described in a mouse model of PM. Here, we used two complementary, holistic in vivo approaches: high throughput DNA sequencing of multiple PM genes in human patients with PM, and genome-edited zebrafish modeling for the digenic inheritance of PM. Exomes of patients with PM showed a significant burden of variants in 75 PM genes, that persisted after removing monogenic causes of PM (e.g., biallelic pathogenic variants in CEP152). This observation was replicated in an independent cohort of patients with PM, where a PM gene panel showed in addition that the burden was carried by six centrosomal genes. Allelic frequencies were consistent with digenic inheritance. In zebrafish, non-centrosomal gene casc5 -/- produced a severe PM phenotype, that was not modified by centrosomal genes aspm or wdr62 invalidation. A digenic, quadriallelic PM phenotype was produced by aspm and wdr62. Our observations provide strong evidence for digenic inheritance of human PM, involving centrosomal genes. Absence of genetic interaction between casc5 and aspm or wdr62 further delineates centrosomal and non-centrosomal pathways in PM. 
  |  https://doi.org/10.1002/humu.23948  |  
------------------------------------------- 
10.1148/radiol.2019192515  |    |  http://pubs.rsna.org/doi/10.1148/radiol.2019192515?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1002/adma.201906493  |   Development of stimuli-responsive materials with complex practical functions is significant for achieving bioinspired artificial intelligence. It is challenging to fabricate stimuli-responsive hydrogels showing simultaneous changes in fluorescence color, brightness, and shape in response to a single stimulus. Herein, a bilayer hydrogel strategy is designed by utilizing an aggregation-induced emission luminogen, tetra-(4-pyridylphenyl)ethylene (TPE-4Py), to fabricate hydrogels with the above capabilities. Bilayer hydrogel actuators with the ionomer of poly(acrylamide-r-sodium 4-styrenesulfonate) (PAS) as a matrix of both active and passive layers and TPE-4Py as the core function element in the active layer are prepared. At acidic pH, the protonation of TPE-4Py leads to fluorescence color and brightness changes of the actuators and the electrostatic interactions between the protonated TPE-4Py and benzenesulfonate groups of the PAS chains in the active layer cause the actuators to deform. The proposed TPE-4Py/PAS-based bilayer hydrogel actuators with such responsiveness to stimulus provide insights in the design of intelligent systems and are highly attractive material candidates in the fields of 3D/4D printing, soft robots, and smart wearable devices. 
  |  https://doi.org/10.1002/adma.201906493  |  
------------------------------------------- 
10.3390/ijerph17031093  |   <i>Background</i>: The primary care service in Catalonia has operated an asynchronous teleconsulting service between GPs and patients since 2015 (eConsulta), which has generated some 500,000 messages. New developments in big data analysis tools, particularly those involving natural language, can be used to accurately and systematically evaluate the impact of the service. <i>Objective</i>: The study was intended to assess the predictive potential of eConsulta messages through different combinations of vector representation of text and machine learning algorithms and to evaluate their performance. <i>Methodology</i>: Twenty machine learning algorithms (based on five types of algorithms and four text representation techniques) were trained using a sample of 3559 messages (169,102 words) corresponding to 2268 teleconsultations (1.57 messages per teleconsultation) in order to predict the three variables of interest (avoiding the need for a face-to-face visit, increased demand and type of use of the teleconsultation). The performance of the various combinations was measured in terms of precision, sensitivity, F-value and the ROC curve. <i>Results</i>: The best-trained algorithms are generally effective, proving themselves to be more robust when approximating the two binary variables "avoiding the need of a face-to-face visit" and "increased demand" (precision = 0.98 and 0.97, respectively) rather than the variable "type of query" (precision = 0.48). <i>Conclusion</i>: To the best of our knowledge, this study is the first to investigate a machine learning strategy for text classification using primary care teleconsultation datasets. The study illustrates the possible capacities of text analysis using artificial intelligence. The development of a robust text classification tool could be feasible by validating it with more data, making it potentially more useful for decision support for health professionals. 
  |  http://www.mdpi.com/resolver?pii=ijerph17031093  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32050435/  |  
------------------------------------------- 
10.2196/15510  |    Background:  Artificial intelligence-enabled electronic health record (EHR) analysis can revolutionize medical practice from the diagnosis and prediction of complex diseases to making recommendations in patient care, especially for chronic conditions such as chronic kidney disease (CKD), which is one of the most frequent complications in patients with diabetes and is associated with substantial morbidity and mortality. 
  Objective:  The longitudinal prediction of health outcomes requires effective representation of temporal data in the EHR. In this study, we proposed a novel temporal-enhanced gradient boosting machine (GBM) model that dynamically updates and ensembles learners based on new events in patient timelines to improve the prediction accuracy of CKD among patients with diabetes. 
  Methods:  Using a broad spectrum of deidentified EHR data on a retrospective cohort of 14,039 adult patients with type 2 diabetes and GBM as the base learner, we validated our proposed Landmark-Boosting model against three state-of-the-art temporal models for rolling predictions of 1-year CKD risk. 
  Results:  The proposed model uniformly outperformed other models, achieving an area under receiver operating curve of 0.83 (95% CI 0.76-0.85), 0.78 (95% CI 0.75-0.82), and 0.82 (95% CI 0.78-0.86) in predicting CKD risk with automatic accumulation of new data in later years (years 2, 3, and 4 since diabetes mellitus onset, respectively). The Landmark-Boosting model also maintained the best calibration across moderate- and high-risk groups and over time. The experimental results demonstrated that the proposed temporal model can not only accurately predict 1-year CKD risk but also improve performance over time with additionally accumulated data, which is essential for clinical use to improve renal management of patients with diabetes. 
  Conclusions:  Incorporation of temporal information in EHR data can significantly improve predictive model performance and will particularly benefit patients who follow-up with their physicians as recommended. 
  |  https://medinform.jmir.org/2020/1/e15510/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012067/  |  
------------------------------------------- 
10.1038/s41598-020-62676-7  |   Generative Adversarial Network (GAN) requires extensive computing resources making its implementation in edge devices with conventional microprocessor hardware a slow and difficult, if not impossible task. In this paper, we propose to accelerate these intensive neural computations using memristive neural networks in analog domain. The implementation of Analog Memristive Deep Convolutional GAN (AM-DCGAN) using Generator as deconvolutional and Discriminator as convolutional memristive neural network is presented. The system is simulated at circuit level with 1.7 million memristor devices taking into account memristor non-idealities, device and circuit parameters. The design is modular with crossbar arrays having a minimum average power consumption per neural computation of 47nW. The design exclusively uses the principles of neural network dropouts resulting in regularization and lowering the power consumption. The SPICE level simulation of GAN is performed with 0.18 μm CMOS technology and WO<sub>x</sub> memristive devices with R<sub>ON</sub> = 40 kΩ and R<sub>OFF</sub> = 250 kΩ, threshold voltage 0.8 V and write voltage at 1.0 V. 
  |  http://dx.doi.org/10.1038/s41598-020-62676-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32246103/  |  
------------------------------------------- 
10.1098/rsta.2019.0163  |   This paper presents the design of an ultra-low energy neural network that uses time-mode signal processing). Handwritten digit classification using a single-layer artificial neural network (ANN) with a Softmin-based activation function is described as an implementation example. To realize time-mode operation, the presented design makes use of monostable multivibrator-based multiplying analogue-to-time converters, fixed-width pulse generators and basic digital gates. The time-mode digit classification ANN was designed in a standard CMOS 0.18 μm IC process and operates from a supply voltage of 0.6 V. The system operates on the MNIST database of handwritten digits with quantized neuron weights and has a classification accuracy of 88%, which is typical for single-layer ANNs, while dissipating 65.74 pJ per classification with a speed of 2.37 k classifications per second. This article is part of the theme issue 'Harmonizing energy-autonomous computing and intelligence'. 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rsta.2019.0163?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1002/hbm.24968  |   There is an ongoing debate about whether, and to what extent, males differ from females in their language skills. In the case of handwriting, a composite language skill involving language and motor processes, behavioral observations consistently show robust sex differences but the mechanisms underlying the effect are unclear. Using functional magnetic resonance imaging (fMRI) in a copying task, the present study examined the neural basis of sex differences in handwriting in 53 healthy adults (ages 19-28, 27 males). Compared to females, males showed increased activation in the left posterior middle frontal gyrus (Exner's area), a region thought to support the conversion between orthographic and graphomotor codes. Functional connectivity between Exner's area and the right cerebellum was greater in males than in females. Furthermore, sex differences in brain activity related to handwriting were independent of language material. This study identifies a novel neural signature of sex differences in a hallmark of human behavior, and highlights the importance of considering sex as a factor in scientific research and clinical applications involving handwriting. 
  |  https://doi.org/10.1002/hbm.24968  |  
------------------------------------------- 
10.1136/gutjnl-2019-318860  |    Objective:  Pancreatic ductal adenocarcinoma (PDAC) is difficult to diagnose at resectable stage. Recent studies have suggested that extracellular vesicles (EVs) contain long RNAs. The aim of this study was to develop a diagnostic (d-)signature for the detection of PDAC based on EV long RNA (exLR) profiling. 
  Design:  We conducted a case-control study with 501 participants, including 284 patients with PDAC, 100 patients with chronic pancreatitis (CP) and 117 healthy subjects. The exLR profile of plasma samples was analysed by exLR sequencing. The d-signature was identified using a support vector machine algorithm and a training cohort (n=188) and was validated using an internal validation cohort (n=135) and an external validation cohort (n=178). 
  Results:  We developed a d-signature that comprised eight exLRs, including FGA, KRT19, HIST1H2BK, ITIH2, MARCH2, CLDN1, MAL2 and TIMP1, for PDAC detection. The d-signature showed high accuracy, with an area under the receiver operating characteristic curve (AUC) of 0.960, 0.950 and 0.936 in the training, internal validation and external validation cohort, respectively. The d-signature was able to identify resectable stage I/II cancer with an AUC of 0.949 in the combined three cohorts. In addition, the d-signature showed superior performance to carbohydrate antigen 19-9 in distinguishing PDAC from CP (AUC 0.931 vs 0.873, p=0.028). 
  Conclusion:  This study is the first to characterise the plasma exLR profile in PDAC and to report an exLR signature for the detection of pancreatic cancer. This signature may improve the prognosis of patients who would have otherwise missed the curative treatment window. 
  |  http://gut.bmj.com/cgi/pmidlookup?view=long&pmid=31562239  |  
------------------------------------------- 
10.1007/s11571-019-09560-x  |   Motor imagery (MI) is a mental representation of motor behavior and has been widely used in electroencephalogram based brain-computer interfaces (BCIs). Several studies have demonstrated the efficacy of MI-based BCI-feedback training in post-stroke rehabilitation. However, in the earliest stage of the training, calibration data typically contain insufficient discriminability, resulting in unreliable feedback, which may decrease subjects' motivation and even hinder their training. To improve the performance in the early stages of MI training, a novel hybrid BCI paradigm based on MI and P300 is proposed in this study. In this paradigm, subjects are instructed to imagine writing the Chinese character following the flash order of the desired Chinese character displayed on the screen. The event-related desynchronization/synchronization (ERD/ERS) phenomenon is produced with writing based on one's imagination. Simultaneously, the P300 potential is evoked by the flash of each stroke. Moreover, a fusion method of P300 and MI classification is proposed, in which unreliable P300 classifications are corrected by reliable MI classifications. Twelve healthy naïve MI subjects participated in this study. Results demonstrated that the proposed hybrid BCI paradigm yielded significantly better performance than the single-modality BCI paradigm. The recognition accuracy of the fusion method is significantly higher than that of P300 (<i>p</i> &lt; 0.05) and MI (<i>p</i> &lt; 0.01). Moreover, the training data size can be reduced through fusion of these two modalities. 
  |  https://dx.doi.org/10.1007/s11571-019-09560-x  |  
------------------------------------------- 
10.1016/j.ijporl.2019.109833  |    Objective:  To summarize recently published key articles on the topics of biomedical engineering, biotechnology and new models in relation to otitis media (OM). 
  Data sources:  Electronic databases: PubMed, Ovid Medline, Cochrane Library and Clinical Evidence (BMJ Publishing). 
  Review methods:  Articles on biomedical engineering, biotechnology, material science, mechanical and animal models in OM published between May 2015 and May 2019 were identified and subjected to review. A total of 132 articles were ultimately included. 
  Results:  New imaging technologies for the tympanic membrane (TM) and the middle ear cavity are being developed to assess TM thickness, identify biofilms and differentiate types of middle ear effusions. Artificial intelligence (AI) has been applied to train software programs to diagnose OM with a high degree of certainty. Genetically modified mice models for OM have further investigated what predisposes some individuals to OM and consequent hearing loss. New vaccine candidates protecting against major otopathogens are being explored and developed, especially combined vaccines, targeting more than one pathogen. Transcutaneous vaccination against non-typeable Haemophilus influenzae has been successfully tried in a chinchilla model. In terms of treatment, novel technologies for trans-tympanic drug delivery are entering the clinical domain. Various growth factors and grafting materials aimed at improving healing of TM perforations show promising results in animal models. 
  Conclusion:  New technologies and AI applications to improve the diagnosis of OM have shown promise in pre-clinical models and are gradually entering the clinical domain. So are novel vaccines and drug delivery approaches that may allow local treatment of OM. 
  Implications for practice:  New diagnostic methods, potential vaccine candidates and the novel trans-tympanic drug delivery show promising results, but are not yet adapted to clinical use. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0165-5876(19)30586-5  |  
------------------------------------------- 
10.1016/j.msard.2020.102034  |    Background:  Multiple sclerosis is a progressive disease responsible for gait disabilities and cognitive impairment, which affect functional performance. Robot-assisted gait training is an emerging training method to facilitate body-weight-supported treadmill training in many neurologic diseases. Through this study, we aimed to determine the efficacy of robot-assisted gait training in patients with multiple sclerosis. 
  Methods:  We performed a systematic review and meta-analysis of randomized controlled trials evaluating the effect of robot-assisted gait training for multiple sclerosis. We searched PubMed, EMBASE, the Cochrane Library, and ClinicalTrials.gov registry for articles published before May 2019. The primary outcome was walking performance (gait parameters, balance, and ambulation capability). The secondary outcomes were changes in perceived fatigue, severity of spasticity, global mobility, physical and mental quality of life, severity of pain, activities of daily living, and treatment acceptance. 
  Results:  We identified 10 studies (9 different trials) that included patients with multiple sclerosis undergoing robot-assisted gait training or conventional walk training. The meta-analysis showed comparable effectiveness between robot-assisted gait training and conventional walking therapy in walking performance, quality of life, pain, or activities of daily living. The robot-assisted gait training was even statistically superior to conventional walking therapy in improving perceived fatigue (pooled SMD: 0.34, 95% CI: 0.02-0.67), spasticity (pooled SMD: 0.70, 95% CI: 0.08-1.33, I² = 53%), and global mobility (borderline) after the intervention. 
  Conclusion:  Our results provide the most up-to-date evidence regarding the robot-assisted gait training on multiple sclerosis. In addition to the safety and good tolerance, its efficacy on multiple sclerosis is comparable to that of conventional walking training and is even superior in improving fatigue and spasticity. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2211-0348(20)30110-3  |  
------------------------------------------- 
10.1001/jamanetworkopen.2020.0255  |    Importance:  Mobile applications (apps) may help improve hypertension self-management. 
  Objective:  To investigate the effect of an artificial intelligence smartphone coaching app to promote home monitoring and hypertension-related behaviors on systolic blood pressure level compared with a blood pressure tracking app. 
  Design, setting, and participants:  This was a 2-group, open, randomized clinical trial. Participants with uncontrolled hypertension were recruited in 2016 and 2017 and were followed up for 6 months. Data analysis was performed from April 2019 to December 2019. 
  Interventions:  Intervention group participants received a smartphone coaching app to promote home monitoring and behavioral changes associated with hypertension self-management plus a home blood pressure monitor. Control participants received a blood pressure tracking app plus a home blood pressure monitor. 
  Main outcomes and measures:  The primary study outcome was systolic blood pressure at 6 months. Secondary outcomes included self-reported antihypertensive medication adherence, home monitoring and self-management practices, measures of self-efficacy associated with blood pressure, weight, and self-reported health behaviors. 
  Results:  There were 333 participants randomized, and 297 completed the follow-up assessment. Among the participants who completed the study, the mean (SD) age was 58.9 (12.8) years, 182 (61.3%) were women, and 103 (34.7%) were black. Baseline mean (SD) systolic blood pressure was 140.6 (12.2) mm Hg among intervention participants and 141.8 (13.4) mm Hg among control participants. After 6 months, the corresponding mean (SD) systolic blood pressures were 132.3 (15.0) mm Hg and 135.0 (13.9) mm Hg, with a between-group adjusted difference of -2.0 mm Hg (95% CI, -4.9 mm Hg to 0.8 mm Hg; P = .16). At 6 months, self-confidence in controlling blood pressure was greater in the intervention group (0.36 point on a 5-point scale; 95% CI, 0.18 point to 0.54 point; P &lt; .001). There were no significant differences between the 2 groups in other secondary outcomes. The adjusted difference in self-reported physical activity was 26.7 minutes per week (95% CI, -5.4 minutes per week to 58.8 minutes per week; P = .10). Subgroup analysis raised the possibility that intervention effects differed by age. 
  Conclusions and relevance:  Among individuals with uncontrolled hypertension, those randomized to a smartphone coaching app plus home monitor had similar systolic blood pressure compared with those who received a blood pressure tracking app plus home monitor. Given the direction of the difference in systolic blood pressure between groups and the possibility for differences in treatment effects across subgroups, future studies are warranted. 
  Trial registration:  ClinicalTrials.gov Identifier: <a href="http://clinicaltrials.gov/show/NCT03288142" title="See in ClinicalTrials.gov">NCT03288142</a>. 
  |  https://jamanetwork.com/journals/jamanetworkopen/fullarticle/10.1001/jamanetworkopen.2020.0255  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32119093/  |  
------------------------------------------- 
10.1212/WNL.0000000000009068  |    Objective:  Genetic diagnosis of muscular dystrophies (MDs) has classically been guided by clinical presentation, muscle biopsy, and muscle MRI data. Muscle MRI suggests diagnosis based on the pattern of muscle fatty replacement. However, patterns overlap between different disorders and knowledge about disease-specific patterns is limited. Our aim was to develop a software-based tool that can recognize muscle MRI patterns and thus aid diagnosis of MDs. 
  Methods:  We collected 976 pelvic and lower limbs T1-weighted muscle MRIs from 10 different MDs. Fatty replacement was quantified using Mercuri score and files containing the numeric data were generated. Random forest supervised machine learning was applied to develop a model useful to identify the correct diagnosis. Two thousand different models were generated and the one with highest accuracy was selected. A new set of 20 MRIs was used to test the accuracy of the model, and the results were compared with diagnoses proposed by 4 specialists in the field. 
  Results:  A total of 976 lower limbs MRIs from 10 different MDs were used. The best model obtained had 95.7% accuracy, with 92.1% sensitivity and 99.4% specificity. When compared with experts on the field, the diagnostic accuracy of the model generated was significantly higher in a new set of 20 MRIs. 
  Conclusion:  Machine learning can help doctors in the diagnosis of muscle dystrophies by analyzing patterns of muscle fatty replacement in muscle MRI. This tool can be helpful in daily clinics and in the interpretation of the results of next-generation sequencing tests. 
  Classification of evidence:  This study provides Class II evidence that a muscle MRI-based artificial intelligence tool accurately diagnoses muscular dystrophies. 
  |  http://www.neurology.org/cgi/pmidlookup?view=long&pmid=32029545  |  
------------------------------------------- 
10.3390/v12030268  |   Migration is associated with HIV-1 vulnerability. 
  Objectives:  To identify long-term trends in HIV-1 molecular epidemiology and antiretroviral drug resistance (ARV) among migrants followed up in Portugal Methods: 5177 patients were included between 2001 and 2017. Rega, Scuel, Comet, and jPHMM algorithms were used for subtyping. Transmitted drug resistance (TDR) and Acquired drug resistance (ADR) were defined as the presence of surveillance drug resistance mutations (SDRMs) and as mutations of the IAS-USA 2015 algorithm, respectively. Statistical analyses were performed. 
  Results:  HIV-1 subtypes infecting migrants were consistent with the ones prevailing in their countries of origin. Over time, overall TDR significantly increased and specifically for Non-nucleoside reverse transcriptase inhibitor (NNRTIs) and Nucleoside reverse transcriptase inhibitor (NRTIs). TDR was higher in patients from Mozambique. Country of origin Mozambique and subtype B were independently associated with TDR. Overall, ADR significantly decreased over time and specifically for NRTIs and Protease Inhibitors (PIs). Age, subtype B, and viral load were independently associated with ADR. 
  Conclusions:  HIV-1 molecular epidemiology in migrants suggests high levels of connectivity with their country of origin. The increasing levels of TDR in migrants could indicate an increase also in their countries of origin, where more efficient surveillance should occur. 
  |  http://www.mdpi.com/resolver?pii=v12030268  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32121161/  |  
------------------------------------------- 
10.1089/omi.2019.0220  |   Precision/personalized medicine is a hot topic in health care. Often presented with the motto "the right drug, for the right patient, at the right dose, and the right time," precision medicine is a theory for rational therapeutics as well as practice to individualize health interventions (e.g., drugs, food, vaccines, medical devices, and exercise programs) using biomarkers. Yet, an alien visitor to planet Earth reading the contemporary textbooks on diagnostics might think precision medicine requires only two biomolecules omnipresent in the literature: nucleic acids (e.g., DNA) and proteins, known as the first and second alphabet of biology, respectively. However, the precision/personalized medicine community has tended to underappreciate the third alphabet of life, the "sugar code" (i.e., the information stored in glycans, glycoproteins, and glycolipids). This article brings together experts in precision/personalized medicine science, pharmacoglycomics, emerging technology governance, cultural studies, contemporary art, and responsible innovation to critically comment on the sociomateriality of the three alphabets of life together. First, the current transformation of targeted therapies with personalized glycomedicine and glycan biomarkers is examined. Next, we discuss the reasons as to why unraveling of the sugar code might have lagged behind the DNA and protein codes. While social scientists have historically noted the importance of constructivism (e.g., how people interpret technology and build their values, hopes, and expectations into emerging technologies), life scientists relied on the material properties of technologies in explaining why some innovations emerge rapidly and are more popular than others. The concept of sociomateriality integrates these two explanations by highlighting the inherent entanglement of the social and the material contributions to knowledge and what is presented to us as reality from everyday laboratory life. Hence, we present a hypothesis based on a sociomaterial conceptual lens: because materiality and synthesis of glycans are not directly driven by a template, and thus more complex and open ended than sequencing of a finite length genome, social construction of expectations from unraveling of the sugar code versus the DNA code might have evolved differently, as being future-uncertain versus future-proof, respectively, thus potentially explaining the "sugar lag" in precision/personalized medicine diagnostics over the past decades. We conclude by introducing systems scientists, physicians, and biotechnology industry to the concept, practice, and value of responsible innovation, while glycomedicine and other emerging biomarker technologies (e.g., metagenomics and pharmacomicrobiomics) transition to applications in health care, ecology, pharmaceutical/diagnostic industries, agriculture, food, and bioengineering, among others. 
  |  https://www.liebertpub.com/doi/full/10.1089/omi.2019.0220?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.annonc.2020.04.003  |    Background:  Preoperative evaluation of the number of lymph node metastasis (LNM) is the basis of individual treatment of locally advanced gastric cancer (LAGC). However, the routinely used preoperative determination method is not accurate enough. 
  Patients and methods:  We enrolled 730 LAGC patients from 5 centers in China and 1 center in Italy, and divided them into 1 primary cohort, 3 external validation cohorts, and 1 international validation cohort. A deep learning radiomic nomogram (DLRN) was built based on the images from multi-phase computed tomography (CT) for preoperatively determining the number of LNM in LAGC. We comprehensively tested the DLRN and compared it with three state-of-the-art methods. Moreover, we investigated the value of the DLRN in survival analysis. 
  Results:  The DLRN showed good discrimination of the number of LNM on all cohorts (overall C-indexes: 0.821, 95% CI: 0.785-0.858 in the primary cohort; 0.797, 95% CI: 0.771-0.823 in the external validation cohorts; and 0.822, 95% CI: 0.756-0.887 in the international validation cohort). The nomogram performed significantly better than the routinely used clinical N stages, tumor size, and clinical model (p&lt;0.05). Besides, DLRN is significantly associated with the overall survival of LAGC patients (n=271). 
  Conclusion:  A deep learning-based radiomic nomogram had good predictive value for LNM in LAGC. In staging-oriented treatment of gastric cancer, this preoperative nomogram could provide baseline information for individual treatment of LAGC. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0923-7534(20)39294-2  |  
------------------------------------------- 
10.1038/s41598-019-56881-2  |   The exposure of germ cells to radiation introduces mutations in the genomes of offspring, and a previous whole-genome sequencing study indicated that the irradiation of mouse sperm induces insertions/deletions (indels) and multisite mutations (clustered single nucleotide variants and indels). However, the current knowledge on the mutation spectra is limited, and the effects of radiation exposure on germ cells at stages other than the sperm stage remain unknown. Here, we performed whole-genome sequencing experiments to investigate the exposure of spermatogonia and mature oocytes. We compared de novo mutations in a total of 24 F1 mice conceived before and after the irradiation of their parents. The results indicated that radiation exposure, 4 Gy of gamma rays, induced 9.6 indels and 2.5 multisite mutations in spermatogonia and 4.7 indels and 3.1 multisite mutations in mature oocytes in the autosomal regions of each F1 individual. Notably, we found two types of deletions, namely, small deletions (mainly 1~12 nucleotides) in non-repeat sequences, many of which showed microhomology at the breakpoint junction, and single-nucleotide deletions in mononucleotide repeat sequences. The results suggest that these deletions and multisite mutations could be a typical signature of mutations induced by parental irradiation in mammals. 
  |  http://dx.doi.org/10.1038/s41598-019-56881-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31913321/  |  
------------------------------------------- 
10.1371/journal.pone.0229819  |   This large, retrospective case-control study of electronic health records from 56 million unique adult patients examined whether or not treatment with a Tumor Necrosis Factor (TNF) blocking agent is associated with lower risk for Alzheimer's disease (AD) in patients with rheumatoid arthritis (RA), psoriasis, and other inflammatory diseases which are mediated in part by TNF and for which a TNF blocker is an approved treatment. The analysis compared the diagnosis of AD as an outcome measure in patients receiving at least one prescription for a TNF blocking agent (etanercept, adalimumab, and infliximab) or for methotrexate. Adjusted odds ratios (AORs) were estimated using the Cochran-Mantel-Haenszel (CMH) method and presented with 95% confidence intervals (CIs) and p-values. RA was associated with a higher risk for AD (Adjusted Odds Ratio (AOR) = 2.06, 95% Confidence Interval: (2.02-2.10), P-value &lt;0.0001) as did psoriasis (AOR = 1.37 (1.31-1.42), P &lt;0.0001), ankylosing spondylitis (AOR = 1.57 (1.39-1.77), P &lt;0.0001), inflammatory bowel disease (AOR = 2.46 (2.33-2.59), P &lt; 0.0001), ulcerative colitis (AOR = 1.82 (1.74-1.91), P &lt;0.0001), and Crohn's disease (AOR = 2.33 (2.22-2.43), P &lt;0.0001). The risk for AD in patients with RA was lower among patients treated with etanercept (AOR = 0.34 (0.25-0.47), P &lt;0.0001), adalimumab (AOR = 0.28 (0.19-0.39), P &lt; 0.0001), or infliximab (AOR = 0.52 (0.39-0.69), P &lt;0.0001). Methotrexate was also associated with a lower risk for AD (AOR = 0.64 (0.61-0.68), P &lt;0.0001), while lower risk was found in patients with a prescription history for both a TNF blocker and methotrexate. Etanercept and adalimumab also were associated with lower risk for AD in patients with psoriasis: AOR = 0.47 (0.30-0.73 and 0.41 (0.20-0.76), respectively. There was no effect of gender or race, while younger patients showed greater benefit from a TNF blocker than did older patients. This study identifies a subset of patients in whom systemic inflammation contributes to risk for AD through a pathological mechanism involving TNF and who therefore may benefit from treatment with a TNF blocking agent. 
  |  http://dx.plos.org/10.1371/journal.pone.0229819  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32203525/  |  
------------------------------------------- 
10.1007/s00330-020-06699-8  |    Objectives:  To take advantage of the deep learning algorithms to detect and calculate clot burden of acute pulmonary embolism (APE) on computed tomographic pulmonary angiography (CTPA). 
  Materials and methods:  The training set in this retrospective study consisted of 590 patients (460 with APE and 130 without APE) who underwent CTPA. A fully deep learning convolutional neural network (DL-CNN), called U-Net, was trained for the segmentation of clot. Additionally, an in-house validation set consisted of 288 patients (186 with APE and 102 without APE). In this study, we set different probability thresholds to test the performance of U-Net for the clot detection and selected sensitivity, specificity, and area under the curve (AUC) as the metrics of performance evaluation. Furthermore, we investigated the relationship between the clot burden assessed by the Qanadli score, Mastora score, and other imaging parameters on CTPA and the clot burden calculated by the DL-CNN model. 
  Results:  There was no statistically significant difference in AUCs with the different probability thresholds. When the probability threshold for segmentation was 0.1, the sensitivity and specificity of U-Net in detecting clot respectively were 94.6% and 76.5% while the AUC was 0.926 (95% CI 0.884-0.968). Moreover, this study displayed that the clot burden measured with U-Net was significantly correlated with the Qanadli score (r = 0.819, p &lt; 0.001), Mastora score (r = 0.874, p &lt; 0.001), and right ventricular functional parameters on CTPA. 
  Conclusions:  DL-CNN achieved a high AUC for the detection of pulmonary emboli and can be applied to quantitatively calculate the clot burden of APE patients, which may contribute to reducing the workloads of clinicians. 
  Key points:  • Deep learning can detect APE with a good performance and efficiently calculate the clot burden to reduce the physicians' workload. • Clot burden measured with deep learning highly correlates with Qanadli and Mastora scores of CTPA. • Clot burden measured with deep learning correlates with parameters of right ventricular function on CTPA. 
  |  https://dx.doi.org/10.1007/s00330-020-06699-8  |  
------------------------------------------- 
10.3390/s20082376  |   This paper presents a more detailed concept of Human-Robot Interaction systems architecture. One of the main differences between the proposed architecture and other ones is the methodology of information acquisition regarding the robot's interlocutor. In order to obtain as much information as possible before the actual interaction took place, a custom Internet-of-Things-based sensor subsystems connected to Smart Infrastructure was designed and implemented, in order to support the interlocutor identification and acquisition of initial interaction parameters. The Artificial Intelligence interaction framework of the developed robotic system (including humanoid Pepper with its sensors and actuators, additional local, remote and cloud computing services) is being extended with the use of custom external subsystems for additional knowledge acquisition: device-based human identification, visual identification and audio-based interlocutor localization subsystems. These subsystems were deeply introduced and evaluated in this paper, presenting the benefits of integrating them into the robotic interaction system. In this paper a more detailed analysis of one of the external subsystems-Bluetooth Human Identification Smart Subsystem-was also included. The idea, use case, and a prototype, integration of elements of Smart Infrastructure systems and the prototype implementation were performed in a small front office of the Weegree company as a decent test-bed application area. 
  |  http://www.mdpi.com/resolver?pii=s20082376  |  
------------------------------------------- 
10.1007/s11571-019-09541-0  |   Many studies reported that ERP-based BCIs can provide communication for some people with amyotrophic lateral sclerosis (ALS). ERP-based BCIs often present characters within a matrix that occupies the center of the visual field. However, several studies have identified some concerns with the matrix-based approach. This approach may lead to fatigue and errors resulting from flashing adjacent stimuli, and is impractical for users who might want to use the BCI in tandem with other software or feedback in the center of the monitor. In this paper, we introduce and validate an alternate ERP-based BCI display approach. By presenting stimuli near the periphery of the display, we reduce the adjacency problem and leave the center of the display available for feedback or other applications. Two ERP-based display approaches were tested on 18 ALS patients to: (1) compare performance between a conventional matrix speller paradigm (Matrix-P, mean visual angle 6°) and a new speller paradigm with peripherally distributed stimuli (Peripheral-P, mean visual angle 8.8°); and (2) assess performance while spelling 42 characters online continuously, without a break. In the Peripheral-P condition, 12 subjects attained higher than 80% feedback accuracy during online performance, and 7 of these subjects obtained higher than 90% accuracy. The experimental results showed that the Peripheral-P condition yielded performance comparable to the conventional Matrix-P condition (<i>p </i>&gt; 0.05) in accuracy and information transfer rate. This paper introduces a new display approach that leaves the center of the monitor open for feedback and/or other display elements, such as movies, games, art, or displays from other AAC software or conventional software tools. 
  |  https://dx.doi.org/10.1007/s11571-019-09541-0  |  
------------------------------------------- 
10.1038/s41467-020-14695-1  |   One key aspect of domain-general thought is the ability to integrate information across different cognitive domains. Here, we tested whether kea (Nestor notabilis) can use relative quantities when predicting sampling outcomes, and then integrate both physical information about the presence of a barrier, and social information about the biased sampling of an experimenter, into their predictions. Our results show that kea exhibit three signatures of statistical inference, and therefore can integrate knowledge across different cognitive domains to flexibly adjust their predictions of sampling events. This result provides evidence that true statistical inference is found outside of the great apes, and that aspects of domain-general thinking can convergently evolve in brains with a highly different structure from primates. This has important implications not only for our understanding of how intelligence evolves, but also for research focused on how to create artificial domain-general thought processes. 
  |  http://dx.doi.org/10.1038/s41467-020-14695-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32127523/  |  
------------------------------------------- 
10.3390/cancers12020379  |   Colorectal cancer treatment has advanced over the past decade. The drug 5-fluorouracil is still used with a wide percentage of patients who do not respond. Therefore, a challenge is the identification of predictive biomarkers. The protein kinase R (PKR also called EIF2AK2) and its regulator, the non-coding pre-mir-nc886, have multiple effects on cells in response to numerous types of stress, including chemotherapy. In this work, we performed an ambispective study with 197 metastatic colon cancer patients with unresectable metastases to determine the relative expression levels of both nc886 and PKR by qPCR, as well as the location of PKR by immunohistochemistry in tumour samples and healthy tissues (plasma and colon epithelium). As primary end point, the expression levels were related to the objective response to first-line chemotherapy following the response evaluation criteria in solid tumours (RECIST) and, as the second end point, with survival at 18 and 36 months. Hierarchical agglomerative clustering was performed to accommodate the heterogeneity and complexity of oncological patients' data. High expression levels of nc886 were related to the response to treatment and allowed to identify clusters of patients. Although the PKR mRNA expression was not associated with chemotherapy response, the absence of PKR location in the nucleolus was correlated with first-line chemotherapy response. Moreover, a relationship between survival and the expression of both PKR and nc886 in healthy tissues was found. Therefore, this work evaluated the best way to analyse the potential biomarkers PKR and nc886 in order to establish clusters of patients depending on the cancer outcomes using algorithms for complex and heterogeneous data. 
  |  http://www.mdpi.com/resolver?pii=cancers12020379  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32045987/  |  
------------------------------------------- 
10.1002/jcla.23054  |    Background:  Centronuclear myopathy (CNM), a subtype of congenital myopathy (CM), is a group of clinical and genetically heterogeneous muscle disorders. Centronuclear myopathy is a kind of disease difficult to diagnose due to its genetic diversity. Since the discovery of the SPEG gene and disease-causing variants, only a few additional patients have been reported. 
  Methods:  A radiograph test, ultrasonic test, and biochemical tests were applied to clinical diagnosis of CNM. We performed trio medical exome sequencing of the family and conservation analysis to identify variants. 
  Results:  We report a pair of severe CNM twins with the same novel homozygous SPEG variant c. 8710A&gt;G (p.Thr2904Ala) identified by clinical trio medical exome sequencing of the family and conservation analysis. The twins showed clinical symptoms of facial weakness, hypotonia, arthrogryposis, strephenopodia, patent ductus arteriosus, and pulmonary arterial hypertension. 
  Conclusions:  Our report expands the clinical and molecular repertoire of CNM and enriches the variant spectrum of the SPEG gene in the Chinese population and helps us further understand the pathogenesis of CNM. 
  |  https://doi.org/10.1002/jcla.23054  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31625632/  |  
------------------------------------------- 
10.3390/nano10040708  |   Transcriptomics data are relevant to address a number of challenges in Toxicogenomics (TGx). After careful planning of exposure conditions and data preprocessing, the TGx data can be used in predictive toxicology, where more advanced modelling techniques are applied. The large volume of molecular profiles produced by omics-based technologies allows the development and application of artificial intelligence (AI) methods in TGx. Indeed, the publicly available omics datasets are constantly increasing together with a plethora of different methods that are made available to facilitate their analysis, interpretation and the generation of accurate and stable predictive models. In this review, we present the state-of-the-art of data modelling applied to transcriptomics data in TGx. We show how the benchmark dose (BMD) analysis can be applied to TGx data. We review read across and adverse outcome pathways (AOP) modelling methodologies. We discuss how network-based approaches can be successfully employed to clarify the mechanism of action (MOA) or specific biomarkers of exposure. We also describe the main AI methodologies applied to TGx data to create predictive classification and regression models and we address current challenges. Finally, we present a short description of deep learning (DL) and data integration methodologies applied in these contexts. Modelling of TGx data represents a valuable tool for more accurate chemical safety assessment. This review is the third part of a three-article series on Transcriptomics in Toxicogenomics. 
  |  http://www.mdpi.com/resolver?pii=nano10040708  |  
------------------------------------------- 
10.1038/s41587-019-0395-5  |   An amendment to this paper has been published and can be accessed via a link at the top of the paper. 
  |  https://dx.doi.org/10.1038/s41587-019-0395-5  |  
------------------------------------------- 
10.3390/s20030885  |   The prevalence of micro-holes is widespread in mechanical, electronic, optical, ornaments, micro-fluidic devices, etc. However, monitoring and detection tool wear and tool breakage are imperative to achieve improved hole quality and high productivity in micro-drilling. The various multi-sensor signals are used to monitor the condition of the tool. In this work, the vibration signals and cutting force signals have been applied individually as well as in combination to determine their effectiveness for tool-condition monitoring applications. Moreover, they have been used to determine the best strategies for tool-condition monitoring by prediction of hole quality during micro-drilling operations with 0.4 mm micro-drills. Furthermore, this work also developed an adaptive neuro fuzzy inference system (ANFIS) model using different time domains and wavelet packet features of these sensor signals for the prediction of the hole quality. The best prediction of hole quality was obtained by a combination of different sensor features in wavelet domain of vibration signal. The model's predicted results were found to exert a good agreement with the experimental results. 
  |  http://www.mdpi.com/resolver?pii=s20030885  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046037/  |  
------------------------------------------- 
10.1016/bs.apcsb.2019.11.013  |   In the era of big data, the interplay of artificial and human intelligence is the demanding job to address the concerns involving exchange of decisions between both sides. Drug discovery is one of the key sources of the big data, which involves synergy among various computational methods to achieve a clinical success. Rightful acquisition, mining and analysis of the data related to ligand and targets are crucial to accomplish reliable outcomes in the entire process. Novel designing and screening tactics are necessary to substantiate a potent and efficient lead compounds. Such methods are emphasized and portrayed in the current review targeting protein-ligand and protein-protein interactions involved in various diseases with potential applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1876-1623(19)30094-X  |  
------------------------------------------- 
10.1111/ejn.14724  |   Astrocytes are key players in the regulation of brain development and function. They sense and respond to the surrounding activity by elevating their intracellular calcium (Ca<sup>2+</sup> ) levels. These astrocytic Ca<sup>2+</sup> elevations emerge from different sources and display complex spatio-temporal properties. Ca<sup>2+</sup> elevations are spatially distributed in global (soma and main processes) and/or focal regions (microdomains). The inositol 1,4,5-trisphosphate receptor type 2 knockout (IP<sub>3</sub> R2 KO) mouse model lacks global Ca<sup>2+</sup> elevations in astrocytes, and it has been used by different laboratories. However, the constitutive deletion of IP<sub>3</sub> R2 during development may trigger compensating phenotypes, which could bias the results of experiments using developing or adult mice. To address this issue, we performed a detailed neurodevelopmental evaluation of male and female IP<sub>3</sub> R2 KO mice, during the first 21 days of life, as well as an evaluation of motor function, strength and neurological reflexes in adult mice. Our results show that male and female IP<sub>3</sub> R2 KO mice display a normal acquisition of developmental milestones, as compared with wild-type (WT) mice. We also show that IP<sub>3</sub> R2 KO mice display normal motor coordination, strength and neurological reflexes in adulthood. To exclude a potential compensatory overexpression of other IP<sub>3</sub> Rs, we quantified the relative mRNA levels of all 3 subtypes, in brain tissue. We found that, along with the complete deletion of Itpr2, there is no compensatory expression of Itpr1 or Itrp3. Overall, our results show that the IP<sub>3</sub> R2 KO mouse is a reliable model to study the functional impact of global IP<sub>3</sub> R2-dependent astrocytic Ca<sup>2+</sup> elevations. 
  |  https://doi.org/10.1111/ejn.14724  |  
------------------------------------------- 
10.1007/s11548-019-02057-2  |    Purpose:  Brain shift during tumor resection can progressively invalidate the accuracy of neuronavigation systems and affect neurosurgeons' ability to achieve optimal resections. This paper compares two methods that have been presented in the literature to compensate for brain shift: a thin-plate spline deformation model and a finite element method (FEM). For this comparison, both methods are driven by identical sparse data. Specifically, both methods are driven by displacements between automatically detected and matched feature points from intraoperative 3D ultrasound (iUS). Both methods have been shown to be fast enough for intraoperative brain shift correction (Machado et al. in Int J Comput Assist Radiol Surg 13(10):1525-1538, 2018; Luo et al. in J Med Imaging (Bellingham) 4(3):035003, 2017). However, the spline method requires no preprocessing and ignores physical properties of the brain while the FEM method requires significant preprocessing and incorporates patient-specific physical and geometric constraints. The goal of this work was to explore the relative merits of these methods on recent clinical data. 
  Methods:  Data acquired during 19 sequential tumor resections in Brigham and Women's Hospital's Advanced Multi-modal Image-Guided Operating Suite between December 2017 and October 2018 were considered for this retrospective study. Of these, 15 cases and a total of 24 iUS to iUS image pairs met inclusion requirements. Automatic feature detection (Machado et al. in Int J Comput Assist Radiol Surg 13(10):1525-1538, 2018) was used to detect and match features in each pair of iUS images. Displacements between matched features were then used to drive both the spline model and the FEM method to compensate for brain shift between image acquisitions. The accuracies of the resultant deformation models were measured by comparing the displacements of manually identified landmarks before and after deformation. 
  Results:  The mean initial subcortical registration error between preoperative MRI and the first iUS image averaged 5.3 ± 0.75 mm. The mean subcortical brain shift, measured using displacements between manually identified landmarks in pairs of iUS images, was 2.5 ± 1.3 mm. Our results showed that FEM was able to reduce subcortical registration error by a small but statistically significant amount (from 2.46 to 2.02 mm). A large variability in the results of the spline method prevented us from demonstrating either a statistically significant reduction in subcortical registration error after applying the spline method or a statistically significant difference between the results of the two methods. 
  Conclusions:  In this study, we observed less subcortical brain shift than has previously been reported in the literature (Frisken et al., in: Miller (ed) Biomechanics of the brain, Springer, Cham, 2019). This may be due to the fact that we separated out the initial misregistration between preoperative MRI and the first iUS image from our brain shift measurements or it may be due to modern neurosurgical practices designed to reduce brain shift, including reduced craniotomy sizes and better control of intracranial pressure with the use of mannitol and other medications. It appears that the FEM method and its use of geometric and biomechanical constraints provided more consistent brain shift correction and better correction farther from the driving feature displacements than the simple spline model. The spline-based method was simpler and tended to give better results for small deformations. However, large variability in the spline results and relatively small brain shift prevented this study from demonstrating a statistically significant difference between the results of the two methods. 
  |  https://dx.doi.org/10.1007/s11548-019-02057-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31444624/  |  
------------------------------------------- 
10.1126/science.aaz4439  |   Venus has a thick atmosphere that rotates 60 times as fast as the surface, a phenomenon known as super-rotation. We use data obtained from the orbiting Akatsuki spacecraft to investigate how the super-rotation is maintained in the cloud layer, where the rotation speed is highest. A thermally induced latitudinal-vertical circulation acts to homogenize the distribution of the angular momentum around the rotational axis. Maintaining the super-rotation requires this to be counteracted by atmospheric waves and turbulence. Among those effects, thermal tides transport the angular momentum, which maintains the rotation peak, near the cloud top at low latitudes. Other planetary-scale waves and large-scale turbulence act in the opposite direction. We suggest that hydrodynamic instabilities adjust the angular-momentum distribution at mid-latitudes. 
  |  http://www.sciencemag.org/cgi/pmidlookup?view=long&pmid=32327594  |  
------------------------------------------- 
10.1111/aas.13527  |    Background:  Mortality prediction models are applied in the intensive care unit (ICU) to stratify patients into different risk categories and to facilitate benchmarking. To ensure that the correct prediction models are applied for these purposes, the best performing models must be identified. As a first step, we aimed to establish a systematic review of mortality prediction models in critically ill patients. 
  Methods:  Mortality prediction models were searched in four databases using the following criteria: developed for use in adult ICU patients in high-income countries, with mortality as primary or secondary outcome. Characteristics and performance measures of the models were summarized. Performance was presented in terms of discrimination, calibration and overall performance measures presented in the original publication. 
  Results:  In total, 43 mortality prediction models were included in the final analysis. In all, 15 models were only internally validated (35%), 13 externally (30%) and 10 (23%) were both internally and externally validated by the original researchers. Discrimination was assessed in 42 models (98%). Commonly used calibration measures were the Hosmer-Lemeshow test (60%) and the calibration plot (28%). Calibration was not assessed in 11 models (26%). Overall performance was assessed in the Brier score (19%) and the Nagelkerke's R<sup>2</sup> (4.7%). 
  Conclusions:  Mortality prediction models have varying methodology, and validation and performance of individual models differ. External validation by the original researchers is often lacking and head-to-head comparisons are urgently needed to identify the best performing mortality prediction models for guiding clinical care and research in different settings and populations. 
  |  https://doi.org/10.1111/aas.13527  |  
------------------------------------------- 
10.1177/1073110520916994  |   Health care is transitioning from genetics to genomics, in which single-gene testing for diagnosis is being replaced by multi-gene panels, genome-wide sequencing, and other multi-genic tests for disease diagnosis, prediction, prognosis, and treatment. This health care transition is spurring a new set of increased or novel liability risks for health care providers and test laboratories. This article describes this transition in both medical care and liability, and addresses 11 areas of potential increased or novel liability risk, offering recommendations to both health care and legal actors to address and manage those liability risks. 
  |  http://journals.sagepub.com/doi/full/10.1177/1073110520916994?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1001/jamanetworkopen.2020.2142  |    Importance:  Studies have shown that adverse events are associated with increasing inpatient care expenditures, but contemporary data on the association between expenditures and adverse events beyond inpatient care are limited. 
  Objective:  To evaluate whether hospital-specific adverse event rates are associated with hospital-specific risk-standardized 30-day episode-of-care Medicare expenditures for fee-for-service patients discharged with acute myocardial infarction (AMI), heart failure (HF), or pneumonia. 
  Design, setting, and participants:  This cross-sectional study used the 2011 to 2016 hospital-specific risk-standardized 30-day episode-of-care expenditure data from the Centers for Medicare &amp; Medicaid Services and medical record-abstracted in-hospital adverse event data from the Medicare Patient Safety Monitoring System. The setting was acute care hospitals treating at least 25 Medicare fee-for-service patients for AMI, HF, or pneumonia in the United States. Participants were Medicare fee-for-service patients 65 years or older hospitalized for AMI, HF, or pneumonia included in the Medicare Patient Safety Monitoring System in 2011 to 2016. The dates of analysis were July 16, 2017, to May 21, 2018. 
  Main outcomes and measures:  Hospitals' risk-standardized 30-day episode-of-care expenditures and the rate of occurrence of adverse events for which patients were at risk. 
  Results:  The final study sample from 2194 unique hospitals included 44 807 patients (26.1% AMI, 35.6% HF, and 38.3% pneumonia) with a mean (SD) age of 79.4 (8.6) years, and 52.0% were women. The patients represented 84 766 exposures for AMI, 96 917 exposures for HF, and 109 641 exposures for pneumonia. Patient characteristics varied by condition but not by expenditure category. The mean (SD) risk-standardized expenditures were $22 985 ($1579) for AMI, $16 020 ($1416) for HF, and $16 355 ($1995) for pneumonia per hospitalization. The mean risk-standardized rates of occurrence of adverse events for which patients were at risk were 3.5% (95% CI, 3.4%-3.6%) for AMI, 2.5% (95% CI, 2.5%-2.5%) for HF, and 3.0% (95% CI, 2.9%-3.0%) for pneumonia. An increase by 1 percentage point in the rate of occurrence of adverse events was associated with an increase in risk-standardized expenditures of $103 (95% CI, $57-$150) for AMI, $100 (95% CI, $29-$172) for HF, and $152 (95% CI, $73-$232) for pneumonia per discharge. 
  Conclusions and relevance:  Hospitals with high adverse event rates were more likely to have high 30-day episode-of-care Medicare expenditures for patients discharged with AMI, HF, or pneumonia. 
  |  https://jamanetwork.com/journals/jamanetworkopen/fullarticle/10.1001/jamanetworkopen.2020.2142  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32259263/  |  
------------------------------------------- 
10.1007/s00415-019-09613-5  |    Objective:  Posterior circulation ischemic stroke (PCiS) constitutes 20-30% of ischemic stroke cases. Detailed information about differences between PCiS and anterior circulation ischemic stroke (ACiS) remains scarce. Such information might guide clinical decision making and prevention strategies. We studied risk factors and ischemic stroke subtypes in PCiS vs. ACiS and lesion location on magnetic resonance imaging (MRI) in PCiS. 
  Methods:  Out of 3,301 MRIs from 12 sites in the National Institute of Neurological Disorders and Stroke (NINDS) Stroke Genetics Network (SiGN), we included 2,381 cases with acute DWI lesions. The definition of ACiS or PCiS was based on lesion location. We compared the groups using Chi-squared and logistic regression. 
  Results:  PCiS occurred in 718 (30%) patients and ACiS in 1663 (70%). Diabetes and male sex were more common in PCiS vs. ACiS (diabetes 27% vs. 23%, p &lt; 0.05; male sex 68% vs. 58%, p &lt; 0.001). Both were independently associated with PCiS (diabetes, OR = 1.29; 95% CI 1.04-1.61; male sex, OR = 1.46; 95% CI 1.21-1.78). ACiS more commonly had large artery atherosclerosis (25% vs. 20%, p &lt; 0.01) and cardioembolic mechanisms (17% vs. 11%, p &lt; 0.001) compared to PCiS. Small artery occlusion was more common in PCiS vs. ACiS (20% vs. 14%, p &lt; 0.001). Small artery occlusion accounted for 47% of solitary brainstem infarctions. 
  Conclusion:  Ischemic stroke subtypes differ between the two phenotypes. Diabetes and male sex have a stronger association with PCiS than ACiS. Definitive MRI-based PCiS diagnosis aids etiological investigation and contributes additional insights into specific risk factors and mechanisms of injury in PCiS. 
  |  https://dx.doi.org/10.1007/s00415-019-09613-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31709475/  |  
------------------------------------------- 
10.1186/s12872-020-01455-8  |    Background:  Chest pain is one of the most common complaints among patients presenting to the emergency department (ED). Causes of chest pain can be benign or life threatening, making accurate risk stratification a critical issue in the ED. In addition to the use of established clinical scores, prior studies have attempted to create predictive models with heart rate variability (HRV). In this study, we proposed heart rate n-variability (HRnV), an alternative representation of beat-to-beat variation in electrocardiogram (ECG), and investigated its association with major adverse cardiac events (MACE) in ED patients with chest pain. 
  Methods:  We conducted a retrospective analysis of data collected from the ED of a tertiary hospital in Singapore between September 2010 and July 2015. Patients &gt; 20 years old who presented to the ED with chief complaint of chest pain were conveniently recruited. Five to six-minute single-lead ECGs, demographics, medical history, troponin, and other required variables were collected. We developed the HRnV-Calc software to calculate HRnV parameters. The primary outcome was 30-day MACE, which included all-cause death, acute myocardial infarction, and revascularization. Univariable and multivariable logistic regression analyses were conducted to investigate the association between individual risk factors and the outcome. Receiver operating characteristic (ROC) analysis was performed to compare the HRnV model (based on leave-one-out cross-validation) against other clinical scores in predicting 30-day MACE. 
  Results:  A total of 795 patients were included in the analysis, of which 247 (31%) had MACE within 30 days. The MACE group was older, with a higher proportion being male patients. Twenty-one conventional HRV and 115 HRnV parameters were calculated. In univariable analysis, eleven HRV and 48 HRnV parameters were significantly associated with 30-day MACE. The multivariable stepwise logistic regression identified 16 predictors that were strongly associated with MACE outcome; these predictors consisted of one HRV, seven HRnV parameters, troponin, ST segment changes, and several other factors. The HRnV model outperformed several clinical scores in the ROC analysis. 
  Conclusions:  The novel HRnV representation demonstrated its value of augmenting HRV and traditional risk factors in designing a robust risk stratification tool for patients with chest pain in the ED. 
  |  https://bmccardiovascdisord.biomedcentral.com/articles/10.1186/s12872-020-01455-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32276602/  |  
------------------------------------------- 
10.1039/d0cs00098a  |   Prediction of chemical bioactivity and physical properties has been one of the most important applications of statistical and more recently, machine learning and artificial intelligence methods in chemical sciences. This field of research, broadly known as quantitative structure-activity relationships (QSAR) modeling, has developed many important algorithms and has found a broad range of applications in physical organic and medicinal chemistry in the past 55+ years. This Perspective summarizes recent technological advances in QSAR modeling but it also highlights the applicability of algorithms, modeling methods, and validation practices developed in QSAR to a wide range of research areas outside of traditional QSAR boundaries including synthesis planning, nanotechnology, materials science, biomaterials, and clinical informatics. As modern research methods generate rapidly increasing amounts of data, the knowledge of robust data-driven modelling methods professed within the QSAR field can become essential for scientists working both within and outside of chemical research. We hope that this contribution highlighting the generalizable components of QSAR modeling will serve to address this challenge. 
  |  https://doi.org/10.1039/d0cs00098a  |  
------------------------------------------- 
10.7554/eLife.52951  |   Clones of excitatory neurons derived from a common progenitor have been proposed to serve as elementary information processing modules in the neocortex. To characterize the cell types and circuit diagram of clonally related excitatory neurons, we performed multi-cell patch clamp recordings and Patch-seq on neurons derived from <i>Nestin</i>-positive progenitors labeled by tamoxifen induction at embryonic day 10.5. The resulting clones are derived from two radial glia on average, span cortical layers 2-6, and are composed of a random sampling of transcriptomic cell types. We find an interaction between shared lineage and connection type: related neurons are more likely to be connected vertically across cortical layers, but not laterally within the same layer. These findings challenge the view that related neurons show uniformly increased connectivity and suggest that integration of vertical <i>intra</i>-clonal input with lateral <i>inter</i>-clonal input may represent a developmentally programmed connectivity motif supporting the emergence of functional circuits. 
  |  https://doi.org/10.7554/eLife.52951  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32134385/  |  
------------------------------------------- 
10.1038/s41587-019-0364-z  |   Tumor DNA sequencing data can be interpreted by computational methods that analyze genomic heterogeneity to infer evolutionary dynamics. A growing number of studies have used these approaches to link cancer evolution with clinical progression and response to therapy. Although the inference of tumor phylogenies is rapidly becoming standard practice in cancer genome analyses, standards for evaluating them are lacking. To address this need, we systematically assess methods for reconstructing tumor subclonality. First, we elucidate the main algorithmic problems in subclonal reconstruction and develop quantitative metrics for evaluating them. Then we simulate realistic tumor genomes that harbor all known clonal and subclonal mutation types and processes. Finally, we benchmark 580 tumor reconstructions, varying tumor read depth, tumor type and somatic variant detection. Our analysis provides a baseline for the establishment of gold-standard methods to analyze tumor heterogeneity. 
  |  https://dx.doi.org/10.1038/s41587-019-0364-z  |  
------------------------------------------- 
10.1016/j.media.2019.101561  |   Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on "Diabetic Retinopathy - Segmentation and Grading" was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(19)30103-3  |  
------------------------------------------- 
10.1148/radiol.2020201491  |   Background COVID-19 and pneumonia of other etiology share similar CT characteristics, contributing to the challenges in differentiating them with high accuracy. Purpose To establish and evaluate an artificial intelligence (AI) system in differentiating COVID-19 and other pneumonia on chest CT and assess radiologist performance without and with AI assistance. Methods 521 patients with positive RT-PCR for COVID-19 and abnormal chest CT findings were retrospectively identified from ten hospitals from January 2020 to April 2020. 665 patients with non-COVID-19 pneumonia and definite evidence of pneumonia on chest CT were retrospectively selected from three hospitals between 2017 and 2019. To classify COVID-19 versus other pneumonia for each patient, abnormal CT slices were input into the EfficientNet B4 deep neural network architecture after lung segmentation, followed by two-layer fully-connected neural network to pool slices together. Our final cohort of 1,186 patients (132,583 CT slices) was divided into training, validation and test sets in a 7:2:1 and equal ratio. Independent testing was performed by evaluating model performance on separate hospitals. Studies were blindly reviewed by six radiologists without and then with AI assistance. Results Our final model achieved a test accuracy of 96% (95% CI: 90-98%), sensitivity 95% (95% CI: 83-100%) and specificity of 96% (95% CI: 88-99%) with Receiver Operating Characteristic (ROC) AUC of 0.95 and Precision-Recall (PR) AUC of 0.90. On independent testing, our model achieved an accuracy of 87% (95% CI: 82-90%), sensitivity of 89% (95% CI: 81-94%) and specificity of 86% (95% CI: 80-90%) with ROC AUC of 0.90 and PR AUC of 0.87. Assisted by the models' probabilities, the radiologists achieved a higher average test accuracy (90% vs. 85%, Δ=5, p&lt;0.001), sensitivity (88% vs. 79%, Δ=9, p&lt;0.001) and specificity (91% vs. 88%, Δ=3, p=0.001). Conclusion AI assistance improved radiologists' performance in distinguishing COVID-19 from non-COVID-19 pneumonia on chest CT. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020201491?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2196/15022  |    Background:  Alternative evidence-based cardiac rehabilitation (CR) delivery models that overcome significant barriers to access and delivery are needed to address persistent low utilization. Models utilizing contemporary digital technologies could significantly improve reach and fidelity as complementary alternatives to traditional center-based programs. 
  Objective:  The aim of this study is to compare the effects and costs of the innovative Smartphone Cardiac Rehabilitation, Assisted self-Management (SCRAM) intervention with usual care CR. 
  Methods:  In this investigator-, assessor-, and statistician-blinded parallel 2-arm randomized controlled trial, 220 adults (18+ years) with coronary heart disease are being recruited from 3 hospitals in metropolitan and regional Victoria, Australia. Participants are randomized (1:1) to receive advice to engage with usual care CR or the SCRAM intervention. SCRAM is a 24-week dual-phase intervention that includes 12 weeks of real-time remote exercise supervision and coaching from exercise physiologists, which is followed by 12 weeks of data-driven nonreal-time remote coaching via telephone. Both intervention phases include evidence- and theory-based multifactorial behavior change support delivered via smartphone push notifications. Outcomes assessed at baseline, 12 weeks, and 24 weeks include maximal aerobic exercise capacity (primary outcome at 24 weeks), modifiable cardiovascular risk factors, exercise adherence, secondary prevention self-management behaviors, health-related quality of life, and adverse events. Economic and process evaluations will determine cost-effectiveness and participant perceptions of the treatment arms, respectively. 
  Results:  The trial was funded in November 2017 and received ethical approval in June 2018. Recruitment began in November 2018. As of September 2019, 54 participants have been randomized into the trial. 
  Conclusions:  The innovative multiphase SCRAM intervention delivers real-time remote exercise supervision and evidence-based self-management behavioral support to participants, regardless of their geographic proximity to traditional center-based CR facilities. Our trial will provide unique and valuable information about effects of SCRAM on outcomes associated with cardiac and all-cause mortality, as well as acceptability and cost-effectiveness. These findings will be important to inform health care providers about the potential for innovative program delivery models, such as SCRAM, to be implemented at scale, as a complement to existing CR programs. The inclusion of a cohort comprising metropolitan-, regional-, and rural-dwelling participants will help to understand the role of this delivery model across health care contexts with diverse needs. 
  Trial registration:  Australian New Zealand Clinical Trials Registry (ACTRN): 12618001458224; anzctr.org.au/Trial/Registration/TrialReview.aspx?id=374508. 
  International registered report identifier (irrid):  DERR1-10.2196/15022. 
  |  https://www.researchprotocols.org/2020/1/e15022/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012103/  |  
------------------------------------------- 
10.1038/s10038-020-0733-y  |   Recently, a recessively inherited intronic repeat expansion in replication factor C1 (RFC1) was identified in cerebellar ataxia with neuropathy and bilateral vestibular areflexia syndrome (CANVAS). Here, we describe a Japanese case of genetically confirmed CANVAS with autonomic failure and auditory hallucination. The case showed impaired uptake of iodine-123-metaiodobenzylguanidine and <sup>123</sup>I-ioflupane in the cardiac sympathetic nerve and dopaminergic neurons, respectively, by single-photon emission computed tomography. Long-read sequencing identified biallelic pathogenic (AAGGG)n nucleotide repeat expansion in RFC1 and heterozygous benign (TAAAA)n and (TAGAA)n expansions in brain expressed, associated with NEDD4 (BEAN1). Enrichment of the repeat regions in RFC1 and BEAN1 using a Cas9-mediated system clearly distinguished between pathogenic and benign repeat expansions. The haplotype around RFC1 indicated that the (AAGGG)n expansion in our case was on the same ancestral allele as that of European cases. Thus, long-read sequencing facilitates precise genetic diagnosis of diseases with complex repeat structures and various expansions. 
  |  http://dx.doi.org/10.1038/s10038-020-0733-y  |  
------------------------------------------- 
10.1002/hbm.25023  |   Alzheimer's disease (AD) is associated with disruptions in brain activity and networks. However, there is substantial inconsistency among studies that have investigated functional brain alterations in AD; such contradictions have hindered efforts to elucidate the core disease mechanisms. In this study, we aim to comprehensively characterize AD-associated functional brain alterations using one of the world's largest resting-state functional MRI (fMRI) biobank for the disorder. The biobank includes fMRI data from six neuroimaging centers, with a total of 252 AD patients, 221 mild cognitive impairment (MCI) patients and 215 healthy comparison individuals. Meta-analytic techniques were used to unveil reliable differences in brain function among the three groups. Relative to the healthy comparison group, AD was associated with significantly reduced functional connectivity and local activity in the default-mode network, basal ganglia and cingulate gyrus, along with increased connectivity or local activity in the prefrontal lobe and hippocampus (p &lt; .05, Bonferroni corrected). Moreover, these functional alterations were significantly correlated with the degree of cognitive impairment (AD and MCI groups) and amyloid-β burden. Machine learning models were trained to recognize key fMRI features to predict individual diagnostic status and clinical score. Leave-one-site-out cross-validation established that diagnostic status (mean area under the receiver operating characteristic curve: 0.85) and clinical score (mean correlation coefficient between predicted and actual Mini-Mental State Examination scores: 0.56, p &lt; .0001) could be predicted with high accuracy. Collectively, our findings highlight the potential for a reproducible and generalizable functional brain imaging biomarker to aid the early diagnosis of AD and track its progression. 
  |  None  |  
------------------------------------------- 
10.1038/s41598-020-59661-5  |   With the advent of personalized medicine, there is a movement to develop "smaller" and "smarter" microdevices that are able to distinguish similar cancer subtypes. Tumor cells display major differences when compared to their natural counterparts, due to alterations in fundamental cellular processes such as glycosylation. Glycans are involved in tumor cell biology and they have been considered to be suitable cancer biomarkers. Thus, more selective cancer screening assays can be developed through the detection of specific altered glycans on the surface of circulating cancer cells. Currently, this is only possible through time-consuming assays. In this work, we propose the "intelligent" Lab on Fiber (iLoF) device, that has a high-resolution, and which is a fast and portable method for tumor single-cell type identification and isolation. We apply an Artificial Intelligence approach to the back-scattered signal arising from a trapped cell by a micro-lensed optical fiber. As a proof of concept, we show that iLoF is able to discriminate two human cancer cell models sharing the same genetic background but displaying a different surface glycosylation profile with an accuracy above 90% and a speed rate of 2.3 seconds. We envision the incorporation of the iLoF in an easy-to-operate microchip for cancer identification, which would allow further biological characterization of the captured circulating live cells. 
  |  http://dx.doi.org/10.1038/s41598-020-59661-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32081911/  |  
------------------------------------------- 
10.1056/NEJMoa1917130  |    Background:  Nonophthalmologist physicians do not confidently perform direct ophthalmoscopy. The use of artificial intelligence to detect papilledema and other optic-disk abnormalities from fundus photographs has not been well studied. 
  Methods:  We trained, validated, and externally tested a deep-learning system to classify optic disks as being normal or having papilledema or other abnormalities from 15,846 retrospectively collected ocular fundus photographs that had been obtained with pharmacologic pupillary dilation and various digital cameras in persons from multiple ethnic populations. Of these photographs, 14,341 from 19 sites in 11 countries were used for training and validation, and 1505 photographs from 5 other sites were used for external testing. Performance at classifying the optic-disk appearance was evaluated by calculating the area under the receiver-operating-characteristic curve (AUC), sensitivity, and specificity, as compared with a reference standard of clinical diagnoses by neuro-ophthalmologists. 
  Results:  The training and validation data sets from 6779 patients included 14,341 photographs: 9156 of normal disks, 2148 of disks with papilledema, and 3037 of disks with other abnormalities. The percentage classified as being normal ranged across sites from 9.8 to 100%; the percentage classified as having papilledema ranged across sites from zero to 59.5%. In the validation set, the system discriminated disks with papilledema from normal disks and disks with nonpapilledema abnormalities with an AUC of 0.99 (95% confidence interval [CI], 0.98 to 0.99) and normal from abnormal disks with an AUC of 0.99 (95% CI, 0.99 to 0.99). In the external-testing data set of 1505 photographs, the system had an AUC for the detection of papilledema of 0.96 (95% CI, 0.95 to 0.97), a sensitivity of 96.4% (95% CI, 93.9 to 98.3), and a specificity of 84.7% (95% CI, 82.3 to 87.1). 
  Conclusions:  A deep-learning system using fundus photographs with pharmacologically dilated pupils differentiated among optic disks with papilledema, normal disks, and disks with nonpapilledema abnormalities. (Funded by the Singapore National Medical Research Council and the SingHealth Duke-NUS Ophthalmology and Visual Sciences Academic Clinical Program.). 
  |  http://www.nejm.org/doi/full/10.1056/NEJMoa1917130?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41746-020-0266-y  |   Pulmonary embolism (PE) is a life-threatening clinical problem and computed tomography pulmonary angiography (CTPA) is the gold standard for diagnosis. Prompt diagnosis and immediate treatment are critical to avoid high morbidity and mortality rates, yet PE remains among the diagnoses most frequently missed or delayed. In this study, we developed a deep learning model-PENet, to automatically detect PE on volumetric CTPA scans as an end-to-end solution for this purpose. The PENet is a 77-layer 3D convolutional neural network (CNN) pretrained on the Kinetics-600 dataset and fine-tuned on a retrospective CTPA dataset collected from a single academic institution. The PENet model performance was evaluated in detecting PE on data from two different institutions: one as a hold-out dataset from the same institution as the training data and a second collected from an external institution to evaluate model generalizability to an unrelated population dataset. PENet achieved an AUROC of 0.84 [0.82-0.87] on detecting PE on the hold out internal test set and 0.85 [0.81-0.88] on external dataset. PENet also outperformed current state-of-the-art 3D CNN models. The results represent successful application of an end-to-end 3D CNN model for the complex task of PE diagnosis without requiring computationally intensive and time consuming preprocessing and demonstrates sustained performance on data from an external institution. Our model could be applied as a triage tool to automatically identify clinically important PEs allowing for prioritization for diagnostic radiology interpretation and improved care pathways via more efficient diagnosis. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32352039/  |  
------------------------------------------- 
10.1016/j.cmpb.2019.105153  |    Background and objectives:  Malignant lymphomas are cancers of the immune system and are characterized by enlarged lymph nodes that typically spread across many different sites. Many different histological subtypes exist, whose diagnosis is typically based on sampling (biopsy) of a single tumor site, whereas total body examinations with computed tomography and positron emission tomography, though not diagnostic, are able to provide a comprehensive picture of the patient. In this work, we exploit a data-driven approach based on multiple-instance learning algorithms and texture analysis features extracted from positron emission tomography, to predict differential diagnosis of the main malignant lymphomas subtypes. 
  Methods:  We exploit a multiple-instance learning setting where support vector machines and random forests are used as classifiers both at the level of single VOIs (instances) and at the level of patients (bags). We present results on two datasets comprising patients that suffer from four different types of malignant lymphomas, namely diffuse large B cell lymphoma, follicular lymphoma, Hodgkin's lymphoma, and mantle cell lymphoma. 
  Results:  Despite the complexity of the task, experimental results show that, with sufficient data samples, some cancer subtypes, such as the Hodgkin's lymphoma, can be identified from texture information: in particular, we achieve a 97.0% of sensitivity (recall) and a 94.1% of predictive positive value (precision) on a dataset that consists in 60 patients. 
  Conclusions:  The presented study indicates that texture analysis features extracted from positron emission tomography, combined with multiple-instance machine learning algorithms, can be discriminating for different malignant lymphomas subtypes. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)30205-6  |  
------------------------------------------- 
10.1002/advs.201902699  |   Terahertz (THz) photon detection is of particular appealing for myriad applications, but it still lags behind efficient manipulation with electronics and photonics due to the lack of a suitable principle satisfying both high sensitivity and fast response at room temperature. Here, a new strategy is proposed to overcome these limitations by exploring the photothermoelectric (PTE) effect in an ultrashort (down to 30 nm) channel with black phosphorus as a photoactive material. The preferential flow of hot carriers is enabled by the asymmetric Cr/Au and Ti/Au metallization with the titled-angle evaporation technique. Most intriguingly, orders of magnitude field-enhancement beyond the skin-depth limit and photon absorption across a broadband frequency can be achieved. The PTE detector has excellent sensitivity of 297 V W<sup>-1</sup>, noise equivalent power less than 58 pW/Hz<sup>0.5</sup>, and response time below 0.8 ms, which is superior to other thermal-based detectors at room temperature. A rigorous comparison with existing THz detectors, together with verification by further optical-pumping and imaging experiments, substantiates the importance of the localized field effect in the skin-depth limit. The results allow solid understanding on the role of PTE effect played in the THz photoresponse, opening up new opportunities for developing highly sensitive THz detectors for addressing targeted applications. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32154074/  |  
------------------------------------------- 
10.1002/cti2.1105  |    Objectives:  T cells play an essential role in controlling the development of B-cell lymphoproliferative disorders (BLPDs), but the dysfunction of T cells in BLPDs largely remains elusive. 
  Methods:  Using multiplexed flow cytometry, we quantified all major subsets of CD4<sup>+</sup> helper T cells (Th) and CD8<sup>+</sup> cytotoxic T cells (Tc) in 94 BLPD patients and 66 healthy controls. Statistics was utilised to rank T-cell signatures that distinguished BLPDs from healthy controls and differentially presented between indolent and aggressive categories. 
  Results:  By comparing with healthy controls, we found that the indolent but not aggressive type of BLPDs demonstrated a high degree of T-cell activation, showing the increase in type I helper T (Th1) cells and follicular B-helper T (Tfh) cells, both of which strongly associated with the enhanced differentiation of exhaustion-like effector cytotoxic CD8<sup>+</sup> T cells expressing PD-1 (Tc exhaustion-like) in indolent BLPDs. Random forest modelling selected a module of T-cell immune signatures best performing binary classification of all BLPD patients. This signature module was composed of low naïve Th cells and high Th1, Tfh and Tc exhaustion-like cells which efficiently identified &gt; 85% indolent cases and was, therefore, assigned as the Indolent Dominant Module of T-cell immune signature. In indolent BLPD patients, a strong bias towards such signatures was found to associate with clinical characteristics of worse prognosis. 
  Conclusion:  Our study identified a prominent signature of T-cell dysregulation specifically for indolent BLPDs, suggesting Th1, Tfh and Tc exhaustion-like cells represent potential prognostic biomarkers and targets for immunotherapies. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31993200/  |  
------------------------------------------- 
10.3988/jcn.2020.16.2.202  |    Background and purpose:  Mild cognitive impairment (MCI) is a condition with diverse clinical outcomes and subgroups. Here we investigated the topographic distribution of tau in vivo using the positron emission tomography (PET) tracer [¹⁸F]THK5351 in MCI subgroups. 
  Methods:  This study included 96 participants comprising 38 with amnestic MCI (aMCI), 21 with nonamnestic MCI (naMCI), and 37 with normal cognition (NC) who underwent 3.0-T MRI, [¹⁸F]THK5351 PET, and detailed neuropsychological tests. [¹⁸F]flutemetamol PET was also performed in 62 participants. The aMCI patients were further divided into three groups: 1) verbal-aMCI, only verbal memory impairment; 2) visual-aMCI, only visual memory impairment; and 3) both-aMCI, both visual and verbal memory impairment. Voxel-wise statistical analysis and region-of-interest -based analyses were performed to evaluate the retention of [¹⁸F]THK5351 in the MCI subgroups. Subgroup analysis of amyloid-positive and -negative MCI patients was also performed. Correlations between [¹⁸F]THK5351 retention and different neuropsychological tests were evaluated using statistical parametric mapping analyses. 
  Results:  [¹⁸F]THK5351 retention in the lateral temporal, mesial temporal, parietal, frontal, posterior cingulate cortices and precuneus was significantly greater in aMCI patients than in NC subjects, whereas it did not differ significantly between naMCI and NC participants. [¹⁸F] THK5351 retention was greater in the both-aMCI group than in the verbal-aMCI and visualaMCI groups, and greater in amyloid-positive than amyloid-negative MCI patients. The cognitive function scores were significantly correlated with cortical [¹⁸F]THK5351 retention. 
  Conclusions:  [¹⁸F]THK5351 PET might be useful for identifying distinct topographic patterns of [¹⁸F]THK5351 retention in subgroups of MCI patients who are at greater risk of the progression to Alzheimer's dementia. 
  |  https://thejcn.com/DOIx.php?id=10.3988/jcn.2020.16.2.202  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32319236/  |  
------------------------------------------- 
10.1210/clinem/dgz141  |    Context:  Urine steroid metabolomics, combining mass spectrometry-based steroid profiling and machine learning, has been described as a novel diagnostic tool for detection of adrenocortical carcinoma (ACC). 
  Objective, design, setting:  This proof-of-concept study evaluated the performance of urine steroid metabolomics as a tool for postoperative recurrence detection after microscopically complete (R0) resection of ACC. 
  Patients and methods:  135 patients from 14 clinical centers provided postoperative urine samples, which were analyzed by gas chromatography-mass spectrometry. We assessed the utility of these urine steroid profiles in detecting ACC recurrence, either when interpreted by expert clinicians or when analyzed by random forest, a machine learning-based classifier. Radiological recurrence detection served as the reference standard. 
  Results:  Imaging detected recurrent disease in 42 of 135 patients; 32 had provided pre- and post-recurrence urine samples. 39 patients remained disease-free for ≥3 years. The urine "steroid fingerprint" at recurrence resembled that observed before R0 resection in the majority of cases. Review of longitudinally collected urine steroid profiles by 3 blinded experts detected recurrence by the time of radiological diagnosis in 50% to 72% of cases, improving to 69% to 92%, if a preoperative urine steroid result was available. Recurrence detection by steroid profiling preceded detection by imaging by more than 2 months in 22% to 39% of patients. Specificities varied considerably, ranging from 61% to 97%. The computational classifier detected ACC recurrence with superior accuracy (sensitivity = specificity = 81%). 
  Conclusion:  Urine steroid metabolomics is a promising tool for postoperative recurrence detection in ACC; availability of a preoperative urine considerably improves the ability to detect ACC recurrence. 
  |  https://academic.oup.com/jcem/article-lookup/doi/10.1210/clinem/dgz141  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31665449/  |  
------------------------------------------- 
10.1016/j.media.2020.101652  |   Detection of early stages of Alzheimer's disease (AD) (i.e., mild cognitive impairment (MCI)) is important to maximize the chances to delay or prevent progression to AD. Brain connectivity networks inferred from medical imaging data have been commonly used to distinguish MCI patients from normal controls (NC). However, existing methods still suffer from limited performance, and classification remains mainly based on single modality data. This paper proposes a new model to automatically diagnosing MCI (early MCI (EMCI) and late MCI (LMCI)) and its earlier stages (i.e., significant memory concern (SMC)) by combining low-rank self-calibrated functional brain networks and structural brain networks for joint multi-task learning. Specifically, we first develop a new functional brain network estimation method. We introduce data quality indicators for self-calibration, which can improve data quality while completing brain network estimation, and perform correlation analysis combined with low-rank structure. Second, functional and structural connected neuroimaging patterns are integrated into our multi-task learning model to select discriminative and informative features for fine MCI analysis. Different modalities are best suited to undertake distinct classification tasks, and similarities and differences among multiple tasks are best determined through joint learning to determine most discriminative features. The learning process is completed by non-convex regularizer, which effectively reduces the penalty bias of trace norm and approximates the original rank minimization problem. Finally, the most relevant disease features classified using a support vector machine (SVM) for MCI identification. Experimental results show that our method achieves promising performance with high classification accuracy and can effectively discriminate between different sub-stages of MCI. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(20)30019-0  |  
------------------------------------------- 
10.1016/j.jcmg.2019.06.027  |    Objectives:  This study was conducted to investigate the influence of coronary artery calcium (CAC) score on the diagnostic performance of machine-learning-based coronary computed tomography (CT) angiography (cCTA)-derived fractional flow reserve (CT-FFR). 
  Background:  CT-FFR is used reliably to detect lesion-specific ischemia. Novel CT-FFR algorithms using machine-learning artificial intelligence techniques perform fast and require less complex computational fluid dynamics. Yet, influence of CAC score on diagnostic performance of the machine-learning approach has not been investigated. 
  Methods:  A total of 482 vessels from 314 patients (age 62.3 ± 9.3 years, 77% male) who underwent cCTA followed by invasive FFR were investigated from the MACHINE (Machine Learning based CT Angiography derived FFR: a Multi-center Registry) registry data. CAC scores were quantified using the Agatston convention. The diagnostic performance of CT-FFR to detect lesion-specific ischemia was assessed across all Agatston score categories (CAC 0, &gt;0 to &lt;100, 100 to &lt;400, and ≥400) on a per-vessel level with invasive FFR as the reference standard. 
  Results:  The diagnostic accuracy of CT-FFR versus invasive FFR was superior to cCTA alone on a per-vessel level (78% vs. 60%) and per patient level (83% vs. 73%) across all Agatston score categories. No statistically significant differences in the diagnostic accuracy, sensitivity, or specificity of CT-FFR were observed across the categories. CT-FFR showed good discriminatory power in vessels with high Agatston scores (CAC ≥400) and high performance in low-to-intermediate Agatston scores (CAC &gt;0 to &lt;400) with a statistically significant difference in the area under the receiver-operating characteristic curve (AUC) (AUC: 0.71 [95% confidence interval (CI): 0.57 to 0.85] vs. 0.85 [95% CI: 0.82 to 0.89], p = 0.04). CT-FFR showed superior diagnostic value over cCTA in vessels with high Agatston scores (CAC ≥ 400: AUC 0.71 vs. 0.55, p = 0.04) and low-to-intermediate Agatston scores (CAC &gt;0 to &lt;400: AUC 0.86 vs. 0.63, p &lt; 0.001). 
  Conclusions:  Machine-learning-based CT-FFR showed superior diagnostic performance over cCTA alone in CAC with a significant difference in the performance of CT-FFR as calcium burden/Agatston calcium score increased. (Machine Learning Based CT Angiography Derived FFR: a Multicenter, Registry [MACHINE] <a href="http://clinicaltrials.gov/show/NCT02805621" title="See in ClinicalTrials.gov">NCT02805621</a>). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1936-878X(19)30635-7  |  
------------------------------------------- 
10.1186/s13041-020-00587-4  |    Aim:  A hallmark of classical conditioning is that conditioned stimulus (CS) must be tightly coupled with unconditioned stimulus (US), often requiring temporal overlap between the two, or a short gap of several seconds. In this study, we investigate the temporal requirements for fear conditioning association between a strong artificial CS, high-frequency optogenetic activation of inputs into the lateral amygdala of rats, and a foot-shock to the animal with delays up to many minutes. 
  Methods:  AAV-oChIEF-tdTomato viruses were injected into the auditory cortex and the medial geniculate nucleus of rats. An optical fiber was implanted just above the lateral amygdala of the animal. Optogenetic high-frequency stimuli (oHFS; containing five 1-s trains of 100 Hz laser pulses) were delivered to the lateral amygdala, before or after (with varying intervals) a foot-shock that elicits fear responses in the animal. Pre-trained lever-press behavior was used to assess the degree of fear recall by optogenetic test stimuli (OTS; 10 Hz for 2 min) 24 h after the association experiment. 
  Results:  In contrast to the tight temporal requirement for classical conditioning with paired optogenetic moderate-frequency stimuli (oMFS; 10 Hz for 20 s) and foot-shock, oHFS followed by foot-shock with a 5-min or even 1-h (but not 3-h) interval could successfully establish an association to be recalled by OTS the next day. Meanwhile, foot-shock followed by oHFS with a 5-min (but not 1-h) interval could also establish the conditioning. Thus, distant association may be formed between temporally distant stimuli when the CS is strong. 
  |  https://molecularbrain.biomedcentral.com/articles/10.1186/s13041-020-00587-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32197621/  |  
------------------------------------------- 
PMID:32060190  |    Objective:  To contrast how Brazil's and Canada's different jurisdictional and judicial realities have led to different types of telemedicine and how further scale and improvement can be achieved. 
  Composition of the committee:  A subgroup of the Besrour Centre of the College of Family Physicians of Canada and Canadian telemedicine experts developed connections with colleagues in Porto Alegre, Brazil, and collaborated to undertake a between-country comparison of their respective telemedicine programs. 
  Methods:  Following a literature review, the authors collectively reflected on their experiences in an attempt to explore the past and current state of telemedicine in Canada and Brazil. 
  Report:  Both Brazil and Canada share expansive geographies, creating substantial barriers to health for rural patients. Telemedicine is an important part of a universal health system. Both countries have achieved telemedicine programs that have scaled up across large regions and are showing important effects on health care costs and outcomes. However, each system is unique in design and implementation and faces unique challenges for further scale and improvement. Addressing regional differences, the normalization of telemedicine, and potential alignment of telemedicine and artificial intelligence technologies for health care are seen as promising approaches to scaling up and improving telemedicine in both countries. 
  Objectif:  Comparer la manière dont les différentes réalités territoriales et judiciaires du Brésil et du Canada ont mené à différents types de télémédecine et déterminer comment l’expansion à plus grande échelle, ainsi que des améliorations peuvent être réalisées. 
  Composition du comité:  Un sous-groupe du Centre Besrour du Collège des médecins de famille du Canada et des experts canadiens en télémédecine ont établi des liens avec des collègues de Porto Alegre, au Brésil, et ont collaboré pour entreprendre une comparaison entre les programmes de télémédecine des deux pays. 
  Méthodes:  Après une revue de la documentation, les auteurs ont fait une réflexion collective sur leurs expériences afin d’explorer l’état passé et actuel de la télémédecine au Canada et au Brésil. 
  Rapport:  Le Brésil et le Canada couvrent tous deux de vastes territoires géographiques, ce qui crée des obstacles importants à la santé des patients des zones rurales. La télémédecine est un élément important d’un système de santé universel. Les deux pays ont mis en place des programmes de télémédecine qui s’étendent sur de vastes régions et qui ont des effets importants sur les coûts des soins et la santé des populations. Toutefois, chaque système est unique dans sa conception et sa mise en œuvre et se heurte à des difficultés particulières qui entravent son expansion. La prise en compte des différences régionales, la normalisation de la télémédecine et l’harmonisation potentielle des technologies de télémédecine et d’intelligence artificielle pour les soins de santé sont considérées comme des approches prometteuses pour le développement et l’amélioration de la télémédecine dans ces deux pays. 
  |  http://www.cfp.ca/cgi/pmidlookup?view=long&pmid=32060190  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32060190/  |  
------------------------------------------- 
10.1155/2020/3658795  |   Recently, brain-machine interfacing is very popular that link humans and artificial devices through brain signals which lead to corresponding mobile application as supplementary. The Android platform has developed rapidly because of its good user experience and openness. Meanwhile, these characteristics of this platform, which cause the amazing pace of Android malware, pose a great threat to this platform and data correction during signal transmission of brain-machine interfacing. Many previous works employ various behavioral characteristics to analyze Android application (or app) and detect Android malware to protect signal data secure. However, with the development of Android app, category of Android app tends to be diverse, and the Android malware behavior tends to be complex. This situation makes existing Android malware detections complicated and inefficient. In this paper, we propose a broad analysis, gathering as many behavior characteristics of an app as possible and compare these behavior characteristics in several metrics. First, we extract static and dynamic behavioral characteristic from Android app in an automatic manner. Second, we explain the decision we made in each kind of behavioral characteristic we choose for Android app analysis and Android malware detection. Third, we design a detailed experiment, which compare the efficiency of each kind of behavior characteristic in different aspects. The results of experiment also show Android malware detection performance of these behavior characteristics combine with well-known machine learning algorithms. 
  |  https://doi.org/10.1155/2020/3658795  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32300372/  |  
------------------------------------------- 
10.1007/s10439-019-02332-y  |   Robots in orthopedic surgery have been developed rapidly for decades and bring significant benefits to the patients and healthcare providers. However, robotics in fracture reduction remains at the infant stage. As essential components of the current robotic system, external fixators were used in fracture reduction, including the unilateral and Ilizarov-like ring fixators. With emerging of the industrial robots and mechanical arms, their sterilized variants were developed as the serial robots, including the traction device and robotic arm, for fracture reduction. Besides, parallel robots (e.g., Gough-Stewart platform) were devised for lower extremity traction and fracture reduction. After combining the advantages of the serial and parallel mechanisms, hybrid robots can fulfill specific clinical requirements (e.g., the joint fracture, including multiple major fragments). Furthermore, with the aid of intra-operative navigation systems, fracture reduction can be performed under real-time guidance. The paper presents a comprehensive overview of the advancement of the robots in fracture reduction and evaluates research challenges and future perspectives, including ergonomic and economic issues, operation time, artificial realities and intelligence, and telesurgery. 
  |  https://doi.org/10.1007/s10439-019-02332-y  |  
------------------------------------------- 
10.1021/acs.jcim.9b01030  |   Farnesoid X receptor (FXR) agonists can reverse dysregulated bile acid metabolism, and thus, they are potential therapeutics to prevent and treat nonalcoholic fatty liver disease. The low success rate of FXR agonists' R&amp;D and the side effects of clinical candidates such as obeticholic acid make it urgent to discover new chemotypes. Unfortunately, structure-based virtual screening (SBVS) that can speed up drug discovery has rarely been reported with success for FXR, which was likely hindered by the failure in addressing protein flexibility. To address this issue, we devised human FXR (hFXR)-specific ensemble learning models based on pose filters from 24 agonist-bound hFXR crystal structures and coupled them to traditional SBVS approaches of the FRED docking plus Chemgauss4 scoring function. It turned out that the hFXR-specific pose filter ensemble (PFE) was able to improve ligand enrichment significantly, which rendered 3RUT-based SBVS with its PFE the ideal approach for FXR agonist discovery. By screening of the Specs chemical library and in vitro FXR transactivation bioassay, we identified a new class of FXR agonists with compound XJ034 as the representative, which would have been missed if the PFE was not coupled. Following that, we performed in-depth biological studies which demonstrated that XJ034 resulted in a downtrend of intracellular triglyceride in vitro, significantly decreased the serum/liver TG in high fat diet-induced C57BL/6J obese mice, and more importantly, showed metabolic stabilities in both plasma and liver microsomes. To provide insight into further structure-based lead optimization, we solved the crystal structure of hFXR complexed with compound XJ034, uncovering a unique hydrogen bond between compound XJ034 and residue Y375. The current work highlights the power of our pose filter-based ensemble learning approach in terms of scaffold hopping and provides a promising lead compound for further development. 
  |  https://dx.doi.org/10.1021/acs.jcim.9b01030  |  
------------------------------------------- 
10.1038/s41467-020-14649-7  |   Recent studies suggest that attention samples space rhythmically through oscillatory interactions in the frontoparietal network. How these attentional fluctuations coincide with spatial exploration/displacement and exploitation/selection by a dynamic attentional spotlight under top-down control is unclear. Here, we show a direct contribution of prefrontal attention selection mechanisms to a continuous space exploration. Specifically, we provide a direct high spatio-temporal resolution prefrontal population decoding of the covert attentional spotlight. We show that it continuously explores space at a 7-12 Hz rhythm. Sensory encoding and behavioral reports are increased at a specific optimal phase w/ to this rhythm. We propose that this prefrontal neuronal rhythm reflects an alpha-clocked sampling of the visual environment in the absence of eye movements. These attentional explorations are highly flexible, how they spatially unfold depending both on within-trial and across-task contingencies. These results are discussed in the context of exploration-exploitation strategies and prefrontal top-down attentional control. 
  |  http://dx.doi.org/10.1038/s41467-020-14649-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32066740/  |  
------------------------------------------- 
10.15252/emmm.201911622  |   Chemotherapy still constitutes the standard of care for the treatment of most neoplastic diseases. Certain chemotherapeutics from the oncological armamentarium are able to trigger pre-mortem stress signals that lead to immunogenic cell death (ICD), thus inducing an antitumor immune response and mediating long-term tumor growth reduction. Here, we used an established model, built on artificial intelligence to identify, among a library of 50,000 compounds, anticancer agents that, based on their molecular descriptors, were predicted to induce ICD. This algorithm led us to the identification of dactinomycin (DACT, best known as actinomycin D), a highly potent cytotoxicant and ICD inducer that mediates immune-dependent anticancer effects in vivo. Since DACT is commonly used as an inhibitor of DNA to RNA transcription, we investigated whether other experimentally established or algorithm-selected, clinically employed ICD inducers would share this characteristic. As a common leitmotif, a panel of pharmacological ICD stimulators inhibited transcription and secondarily translation. These results establish the inhibition of RNA synthesis as an initial event for ICD induction. 
  |  https://doi.org/10.15252/emmm.201911622  |  
------------------------------------------- 
10.1055/a-1111-2431  |   This review is intended to present the latest developments in the prevention and treatment of early breast cancer. The risk of breast cancer can be increasingly better characterised with large epidemiological studies on genetic and non-genetic risk factors. Through new analyses, the evidence for high-penetrance genes as well as for low-penetrance genes was able to be improved. New data on denosumab and atezolizumab are available in the neoadjuvant situation as is a pooled appraisal of numerous studies on capecitabine in the curative situation. There is also an update to the overall survival data of pertuzumab in the adjuvant situation with a longer follow-up observation period. Finally, digital medicine is steadily finding its way into science. A recently conducted study on automated breast cancer detection using artificial intelligence establishes the basis for a future review in clinical studies. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/a-1111-2431  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32139917/  |  
------------------------------------------- 
10.1053/j.gastro.2020.02.036  |    Background &amp; aims:  Narrow-band imaging (NBI) can be used to determine whether colorectal polyps are adenomatous or hyperplastic. We investigated whether an artificial intelligence (AI) system can increase the accuracy of characterizations of polyps by endoscopists of different skill levels. 
  Methods:  We developed convolutional neural networks (CNNs) for evaluation of diminutive colorectal polyps, based on efficient neural architecture searches via parameter sharing with augmentation using narrow-band images of diminutive (≤5 mm) polyps, collected from October 2015 through October 2017 at the Seoul National University Hospital, Healthcare System Gangnam Center (training set). We trained the CNN using images from 1100 adenomatous polyps and 1050 hyperplastic polyps from 1379 patients. We then tested the system using 300 images of 180 adenomatous polyps and 120 hyperplastic polyps, obtained from January 2018 to May 2019. We compared the accuracy of 22 endoscopists of different skill levels (7 novices, 4 experts, and 11 NBI-trained experts) vs the CNN in evaluation of images (adenomatous vs hyperplastic) from 180 adenomatous and 120 hyperplastic polyps. The endoscopists then evaluated the polyp images with knowledge of the CNN-processed results. We conducted mixed-effect logistic and linear regression analyses to determine the effects of AI assistance on the accuracy of analysis of diminutive colorectal polyps by endoscopists (primary outcome). 
  Results:  The CNN distinguished adenomatous vs hyperplastic diminutive polyps with 86.7% accuracy, based on histologic analysis as the reference standard. Endoscopists distinguished adenomatous vs hyperplastic diminutive polyps with 82.5% overall accuracy (novices, 73.8% accuracy; experts, 83.8% accuracy; and NBI-trained experts, 87.6% accuracy). With knowledge of the CNN-processed results, the overall accuracy of the endoscopists increased to 88.5% (P&lt;.05). With knowledge of the CNN-processed results, the accuracy of novice endoscopists increased to 85.6% (P&lt;.05). The CNN-processed results significantly reduced endoscopist time of diagnosis (from 3.92 to 3.37 seconds per polyp, P=.042). 
  Conclusions:  We developed a CNN that significantly increases the accuracy of evaluation of diminutive colorectal polyps (as adenomatous vs hyperplastic) and reduces the time of diagnosis by endoscopists. This AI assistance system significantly increased the accuracy of analysis by novice endoscopists, who achieved near-expert levels of accuracy without extra training. The CNN assistance system can reduce the skill-level dependence of endoscopists and costs. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5085(20)30263-8  |  
------------------------------------------- 
10.1016/j.ajo.2020.04.003  |    Purpose:  o investigate the association between retinal microstructure and cone- and rod-function in geographic atrophy (GA) secondary to age-related macular degeneration (AMD) using artificial-intelligence-(AI) algorithms. Design; Prospective, observational case series METHODS: Forty-one eyes of 41 patients (75.8±8.4 years; 22 female) from a tertiary referral hospital were included (Directional-Spread-in-Geographic-Atrophy (DSGA) natural history study; <a href="http://clinicaltrials.gov/show/NCT02051998" title="See in ClinicalTrials.gov">NCT02051998</a>). Mesopic, dark-adapted (DA) cyan and red sensitivity were assessed using fundus-controlled perimetry ("microperimetry"); retinal microstructure using spectral-domain optical-coherence-tomography (SD-OCT), fundus autofluorescence (FAF) and near-infrared-reflectance (IR) imaging. Layer-thicknesses and -intensities and FAF- and IR-intensities were extracted for each test-point. We evaluated the cross-validated mean absolute error (MAE) for random-forest-based predictions of retinal sensitivity with and without patient-specific training-data and the increase mean-squared error (%IncMSE) as measure of feature-importance. 
  Results:  Retinal sensitivity was predicted with a MAE of 4.64 dB for mesopic, 4.89 dB for DA cyan and 4.40 dB for DA red testing in absence of patient-specific data. Partial addition of patient-specific sensitivity data to the training sets decreased the MAE to 2.89 dB, 2.86 dB and 2.77 dB. For all three types of testing, the outer nuclear layer-thickness constituted the most important predictive feature (35.0, 42.22 and 53.74 %IncMSE). Spatially-resolved mapping of "inferred sensitivity" revealed regions with differential degrees of mesopic and DA cyan sensitivity loss outside of the GA lesions. 
  Conclusions:  "Inferred sensitivity" accurately reflected retinal function in patients with GA. Mapping of "inferred sensitivity" could facilitate monitoring of disease progression and serve as "quasi functional" surrogate outcome in clinical trials, especially in consideration of retinal regions beyond areas of GA. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9394(20)30170-7  |  
------------------------------------------- 
10.1016/j.cmpb.2019.105059  |    Background and objective:  With the rapid development of medical imaging and intelligent diagnosis, artificial intelligence methods have become a research hotspot of radiography processing technology in recent years. The low definition of knee magnetic resonance image texture seriously affects the diagnosis of knee osteoarthritis. This paper presents a super-resolution reconstruction method to address this problem. 
  Methods:  In this paper, we propose an efficient medical image super-resolution (EMISR) method, in which we mainly adopted three hidden layers of super-resolution convolution neural network (SRCNN) and a sub-pixel convolution layer of efficient sub-pixel convolution neural network (ESPCN). The addition of the efficient sub-pixel convolutional layer in the hidden layer and the small network replacement consisting of concatenated convolutions to address low-resolution images but not high-resolution images are important. The EMISR method also uses cascaded small convolution kernels to improve reconstruction speed and deepen the convolution neural network to improve reconstruction quality. 
  Results:  The proposed method is tested in the public dataset IDI, and the reconstruction quality of the algorithm is higher than that of the sparse coding-based network (SCN) method, the SRCNN method, and the ESPCN method (+ 2.306 dB, + 2.540 dB, + 1.089 dB improved); moreover, the reconstruction speed is faster than its counterparts (+ 4.272 s, + 1.967 s, and + 0.073 s improved). 
  Conclusion:  The experimental results show that our EMISR framework has improved performance and greatly reduces the number of parameters and training time. Furthermore, the reconstructed image presents more details, and the edges are more complete. Therefore, the EMISR technique provides a more powerful medical analysis in knee osteoarthritis examinations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31241-6  |  
------------------------------------------- 
10.1002/hbm.24856  |   Thalamic atrophy is a common feature across all forms of FTD but little is known about specific nuclei involvement. We aimed to investigate in vivo atrophy of the thalamic nuclei across the FTD spectrum. A cohort of 402 FTD patients (age: mean(SD) 64.3(8.2) years; disease duration: 4.8(2.8) years) was compared with 104 age-matched controls (age: 62.5(10.4) years), using an automated segmentation of T1-weighted MRIs to extract volumes of 14 thalamic nuclei. Stratification was performed by clinical diagnosis (180 behavioural variant FTD (bvFTD), 85 semantic variant primary progressive aphasia (svPPA), 114 nonfluent variant PPA (nfvPPA), 15 PPA not otherwise specified (PPA-NOS), and 8 with associated motor neurone disease (FTD-MND), genetic diagnosis (27 MAPT, 28 C9orf72, 18 GRN), and pathological confirmation (37 tauopathy, 38 TDP-43opathy, 4 FUSopathy). The mediodorsal nucleus (MD) was the only nucleus affected in all FTD subgroups (16-33% smaller than controls). The laterodorsal nucleus was also particularly affected in genetic cases (28-38%), TDP-43 type A (47%), tau-CBD (44%), and FTD-MND (53%). The pulvinar was affected only in the C9orf72 group (16%). Both the lateral and medial geniculate nuclei were also affected in the genetic cases (10-20%), particularly the LGN in C9orf72 expansion carriers. Use of individual thalamic nuclei volumes provided higher accuracy in discriminating between FTD groups than the whole thalamic volume. The MD is the only structure affected across all FTD groups. Differential involvement of the thalamic nuclei among FTD forms is seen, with a unique pattern of atrophy in the pulvinar in C9orf72 expansion carriers. 
  |  https://doi.org/10.1002/hbm.24856  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.137320  |   Predictive capability of landslide susceptibilities is assumed to be varied with different sampling techniques, such as (a) the landslide scarp centroid, (b) centroid of landslide body, (c) samples of the scrap region representing the scarp polygon, and (d) samples of the landslide body representing the entire landslide body. However, new advancements in statistical and machine learning algorithms continuously being updated the landslide susceptibility paradigm. This paper explores the predictive performance power of different sampling techniques in landslide susceptibility mapping in the wake of increased usage of artificial intelligence. We used logistic regression (LR), neural network (NNET), and deep learning neural network (DNN) model for testing and validation of the models. The tests were applied to the 2018 Hokkaido Earthquake affected areas using a set of 11 predictor variables (seismic, topographic, and hydrological). We found that the prediction rates are inconsequential with the DNN model irrespective of the sampling technique (AUC: 0.904 - 0.919). Whereas, testing with LR (AUC: 0.825 - 0.785) and NNET (AUC: 0.882 - 0.858) produces larger differences in the accuracies between the four datasets. Nonetheless, the highest success rates were obtained for samples within the landslide scarp area. The analogy was then validated with a published landslide inventory from the 2015 Gorkha earthquake. We, therefore, suggest that DNN models as an appropriate technique to increase the predictive performance of landslide susceptibilities if the landslide scarp and body are not characterized properly in an inventory. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)30830-5  |  
------------------------------------------- 
10.1002/adma.202000497  |   Bioinspired elastomeric fibrillar surfaces have significant potential as reversible dry adhesives, but their adhesion performance is sensitive to the presence of liquids at the contact interface. Like their models in nature, many artificial mimics can effectively repel water, but fail when low-surface-tension liquids are introduced at the contact interface. A bioinspired fibrillar adhesive surface that is liquid-superrepellent even toward ultralow-surface-tension liquids while retaining its adhesive properties is proposed herein. This surface combines the effective adhesion principle of mushroom-shaped fibrillar arrays with liquid repellency based on double re-entrant fibril tip geometry. The adhesion performance of the proposed microfibril structures is retained even when low-surface-tension liquids are added to the contact interface. The extreme liquid repellency enables real-world applications of fibrillar adhesives for surfaces covered with water, oil, and other liquids. Moreover, fully elastomeric liquid-superrepellent surfaces are mechanically not brittle, highly robust against physical contact, and highly deformable and stretchable, which can increase the real-world uses of such antiwetting surfaces. 
  |  https://doi.org/10.1002/adma.202000497  |  
------------------------------------------- 
10.1038/s41598-020-59541-y  |   Modern society characterized by a 24/7 lifestyle leads to misalignment between environmental cycles and endogenous circadian rhythms. Persisting circadian misalignment leads to deleterious effects on health and healthspan. However, the underlying mechanism remains not fully understood. Here, we subjected adult, wild-type mice to distinct chronic jet-lag paradigms, which showed that long-term circadian misalignment induced significant early mortality. Non-biased RNA sequencing analysis using liver and kidney showed marked activation of gene regulatory pathways associated with the immune system and immune disease in both organs. In accordance, we observed enhanced steatohepatitis with infiltration of inflammatory cells. The investigation of senescence-associated immune cell subsets from the spleens and mesenteric lymph nodes revealed an increase in PD-1<sup>+</sup>CD44<sup>high</sup> CD4 T cells as well as CD95<sup>+</sup>GL7<sup>+</sup> germinal center B cells, indicating that the long-term circadian misalignment exacerbates immune senescence and consequent chronic inflammation. Our results underscore immune homeostasis as a pivotal interventional target against clock-related disorders. 
  |  http://dx.doi.org/10.1038/s41598-020-59541-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32054990/  |  
------------------------------------------- 
10.2174/0929867327666200408115817  |   Background：Hydrogel has a three-dimensional network structure that is able to absorb large amount of water/liquid and maintain its original structure. Hemicellulose (HC) is the second most abundant polysaccharide after cellulose in plants and a heterogeneous polysaccharide consisting of various saccharide units. The unique physical and chemical properties of hemicellulose make it a promising material for hydrogels. 
  Methods:  This review first summarizes the three research hotspots on the hemicellulose-based hydrogels: intelligence, biodegradability and biocompatibility. Overviews the progress in the fabrication and applications of hemicellulose hydrogels in drug delivery system and tissue engineering (articular cartilage, cell immobilization, and wound dressing). 
  Results:  Hemicellulose-based hydrogels have many unique properties, such as stimuli-responsibility, biodegradability and biocompatibility. Interpenetrating networking can endow appropriate mechanical properties to hydrogels. These properties make the hemicellulose-based hydrogels promising materials in biomedical applications such as drug delivery system and tissue engineering (articular cartilage, cell immobilization, and wound dressing). 
  Conclusion:  Hydrogels have been widely used in biomedicine and tissue engineering areas, such as tissue fillers, drug release agents, enzyme encapsulation, protein electrophoresis, contact lenses, artificial plasma, artificial skin, and tissue engineering scaffold materials. This article reviews the recent progress in the fabrication and applications of hemicellulose-based hydrogels in the biomedical field. 
  |  http://www.eurekaselect.com/180789/article  |  
------------------------------------------- 
10.1002/mp.13986  |    Purpose:  Beam orientation selection, whether manual or protocol-based, is the current clinical standard in radiation therapy treatment planning, but it is tedious and can yield suboptimal results. Many algorithms have been designed to optimize beam orientation selection because of its impact on treatment plan quality, but these algorithms suffer from slow calculation of the dose influence matrices of all candidate beams. We propose a fast beam orientation selection method, based on deep learning neural networks (DNN), capable of developing a plan comparable to those developed by the state-of-the-art column generation (CG) method. Our model's novelty lies in its supervised learning structure (using CG to teach the network), DNN architecture, and ability to learn from anatomical features to predict dosimetrically suitable beam orientations without using dosimetric information from the candidate beams. This may save hours of computation. 
  Methods:  A supervised DNN is trained to mimic the CG algorithm, which iteratively chooses beam orientations one-by-one by calculating beam fitness values based on Karush-Kush-Tucker optimality conditions at each iteration. The DNN learns to predict these values. The dataset contains 70 prostate cancer patients - 50 training, 7 validation, and 13 test patients - to develop and test the model. Each patient's data contains 6 contours: PTV, body, bladder, rectum, and left and right femoral heads. Column generation was implemented with a GPU-based Chambolle-Pock algorithm, a first-order primal-dual proximal-class algorithm, to create 6270 plans. The DNN trained over 400 epochs, each with 2500 steps and a batch size of 1, using the Adam optimizer at a learning rate of 1 × 10<sup>-5</sup> and a sixfold cross-validation technique. 
  Results:  The average and standard deviation of training, validation, and testing loss functions among the six folds were 0.62 ± 0.09%, 1.04 ± 0.06%, and 1.44 ± 0.11%, respectively. Using CG and supervised DNN, we generated two sets of plans for each scenario in the test set. The proposed method took at most 1.5 s to select a set of five beam orientations and 300 s to calculate the dose influence matrices for 5 beams and finally 20 s to solve the fluence map optimization (FMO). However, CG needed around 15 h to calculate the dose influence matrices of all beams and at least 400 s to solve both the beam orientation selection and FMO problems. The differences in the dose coverage of PTV between plans generated by CG and by DNN were 0.2%. The average dose differences received by organs at risk were between 1 and 6 percent: Bladder had the smallest average difference in dose received (0.956 ± 1.184%), then Rectum (2.44 ± 2.11%), Left Femoral Head (6.03 ± 5.86%), and Right Femoral Head (5.885 ± 5.515%). The dose received by Body had an average difference of 0.10 ± 0.1% between the generated treatment plans. 
  Conclusions:  We developed a fast beam orientation selection method based on a DNN that selects beam orientations in seconds and is therefore suitable for clinical routines. In the training phase of the proposed method, the model learns the suitable beam orientations based on patients' anatomical features and omits time intensive calculations of dose influence matrices for all possible candidate beams. Solving the FMO to get the final treatment plan requires calculating dose influence matrices only for the selected beams. 
  |  https://doi.org/10.1002/mp.13986  |  
------------------------------------------- 
10.1186/s41199-020-0047-y  |   For many years, head and neck squamous cell carcinoma (HNSCC) has been considered as a single entity. However, in the last decades HNSCC complexity and heterogeneity have been recognized. In parallel, high-throughput <i>omics</i> techniques had allowed picturing a larger spectrum of the behavior and characteristics of molecules in cancer and a large set of omics web-based tools and informative repository databases have been developed. The objective of the present review is to provide an overview on biological, prognostic and predictive molecular signatures in HNSCC. To contextualize the selected data, our literature survey includes a short summary of the main characteristics of omics data repositories and web-tools for data analyses. The timeframe of our analysis was fixed, encompassing papers published between January 2015 and January 2019. From more than 1000 papers evaluated, 61 omics studies were selected: 33 investigating mRNA signatures, 11 and 13 related to miRNA and other non-coding-RNA signatures and 4 analyzing DNA methylation signatures. More than half of identified signatures (36) had a prognostic value but only in 10 studies selection of a specific anatomical sub-site (8 oral cavity, 1 oropharynx and 1 both oral cavity and oropharynx) was performed. Noteworthy, although the sample size included in many studies was limited, about one-half of the retrieved studies reported an external validation on independent dataset(s), strengthening the relevance of the obtained data. Finally, we highlighted the development and exploitation of three gene-expression signatures, whose clinical impact on prognosis/prediction of treatment response could be high. Based on this overview on omics<i>-related</i> literature in HNSCC, we identified some limits and strengths. The major limits are represented by the low number of signatures associated to DNA methylation and to non-coding RNA (miRNA, lncRNA and piRNAs) and the availability of a single dataset with multiple omics on more than 500 HNSCC (i.e. TCGA). The major strengths rely on the integration of multiple datasets through meta-analysis approaches and on the growing integration among <i>omics</i> data obtained on the same cohort of patients. Moreover, new approaches based on artificial intelligence and informatic analyses are expected to be available in the next future. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31988797/  |  
------------------------------------------- 
10.3171/2020.1.FOCUS19914  |    Objective:  The semiology of cingulate gyrus epilepsy is varied and may involve the paracentral area, the adjacent limbic system, and/or the orbitofrontal gyrus. Invasive electroencephalography (iEEG) recording is usually required for patients with deeply located epileptogenic foci. This paper reports on the authors' experiences in the diagnosis and surgical treatment of patients with focal epilepsy originating in the cingulate gyrus. 
  Methods:  Eighteen patients (median age 24 years, range 5-53 years) with a mean seizure history of 23 years (range 2-32 years) were analyzed retrospectively. The results of presurgical evaluation, surgical strategy, and postoperative pathology are reported, as well as follow-up concerning functional morbidity and seizures (median follow-up 7 years, range 2-12 years). 
  Results:  Patients with cingulate gyrus epilepsy presented with a variety of semiologies and scalp EEG patterns. Prior to ictal onset, 11 (61%) of the patients presented with aura. Initial ictal symptoms included limb posturing in 12 (67%), vocalization in 5, and hypermotor movement in 4. In most patients (n = 16, 89%), ictal EEG presented as widespread patterns with bilateral hemispheric origin, as well as muscle artifacts obscuring the onset of EEG during the ictal period in 11 patients. Among the 18 patients who underwent resection, the pathology revealed mild malformation of cortical development in 2, focal cortical dysplasia (FCD) Ib in 4, FCD IIa in 4, FCD IIb in 4, astrocytoma in 1, ganglioglioma in 1, and gliosis in 2. The seizure outcome after surgery was satisfactory: Engel class IA in 12 patients, IIB in 3, IIIA in 1, IIIB in 1, and IVB in 1 at the 2-year follow-up. 
  Conclusions:  In this study, the authors exploited the improved access to the cingulate epileptogenic network made possible by the use of 3D electrodes implanted using stereoelectroencephalography methodology. Under iEEG recording and intraoperative neuromonitoring, epilepsy surgery on lesions in the cingulate gyrus can result in good outcomes in terms of seizure recurrence and the incidence of postoperative permanent deficits. 
  |  https://thejns.org/doi/10.3171/2020.1.FOCUS19914  |  
------------------------------------------- 
10.1007/s00330-019-06652-4  |    Purpose:  This study aimed to validate a deep learning model's diagnostic performance in using computed tomography (CT) to diagnose cervical lymph node metastasis (LNM) from thyroid cancer in a large clinical cohort and to evaluate the model's clinical utility for resident training. 
  Methods:  The performance of eight deep learning models was validated using 3838 axial CT images from 698 consecutive patients with thyroid cancer who underwent preoperative CT imaging between January and August 2018 (3606 and 232 images from benign and malignant lymph nodes, respectively). Six trainees viewed the same patient images (n = 242), and their diagnostic performance and confidence level (5-point scale) were assessed before and after computer-aided diagnosis (CAD) was included. 
  Results:  The overall area under the receiver operating characteristics (AUROC) of the eight deep learning algorithms was 0.846 (range 0.784-0.884). The best performing model was Xception, with an AUROC of 0.884. The diagnostic accuracy, sensitivity, specificity, positive predictive value, and negative predictive value of Xception were 82.8%, 80.2%, 83.0%, 83.0%, and 80.2%, respectively. After introducing the CAD system, underperforming trainees received more help from artificial intelligence than the higher performing trainees (p = 0.046), and overall confidence levels significantly increased from 3.90 to 4.30 (p &lt; 0.001). 
  Conclusion:  The deep learning-based CAD system used in this study for CT diagnosis of cervical LNM from thyroid cancer was clinically validated with an AUROC of 0.884. This approach may serve as a training tool to help resident physicians to gain confidence in diagnosis. 
  Key points:  • A deep learning-based CAD system for CT diagnosis of cervical LNM from thyroid cancer was validated using data from a clinical cohort. The AUROC for the eight tested algorithms ranged from 0.784 to 0.884. • Of the eight models, the Xception algorithm was the best performing model for the external validation dataset with 0.884 AUROC. The accuracy, sensitivity, specificity, positive predictive value, and negative predictive value were 82.8%, 80.2%, 83.0%, 83.0%, and 80.2%, respectively. • The CAD system exhibited potential to improve diagnostic specificity and accuracy in underperforming trainees (3 of 6 trainees, 50.0%). This approach may have clinical utility as a training tool to help trainees to gain confidence in diagnoses. 
  |  https://dx.doi.org/10.1007/s00330-019-06652-4  |  
------------------------------------------- 
10.1158/1078-0432.CCR-19-1376  |    Purpose:  Non-small cell lung cancer (NSCLC) is the most common cause of cancer-related deaths worldwide. There is an unmet need to develop novel clinically relevant models of NSCLC to accelerate identification of drug targets and our understanding of the disease. 
  Experimental design:  Thirty surgically resected NSCLC primary patient tissue and 35 previously established patient-derived xenograft (PDX) models were processed for organoid culture establishment. Organoids were histologically and molecularly characterized by cytology and histology, exome sequencing, and RNA-sequencing analysis. Tumorigenicity was assessed through subcutaneous injection of organoids in NOD/SCID mice. Organoids were subjected to drug testing using EGFR, FGFR, and MEK-targeted therapies. 
  Results:  We have identified cell culture conditions favoring the establishment of short-term and long-term expansion of NSCLC organoids derived from primary lung patient and PDX tumor tissue. The NSCLC organoids recapitulated the histology of the patient and PDX tumor. They also retained tumorigenicity, as evidenced by cytologic features of malignancy, xenograft formation, preservation of mutations, copy number aberrations, and gene expression profiles between the organoid and matched parental tumor tissue by whole-exome and RNA sequencing. NSCLC organoid models also preserved the sensitivity of the matched parental tumor to targeted therapeutics, and could be used to validate or discover biomarker-drug combinations. 
  Conclusions:  Our panel of NSCLC organoids closely recapitulates the genomics and biology of patient tumors, and is a potential platform for drug testing and biomarker validation. 
  |  http://clincancerres.aacrjournals.org/cgi/pmidlookup?view=long&pmid=31694835  |  
------------------------------------------- 
10.1186/s12859-020-3430-0  |    Background:  Methylated RNA immunoprecipitation sequencing (MeRIP-Seq) is a popular sequencing method for studying RNA modifications and, in particular, for N6-methyladenosine (m6A), the most abundant RNA methylation modification found in various species. The detection of enriched regions is a main challenge of MeRIP-Seq analysis, however current tools either require a long time or do not fully utilize features of RNA sequencing such as strand information which could cause ambiguous calling. On the other hand, with more attention on the treatment experiments of MeRIP-Seq, biologists need intuitive evaluation on the treatment effect from comparison. Therefore, efficient and user-friendly software that can solve these tasks must be developed. 
  Results:  We developed a software named "model-based analysis and inference of MeRIP-Seq (MoAIMS)" to detect enriched regions of MeRIP-Seq and infer signal proportion based on a mixture negative-binomial model. MoAIMS is designed for transcriptome immunoprecipitation sequencing experiments; therefore, it is compatible with different RNA sequencing protocols. MoAIMS offers excellent processing speed and competitive performance when compared with other tools. When MoAIMS is applied to studies of m6A, the detected enriched regions contain known biological features of m6A. Furthermore, signal proportion inferred from MoAIMS for m6A treatment datasets (perturbation of m6A methyltransferases) showed a decreasing trend that is consistent with experimental observations, suggesting that the signal proportion can be used as an intuitive indicator of treatment effect. 
  Conclusions:  MoAIMS is efficient and easy-to-use software implemented in R. MoAIMS can not only detect enriched regions of MeRIP-Seq efficiently but also provide intuitive evaluation on treatment effect for MeRIP-Seq treatment datasets. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3430-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32171255/  |  
------------------------------------------- 
10.1016/j.healthplace.2019.102243  |   Spatial lifecourse epidemiology is an interdisciplinary field that utilizes advanced spatial, location-based, and artificial intelligence technologies to investigate the long-term effects of environmental, behavioural, psychosocial, and biological factors on health-related states and events and the underlying mechanisms. With the growing number of studies reporting findings from this field and the critical need for public health and policy decisions to be based on the strongest science possible, transparency and clarity in reporting in spatial lifecourse epidemiologic studies is essential. A task force supported by the International Initiative on Spatial Lifecourse Epidemiology (ISLE) identified a need for guidance in this area and developed a Spatial Lifecourse Epidemiology Reporting Standards (ISLE-ReSt) Statement. The aim is to provide a checklist of recommendations to improve and make more consistent reporting of spatial lifecourse epidemiologic studies. The STrengthening the Reporting of Observational Studies in Epidemiology (STROBE) Statement for cohort studies was identified as an appropriate starting point to provide initial items to consider for inclusion. Reporting standards for spatial data and methods were then integrated to form a single comprehensive checklist of reporting recommendations. The strength of our approach has been our international and multidisciplinary team of content experts and contributors who represent a wide range of relevant scientific conventions, and our adherence to international norms for the development of reporting guidelines. As spatial, location-based, and artificial intelligence technologies used in spatial lifecourse epidemiology continue to evolve at a rapid pace, it will be necessary to revisit and adapt the ISLE-ReSt at least every 2-3 years from its release. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1353-8292(19)30635-5  |  
------------------------------------------- 
10.1038/s41746-019-0205-y  |   Complex health problems require multi-strategy, multi-target interventions. We present a method that uses machine learning techniques to choose optimal interventions from a set of possible interventions within a case study aiming to increase General Practitioner (GP) discussions of physical activity (PA) with their patients. Interventions were developed based on a causal loop diagram with 26 GPs across 13 clinics in Geelong, Australia. GPs prioritised eight from more than 80 potential interventions to increase GP discussion of PA with patients. Following a 2-week baseline, a multi-arm bandit algorithm was used to assign optimal strategies to GP clinics with the target outcome being GP PA discussion rates. The algorithm was updated weekly and the process iterated until the more promising strategies emerged (a duration of seven weeks). The top three performing strategies were continued for 3 weeks to improve the power of the hypothesis test of effectiveness for each strategy compared to baseline. GPs recorded a total of 11,176 conversations about PA. GPs identified 15 factors affecting GP PA discussion rates with patients including GP skills and awareness, fragmentation of care and fear of adverse outcomes. The two most effective strategies were correctly identified within seven weeks of the algorithm-based assignment of strategies. These were clinic reception staff providing PA information to patients at check in and PA screening questionnaires completed in the waiting room. This study demonstrates an efficient way to test and identify optimal strategies from multiple possible solutions. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31993505/  |  
------------------------------------------- 
10.1002/jmri.27104  |    Grant support:  This project was funded by the Research Council of Norway. 
  Background:  Oxygen uptake through the gastrointestinal tract after oral administration of oxygenated water in humans is not well studied and is debated in the literature. Due to the paramagnetic properties of oxygen and deoxyhemoglobin, MRI as a technique might be able to detect changes in relaxometry values caused by increased oxygen levels in the blood. 
  Purpose:  To assess whether oxygen dissolved in water is absorbed from the gastrointestinal tract and transported into the bloodstream after oral administration. 
  Study type:  A randomized, double-blinded, placebo-controlled crossover trial. 
  Population/subjects:  Thirty healthy male volunteers age 20-35. 
  Field strength/sequence:  3T/Modified Look-Locker inversion recovery (MOLLI) T<sub>1</sub> -mapping and multi fast field echo (mFFE) T<sub>2</sub> *-mapping. 
  Assessment:  Each volunteer was scanned in two separate sessions. T<sub>1</sub> and T<sub>2</sub> * maps were acquired repeatedly covering the hepatic portal vein (HPV) and vena cava inferior (VCI, control vein) before and after intake of oxygenated or control water. Assessments were done by placing a region of interest in the HPV and VCI. 
  Statistical test:  A mixed linear model was performed to the compare control vs. oxygen group. 
  Results:  Drinking caused a mean 1.6% 95% CI (1.1-2.0% P &lt; 0.001) increase in T<sub>1</sub> of HPV blood and water oxygenation attributed another 0.70% 95% confidence interval (CI) (0.07-1.3% P = 0.028) increase. Oxygenation did not change T<sub>1</sub> in VCI blood. Mean T<sub>2</sub> * increased 9.6% 95% CI (1.7-17.5% P = 0.017) after ingestion of oxygenated water and 1.2% 95% CI (-4.3-6.8% P = 0.661) after ingestion of control water. The corresponding changes in VCI blood were not significant. 
  Data conclusion:  Ingestion of water caused changes in T<sub>1</sub> and T<sub>2</sub> * of HPV blood compatible with dilution due to water absorption. The effects were enhanced by oxygen. Assessment of oxygen enrichment of HPV blood was not possible due to the dilution effect. 
  Level of evidence:  2 TECHNICAL EFFICACY STAGE: 2. 
  |  https://doi.org/10.1002/jmri.27104  |  
------------------------------------------- 
10.2196/17125  |    Background:  Coding of underlying causes of death from death certificates is a process that is nowadays undertaken mostly by humans with potential assistance from expert systems, such as the Iris software. It is, consequently, an expensive process that can, in addition, suffer from geospatial discrepancies, thus severely impairing the comparability of death statistics at the international level. The recent advances in artificial intelligence, specifically the rise of deep learning methods, has enabled computers to make efficient decisions on a number of complex problems that were typically considered out of reach without human assistance; they require a considerable amount of data to learn from, which is typically their main limiting factor. However, the CépiDc (Centre d'épidémiologie sur les causes médicales de Décès) stores an exhaustive database of death certificates at the French national scale, amounting to several millions of training examples available for the machine learning practitioner. 
  Objective:  This article investigates the application of deep neural network methods to coding underlying causes of death. 
  Methods:  The investigated dataset was based on data contained from every French death certificate from 2000 to 2015, containing information such as the subject's age and gender, as well as the chain of events leading to his or her death, for a total of around 8 million observations. The task of automatically coding the subject's underlying cause of death was then formulated as a predictive modelling problem. A deep neural network-based model was then designed and fit to the dataset. Its error rate was then assessed on an exterior test dataset and compared to the current state-of-the-art (ie, the Iris software). Statistical significance of the proposed approach's superiority was assessed via bootstrap. 
  Results:  The proposed approach resulted in a test accuracy of 97.8% (95% CI 97.7-97.9), which constitutes a significant improvement over the current state-of-the-art and its accuracy of 74.5% (95% CI 74.0-75.0) assessed on the same test example. Such an improvement opens up a whole field of new applications, from nosologist-level batch-automated coding to international and temporal harmonization of cause of death statistics. A typical example of such an application is demonstrated by recoding French overdose-related deaths from 2000 to 2010. 
  Conclusions:  This article shows that deep artificial neural networks are perfectly suited to the analysis of electronic health records and can learn a complex set of medical rules directly from voluminous datasets, without any explicit prior knowledge. Although not entirely free from mistakes, the derived algorithm constitutes a powerful decision-making tool that is able to handle structured medical data with an unprecedented performance. We strongly believe that the methods developed in this article are highly reusable in a variety of settings related to epidemiology, biostatistics, and the medical sciences in general. 
  |  https://medinform.jmir.org/2020/4/e17125/  |  
------------------------------------------- 
10.1002/mp.14129  |    Purpose:  Image-based breast lesion detection is a powerful clinical diagnosis technology. In recent years, deep learning architectures have achieved considerable success in medical image analysis however, they always require large-scale samples. In mammography images, breast lesions are inconspicuous, multiscale, and have blurred edges. Moreover, few well-labeled images exist. Because of these factors, the detection accuracy of conventional deep learning methods is low. Therefore, we attempted to improve the accuracy of mammary lesion detection by introducing transfer learning (TL) into a deep learning framework for the few-shot learning task and thus provide a method that will further assist physicians in detecting breast lesions. 
  Methods:  In this paper, we propose a method called "few-shot learning with deformable convolution for multiscale lesion detection in mammography," named FDMNet. Deformable convolution is introduced for enhancing the network's ability to detect lesions, and the sensitivity of the multiscale feature space is reinforced by using a feature pyramid method. Furthermore, by introducing location information in the predictor, the sensitivity of the model to lesion location is also enhanced. The proposed method, through the TL technique that is applied mines the potentially common knowledge of features in the source domain and transfers it into the target domain to improve the accuracy of breast lesion detection in the few-shot learning task. 
  Results:  On the publicly available datasets for screening mammography CBIS-DDSM and Mini-MIAS, the proposed method performs better than five widely used detection methods. On the CBIS-DDSM dataset, its comprehensive scores, sensitivity, precision, and the mean dice similarity coefficient are 0.911, 0.949, 0.873, and 0.913, respectively, and on the Mini-MIAS dataset, these values are 0.931, 0.966, 0.882, and 0.941, respectively. 
  Conclusions:  To achieve the few-shot learning required for medical image analysis, the proposed method uses TL to execute feature knowledge transformation and includes deformable convolution to build a feature pyramid structure, which enhances the learning performance of the network for lesions. The results of comparative numerical experiments show that the proposed method outperforms some state-of-the-art methods. 
  |  https://doi.org/10.1002/mp.14129  |  
------------------------------------------- 
10.20900/jpbs.20200001  |    Background:  The majority of individuals with Opioid Use Disorder (OUD) do not receive any formal substance use treatment. Due to limited engagement and access to traditional treatment, there is increasing evidence that patients with OUDs turn to online social platforms to access peer support and obtain health-related information about addiction and recovery. Interacting with peers before and during recovery is a key component of many evidence-based addiction recovery programs, and may improve self-efficacy and treatment engagement as well as reduce relapse. Commonly-used online social platforms are limited in utility and scalability as an adjunct to addiction treatment; lack effective content moderation (e.g., misinformed advice, maliciousness or "trolling"); and lack common security and ethical safeguards inherent to clinical care. 
  Methods:  This present study will develop a novel, artificial-intelligence (AI) enabled, mobile treatment delivery method that fulfills the need for a robust, secure, technology-based peer support platform to support patients with OUD. Forty adults receiving outpatient buprenorphine treatment for OUD will be asked to pilot a smartphone-based mobile peer support application, the "Marigold App", for a duration of six weeks. The program will use (1) a prospective cohort study to obtain text message content and feasibility metrics, and (2) qualitative interviews to evaluate usability and acceptability of the mobile platform. 
  Anticipated findings and future directions:  The Marigold mobile platform will allow patients to access a tailored chat support group 24/7 as a complement to different forms of clinical OUD treatment. Marigold can keep groups safe and constructive by augmenting chats with AI tools capable of understanding the emotional sentiment in messages, automatically "flagging" critical or clinically relevant content. This project will demonstrate the robustness of these AI tools by adapting them to catch OUD-specific "flags" in peer messages while also examining the adoptability of the platform itself within OUD patients. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32149192/  |  
------------------------------------------- 
10.2196/15963  |    Background:  Bone marrow aspiration and biopsy remain the gold standard for the diagnosis of hematological diseases despite the development of flow cytometry (FCM) and molecular and gene analyses. However, the interpretation of the results is laborious and operator dependent. Furthermore, the obtained results exhibit inter- and intravariations among specialists. Therefore, it is important to develop a more objective and automated analysis system. Several deep learning models have been developed and applied in medical image analysis but not in the field of hematological histology, especially for bone marrow smear applications. 
  Objective:  The aim of this study was to develop a deep learning model (BMSNet) for assisting hematologists in the interpretation of bone marrow smears for faster diagnosis and disease monitoring. 
  Methods:  From January 1, 2016, to December 31, 2018, 122 bone marrow smears were photographed and divided into a development cohort (N=42), a validation cohort (N=70), and a competition cohort (N=10). The development cohort included 17,319 annotated cells from 291 high-resolution photos. In total, 20 photos were taken for each patient in the validation cohort and the competition cohort. This study included eight annotation categories: erythroid, blasts, myeloid, lymphoid, plasma cells, monocyte, megakaryocyte, and unable to identify. BMSNet is a convolutional neural network with the YOLO v3 architecture, which detects and classifies single cells in a single model. Six visiting staff members participated in a human-machine competition, and the results from the FCM were regarded as the ground truth. 
  Results:  In the development cohort, according to 6-fold cross-validation, the average precision of the bounding box prediction without consideration of the classification is 67.4%. After removing the bounding box prediction error, the precision and recall of BMSNet were similar to those of the hematologists in most categories. In detecting more than 5% of blasts in the validation cohort, the area under the curve (AUC) of BMSNet (0.948) was higher than the AUC of the hematologists (0.929) but lower than the AUC of the pathologists (0.985). In detecting more than 20% of blasts, the AUCs of the hematologists (0.981) and pathologists (0.980) were similar and were higher than the AUC of BMSNet (0.942). Further analysis showed that the performance difference could be attributed to the myelodysplastic syndrome cases. In the competition cohort, the mean value of the correlations between BMSNet and FCM was 0.960, and the mean values of the correlations between the visiting staff and FCM ranged between 0.952 and 0.990. 
  Conclusions:  Our deep learning model can assist hematologists in interpreting bone marrow smears by facilitating and accelerating the detection of hematopoietic cells. However, a detailed morphological interpretation still requires trained hematologists. 
  |  https://medinform.jmir.org/2020/4/e15963/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32267237/  |  
------------------------------------------- 
10.1055/a-1035-9088  |   <b>Background and study aims</b> Capsule endoscopy (CE) is the preferred method for small bowel (SB) exploration. With a mean number of 50,000 SB frames per video, SBCE reading is time-consuming and tedious (30 to 60 minutes per video). We describe a large, multicenter database named CAD-CAP (Computer-Assisted Diagnosis for CAPsule Endoscopy, CAD-CAP). This database aims to serve the development of CAD tools for CE reading. <b>Materials and methods</b> Twelve French endoscopy centers were involved. All available third-generation SB-CE videos (Pillcam, Medtronic) were retrospectively selected from these centers and deidentified. Any pathological frame was extracted and included in the database. Manual segmentation of findings within these frames was performed by two pre-med students trained and supervised by an expert reader. All frames were then classified by type and clinical relevance by a panel of three expert readers. An automated extraction process was also developed to create a dataset of normal, proofread, control images from normal, complete, SB-CE videos. <b>Results</b> Four-thousand-one-hundred-and-seventy-four SB-CE were included. Of them, 1,480 videos (35 %) containing at least one pathological finding were selected. Findings from 5,184 frames (with their short video sequences) were extracted and delimited: 718 frames with fresh blood, 3,097 frames with vascular lesions, and 1,369 frames with inflammatory and ulcerative lesions. Twenty-thousand normal frames were extracted from 206 SB-CE normal videos. CAD-CAP has already been used for development of automated tools for angiectasia detection and also for two international challenges on medical computerized analysis. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/a-1035-9088  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32118115/  |  
------------------------------------------- 
10.3389/fonc.2020.00093  |   <b>Background:</b> Neoadjuvant chemotherapy (NAC) is commonly utilized in preoperative treatment for local breast cancer, and it gives high clinical response rates and can result in pathologic complete response (pCR) in 6-25% of patients. In recent years, dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has been increasingly used to assess the pathological response of breast cancer to NAC. In present analysis, we assess the diagnostic performance of DCE-MRI in evaluating the pathological response of breast cancer to NAC. <b>Materials and Methods:</b> A systematic search in PubMed, the Cochrane Library, and Web of Science for original studies was performed. The Quality Assessment of Diagnostic Accuracy Studies-2 tool was used to assess the methodological quality of the included studies. Patient, study, and imaging characteristics were extracted, and sufficient data to reconstruct 2 × 2 tables were obtained. Data pooling, heterogeneity testing, forest plot construction, meta-regression analysis and sensitivity analysis were performed using Stata version 12.0 (StataCorp LP, College Station, TX). <b>Results:</b> Eighteen studies (969 patients with breast cancer) were included in the present meta-analysis. The pooled sensitivity and specificity of DCE-MRI were 0.80 (95% confidence interval [CI]: 0.70, 0.88) and 0.84 (95% [CI]: 0.79, 0.88), respectively. Meta-regression analysis found no significant factors affecting heterogeneity. Sensitivity analysis showed that studies that set pathological complete response (pCR) (<i>n</i> = 14) as a responder showed a tendency for higher sensitivity compared with those that set pCR and near pCR together (<i>n</i> = 5) as a responder (0.83 vs. 0.72), and studies (<i>n</i> = 14) that used DCE-MRI to early predict the pathological response of breast cancer had a higher sensitivity (0.83 vs. 0.71) and equivalent specificity (0.80 vs. 0.86) compared to studies (<i>n</i> = 5) that assessed the response after NAC completion. <b>Conclusion:</b> Our results indicated that DCE-MRI could be considered an important auxiliary method for evaluating the pathological response of breast cancer to NAC and used as an effective method for dynamically monitoring the efficacy during NAC. DCE-MRI also performed well in predicting the pCR of breast cancer to NAC. However, due to the heterogeneity of the included studies, caution should be exercised in applying our results. 
  |  https://doi.org/10.3389/fonc.2020.00093  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32117747/  |  
------------------------------------------- 
10.1001/jamanetworkopen.2020.0265  |    Importance:  Mammography screening currently relies on subjective human interpretation. Artificial intelligence (AI) advances could be used to increase mammography screening accuracy by reducing missed cancers and false positives. 
  Objective:  To evaluate whether AI can overcome human mammography interpretation limitations with a rigorous, unbiased evaluation of machine learning algorithms. 
  Design, setting, and participants:  In this diagnostic accuracy study conducted between September 2016 and November 2017, an international, crowdsourced challenge was hosted to foster AI algorithm development focused on interpreting screening mammography. More than 1100 participants comprising 126 teams from 44 countries participated. Analysis began November 18, 2016. 
  Main outcomes and measurements:  Algorithms used images alone (challenge 1) or combined images, previous examinations (if available), and clinical and demographic risk factor data (challenge 2) and output a score that translated to cancer yes/no within 12 months. Algorithm accuracy for breast cancer detection was evaluated using area under the curve and algorithm specificity compared with radiologists' specificity with radiologists' sensitivity set at 85.9% (United States) and 83.9% (Sweden). An ensemble method aggregating top-performing AI algorithms and radiologists' recall assessment was developed and evaluated. 
  Results:  Overall, 144 231 screening mammograms from 85 580 US women (952 cancer positive ≤12 months from screening) were used for algorithm training and validation. A second independent validation cohort included 166 578 examinations from 68 008 Swedish women (780 cancer positive). The top-performing algorithm achieved an area under the curve of 0.858 (United States) and 0.903 (Sweden) and 66.2% (United States) and 81.2% (Sweden) specificity at the radiologists' sensitivity, lower than community-practice radiologists' specificity of 90.5% (United States) and 98.5% (Sweden). Combining top-performing algorithms and US radiologist assessments resulted in a higher area under the curve of 0.942 and achieved a significantly improved specificity (92.0%) at the same sensitivity. 
  Conclusions and relevance:  While no single AI algorithm outperformed radiologists, an ensemble of AI algorithms combined with radiologist assessment in a single-reader screening environment improved overall accuracy. This study underscores the potential of using machine learning methods for enhancing mammography screening interpretation. 
  |  https://jamanetwork.com/journals/jamanetworkopen/fullarticle/10.1001/jamanetworkopen.2020.0265  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32119094/  |  
------------------------------------------- 
10.1007/s00330-020-06856-z  |    Objectives:  To develop and evaluate the performance of a deep learning-based system for automatic patellar height measurements using knee radiographs. 
  Methods:  The deep learning-based algorithm was developed with a data set consisting of 1018 left knee radiographs for the prediction of patellar height parameters, specifically the Insall-Salvati index (ISI), Caton-Deschamps index (CDI), modified Caton-Deschamps index (MCDI), and Keerati index (KI). The performance and generalizability of the algorithm were tested with 200 left knee and 200 right knee radiographs, respectively. The intra-class correlation coefficient (ICC), Pearson correlation coefficient, mean absolute difference (MAD), root mean square (RMS), and Bland-Altman plots for predictions by the system were evaluated in comparison with manual measurements as the reference standard. 
  Results:  Compared with the reference standard, the deep learning-based algorithm showed high accuracy in predicting the ISI, CDI, and KI (left knee ICC = 0.91-0.95, r = 0.84-0.91, MAD = 0.02-0.05, RMS = 0.02-0.07; right knee ICC = 0.87-0.96, r = 0.78-0.92, MAD = 0.02-0.06, RMS = 0.02-0.10), but not the MCDI (left knee ICC = 0.65, r = 0.50, MAD = 0.14, RMS = 0.18; right knee ICC = 0.62, r = 0.47, MAD = 0.15, RMS = 0.20). The performance of the algorithm met or exceeded that of manual determination of ISI, CDI, and KI by radiologists. 
  Conclusions:  In its current state, the developed system can predict the ISI, CDI, and KI for both left and right knee radiographs as accurately as radiologists. Training the system further with more data would increase its utility in helping radiologists measure patellar height in clinical practice. 
  Key points:  • Objective and reliable measurement of patellar height parameters is important for clinical diagnosis and the development of a treatment strategy. • Deep learning can be used to create an automatic patellar height measurement system based on knee radiographs. • The deep learning-based patellar height measurement system achieves comparable performance to radiologists in measuring ISI, CDI, and KI. 
  |  https://dx.doi.org/10.1007/s00330-020-06856-z  |  
------------------------------------------- 
10.3389/fonc.2020.00235  |   <b>Purpose:</b> The majority of patients with low-grade gliomas (LGGs) experience tumor-related epilepsy during the disease course. Our study aimed to build a radiomic prediction model for LGG-related epilepsy type based on magnetic resonance imaging (MRI) data. <b>Methods:</b> A total of 205 cases with LGG-related epilepsy were enrolled in the retrospective study and divided into training and validation cohorts (1:1) according to their surgery time. Seven hundred thirty-four radiomic features were extracted from T2-weighted imaging, including six location features. Pearson correlation coefficient, univariate area under curve (AUC) analysis, and least absolute shrinkage and selection operator regression were adopted to select the most relevant features for the epilepsy type to build a radiomic signature. Furthermore, a novel radiomic nomogram was developed for clinical application using the radiomic signature and clinical variables from all patients. <b>Results:</b> Four MRI-based features were selected from the 734 radiomic features, including one location feature. Good discriminative performances were achieved in both training (AUC = 0.859, 95% CI = 0.787-0.932) and validation cohorts (AUC = 0.839, 95% CI = 0.761-0.917) for the type of epilepsy. The accuracies were 80.4 and 80.6%, respectively. The radiomic nomogram also allowed for a high degree of discrimination. All models presented favorable calibration curves and decision curve analyses. <b>Conclusion:</b> Our results suggested that the MRI-based radiomic analysis may predict the type of LGG-related epilepsy to enable individualized therapy for patients with LGG-related epilepsy. 
  |  https://doi.org/10.3389/fonc.2020.00235  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231995/  |  
------------------------------------------- 
10.1007/s11682-019-00252-y  |   Vascular cognitive impairment, no dementia (VCIND) refers to cognitive deficits associated with underlying vascular causes that are insufficient to confirm a diagnosis of dementia. The default mode network (DMN) is a large-scale brain network of interacting brain regions involved in attention, working memory and executive function. The role of DMN white matter integrity in cognitive deficits of VCIND patients is unclear. Using diffusion tensor imaging (DTI), this study was carried out to investigate white matter microstructural changes in the DMN in VCIND patients and their contributions to cognitive deficits. Thirty-one patients with subcortical VCIND and twenty-two healthy elderly subjects were recruited. All patients underwent neuropsychological assessments and DTI examination. Voxel-based analyses were performed to extract fractional anisotropy (FA) and mean diffusivity (MD) measures in the DMN. Compared with the healthy elderly subjects, patients diagnosed with subcortical VCIND presented with abnormal white matter integrity in several key hubs of the DMN. The severity of damage in the white matter microstructure in the DMN significantly correlated with cognitive dysfunction. Mediation analyses demonstrated that DTI values could account for attention, executive and language impairments, and partly mediated global cognitive dysfunction in the subcortical VCIND patients. DMN integrity is significantly impaired in subcortical VCIND patients. The disrupted DMN connectivity could explain the attention, language and executive dysfunction, which indicates that the white matter integrity of the DMN may be a neuroimaging marker for VCIND. 
  |  https://dx.doi.org/10.1007/s11682-019-00252-y  |  
------------------------------------------- 
10.1038/s41586-020-2284-y  |   Sudden, large-scale, and diffuse human migration can amplify localized outbreaks into widespread epidemics.<sup>1-4</sup> Rapid and accurate tracking of aggregate population flows may therefore be epidemiologically informative. Here, we use mobile-phone-data-based counts of 11,478,484 people egressing or transiting through the prefecture of Wuhan between 1 January and 24 January 2020 as they moved to 296 prefectures throughout China. First, we document the efficacy of quarantine in ceasing movement. Second, we show that the distribution of population outflow from Wuhan accurately predicts the relative frequency and geographic distribution of COVID-19 infections through February 19, 2020, across all of China. Third, we develop a spatio-temporal "risk source" model that leverages population flow data (which operationalizes risk emanating from epidemic epicenters) to not only forecast confirmed cases, but also to identify high-transmission-risk locales at an early stage. Fourth, we use this risk source model to statistically derive the geographic spread of COVID-19 and the growth pattern based on the population outflow from Wuhan; the model yields a benchmark trend and an index for assessing COVID-19 community transmission risk over time for different locations. This approach can be used by policy-makers in any nation with available data to make rapid and accurate risk assessments and to plan allocation of limited resources ahead of ongoing outbreaks. 
  |  https://doi.org/10.1038/s41586-020-2284-y  |  
------------------------------------------- 
10.1016/j.brainres.2020.146693  |   A direct measure of spoken lexical processing based on neuroimaging technology would provide us useful information to understand the neural mechanisms underlying speech or auditory language processing. The neural mechanisms of spoken word segmentation for English as a second language (ESL) learners remain elusive. The present study, using functional near-infrared spectroscopy (fNIRS), addresses this issue by measuring hemodynamic responses in the temporo-parietal junction (TPJ) and the prefrontal cortex (PFC) in a word-spotting task, designed with two task conditions (easy vs. difficult). Thirty participants, divided into a high listening proficiency group (HLG) and a low listening proficiency group (LLG), were tested. Results revealed significantly less TPJ activation in the HLG than in the LLG. Further analyses supported this result by showing that activation in the TPJ was in a negative correlation with listening proficiency. This association appears to be related to the more efficient use of processing resources in a bottom-up fashion for accurate and efficient sensory representations in high proficient language learners. In contrast, cortical activation in the PFC increased with listening proficiency and was stronger in the difficult task condition than in the easy task condition, implying that recruitment of top-down cognitive control functions might play a role in word segmentation. Our results suggest that the combination of the functions mediated via bottom-up sensory input processing (demonstrated in the TPJ activation) and top-down cognitive processing (demonstrated in the PFC activation) are crucial for ESL listeners' spoken word segmentation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0006-8993(20)30049-4  |  
------------------------------------------- 
10.1186/s12864-020-6542-z  |    Background:  Read coverage of RNA sequencing data reflects gene expression and RNA processing events. Single-cell RNA sequencing (scRNA-seq) methods, particularly "full-length" ones, provide read coverage of many individual cells and have the potential to reveal cellular heterogeneity in RNA transcription and processing. However, visualization tools suited to highlighting cell-to-cell heterogeneity in read coverage are still lacking. 
  Results:  Here, we have developed Millefy, a tool for visualizing read coverage of scRNA-seq data in genomic contexts. Millefy is designed to show read coverage of all individual cells at once in genomic contexts and to highlight cell-to-cell heterogeneity in read coverage. By visualizing read coverage of all cells as a heat map and dynamically reordering cells based on diffusion maps, Millefy facilitates discovery of "local" region-specific, cell-to-cell heterogeneity in read coverage. We applied Millefy to scRNA-seq data sets of mouse embryonic stem cells and triple-negative breast cancers and showed variability of transcribed regions including antisense RNAs, 3 <sup>'</sup> UTR lengths, and enhancer RNA transcription. 
  Conclusions:  Millefy simplifies the examination of cellular heterogeneity in RNA transcription and processing events using scRNA-seq data. Millefy is available as an R package (https://github.com/yuifu/millefy) and as a Docker image for use with Jupyter Notebook (https://hub.docker.com/r/yuifu/datascience-notebook-millefy). 
  |  https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-020-6542-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32122302/  |  
------------------------------------------- 
10.1089/ast.2019.2129  |   One of Saturn's largest moons, Enceladus, possesses a vast extraterrestrial ocean (<i>i.e.,</i> exo-ocean) that is increasingly becoming the hotspot of future research initiatives dedicated to the exploration of putative life. Here, a new bio-exploration concept design for Enceladus' exo-ocean is proposed, focusing on the potential presence of organisms across a wide range of sizes (<i>i.e.,</i> from uni- to multicellular and animal-like), according to state-of-the-art sensor and robotic platform technologies used in terrestrial deep-sea research. In particular, we focus on combined direct and indirect life-detection capabilities, based on optoacoustic imaging and passive acoustics, as well as molecular approaches. Such biologically oriented sampling can be accompanied by concomitant geochemical and oceanographic measurements to provide data relevant to exo-ocean exploration and understanding. Finally, we describe how this multidisciplinary monitoring approach is currently enabled in terrestrial oceans through cabled (fixed) observatories and their related mobile multiparametric platforms (<i>i.e.,</i> Autonomous Underwater and Remotely Operated Vehicles, as well as crawlers, rovers, and biomimetic robots) and how their modified design can be used for exo-ocean exploration. 
  |  https://www.liebertpub.com/doi/full/10.1089/ast.2019.2129?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2196/15411  |    Background:  Preeclampsia and intrauterine growth restriction are placental dysfunction-related disorders (PDDs) that require a referral decision be made within a certain time period. An appropriate prediction model should be developed for these diseases. However, previous models did not demonstrate robust performances and/or they were developed from datasets with highly imbalanced classes. 
  Objective:  In this study, we developed a predictive model of PDDs by machine learning that uses features at 24-37 weeks' gestation, including maternal characteristics, uterine artery (UtA) Doppler measures, soluble fms-like tyrosine kinase receptor-1 (sFlt-1), and placental growth factor (PlGF). 
  Methods:  A public dataset was taken from a prospective cohort study that included pregnant women with PDDs (66/95, 69%) and a control group (29/95, 31%). Preliminary selection of features was based on a statistical analysis using SAS 9.4 (SAS Institute). We used Weka (Waikato Environment for Knowledge Analysis) 3.8.3 (The University of Waikato, Hamilton, NZ) to automatically select the best model using its optimization algorithm. We also manually selected the best of 23 white-box models. Models, including those from recent studies, were also compared by interval estimation of evaluation metrics. We used the Matthew correlation coefficient (MCC) as the main metric. It is not overoptimistic to evaluate the performance of a prediction model developed from a dataset with a class imbalance. Repeated 10-fold cross-validation was applied. 
  Results:  The classification via regression model was chosen as the best model. Our model had a robust MCC (.93, 95% CI .87-1.00, vs .64, 95% CI .57-.71) and specificity (100%, 95% CI 100-100, vs 90%, 95% CI 90-90) compared to each metric of the best models from recent studies. The sensitivity of this model was not inferior (95%, 95% CI 91-100, vs 100%, 95% CI 92-100). The area under the receiver operating characteristic curve was also competitive (0.970, 95% CI 0.966-0.974, vs 0.987, 95% CI 0.980-0.994). Features in the best model were maternal weight, BMI, pulsatility index of the UtA, sFlt-1, and PlGF. The most important feature was the sFlt-1/PlGF ratio. This model used an M5P algorithm consisting of a decision tree and four linear models with different thresholds. Our study was also better than the best ones among recent studies in terms of the class balance and the size of the case class (66/95, 69%, vs 27/239, 11.3%). 
  Conclusions:  Our model had a robust predictive performance. It was also developed to deal with the problem of a class imbalance. In the context of clinical management, this model may improve maternal mortality and neonatal morbidity and reduce health care costs. 
  |  https://doi.org/10.2196/15411  |  
------------------------------------------- 
10.1016/j.drugalcdep.2019.107716  |    Background:  Data from controlled laboratory experiments in adults indicate that the subjective effects of cannabis vary by administration method (e.g., combustible, vaporized). Whether the subjective effects of cannabis experienced in the natural ecology and among adolescents differ by cannabis administration method is unknown. In this observational study, adolescents' retrospective reports of subjective effects after combustible, edible, and vaporized cannabis use were examined. 
  Methods:  Students from ten public schools in Los Angeles, CA, USA (M[SD] age = 16.1 [.43] years) who reported past 6-month use of combustible, edible, or vaporized cannabis (N = 584) were surveyed on subjective effects experienced after use (yes/no). They were provided with a 12 item self-report checklist of six positive (e.g., relaxed, energetic) and six negative (e.g., drowsy, lazy) subjective effects. For each method of administration, affirmative responses were summed in positive (range: 0-6) and negative (range: 0-6) effect composite scores. 
  Results:  Generalized estimating equations adjusted for demographics and recent cannabis use revealed a graded pattern of differences in positive subjective effects across products, with highest scores for combustible (M[SD] = 3.98[1.76]), followed by edible (M[SD] = 3.58 [2.04]) and vaporized (M[SD] = 3.11 [2.21]) cannabis (all pairwise cross-product contrasts p &lt; .01). Mean negative effect score was highest for edible (M[SD] = 2.27 [1.95]), followed by combustible (M[SD] = 1.94 [1.66]), and vaporized (M[SD] = 1.34 [1.73]) cannabis, respectively (all pairwise contrasts p &lt; .02). 
  Conclusion:  Adolescents' reports of subjective effects varied across cannabis administration methods. Combustible cannabis' more desirable subjective effects profile might be indicative of higher abuse liability. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0376-8716(19)30493-4  |  
------------------------------------------- 
10.3390/s20092489  |   Under the conditions of low flow rate and strong noise, the current electromagnetic flowmeter (EMF) cannot satisfy the requirement for measurement or separate the actual flow signal and interference signal accurately. Correlation detection technology can reduce the bandwidth and suppress noise effectively using the periodic transmission of signal and noise randomness. As for the problem that the current anti-interference technology cannot suppress noise effectively, the noise and interference of the electromagnetic flowmeter were analyzed in this paper, and a design of the electromagnetic flowmeter based on differential correlation detection was proposed. Then, in order to verify the feasibility of the electromagnetic flow measurement system based on differential correlation, an experimental platform for the comparison between standard flow and measured flow was established and a verification experiment was carried out under special conditions and with flow calibration measurements. Finally, the data obtained in the experiment were analyzed. The research result showed that an electromagnetic flowmeter based on differential correlation detection satisfies the need for measurement completely. The lower limit of the flow rate of the electromagnetic flowmeter based on the differential correlation principle could reach 0.084 m/s. Under strong external interferences, the electromagnetic flowmeter based on differential correlation had a fluctuation range in output value of only 10 mV. This shows that the electromagnetic flowmeter based on the differential correlation principle has unique advantages in measurements taken under the conditions of strong noise, slurry flow, and low flow rate. 
  |  http://www.mdpi.com/resolver?pii=s20092489  |  
------------------------------------------- 
10.1002/ece3.6147  |   Ecological camera traps are increasingly used by wildlife biologists to unobtrusively monitor an ecosystems animal population. However, manual inspection of the images produced is expensive, laborious, and time-consuming. The success of deep learning systems using camera trap images has been previously explored in preliminary stages. These studies, however, are lacking in their practicality. They are primarily focused on extremely large datasets, often millions of images, and there is little to no focus on performance when tasked with species identification in new locations not seen during training. Our goal was to test the capabilities of deep learning systems trained on camera trap images using modestly sized training data, compare performance when considering unseen background locations, and quantify the gradient of lower bound performance to provide a guideline of data requirements in correspondence to performance expectations. We use a dataset provided by Parks Canada containing 47,279 images collected from 36 unique geographic locations across multiple environments. Images represent 55 animal species and human activity with high-class imbalance. We trained, tested, and compared the capabilities of six deep learning computer vision networks using transfer learning and image augmentation: DenseNet201, Inception-ResNet-V3, InceptionV3, NASNetMobile, MobileNetV2, and Xception. We compare overall performance on "trained" locations where DenseNet201 performed best with 95.6% top-1 accuracy showing promise for deep learning methods for smaller scale research efforts. Using trained locations, classifications with &lt;500 images had low and highly variable recall of 0.750 ± 0.329, while classifications with over 1,000 images had a high and stable recall of 0.971 ± 0.0137. Models tasked with classifying species from untrained locations were less accurate, with DenseNet201 performing best with 68.7% top-1 accuracy. Finally, we provide an open repository where ecologists can insert their image data to train and test custom species detection models for their desired ecological domain. 
  |  https://doi.org/10.1002/ece3.6147  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274005/  |  
------------------------------------------- 
10.1016/j.cmpb.2019.105162  |    Background and objective:  In most patients presenting with respiratory symptoms, the findings of chest radiography play a key role in the diagnosis, management, and follow-up of the disease. Consolidation is a common term in radiology, which indicates focally increased lung density. When the alveolar structures become filled with pus, fluid, blood cells or protein subsequent to a pulmonary pathological process, it may result in different types of lung opacity in chest radiograph. This study aims at detecting consolidations in chest x-ray radiographs, with a certain precision, using artificial intelligence and especially Deep Convolutional Neural Networks to assist radiologist for better diagnosis. 
  Methods:  Medical image datasets usually are relatively small to be used for training a Deep Convolutional Neural Network (DCNN), so transfer learning technique with well-known DCNNs pre-trained with ImageNet dataset are used to improve the accuracy of the models. ImageNet feature space is different from medical images and in the other side, the well-known DCNNs are designed to achieve the best performance on ImageNet. Therefore, they cannot show their best performance on medical images. To overcome this problem, we designed a problem-based architecture which preserves the information of images for detecting consolidation in Pediatric Chest X-ray dataset. We proposed a three-step pre-processing approach to enhance generalization of the models. To demonstrate the correctness of numerical results, an occlusion test is applied to visualize outputs of the model and localize the detected appropriate area. A different dataset as an extra validation is used in order to investigate the generalization of the proposed model. 
  Results:  The best accuracy to detect consolidation is 94.67% obtained by our problem based architecture for the understudy dataset which outperforms the previous works and the other architectures. 
  Conclusions:  The designed models can be employed as computer aided diagnosis tools in real practice. We critically discussed the datasets and the previous works based on them and show that without some considerations the results of them may be misleading. We believe, the output of AI should be only interpreted as focal consolidation. The clinical significance of the finding can not be interpreted without integration of clinical data. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)30696-0  |  
------------------------------------------- 
10.1111/den.13688  |    Objectives:  Detecting early gastric cancer is difficult, and it may even be overlooked by experienced endoscopists. Recently, artificial intelligence based on deep learning through convolutional neural networks (CNNs) has enabled significant advancements in the field of gastroenterology. However, it remains unclear whether a CNN can outperform endoscopists. In this study, we evaluated whether the performance of a CNN in detecting early gastric cancer is better than that of endoscopists. 
  Methods:  The CNN was constructed using 13,584 endoscopic images from 2,639 lesions of gastric cancer. Subsequently, its diagnostic ability was compared to that of 67 endoscopists using an independent test dataset (2,940 images from 140 cases). 
  Results:  The average diagnostic time for analyzing 2,940 test endoscopic images by the CNN and endoscopists were 45.5 ± 1.8 s and 173.0 ± 66.0 min, respectively. The sensitivity, specificity, and positive and negative predictive values for the CNN were 58.4%, 87.3%, 26.0%, and 96.5%, respectively. These values for the 67 endoscopists were 31.9%, 97.2%, 46.2%, and 94.9%, respectively. The CNN had a significantly higher sensitivity than the endoscopists (by 26.5%; 95% confidence interval, 14.9-32.5%). 
  Conclusion:  The CNN detected more early gastric cancer cases in a shorter time than the endoscopists. The CNN needs further training to achieve higher diagnostic accuracy. However, a diagnostic support tool for gastric cancer using a CNN will be realized in the near future. 
  |  https://doi.org/10.1111/den.13688  |  
------------------------------------------- 
10.3390/s20010293  |   To generate indoor as-built building information models (AB BIMs) automatically and economically is a great technological challenge. Many approaches have been developed to address this problem in recent years, but it is far from being settled, particularly for the point cloud segmentation and the extraction of the relationship among different elements due to the complicated indoor environment. This is even more difficult for the low-quality point cloud generated by low-cost scanning equipment. This paper proposes an automatic as-built BIMs generation framework that transforms the noisy 3D point cloud produced by a low-cost RGB-D sensor (about 708 USD for data collection equipment, 379 USD for the Structure sensor and 329 USD for iPad) to the as-built BIMs, without any manual intervention. The experiment results show that the proposed method has competitive robustness and accuracy, compared to the high-quality Terrestrial Lidar System (TLS), with the element extraction accuracy of 100%, mean dimension reconstruction accuracy of 98.6% and mean area reconstruction accuracy of 93.6%. Also, the proposed framework makes the BIM generation workflows more efficient in both data collection and data processing. In the experiments, the time consumption of data collection for a typical room, with an area of 45-67 m 2 , is reduced to 4-6 min with an RGB-D sensor from 50-60 min with TLS. The processing time to generate BIM models is about half minutes automatically, from around 10 min with a conventional semi-manual method. 
  |  http://www.mdpi.com/resolver?pii=s20010293  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31948010/  |  
------------------------------------------- 
10.1097/MLR.0000000000001221  |    Objective:  Experts cautioned that patients affected by the November 2010 withdrawal of the opioid analgesic propoxyphene might receive riskier prescriptions. To explore this, we compared drug receipts and outcomes among propoxyphene users before and aftermarket withdrawal. 
  Study design:  Using OptumLabs data, we studied 3 populations: commercial, Medicare Advantage (MA) aged (age 65+ y) and MA disabled (age below 65 y) enrollees. The exposed enrollees received propoxyphene in the 3 months before market withdrawal (n=13,622); historical controls (unexposed) received propoxyphene 1 year earlier (n=9971). Regression models estimated daily milligrams morphine equivalent (MME), daily prescription acetaminophen dose, potentially toxic acetaminophen doses, nonopioid prescription analgesics receipt, emergency room visits, and diagnosed falls, motor vehicle accidents, and hip fractures. 
  Principal findings:  Aged MA enrollees illustrate the experience of all 3 populations examined. Following the market withdrawal, propoxyphene users in the exposed cohort experienced an abrupt decline of 69% in average daily MME, compared with a 14% decline in the unexposed. Opioids were discontinued by 34% of the exposed cohort and 18% of the unexposed. Tramadol and hydrocodone were the most common opioids substituted for propoxyphene. The proportion of each group receiving ≥4 g of prescription acetaminophen per day decreased from 12% to 2% in the exposed group but increased from 6% to 8% among the unexposed. Adverse events were rare and not significantly different in exposed versus unexposed groups. 
  Conclusions:  After propoxyphene market withdrawal, many individuals experienced abrupt discontinuation of opioids. Policymakers might consider supporting appropriate treatment transitions and monitoring responses following drug withdrawals. 
  |  http://dx.doi.org/10.1097/MLR.0000000000001221  |  
------------------------------------------- 
10.1111/nyas.14320  |   Visual perception involves the rapid formation of a coarse image representation at the onset of visual processing, which is iteratively refined by late computational processes. These early versus late time windows approximately map onto feedforward and feedback processes, respectively. State-of-the-art convolutional neural networks, the main engine behind recent machine vision successes, are feedforward architectures. Their successes and limitations provide critical information regarding which visual tasks can be solved by purely feedforward processes and which require feedback mechanisms. We provide an overview of recent work in cognitive neuroscience and machine vision that highlights the possible role of feedback processes for both visual recognition and beyond. We conclude by discussing important open questions for future research. 
  |  https://doi.org/10.1111/nyas.14320  |  
------------------------------------------- 
PMID:32355515  |   Colorectal cancer (CRC) is one of the most common malignancies, with varying prognoses and a high mortality. There is an urgent need to establish a new prediction model to predict the survival risk of CRC patients. The long non-coding RNAs (lncRNAs) expression profiles and corresponding clinical information of CRC patients were obtained from The Cancer Genome Atlas, TCGA. We identified a total of 1,176 lncRNAs differentially expressed between 480 CRC and 41 normal tissues. In the training test, we combined these differentially expressed lncRNAs with overall survival of CRC patients. Six lncRNAs (AL356270.1, LINC02257, AC020891.2, LINC01485, AC083967.1 and RBAKDN) were finally screened out by using LASSO regression mode to establish a novel prediction model as a prognostic indicator for CRC patients. The area under the curve (AUC) of 3- and 5-year ROC analysis in CRC were 0.6923 and 0.7328 for training set, and were 0.6803 and 0.7035 for testing set, respectively. K-M analysis revealed a significant difference between high risk and low risk in the training set (<i>P</i>-value = 5.0e-05) and testing set (<i>P</i>-value = 0.00052), respectively. Our study shows that the six lncRNAs model can improve the survival prediction mechanism of patients with CRC and provide help for patients through personalized treatment. 
  |  None  |  
------------------------------------------- 
10.1002/jum.15071  |    Objectives:  To explore the value of ultrasomics in temporal monitoring of tumor changes in response to gene therapy in hepatocellular carcinoma compared with methods according to the Response Evaluation Criteria in Solid Tumors (RECIST) and modified RECIST (mRECIST). 
  Methods:  Hepatocellular carcinoma-bearing mice were injected intratumorally with microRNA-122 (miR-122) mimics and an miR-122 negative control in the treatment and control groups, respectively. The injections were performed every 3 days for 5 times (on days 0, 3, 6, 9, and 12). Before each injection and at the experiment ending, 2-dimensional ultrasound imaging was performed for tumor size measurement with RECIST and computing a quantitative imaging analysis with ultrasomics. To analyze the tumor perfusion by mRECIST, perfusion parameters were analyzed offline based on dynamic contrast-enhanced ultrasound image videos using SonoLiver software (TomTec, Unterschleissheim, Germany) on day 13. Tumor miR-122 expression was then analyzed by real-time reverse transcription-polymerase chain reaction experiments. 
  Results:  Tumors in mice treated with miR-122 mimics demonstrated a mean ± SD 763- ± 60-fold increase in miR-122 levels compared with tumors in the control group. With RECIST, a significant therapeutic response evaluated by tumor size changes was detected after day 9 (days 9, 12, and 13; P &lt; .001). With mRECIST, no parameters showed significant differences (P &gt; .05). Significant different features of the 2-dimensional ultrasound images between the groups were detected by the ultrasomics analysis, and the model could be successfully built. The ultrasomics score values between the groups were statistically significant after day 6 (days 6, 9, 12, and 13; P &lt; .05). 
  Conclusions:  Ultrasomics revealed significant changes after the second injection of miR-122, showing the potential as an important imaging biomarker for gene therapy. 
  |  https://doi.org/10.1002/jum.15071  |  
------------------------------------------- 
10.1158/2326-6066.CIR-19-0521  |   CD8<sup>+</sup> T cells can be polarized into several different subsets as defined by the cytokines they produce and the transcription factors that govern their differentiation. Here, we identified the polarizing conditions to induce an IL22-producing CD8<sup>+</sup> Tc22 subset, which is dependent on IL6 and the aryl hydrocarbon receptor transcription factor. Further characterization showed that this subset was highly cytolytic and expressed a distinct cytokine profile and transcriptome relative to other subsets. In addition, polarized Tc22 were able to control tumor growth as well as, if not better than, the traditional IFNγ-producing Tc1 subset. Tc22s were also found to infiltrate the tumors of human patients with ovarian cancer, comprising up to approximately 30% of expanded CD8<sup>+</sup> tumor-infiltrating lymphocytes (TIL). Importantly, IL22 production in these CD8<sup>+</sup> TILs correlated with improved recurrence-free survival. Given the antitumor properties of Tc22 cells, it may be prudent to polarize T cells to the Tc22 lineage when using chimeric antigen receptor (CAR)-T or T-cell receptor (TCR) transduction-based immunotherapies. 
  |  http://cancerimmunolres.aacrjournals.org/cgi/pmidlookup?view=long&pmid=31964625  |  
------------------------------------------- 
10.4070/kcj.2019.0105  |    Background and objectives:  We aim to explore the additional discriminative accuracy of a deep learning (DL) algorithm using repeated-measures data for identifying people at high risk for cardiovascular disease (CVD), compared to Cox hazard regression. 
  Methods:  Two CVD prediction models were developed from National Health Insurance Service-Health Screening Cohort (NHIS-HEALS): a Cox regression model and a DL model. Performance of each model was assessed in the internal and 2 external validation cohorts in Koreans (National Health Insurance Service-National Sample Cohort; NHIS-NSC) and in Europeans (Rotterdam Study). A total of 412,030 adults in the NHIS-HEALS; 178,875 adults in the NHIS-NSC; and the 4,296 adults in Rotterdam Study were included. 
  Results:  Mean ages was 52 years (46% women) and there were 25,777 events (6.3%) in NHIS-HEALS during the follow-up. In internal validation, the DL approach demonstrated a C-statistic of 0.896 (95% confidence interval, 0.886-0.907) in men and 0.921 (0.908-0.934) in women and improved reclassification compared with Cox regression (net reclassification index [NRI], 24.8% in men, 29.0% in women). In external validation with NHIS-NSC, DL demonstrated a C-statistic of 0.868 (0.860-0.876) in men and 0.889 (0.876-0.898) in women, and improved reclassification compared with Cox regression (NRI, 24.9% in men, 26.2% in women). In external validation applied to the Rotterdam Study, DL demonstrated a C-statistic of 0.860 (0.824-0.897) in men and 0.867 (0.830-0.903) in women, and improved reclassification compared with Cox regression (NRI, 36.9% in men, 31.8% in women). 
  Conclusions:  A DL algorithm exhibited greater discriminative accuracy than Cox model approaches. 
  Trial registration:  ClinicalTrials.gov Identifier: <a href="http://clinicaltrials.gov/show/NCT02931500" title="See in ClinicalTrials.gov">NCT02931500</a>. 
  |  https://e-kcj.org/DOIx.php?id=10.4070/kcj.2019.0105  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31456363/  |  
------------------------------------------- 
10.1186/s12916-020-01563-4  |    Background:  Healthcare represents a paradox. While change is everywhere, performance has flatlined: 60% of care on average is in line with evidence- or consensus-based guidelines, 30% is some form of waste or of low value, and 10% is harm. The 60-30-10 Challenge has persisted for three decades. 
  Main body:  Current top-down or chain-logic strategies to address this problem, based essentially on linear models of change and relying on policies, hierarchies, and standardisation, have proven insufficient. Instead, we need to marry ideas drawn from complexity science and continuous improvement with proposals for creating a deep learning health system. This dynamic learning model has the potential to assemble relevant information including patients' histories, and clinical, patient, laboratory, and cost data for improved decision-making in real time, or close to real time. If we get it right, the learning health system will contribute to care being more evidence-based and less wasteful and harmful. It will need a purpose-designed digital backbone and infrastructure, apply artificial intelligence to support diagnosis and treatment options, harness genomic and other new data types, and create informed discussions of options between patients, families, and clinicians. While there will be many variants of the model, learning health systems will need to spread, and be encouraged to do so, principally through diffusion of innovation models and local adaptations. 
  Conclusion:  Deep learning systems can enable us to better exploit expanding health datasets including traditional and newer forms of big and smaller-scale data, e.g. genomics and cost information, and incorporate patient preferences into decision-making. As we envisage it, a deep learning system will support healthcare's desire to continually improve, and make gains on the 60-30-10 dimensions. All modern health systems are awash with data, but it is only recently that we have been able to bring this together, operationalised, and turned into useful information by which to make more intelligent, timely decisions than in the past. 
  |  None  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105475  |    Background and objective:  Skin cancer is among the most common cancer types in the white population and consequently computer aided methods for skin lesion classification based on dermoscopic images are of great interest. A promising approach for this uses transfer learning to adapt pre-trained convolutional neural networks (CNNs) for skin lesion diagnosis. Since pre-training commonly occurs with natural images of a fixed image resolution and these training images are usually significantly smaller than dermoscopic images, downsampling or cropping of skin lesion images is required. This however may result in a loss of useful medical information, while the ideal resizing or cropping factor of dermoscopic images for the fine-tuning process remains unknown. 
  Methods:  We investigate the effect of image size for skin lesion classification based on pre-trained CNNs and transfer learning. Dermoscopic images from the International Skin Imaging Collaboration (ISIC) skin lesion classification challenge datasets are either resized to or cropped at six different sizes ranging from 224 × 224 to 450 × 450. The resulting classification performance of three well established CNNs, namely EfficientNetB0, EfficientNetB1 and SeReNeXt-50 is explored. We also propose and evaluate a multi-scale multi-CNN (MSM-CNN) fusion approach based on a three-level ensemble strategy that utilises the three network architectures trained on cropped dermoscopic images of various scales. 
  Results:  Our results show that image cropping is a better strategy compared to image resizing delivering superior classification performance at all explored image scales. Moreover, fusing the results of all three fine-tuned networks using cropped images at all six scales in the proposed MSM-CNN approach boosts the classification performance compared to a single network or a single image scale. On the ISIC 2018 skin lesion classification challenge test set, our MSM-CNN algorithm yields a balanced multi-class accuracy of 86.2% making it the currently second ranked algorithm on the live leaderboard. 
  Conclusions:  We confirm that the image size has an effect on skin lesion classification performance when employing transfer learning of CNNs. We also show that image cropping results in better performance compared to image resizing. Finally, a straightforward ensembling approach that fuses the results from images cropped at six scales and three fine-tuned CNNs is shown to lead to the best classification performance. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31146-0  |  
------------------------------------------- 
10.1016/j.jvoice.2020.02.016  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0892-1997(20)30068-0  |  
------------------------------------------- 
10.2196/17279  |    Background:  Interprofessional team training is needed to improve nurse-physician communication skills that are lacking in clinical practice. Using simulations has proven to be an effective learning approach for team training. Yet, it has logistical constraints that call for the exploration of virtual environments in delivering team training. 
  Objective:  This study aimed to evaluate a team training program using virtual reality vs conventional live simulations on medical and nursing students' communication skill performances and teamwork attitudes. 
  Methods:  In June 2018, the authors implemented nurse-physician communication team training using communication tools. A randomized controlled trial study was conducted with 120 undergraduate medical and nursing students who were randomly assigned to undertake team training using virtual reality or live simulations. The participants from both groups were tested on their communication performances through team-based simulation assessments. Their teamwork attitudes were evaluated using interprofessional attitude surveys that were administered before, immediately after, and 2 months after the study interventions. 
  Results:  The team-based simulation assessment revealed no significant differences in the communication performance posttest scores (P=.29) between the virtual and simulation groups. Both groups reported significant increases in the interprofessional attitudes posttest scores from the baseline scores, with no significant differences found between the groups over the 3 time points. 
  Conclusions:  Our study outcomes did not show an inferiority of team training using virtual reality when compared with live simulations, which supports the potential use of virtual reality to substitute conventional simulations for communication team training. Future studies can leverage the use of artificial intelligence technology in virtual reality to replace costly human-controlled facilitators to achieve better scalability and sustainability of team-based training in interprofessional education. 
  Trial registration:  ClinicalTrials.gov <a href="http://clinicaltrials.gov/show/NCT04330924" title="See in ClinicalTrials.gov">NCT04330924</a>; https://clinicaltrials.gov/ct2/show/NCT04330924. 
  |  https://www.jmir.org/2020/4/e17279/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32267235/  |  
------------------------------------------- 
10.1002/1878-0261.12635  |   In breast cancer (BC), the presence of cancer stem cells (CSCs) has been related to relapse, metastasis, and radioresistance. Radiotherapy (RT) is an extended BC treatment, but is not always effective. CSCs have several mechanisms of radioresistance in place, and some miRNAs are involved in the cellular response to ionizing radiation (IR). Here, we studied how IR affects the expression of miRNAs related to stemness in different molecular BC subtypes. Exposition of BC cells to radiation doses of 2, 4, or 6 Gy affected their phenotype, functional characteristics, pluripotency gene expression, and in vivo tumorigenic capacity. This held true for various molecular subtypes of BC cells (classified by ER, PR and HER-2 status), and for BC cells either plated in monolayer, or being in suspension as mammospheres. However, the effect of IR on the expression of eight stemness- and radioresistance-related miRNAs (miR-210, miR-10b, miR-182, miR-142, miR-221, miR-21, miR-93, miR-15b) varied, depending on cell line subpopulation and clinicopathological features of BC patients. Therefore, clinicopathological features and, potentially also, chemotherapy regimen should be both taken into consideration, for determining a potential miRNA signature by liquid biopsy in BC patients treated with RT. Personalized and precision RT dosage regimes could improve the prognosis, treatment, and survival of BC patients. 
  |  https://doi.org/10.1002/1878-0261.12635  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31930680/  |  
------------------------------------------- 
10.3390/s20092460  |   Leaf area index (LAI) is an important biophysical parameter, which can be effectively applied in the estimation of vegetation growth status. At present, amounts of studies just focused on the LAI estimation of a single plant type, while plant types are usually mixed rather than single distribution. In this study, the suitability of GF-1 data for multi-species LAI estimation was evaluated by using Gaussian process regression (GPR), and a look-up table (LUT) combined with a PROSAIL radiative transfer model. Then, the performance of the LUT and GPR for multi-species LAI estimation was analyzed in term of 15 different band combinations and 10 published vegetation indices (VIs). Lastly, the effect of the different band combinations and published VIs on the accuracy of LAI estimation was discussed. The results indicated that GF-1 data exhibited a good potential for multi-species LAI retrieval. Then, GPR exhibited better performance than that of LUT for multi-species LAI estimation. What is more, modified soil adjusted vegetation index (MSAVI) was selected based on the GPR algorithm for multi-species LAI estimation with a lower root mean squared error (RMSE = 0.6448 m<sup>2</sup>/m<sup>2</sup>) compared to other band combinations and VIs. Then, this study can provide guidance for multi-species LAI estimation. 
  |  http://www.mdpi.com/resolver?pii=s20092460  |  
------------------------------------------- 
10.1016/j.canrad.2020.01.011  |    Purpose:  Radiomics are a set of methods used to leverage medical imaging and extract quantitative features that can characterize a patient's phenotype. All modalities can be used with several different software packages. Specific informatics methods can then be used to create meaningful predictive models. In this review, we will explain the major steps of a radiomics analysis pipeline and then present the studies published in the context of radiation therapy. 
  Methods:  A literature review was performed on Medline using the search engine PubMed. The search strategy included the search terms "radiotherapy", "radiation oncology" and "radiomics". The search was conducted in July 2019 and reference lists of selected articles were hand searched for relevance to this review. 
  Results:  A typical radiomics workflow always includes five steps: imaging and segmenting, data curation and preparation, feature extraction, exploration and selection and finally modeling. In radiation oncology, radiomics studies have been published to explore different clinical outcome in lung (n=5), head and neck (n=5), esophageal (n=3), rectal (n=3), pancreatic (n=2) cancer and brain metastases (n=2). The quality of these retrospective studies is heterogeneous and their results have not been translated to the clinic. 
  Conclusion:  Radiomics has a great potential to predict clinical outcome and better personalize treatment. But the field is still young and constantly evolving. Improvement in bias reduction techniques and multicenter studies will hopefully allow more robust and generalizable models. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1278-3218(20)30071-8  |  
------------------------------------------- 
10.1016/j.actbio.2020.02.007  |   Throughout the process of aging, dynamic changes of bone material, micro- and macro-architecture result in a loss of strength and therefore in an increased likelihood of fragility fractures. To date, precise contributions of age-related changes in bone (re)modeling and (de)mineralization dynamics to this fragility increase are not completely understood. Here, we present an image-based deep learning approach to quantitatively describe the effects of short-term aging and adaptive response to cyclic loading applied to proximal mouse tibiae and fibulae. Our approach allowed us to perform an end-to-end age prediction based on μCT imaging to determine the dynamic biological process of aging during a two week period, therefore permitting short-term bone aging analysis with 95% accuracy in predicting time points. In a second application, our deep learning analysis reveals that two weeks of in vivo mechanical loading are associated with an underlying rejuvenating effect of 5 days. Additionally, by quantitatively analyzing the learning process, we could, for the first time, identify the localization of the age-relevant encoded information and demonstrate 89% load-induced similarity of these locations in the loaded tibia with younger control bones. These data therefore suggest that our method enables identifying a general prognostic phenotype of a certain skeletal age as well as a temporal and localized loading-treatment effect on this apparent skeletal age for the studied mouse tibia and fibula. Future translational applications of this method may provide an improved decision-support method for osteoporosis treatment at relatively low cost. STATEMENT OF SIGNIFICANCE: Bone is a highly complex and dynamic structure that undergoes changes during the course of aging as well as in response to external stimuli, such as loading. Automatic assessment of "age" and "state" of the bone may lead to early prognosis of deceases such as osteoporosis and enables evaluating the effects of certain treatments. Here, we present an artificial intelligence-based method capable of automatically predicting the skeletal age from μCT images with 95% accuracy. Additionally, we utilize it to demonstrate the rejuvenation effects of in-vivo loading treatment on bones. We further, for the first time, break down aging-related local changes in bone by quantitatively analyzing "what the age assessment model has learned" and use this information to investigate the structural details of rejuvenation process. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1742-7061(20)30085-4  |  
------------------------------------------- 
10.1016/j.brainres.2020.146700  |   The central nervous system (CNS) has a limited auto-regeneration capacity, which makes it challenging for the development of new therapies. Previous studies from our lab have demonstrated the applicability of human bone marrow mesenchymal stem cells (hBM-MSCs) secretome as a possible therapeutic tool for CNS. Astrocytes, glial cells present in all brain regions, are important players in brain function through their vast influence in extracellular homeostasis, neuro-vascular regulation, synaptic modulation and neurogenesis. Thus, in the present work, we aimed to evaluate the specific impact of MSCs secretome on hippocampal proliferation and astrocyte morphology, in both WT and dnSNARE mice, a transgenic model that presents impaired astrocytic exocytosis and consequently impaired astrocytic function. Results demonstrated increased levels of proliferation for WT when treated with secretome. Additionally, it was possible to observe that dnSNARE animals injected with hBM-MSCs secretome disclosed increased levels of proliferating GFAP stained cells at the SGZ. Morphometrical evaluation found increased process hypertrophy and branching of dnSNARE astrocytes when treated with secretome. These results are closely related with the trophic factors present in the secretome, namely FGF-2, BDNF, GDNF, IGF-1, VEGF, CADH2, PEDF and miR-16. Moreover, the impaired exocytosis of astrocytes may also have implications for the response to the proliferative stimulus, given the established autocrine signaling through this mechanism. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0006-8993(20)30056-1  |  
------------------------------------------- 
10.3390/s20041199  |   Research on emotion recognition from facial expressions has found evidence of different muscle movements between genuine and posed smiles. To further confirm discrete movement intensities of each facial segment, we explored differences in facial expressions between spontaneous and posed smiles with three-dimensional facial landmarks. Advanced machine analysis was adopted to measure changes in the dynamics of 68 segmented facial regions. A total of 57 normal adults (19 men, 38 women) who displayed adequate posed and spontaneous facial expressions for happiness were included in the analyses. The results indicate that spontaneous smiles have higher intensities for upper face than lower face. On the other hand, posed smiles showed higher intensities in the lower part of the face. Furthermore, the 3D facial landmark technique revealed that the left eyebrow displayed stronger intensity during spontaneous smiles than the right eyebrow. These findings suggest a potential application of landmark based emotion recognition that spontaneous smiles can be distinguished from posed smiles via measuring relative intensities between the upper and lower face with a focus on left-sided asymmetry in the upper region. 
  |  http://www.mdpi.com/resolver?pii=s20041199  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32098261/  |  
------------------------------------------- 
10.1016/j.jpsychires.2020.03.013  |   Cognitive remediation (CR) is predicated on principles of neuroplasticity, but the actual molecular and neurocircuitry changes underlying cognitive change in individuals with impaired neuroplastic processes is poorly understood. The present study examined epigenetic-neurocircuitry-behavioral outcome measures in schizophrenia, before and after participating in a CR program that targeted higher-order cognitive functions. Outcome measures included DNA methylation of genes central to synaptic plasticity (CpG sites of Reelin promoter and BDNF promoter) from buccal swabs, resting-state functional brain connectivity and topological network efficiency, and global scores of a cognitive battery from 35 inpatients in a rehabilitative ward (18 CR, 17 non-CR) with similar premorbid IQ to 15 healthy controls. Baseline group differences between healthy controls and schizophrenia, group-by-time effects of CR in schizophrenia, and associations between the outcome measures were tested. Baseline functional connectivity abnormalities within the frontal, fronto-temporal and fronto-parietal regions, and trending decreases in global efficiency, but not DNA methylation, were found in schizophrenia; the frontal and fronto-temporal connectivity, and global efficiency correlated with global cognitive performance across all individuals. Notably, CR resulted in differential changes in Reelin promoter CpG methylation levels, altered within-frontal and fronto-temporal functional connectivity, increasing global efficiency and improving cognitive performance in schizophrenia, when compared to non-CR. In the CR inpatients, positive associations between the micro to macro measures: Reelin methylation changes, higher global efficiency and improving global cognitive performance were found. Present findings provide a neurobiological insight into potential CR-led epigenetics-neurocircuitry modifications driving cognitive plasticity. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-3956(19)31081-7  |  
------------------------------------------- 
10.1016/j.jcjo.2020.01.001  |    Objective:  The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. 
  Methods:  Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. 
  Results:  National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. 
  Conclusions:  Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45° image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0008-4182(19)31298-0  |  
------------------------------------------- 
10.3390/s20030639  |   Consumer-grade RGBD sensors that provide both colour and depth information have many potential applications, such as robotics control, localization, and mapping, due to their low cost and simple operation. However, the depth measurement provided by consumer-grade RGBD sensors is still inadequate for many high-precision applications, such as rich 3D reconstruction, accurate object recognition and precise localization, due to the fact that the systematic errors of RGB sensors increase exponentially with the ranging distance. Most existing calibration models for depth measurement must be carried out with different distances. In this paper, we reveal the mechanism of how an infrared (IR) camera and IR projector contribute to the overall non-centrosymmetric distortion of a structured light pattern-based RGBD sensor. Then, a new two-step calibration method for RGBD sensors based on the disparity measurement is proposed, which is range-independent and has full frame coverage. Three independent calibration models are used for the calibration for the three main components of the RGBD sensor errors: the infrared camera distortion, the infrared projection distortion, and the infrared cone-caused bias. Experiments show the proposed calibration method can provide precise calibration results in full-range and full-frame coverage of depth measurement. The offset in the edge area of long-range depth (8 m) is reduced from 86 cm to 30 cm, and the relative error is reduced from 11% to 3% of the range distance. Overall, at far range the proposed calibration method can improve the depth accuracy by 70% in the central region of depth frame and 65% in the edge region. 
  |  http://www.mdpi.com/resolver?pii=s20030639  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31979266/  |  
------------------------------------------- 
10.1016/j.artmed.2019.101763  |   The decompressive laminectomy is a common operation for treatment of lumbar spinal stenosis. The tools for grinding and drilling are used for fenestration and internal fixation, respectively. The state recognition is one of the main technologies in robot-assisted surgery, especially in tele-surgery, because surgeons have limited perception during remote-controlled robot-assisted surgery. The novelty of this paper is that a state recognition system is proposed for the robot-assisted tele-surgery. By combining the learning methods and traditional methods, the robot from the slave-end can think about the current operation state like a surgeon, and provide more information and decision suggestions to the master-end surgeon, which aids surgeons work safer in tele-surgery. For the fenestration, we propose an image-based state recognition method that consists a U-Net derived network, grayscale redistribution and dynamic receptive field assisting in controlling the grinding process to prevent the grinding-bit from crossing the inner edge of the lamina to damage the spinal nerves. For the internal fixation, we propose an audio and force-based state recognition method that consists signal features extraction methods, LSTM-based prediction and information fusion assisting in monitoring the drilling process to prevent the drilling-bit from crossing the outer edge of the vertebral pedicle to damage the spinal nerves. Several experiments are conducted to show the reliability of the proposed system in robot-assisted surgery. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(18)30530-X  |  
------------------------------------------- 
10.1016/j.compmedimag.2019.101685  |   We present the application of limited one-time sampling irregularity map (LOTS-IM): a fully automatic unsupervised approach to extract brain tissue irregularities in magnetic resonance images (MRI), for quantitatively assessing white matter hyperintensities (WMH) of presumed vascular origin, and multiple sclerosis (MS) lesions and their progression. LOTS-IM generates an irregularity map (IM) that represents all voxels as irregularity values with respect to the ones considered "normal". Unlike probability values, IM represents both regular and irregular regions in the brain based on the original MRI's texture information. We evaluated and compared the use of IM for WMH and MS lesions segmentation on T2-FLAIR MRI with the state-of-the-art unsupervised lesions' segmentation method, Lesion Growth Algorithm from the public toolbox Lesion Segmentation Toolbox (LST-LGA), with several well established conventional supervised machine learning schemes and with state-of-the-art supervised deep learning methods for WMH segmentation. In our experiments, LOTS-IM outperformed unsupervised method LST-LGA on WMH segmentation, both in performance and processing speed, thanks to the limited one-time sampling scheme and its implementation on GPU. Our method also outperformed supervised conventional machine learning algorithms (i.e., support vector machine (SVM) and random forest (RF)) and deep learning algorithms (i.e., deep Boltzmann machine (DBM) and convolutional encoder network (CEN)), while yielding comparable results to the convolutional neural network schemes that rank top of the algorithms developed up to date for this purpose (i.e., UResNet and UNet). LOTS-IM also performed well on MS lesions segmentation, performing similar to LST-LGA. On the other hand, the high sensitivity of IM on depicting signal change deems suitable for assessing MS progression, although care must be taken with signal changes not reflective of a true pathology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0895-6111(19)30100-4  |  
------------------------------------------- 
10.1073/pnas.1906364117  |   Researchers and policy makers worldwide are interested in measuring the subjective well-being of populations. When users post on social media, they leave behind digital traces that reflect their thoughts and feelings. Aggregation of such digital traces may make it possible to monitor well-being at large scale. However, social media-based methods need to be robust to regional effects if they are to produce reliable estimates. Using a sample of 1.53 billion geotagged English tweets, we provide a systematic evaluation of word-level and data-driven methods for text analysis for generating well-being estimates for 1,208 US counties. We compared Twitter-based county-level estimates with well-being measurements provided by the Gallup-Sharecare Well-Being Index survey through 1.73 million phone surveys. We find that word-level methods (e.g., Linguistic Inquiry and Word Count [LIWC] 2015 and Language Assessment by Mechanical Turk [LabMT]) yielded inconsistent county-level well-being measurements due to regional, cultural, and socioeconomic differences in language use. However, removing as few as three of the most frequent words led to notable improvements in well-being prediction. Data-driven methods provided robust estimates, approximating the Gallup data at up to <i>r</i> = 0.64. We show that the findings generalized to county socioeconomic and health outcomes and were robust when poststratifying the samples to be more representative of the general US population. Regional well-being estimation from social media data seems to be robust when supervised data-driven methods are used. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=32341156  |  
------------------------------------------- 
10.3390/polym12010163  |   Organic molecules and polymers have a broad range of applications in biomedical, chemical, and materials science fields. Traditional design approaches for organic molecules and polymers are mainly experimentally-driven, guided by experience, intuition, and conceptual insights. Though they have been successfully applied to discover many important materials, these methods are facing significant challenges due to the tremendous demand of new materials and vast design space of organic molecules and polymers. Accelerated and inverse materials design is an ideal solution to these challenges. With advancements in high-throughput computation, artificial intelligence (especially machining learning, ML), and the growth of materials databases, ML-assisted materials design is emerging as a promising tool to flourish breakthroughs in many areas of materials science and engineering. To date, using ML-assisted approaches, the quantitative structure property/activity relation for material property prediction can be established more accurately and efficiently. In addition, materials design can be revolutionized and accelerated much faster than ever, through ML-enabled molecular generation and inverse molecular design. In this perspective, we review the recent progresses in ML-guided design of organic molecules and polymers, highlight several successful examples, and examine future opportunities in biomedical, chemical, and materials science fields. We further discuss the relevant challenges to solve in order to fully realize the potential of ML-assisted materials design for organic molecules and polymers. In particular, this study summarizes publicly available materials databases, feature representations for organic molecules, open-source tools for feature generation, methods for molecular generation, and ML models for prediction of material properties, which serve as a tutorial for researchers who have little experience with ML before and want to apply ML for various applications. Last but not least, it draws insights into the current limitations of ML-guided design of organic molecules and polymers. We anticipate that ML-assisted materials design for organic molecules and polymers will be the driving force in the near future, to meet the tremendous demand of new materials with tailored properties in different fields. 
  |  http://www.mdpi.com/resolver?pii=polym12010163  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31936321/  |  
------------------------------------------- 
10.1002/advs.201902880  |   The distribution and abundance of immune cells, particularly T-cell subsets, play pivotal roles in cancer immunology and therapy. T cells have many subsets with specific function and current methods are limited in estimating them, thus, a method for predicting comprehensive T-cell subsets is urgently needed in cancer immunology research. Here, Immune Cell Abundance Identifier (ImmuCellAI), a gene set signature-based method, is introduced for precisely estimating the abundance of 24 immune cell types including 18 T-cell subsets, from gene expression data. Performance evaluation on both the sequencing data with flow cytometry results and public expression data indicate that ImmuCellAI can estimate the abundance of immune cells with superior accuracy to other methods especially on many T-cell subsets. Application of ImmuCellAI to immunotherapy datasets reveals that the abundance of dendritic cells, cytotoxic T, and gamma delta T cells is significantly higher both in comparisons of on-treatment versus pre-treatment and responders versus non-responders. Meanwhile, an ImmuCellAI result-based model is built for predicting the immunotherapy response with high accuracy (area under curve 0.80-0.91). These results demonstrate the powerful and unique function of ImmuCellAI in tumor immune infiltration estimation and immunotherapy response prediction. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274301/  |  
------------------------------------------- 
10.1080/13548506.2020.1746817  |   <b>Background</b>: As COVID-19 occurs suddenly and is highly contagious, this will inevitably cause people anxiety, depression, etc. The study on the public psychological states and its related factors during the COVID-19 outbreak is of practical significance.<b>Methods</b>: 600 valid questionnaires were received. The Self-Rating Anxiety Scale (SAS) and the Self-Rating Depression Scale (SDS) were used.<b>Results</b>: Females' anxiety risk was 3.01 times compared to males (95% <i>CI</i> 1.39-6.52). Compared with people below 40 years old, the anxiety risk of people above 40 years old was 0.40 times (95% <i>CI</i> 0.16-0.99). SDS results indicated that the difference between education level and occupation was statistically significant (<i>p</i> = 0.024, 0.005). Compared to people with a master's degree or above, those with a bachelor's degree group had a depression risk of 0.39 times (95% <i>CI</i> 0.17-0.87). Compared with professionals, industrial service workers and other staff had a depression risk of 0.31 times (95% <i>CI</i> 0.15-0.65) and 0.38 times (95% <i>CI</i> 0.15-0.93).<b>Conclusions:</b> 600 questionnaire participants were psychologically stable. Non-anxiety and non-depression rates were 93.67% and 82.83%, respectively. There were anxiety in 6.33% and depression in 17.17%. Therefore, we should pay attention to the psychological states of the public. 
  |  None  |  
------------------------------------------- 
10.1016/j.oret.2020.03.007  |    Purpose:  Though the domain of big data and artificial intelligence in health care continues to evolve, there is a lack of systemic methods to improve data quality and streamline the preparation process. To address this, we aimed to develop an automated sorting system (RetiSort) that accurately labels the type and laterality of retinal photographs. 
  Design:  Cross-sectional study. 
  Participants:  RetiSort was developed with retinal photographs from the Singapore Epidemiology of Eye Diseases (SEED) study. 
  Methods:  The development of RetiSort was composed of 3 steps: 2 deep-learning (DL) algorithms and 1 rule-based classifier. For step 1, a DL algorithm was developed to locate the optic disc, the "landmark feature." For step 2, based on the location of the optic disc derived from step 1, a rule-based classifier was developed to sort retinal photographs into 3 types: macular-centered, optic disc-centered, or related to other fields. Step 2 concurrently distinguished laterality (i.e., the left or right eye) of macular-centered photographs. For step 3, an additional DL algorithm was developed to differentiate the laterality of disc-centered photographs. Via the 3 steps, RetiSort sorted and labeled retinal images into (1) right macular-centered, (2) left macular-centered, (3) right optic disc-centered, (4) left optic disc-centered, and (5) images relating to other fields. Subsequently, the accuracy of RetiSort was evaluated on 5000 randomly selected retinal images from SEED as well as on 3 publicly available image databases (DIARETDB0, HEI-MED, and Drishti-GS). The main outcome measure was the accuracy for sorting of retinal photographs. 
  Results:  RetiSort mislabeled 48 out of 5000 retinal images from SEED, representing an overall accuracy of 99.0% (95% confidence interval [CI], 98.7-99.3). In external tests, RetiSort mislabeled 1, 0, and 2 images, respectively, from DIARETDB0, HEI-MED, and Drishti-GS, representing an accuracy of 99.2% (95% CI, 95.8-99.9), 100%, and 98.0% (95% CI, 93.1-99.8), respectively. Saliency maps consistently showed that the DL algorithm in step 3 required pixels in the central left lateral border and optic disc of optic disc-centered retinal photographs to differentiate the laterality. 
  Conclusions:  RetiSort is a highly accurate automated sorting system. It can aid in data preparation and has practical applications in DL research that uses retinal photographs. 
  |  None  |  
------------------------------------------- 
10.3389/fonc.2020.00312  |   <b>Purpose:</b> Following radical prostatectomy, prostate bed radiotherapy (PBRT) has been combined with either long-term androgen deprivation therapy (LT-ADT) or short-term ADT with pelvic lymph node radiotherapy (PLNRT) to provide an oncological benefit in randomized trials. McGill 0913 was designed to characterize the efficacy of combining PBRT, PLNRT, and LT-ADT. It is the first study to do so prospectively. <b>Methods:</b> In a single arm phase II trial conduced from 2010 to 2016, 46 post-prostatectomy prostate cancer patients at a high-risk for relapse (pathological Gleason 8+ or T3) were assessed for treatment with combined LT-ADT (24 months), PBRT, and PLNRT. Patients received PLNRT and PBRT (44 Gy in 22 fractions) followed by a PBRT boost (22 Gy in 11 fractions). The primary endpoint was progression-free survival (PFS). Toxicity and quality of life (QoL) were evaluated using CTCAE V3.0 and EQ-5D-3L questionnaires, respectively. <b>Results:</b> Among the 43 patients were treated as per protocol, median PSA was 0.30 μg/L. On surgical pathology, 51% had positive margins, 40% had Gleason 8+ disease, 42% had seminal vesicle involvement, and 19% had lymph node involvement. At a median follow-up of 5.2 years, there were no deaths or clinical progression. At 5 years, PFS was 78.0% (95% Confidence Interval 63.7-95.5%). Not including erectile dysfunction, patients experienced: 14% grade 2 endocrine toxicity while on ADT, one incident of long-term gynecomastia, 5% grade 2 acute urinary toxicity, 5% grade 2 late Urinary toxicity, and 24% long-term hypogonadism. No comparison between the average or minimum self-reported QoL at baseline, during ADT, nor after ADT demonstrated a statistically significant difference. <b>Conclusions:</b> Combining PBRT, PLNRT, and LT-ADT had an acceptable PFS in patients with significant post-operative risk factors for recurrence. While therapy was well-tolerated, long-term hypogonadism was a substantial risk. Further investigations are needed to determine if this combination is beneficial. <b>Trial registration:</b> <a href="http://clinicaltrials.gov/show/NCT01255891" title="See in ClinicalTrials.gov">NCT01255891</a>. 
  |  https://doi.org/10.3389/fonc.2020.00312  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32226774/  |  
------------------------------------------- 
10.3389/fmed.2020.00099  |   <b>Background:</b> Cardiac sympathetic response (CSR) and malnutrition-inflammation syndrome (MIS) score are validated assessment tools for patients' health condition. We aim to evaluate the joint effect of CSR and MIS on all-cause and cardiovascular (CV) mortality in patients with hemodialysis (HD). <b>Methods:</b> Changes in normalized low frequency (ΔnLF) during HD were utilized for quantification of CSR. Unadjusted and adjusted hazard ratios (aHRs) of mortality risks were analyzed in different groups of ΔnLF and MIS score. <b>Results:</b> In multivariate analysis, higher ΔnLF was related to all-cause, CV and sudden cardiac deaths [aHR: 0.78 (95% confidence interval (CI): 0.72-0.85), 0.78 (95% CI: 0.70-0.87), and 0.74 (95% CI: 0.63-0.87), respectively]. Higher MIS score was associated with incremental risks of all-cause, CV and sudden cardiac deaths [aHR: 1.36 (95% CI: 1.13-1.63), 1.33 (95% CI: 1.06 - 1.38), and 1.50 (95% CI: 1.07-2.11), respectively]. Patients with combined lower ΔnLF (≤6.8 nu) and higher MIS score were at the greatest risk of all-cause and CV mortality [aHR: 5.64 (95% CI: 1.14-18.09) and 5.86 (95% CI: 1.64-13.65), respectively]. <b>Conclusion:</b> Our data indicate a joint evaluation of CSR and MIS score to identify patients at high risk of death is more comprehensive and convincing. Considering the extremely high prevalence of cardiac autonomic neuropathy and malnutrition-inflammation cachexia in HD population, a non-invasive monitoring system composed of CSR analyzer and MIS score calculator should be developed in the artificial intelligence-based prediction of clinical events. 
  |  https://doi.org/10.3389/fmed.2020.00099  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32292788/  |  
------------------------------------------- 
10.1097/RTI.0000000000000491  |    Purpose:  The purpose of this study was to evaluate the accuracy of a novel fully automated deep learning (DL) algorithm implementing a recurrent neural network (RNN) with long short-term memory (LSTM) for the detection of coronary artery calcium (CAC) from coronary computed tomography angiography (CCTA) data. 
  Materials and methods:  Under an IRB waiver and in HIPAA compliance, a total of 194 patients who had undergone CCTA were retrospectively included. Two observers independently evaluated the image quality and recorded the presence of CAC in the right (RCA), the combination of left main and left anterior descending (LM-LAD), and left circumflex (LCx) coronary arteries. Noncontrast CACS scans were allowed to be used in cases of uncertainty. Heart and coronary artery centerline detection and labeling were automatically performed. Presence of CAC was assessed by a RNN-LSTM. The algorithm's overall and per-vessel sensitivity, specificity, and diagnostic accuracy were calculated. 
  Results:  CAC was absent in 84 and present in 110 patients. As regards CCTA, the median subjective image quality, signal-to-noise ratio, and contrast-to-noise ratio were 3.0, 13.0, and 11.4. A total of 565 vessels were evaluated. On a per-vessel basis, the algorithm achieved a sensitivity, specificity, and diagnostic accuracy of 93.1% (confidence interval [CI], 84.3%-96.7%), 82.76% (CI, 74.6%-89.4%), and 86.7% (CI, 76.8%-87.9%), respectively, for the RCA, 93.1% (CI, 86.4%-97.7%), 95.5% (CI, 88.77%-98.75%), and 94.2% (CI. 90.2%-94.6%), respectively, for the LM-LAD, and 89.9% (CI, 80.2%-95.8%), 90.0% (CI, 83.2%-94.7%), and 89.9% (CI, 85.0%-94.1%), respectively, for the LCx. The overall sensitivity, specificity, and diagnostic accuracy were 92.1% (CI, 92.1%-95.2%), 88.9% (CI. 84.9%-92.1%), and 90.3% (CI, 88.0%-90.0%), respectively. When accounting for image quality, the algorithm achieved a sensitivity, specificity, and diagnostic accuracy of 76.2%, 87.5%, and 82.2%, respectively, for poor-quality data sets and 93.3%, 89.2% and 90.9%, respectively, when data sets rated adequate or higher were combined. 
  Conclusion:  The proposed RNN-LSTM demonstrated high diagnostic accuracy for the detection of CAC from CCTA. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000491  |  
------------------------------------------- 
10.1080/13880209.2020.1732429  |   <b>Context:</b> It is common sense that chewing a mint leaf can cause a cooling feeling, while chewing ginger root will produce a burning feeling. In Traditional Chinese Medicine (TCM), this phenomenon is referred to as 'cold/hot' properties of herbs. Herein, it is reported that TCM with different "cold/hot" properties have different effects on the variation of cells.<b>Objective:</b> To explore the intrinsic 'cold/hot' properties of TCM from the perspective of cellular and molecular biology.<b>Materials and methods:</b> A375 cells were selected using Cancer Cell Line Encyclopaedia (CCLE) analysis and western blots. Hypaconitine and baicalin were selected by structural similarity analysis from 56 and 140 compounds, respectively. A wireless thermometry system was used to measure cellular temperature change induced by different compounds. Alteration of intracellular calcium influx was investigated by means of calcium imaging.<b>Results:</b> The IC<sub>50</sub> values of GSK1016790A, HC067047, hypaconitine, and baicalin for A375 cells are 8.363 nM, 816.4 μM, 286.4 μM and 29.84 μM, respectively. And, 8 μM hypaconitine induced obvious calcium influx while 8 μM baicalin inhibited calcium influx induced by TRPV4 activation. Cellular temperature elevated significantly when treated with GSK1016790A or hypaconitine, while the results were reversed when cells were treated with HC067047 or baicalin.<b>Discussion and conclusions:</b> The changes in cellular temperature are speculated to be caused by the alteration of intracellular calcium influx mediated by TRPV4. In addition, the 'cold/hot' properties of compounds in TCM can be classified by using cellular temperature detection. 
  |  http://www.tandfonline.com/doi/full/10.1080/13880209.2020.1732429  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32114881/  |  
------------------------------------------- 
10.1007/s00330-020-06676-1  |    Objective:  The progression of white matter hyperintensities (WMH) varies considerably in adults. In this study, we aimed to predict the progression and related risk factors of WMH based on the radiomics of whole-brain white matter (WBWM). 
  Methods:  A retrospective analysis was conducted on 141 patients with WMH who underwent two consecutive brain magnetic resonance (MR) imaging sessions from March 2014 to May 2018. The WBWM was segmented to extract and score the radiomics features at baseline. Follow-up images were evaluated using the modified Fazekas scale, with progression indicated by scores ≥ 1. Patients were divided into progressive (n = 65) and non-progressive (n = 76) groups. The progressive group was subdivided into any WMH (AWMH), periventricular WMH (PWMH), and deep WMH (DWMH). Independent risk factors were identified using logistic regression. 
  Results:  The area under the curve (AUC) values for the radiomics signatures of the training sets were 0.758, 0.749, and 0.775 for AWMH, PWMH, and DWMH, respectively. The AUC values of the validation set were 0.714, 0.697, and 0.717, respectively. Age and hyperlipidemia were independent predictors of progression for AWMH. Age and body mass index (BMI) were independent predictors of progression for DWMH, while hyperlipidemia was an independent predictor of progression for PWMH. After combining clinical factors and radiomics signatures, the AUC values were 0.848, 0.863, and 0.861, respectively, for the training set, and 0.824, 0.818, and 0.833, respectively, for the validation set. 
  Conclusions:  MRI-based radiomics of WBWM, along with specific risk factors, may allow physicians to predict the progression of WMH. 
  Key points:  • Radiomics features detected by magnetic resonance imaging may be used to predict the progression of white matter hyperintensities. • Radiomics may be used to identify risk factors associated with the progression of white matter hyperintensities. • Radiomics may serve as non-invasive biomarkers to monitor white matter status. 
  |  https://dx.doi.org/10.1007/s00330-020-06676-1  |  
------------------------------------------- 
10.1200/JCO.19.02322  |    Purpose:  National Cancer Institute (NCI)-sponsored clinical trial network studies frequently require biopsy specimens for pharmacodynamic and molecular biomarker analyses, including paired pre- and post-treatment samples. The purpose of this meeting of NCI-sponsored investigators was to identify local institutional standard procedures found to ensure quantitative and qualitative specimen adequacy. 
  Methods:  NCI convened a conference on best biopsy practices, focusing on the clinical research community. Topics discussed were (1) criteria for specimen adequacy in the personalized medicine era, (2) team-based approaches to ensure specimen adequacy and quality control, and (3) risk considerations relevant to academic and community practitioners and their patients. 
  Results and recommendations:  Key recommendations from the convened consensus panel included (1) establishment of infrastructure for multidisciplinary biopsy teams with a formalized information capture process, (2) maintenance of standard operating procedures with regular team review, (3) optimization of tissue collection and yield methodology, (4) incorporation of needle aspiration and other newer techniques, and (5) commitment of stakeholders to use of guideline documents to increase awareness of best biopsy practices, with the goal of universally improving tumor biopsy practices. 
  |  http://ascopubs.org/doi/full/10.1200/JCO.19.02322?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.ijcard.2019.12.069  |   Some cohort studies showed the possibility of renin-angiotensin-aldosterone system (RAAS) blockade in preventing future osteoporotic fractures. The study aimed to evaluate the association between angiotensin converting enzyme inhibitors (ACEIs), angiotensin II receptor blockers (ARBs), and future osteoporotic fracture in a hypertensive population. We queried the Taiwan Longitudinal Health Insurance Database between 2001 and 2012. We used propensity score matching and the total cohort was made up of 57,470 participants (28,735 matched-pairs using or not using RAAS blockers). The mean follow-up period was 6 years. The number of incident fractures was 3757. Hazard ratios (HRs) [95% confidence interval (CI)] of ACEIs and ARBs use with incident fractures were calculated. The incidence of future osteoporotic fracture was significantly lower in the ACEI and ARB user groups but not in the group using an ACEI plus ARB concomitantly, when compared with RAAS blocker nonusers. Comparing ACEI users with RAAS blocker non-users and ARB users with RAAS blocker non-users, the HRs for composite fractures were 0.70 (0.62-0.79) and 0.58 (0.51-0.65), respectively. Sensitivity analysis confirmed a lower incidence of future osteoporotic fracture in patients taking an ACEI for &gt;55 cumulative defined daily doses (cDDDs) and those who received an ARB for &gt;90 cDDDs. These results suggested a lower incidence of future osteoporotic fracture in a hypertensive population who were using an ACEI or ARB compared with RAAS blocker nonusers but not in the group taking an ACEI and ARB concomitantly. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0167-5273(19)32471-4  |  
------------------------------------------- 
10.1161/JAHA.119.014840  |   Background Recommendations have not yet been established for statin therapy in patients on maintenance dialysis. In this study, we aimed to evaluate the effects of statin therapy on all-cause mortality in patients undergoing maintenance hemodialysis. Methods and Results This retrospective cohort study analyzed data from adults, aged ≥30 years, who were on maintenance hemodialysis for end-stage renal disease. Data on statin use, along with other clinical information between 2007 and 2017, were extracted from the Health Insurance Review and Assessment Service database in Korea. In total, 65 404 patients were included, and 41 549 (73.2%) patients had received statin therapy for a mean duration of 3.6±2.6 years. Compared with statin nonusers before and after the initiation of hemodialysis (entry), patients who initiated statin therapy after entry and patients who continued statins from the pre-end-stage renal disease to post-end-stage renal disease period had a lower risk of all-cause mortality; the adjusted hazard ratios (95% CIs) were 0.48 (0.47-0.50; <i>P</i>&lt;0.001) for post-end-stage renal disease only statin users and 0.59 (0.57-0.60; <i>P</i>&lt;0.001) for continuous statin users. However, those discontinuing statins before or at entry showed a higher risk of all-cause mortality. Statin-ezetimibe combinations were associated with better survival benefits than fixed patterns of statin therapy. These results were consistent across various subgroups, including elderly patients aged &gt;75 years, and were maintained even after propensity score matching. Conclusions Our results showed that in adult patients undergoing maintenance hemodialysis, statin therapy, preferably combined with ezetimibe, was associated with a lower risk of all-cause mortality. 
  |  http://www.ahajournals.org/doi/full/10.1161/JAHA.119.014840?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s10038-020-0754-6  |   Chromothripsis is a type of chaotic complex genomic rearrangement caused by a single event of chromosomal shattering and repair processes. Chromothripsis is known to cause rare congenital diseases when it occurs in germline cells, however, current genome analysis technologies have difficulty in detecting and deciphering chromothripsis. It is possible that this type of complex rearrangement may be overlooked in rare-disease patients whose genetic diagnosis is unsolved. We applied long read nanopore sequencing and our recently developed analysis pipeline dnarrange to a patient who has a reciprocal chromosomal translocation t(8;18)(q22;q21) as a result of chromothripsis between the two chromosomes, and fully characterize the complex rearrangements at the translocation site. The patient genome was evidently shattered into 19 fragments, and rejoined into derivative chromosomes in a random order and orientation. The reconstructed patient genome indicates loss of five genomic regions, which all overlap with microarray-detected copy number losses. We found that two disease-related genes RAD21 and EXT1 were lost by chromothripsis. These two genes could fully explain the disease phenotype with facial dysmorphisms and bone abnormality, which is likely a contiguous gene syndrome, Cornelia de Lange syndrome type IV (CdLs-4) and atypical Langer-Giedion syndrome (LGS), also known as trichorhinophalangeal syndrome type II (TRPSII). This provides evidence that our approach based on long read sequencing can fully characterize chromothripsis in a patient's genome, which is important for understanding the phenotype of disease caused by complex genomic rearrangement. 
  |  http://dx.doi.org/10.1038/s10038-020-0754-6  |  
------------------------------------------- 
10.3390/jcm9041206  |   We collated publicly available single-cell expression profiles of circulating tumor cells (CTCs) and showed that CTCs across cancers lie on a near-perfect continuum of epithelial to mesenchymal (EMT) transition. Integrative analysis of CTC transcriptomes also highlighted the inverse gene expression pattern between PD-L1 and MHC, which is implicated in cancer immunotherapy. We used the CTCs expression profiles in tandem with publicly available peripheral blood mononuclear cell (PBMC) transcriptomes to train a classifier that accurately recognizes CTCs of diverse phenotype. Further, we used this classifier to validate circulating breast tumor cells captured using a newly developed microfluidic system for label-free enrichment of CTCs. 
  |  http://www.mdpi.com/resolver?pii=jcm9041206  |  
------------------------------------------- 
10.21037/jtd.2020.02.64  |    Background:  The coronavirus disease 2019 (COVID-19) outbreak originating in Wuhan, Hubei province, China, coincided with <i>chunyun</i>, the period of mass migration for the annual Spring Festival. To contain its spread, China adopted unprecedented nationwide interventions on January 23 2020. These policies included large-scale quarantine, strict controls on travel and extensive monitoring of suspected cases. However, it is unknown whether these policies have had an impact on the epidemic. We sought to show how these control measures impacted the containment of the epidemic. 
  Methods:  We integrated population migration data before and after January 23 and most updated COVID-19 epidemiological data into the Susceptible-Exposed-Infectious-Removed (SEIR) model to derive the epidemic curve. We also used an artificial intelligence (AI) approach, trained on the 2003 SARS data, to predict the epidemic. 
  Results:  We found that the epidemic of China should peak by late February, showing gradual decline by end of April. A five-day delay in implementation would have increased epidemic size in mainland China three-fold. Lifting the Hubei quarantine would lead to a second epidemic peak in Hubei province in mid-March and extend the epidemic to late April, a result corroborated by the machine learning prediction. 
  Conclusions:  Our dynamic SEIR model was effective in predicting the COVID-19 epidemic peaks and sizes. The implementation of control measures on January 23 2020 was indispensable in reducing the eventual COVID-19 epidemic size. 
  |  https://doi.org/10.21037/jtd.2020.02.64  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274081/  |  
------------------------------------------- 
10.1038/s41467-019-14052-x  |   Many primary tumours have low levels of molecular oxygen (hypoxia), and hypoxic tumours respond poorly to therapy. Pan-cancer molecular hallmarks of tumour hypoxia remain poorly understood, with limited comprehension of its associations with specific mutational processes, non-coding driver genes and evolutionary features. Here, as part of the ICGC/TCGA Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium, which aggregated whole genome sequencing data from 2658 cancers across 38 tumour types, we quantify hypoxia in 1188 tumours spanning 27 cancer types. Elevated hypoxia associates with increased mutational load across cancer types, irrespective of underlying mutational class. The proportion of mutations attributed to several mutational signatures of unknown aetiology directly associates with the level of hypoxia, suggesting underlying mutational processes for these signatures. At the gene level, driver mutations in TP53, MYC and PTEN are enriched in hypoxic tumours, and mutations in PTEN interact with hypoxia to direct tumour evolutionary trajectories. Overall, hypoxia plays a critical role in shaping the genomic and evolutionary landscapes of cancer. 
  |  http://dx.doi.org/10.1038/s41467-019-14052-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32024819/  |  
------------------------------------------- 
10.1126/sciadv.aaz1170  |   In human and nonhuman primates, sex differences typically explain much interindividual variability. Male and female behaviors may have played unique roles in the likely coevolution of increasing brain volume and more complex social dynamics. To explore possible divergence in social brain morphology between men and women living in different social environments, we applied probabilistic generative modeling to ~10,000 UK Biobank participants. We observed strong volume effects especially in the limbic system but also in regions of the sensory, intermediate, and higher association networks. Sex-specific brain volume effects in the limbic system were linked to the frequency and intensity of social contact, such as indexed by loneliness, household size, and social support. Across the processing hierarchy of neural networks, different conditions for social interplay may resonate in and be influenced by brain anatomy in sex-dependent ways. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32206722/  |  
------------------------------------------- 
10.1371/journal.pone.0227707  |   Epithelial ovarian cancer (OC) is the most deadly cancer of the female reproductive system. To date, there is no effective screening method for early detection of OC and current diagnostic armamentarium may include sonographic grading of the tumor and analyzing serum levels of tumor markers, Cancer Antigen 125 (CA-125) and Human epididymis protein 4 (HE4). Microorganisms (bacterial, archaeal, and fungal cells) residing in mucosal tissues including the gastrointestinal and urogenital tracts can be altered by different disease states, and these shifts in microbial dynamics may help to diagnose disease states. We hypothesized that the peritoneal microbial environment was altered in patients with OC and that inclusion of selected peritoneal microbial features with current clinical features into prediction analyses will improve detection accuracy of patients with OC. Blood and peritoneal fluid were collected from consented patients that had sonography confirmed adnexal masses and were being seen at SIU School of Medicine Simmons Cancer Institute. Blood was processed and serum HE4 and CA-125 were measured. Peritoneal fluid was collected at the time of surgery and processed for Next Generation Sequencing (NGS) using 16S V4 exon bacterial primers and bioinformatics analyses. We found that patients with OC had a unique peritoneal microbial profile compared to patients with a benign mass. Using ensemble modeling and machine learning pathways, we identified 18 microbial features that were highly specific to OC pathology. Prediction analyses confirmed that inclusion of microbial features with serum tumor marker levels and control features (patient age and BMI) improved diagnostic accuracy compared to currently used models. We conclude that OC pathogenesis alters the peritoneal microbial environment and that these unique microbial features are important for accurate diagnosis of OC. Our study warrants further analyses of the importance of microbial features in regards to oncological diagnostics and possible prognostic and interventional medicine. 
  |  http://dx.plos.org/10.1371/journal.pone.0227707  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31917801/  |  
------------------------------------------- 
10.1002/jmri.26813  |    Background:  White matter hyperintensity (WMH) is widely observed in aging brain and is associated with various diseases. A pragmatic and handy method in the clinic to assess and follow up white matter disease is strongly in need. 
  Purpose:  To develop and validate a radiomics nomogram for the prediction of WMH progression. 
  Study type:  Retrospective. 
  Population:  Brain images of 193 WMH patients from the Picture Archiving and Communication Systems (PACS) database in the A Medical Center (Zhejiang Provincial People's Hospital). MRI data of 127 WMH patients from the PACS database in the B Medical Center (Zhejiang Lishui People's Hospital) were included for external validation. All of the patients were at least 60 years old. 
  Field strength/sequence:  T<sub>1</sub> -fluid attenuated inversion recovery images were acquired using a 3T scanner. 
  Assessment:  WMH was evaluated utilizing the Fazekas scale based on MRI. WMH progression was assessed with a follow-up MRI using a visual rating scale. Three neuroradiologists, who were blinded to the clinical data, assessed the images independently. Moreover, interobserver and intraobserver reproducibility were performed for the regions of interest for segmentation and feature extraction. 
  Statistical tests:  A receiver operating characteristic (ROC) curve, the area under the curve (AUC) of the ROC was calculated, along with sensitivity and specificity. Also, a Hosmer-Lemeshow test was performed. 
  Results:  The AUC of radiomics signature in the primary, internal validation cohort, external validation cohort were 0.886, 0.816, and 0.787, respectively; the specificity were 71.79%, 72.22%, and 81%, respectively; the sensitivity were 92.68%, 87.94% and 78.3%, respectively. The radiomics nomogram in the primary cohort (AUC = 0.899) and the internal validation cohort (AUC = 0.84). The Hosmer-Lemeshow test showed no significant difference between the primary cohort and the internal validation cohort (P &gt; 0.05). The AUC of the radiomics nomogram, radiomics signature, and hyperlipidemia in all patients from the primary and internal validation cohort was 0.878, 0.848, and 0.626, respectively. 
  Data conclusion:  This multicenter study demonstrated the use of a radiomics nomogram in predicting the progression of WMH with elderly adults (an age of at least 60 years) based on conventional MRI. 
  Level of evidence:  3 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:535-546. 
  |  https://doi.org/10.1002/jmri.26813  |  
------------------------------------------- 
10.1089/thy.2019.0780  |   <b><i>Background:</i></b> Accurate preoperative prediction of cervical lymph node (LN) metastasis in patients with papillary thyroid carcinoma (PTC) provides a basis for surgical decision-making and the extent of tumor resection. This study aimed to develop and validate an ultrasound radiomics nomogram for the preoperative assessment of LN status. <b><i>Methods:</i></b> Data from 147 PTC patients at the Wuhan Tongji Hospital and 90 cases at the Hunan Provincial Tumor Hospital between January 2017 and September 2019 were included in our study. They were grouped as the training and external validation set. Radiomics features were extracted from shear-wave elastography (SWE) images and corresponding B-mode ultrasound (BMUS) images. Then, the minimum redundancy maximum relevance algorithm and the least absolute shrinkage and selection operator regression were used to select LN status-related features and construct the SWE and BMUS radiomics score (Rad-score). Multivariate logistic regression was performed using the two radiomics scores together with clinical data, and a nomogram was subsequently developed. The performance of the nomogram was assessed with respect to discrimination, calibration, and clinical usefulness in the training and external validation set. <b><i>Results:</i></b> Both the SWE and BMUS Rad-scores were significantly higher in patients with cervical LN metastasis. Multivariate analysis indicated that the SWE Rad-scores, multifocality, and ultrasound (US)-reported LN status were independent risk factors associated with LN status. The radiomics nomogram, which incorporated the three variables, showed good calibration and discrimination in the training set (area under the receiver operator characteristic curve [AUC] 0.851 [CI 0.791-0.912]) and the validation set (AUC 0.832 [CI 0.749-0.916]). The significantly improved net reclassification improvement and index-integrated discrimination improvement demonstrated that SWE radiomics signature was a very useful marker to predict the LN metastasis in PTC. Decision curve analysis indicated that the SWE radiomics nomogram was clinically useful. Furthermore, the nomogram also showed favorable discriminatory efficacy in the US-reported LN-negative (cN0) subgroup (AUC 0.812 [CI 0.745-0.860]). <b><i>Conclusions:</i></b> The presented radiomics nomogram, which is based on the SWE radiomics signature, shows a favorable predictive value for LN staging in patients with PTC. 
  |  https://www.liebertpub.com/doi/full/10.1089/thy.2019.0780?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3389/fonc.2020.00057  |   <b>Objectives:</b> To identify a computed tomography (CT)-based radiomic signature for predicting progression-free survival (PFS) in stage IV anaplastic lymphoma kinase (<i>ALK</i>)-positive non-small-cell lung cancer (NSCLC) patients treated with tyrosine kinase inhibitor (TKI) crizotinib. <b>Materials and Methods:</b> This retrospective proof-of-concept study included a cohort of 63 stage IV <i>ALK</i>-positive NSCLC patients who had received TKI crizotinib therapy for model construction and validation. Another independent cohort including 105 stage IV <i>EGFR</i>-positive NSCLC patients was also used for external validation in <i>EGFR</i>-TKI treatment. We initially extracted 481 quantitative three-dimensional features derived from manually segmented tumor volumes of interest. Pearson's correlation analysis along with the least absolute shrinkage and selection operator (LASSO) penalized Cox proportional hazards regression was successively performed to select critical radiomic features. A CT-based radiomic signature for PFS prediction was obtained using multivariate Cox regression. The performance evaluation of the radiomic signature was conducted using the concordance index (C-index), time-dependent receiver operating characteristic (ROC) analysis, and Kaplan-Meier survival analysis. <b>Results:</b> A radiomic signature containing three features showed significant prognostic performance for <i>ALK</i>-positive NSCLC patients in both the training cohort (C-index, 0.744; time-dependent AUC, 0.895) and the validation cohort (C-index, 0.717; time-dependent AUC, 0.824). The radiomic signature could significantly risk-stratify <i>ALK</i>-positive NSCLC patients (hazard ratio, 2.181; <i>P</i> &lt; 0.001) and outperformed other prognostic factors. However, no significant association with PFS was captured for the radiomic signature in the <i>EGFR</i>-positive NSCLC cohort (log-rank tests, <i>P</i> = 0.41). <b>Conclusions:</b> The CT-based radiomic features can capture valuable information regarding the tumor phenotype. The proposed radiomic signature was found to be an effective prognostic factor in stage IV <i>ALK</i> mutated nonsynchronous nodules in NSCLC patients treated with a TKI. 
  |  https://doi.org/10.3389/fonc.2020.00057  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32133282/  |  
------------------------------------------- 
10.1371/journal.pone.0229186  |   Acute coronary syndrome (ACS) patients with diabetes have significantly worse cardiovascular outcomes than those without diabetes. This study aimed to compare the performance of The Thrombolysis In Myocardial Infarction (TIMI), Global Registry of Acute Coronary Events (GRACE), Primary Angioplasty in Myocardial Infarction (PAMI), and Controlled Abciximab and Device Investigation to Lower Late Angioplasty Complications (CADILLAC) risk scores in predicting long-term cardiovascular outcomes in diabetic patients with ST-segment elevation myocardial infarction (STEMI). From the Acute Coronary Syndrome-Diabetes Mellitus Registry of the Taiwan Society of Cardiology, patients with STEMI were included. The TIMI, GRACE, PAMI, and CADILLAC risk scores were calculated. The discriminative potential of risk scores was analyzed using the area under the receiver-operating characteristics curve (AUC). In the 455 patients included, all four risk score systems demonstrated predictive accuracy for 6-, 12- and 24-month mortality with AUC values of 0.67-0.82. The CADILLAC score had the best discriminative accuracy, with an AUC of 0.8207 (p&lt;0.0001), 0.8210 (p&lt;0.0001), and 0.8192 (p&lt;0.0001) for 6-, 12-, and 24-month mortality, respectively. It also had the best predictive value for bleeding and acute renal failure, with an AUC of 0.7919 (p&lt;0.05) and 0.9764 (p&lt;0.0001), respectively. Patients with CADILLAC risk scores &gt;8 had poorer 2-year survival than those with lower scores (log-rank p&lt;0.0001). In conclusion, the CADILLAC risk score is more effective than other risk scores in predicting 6-month, 1-year, and 2-year all-cause mortality in diabetic patients with STEMI. It also had the best predictive value for in-hospital bleeding and acute renal failure. 
  |  http://dx.plos.org/10.1371/journal.pone.0229186  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32053694/  |  
------------------------------------------- 
10.1002/adma.201907801  |   Fundamental advances to increase the efficiency as well as stability of organic photovoltaics (OPVs) are achieved by designing ternary blends, which represents a clear trend toward multicomponent active layer blends. The development of high-throughput and autonomous experimentation methods is reported for the effective optimization of multicomponent polymer blends for OPVs. A method for automated film formation enabling the fabrication of up to 6048 films per day is introduced. Equipping this automated experimentation platform with a Bayesian optimization, a self-driving laboratory is constructed that autonomously evaluates measurements to design and execute the next experiments. To demonstrate the potential of these methods, a 4D parameter space of quaternary OPV blends is mapped and optimized for photostability. While with conventional approaches, roughly 100 mg of material would be necessary, the robot-based platform can screen 2000 combinations with less than 10 mg, and machine-learning-enabled autonomous experimentation identifies stable compositions with less than 1 mg. 
  |  https://doi.org/10.1002/adma.201907801  |  
------------------------------------------- 
10.3390/ijms21082839  |   Since the infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was reported in China during December 2019, the coronavirus disease 2019 (COVID-19) has spread on a global scale, causing the World Health Organization (WHO) to issue a warning. While novel vaccines and drugs that target SARS-CoV-2 are under development, this review provides information on therapeutics which are under clinical trials or are proposed to antagonize SARS-CoV-2. Based on the information gained from the responses to other RNA coronaviruses, including the strains that cause severe acute respiratory syndrome (SARS)-coronaviruses and Middle East respiratory syndrome (MERS), drug repurposing might be a viable strategy. Since several antiviral therapies can inhibit viral replication cycles or relieve symptoms, mechanisms unique to RNA viruses will be important for the clinical development of antivirals against SARS-CoV-2. Given that several currently marketed drugs may be efficient therapeutic agents for severe COVID-19 cases, they may be beneficial for future viral pandemics and other infections caused by RNA viruses when standard treatments are unavailable. 
  |  http://www.mdpi.com/resolver?pii=ijms21082839  |  
------------------------------------------- 
10.1186/s13014-020-01502-w  |    Background:  Current chemoradiation regimens for locally advanced cervical cancer are fairly uniform despite a profound diversity of treatment response and recurrence patterns. The wide range of treatment responses and prognoses to standardized concurrent chemoradiation highlights the need for a reliable tool to predict treatment outcomes. We investigated pretreatment magnetic resonance (MR) imaging features of primary tumor and involved lymph node for predicting clinical outcome in cervical cancer patients. 
  Methods:  We included 93 node-positive cervical cancer patients treated with definitive chemoradiotherapy at our institution between 2006 and 2017. The median follow-up period was 38 months (range, 5-128). Primary tumor and involved lymph node were manually segmented on axial gadolinium-enhanced T1-weighted images as well as T2-weighted images and saved as 3-dimensional regions of interest (ROI). After the segmentation, imaging features related to histogram, shape, and texture were extracted from each ROI. Using these features, random survival forest (RSF) models were built to predict local control (LC), regional control (RC), distant metastasis-free survival (DMFS), and overall survival (OS) in the training dataset (n = 62). The generated models were then tested in the validation dataset (n = 31). 
  Results:  For predicting LC, models generated from primary tumor imaging features showed better predictive performance (C-index, 0.72) than those from lymph node features (C-index, 0.62). In contrast, models from lymph nodes showed superior performance for predicting RC, DMFS, and OS compared to models of the primary tumor. According to the 3-year time-dependent receiver operating characteristic analysis of LC, RC, DMFS, and OS prediction, the respective area under the curve values for the predicted risk of the models generated from the training dataset were 0.634, 0.796, 0.733, and 0.749 in the validation dataset. 
  Conclusions:  Our results suggest that tumor and lymph node imaging features may play complementary roles for predicting clinical outcomes in node-positive cervical cancer. 
  |  https://ro-journal.biomedcentral.com/articles/10.1186/s13014-020-01502-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32312283/  |  
------------------------------------------- 
10.1016/j.envint.2020.105745  |    Background:  Recently, the impact of fine particulate matter pollution on cardiovascular system is drawing considerable concern worldwide. The association between ambient fine particulate and the cardiac arrhythmias is not clear now. 
  Objective:  To study associations of ambient fine particulate with incidence of arrhythmias in outpatients. 
  Methods:  Data was collected from the remote electrocardiogram (ECG) system covering 282 community hospitals in Shanghai from June 24th, 2014 to June 23rd, 2016. ECG was performed for patients admitted to above hospitals with complaining of chest discomfort or palpitation, or for regular check-ups. Air quality data during this time period was obtained from China National Environment Monitoring Center. A generalized additive quasi-Poisson model was established to examine the associations between PM2.5 and cardiac arrhythmias. 
  Results:  Cardiac arrhythmias were detected in 202,661 out of 1,016,579 outpatients (19.9%) and fine particulate matter ranged from 6 to 219 μg/m<sup>3</sup> during this period. Positive associations were evidenced between fine particulate matter level and prevalence of cardiac arrhythmia by different lag models. Per 10 μg/m<sup>3</sup> increase in fine particulate matter was associated with a 0.584%(95%CI:0.346-0.689%, p &lt; 0.001) increase of cardiac arrhythmia detected in these patient cohort at lag0-2. For different types of cardiac arrhythmias, an immediate arrhythmogenic effect of fine particulate matter (increase of the estimates of cardiac arrhythmia prevalence detected in daily outpatient visits) was found with paroxysmal supraventricular tachycardia; a lag effect was found with atrial fibrillation; and both immediate and lag effect was found with premature atrial contractions or atrial tachycardia, atrioventricular block. Moreover, the impact of fine particulate matter on cardiac arrhythmias was significantly greater in women (lag3 and lag0-4), and in people aged &lt;65 years (lag0). 
  Conclusion:  Ambient exposure to fine particulate matter is linked with increased risk of arrhythmias in outpatients visiting Shanghai community hospitals, with an immediate or lag effect. The arrhythmogenic effect varies among different types of cardiac arrhythmias. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0160-4120(19)33419-1  |  
------------------------------------------- 
10.1155/2020/9719872  |    Background:  Traditional Chinese medicine (TCM) has long been used to treat chronic kidney disease (CKD) in Asia. Its effectiveness and safety for CKD treatment have been confirmed in documented studies. However, the prescription rule of formulae for Chinese medicinal herbs is complicated and remains uncharacterized. Thus, we used data mining technology to evaluate the treatment principle and coprescription pattern of these formulae in CKD TCM treatment. 
  Methods:  Data on patients with CKD were obtained from the outpatient system of a TCM hospital. We established a Chinese herb knowledge base based on the Chinese Pharmacopoeia and the Chinese Materia Medica. Then, following extraction of prescription information, we deweighted and standardized each prescribed herb according to the knowledge base to establish a database of CKD treatment formulae. We analyzed the frequency with which individual herbs were prescribed, as well as their properties, tastes, meridian tropisms, and categories. Then, we evaluated coprescription patterns and assessed medication rules by performing association rule learning, cluster analysis, and complex network analysis. 
  Results:  We retrospectively analyzed 299 prescriptions of 166 patients with CKD receiving TCM treatment. The most frequently prescribed core herbs for CKD treatment were Rhizoma Dioscoreae (Shanyao), Spreading Hedyotis Herb (Baihuasheshecao), Root of Snow of June (Baimagu), Radix Astragali (Huangqi), Poria (Fulin), Rhizoma Atractylodis Macrocephalae (Baizhu), Radix Pseudostellariae (Taizishen), and Fructus Corni (Shanzhuyu). The TCM properties of the herbs were mainly being warm, mild, and cold. The tastes of the herbs were mainly sweet, followed by bitter. The main meridian tropisms were Spleen Meridian of Foot-Taiyin, Liver Meridian of Foot-Jueyi, Lung Meridian of Hand-Taiyin, Stomach Meridian of Foot-Yangming, and Kidney Meridian of Foot-Shaoyin. The top three categories were deficiency-tonifying, heat-clearing, and dampness-draining diuretic. 
  Conclusion:  Using an integrated analysis method, we confirmed that the primary TCM pathogeneses of kidney disease were deficiency and dampness-heat. The primary treatment principles were tonifying deficiency and eliminating dampness-heat. 
  |  https://doi.org/10.1155/2020/9719872  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32047530/  |  
------------------------------------------- 
10.1016/j.artmed.2020.101814  |    Background:  The accuracy of a prognostic prediction model has become an essential aspect of the quality and reliability of the health-related decisions made by clinicians in modern medicine. Unfortunately, individual institutions often lack sufficient samples, which might not provide sufficient statistical power for models. One mitigation is to expand data collection from a single institution to multiple centers to collectively increase the sample size. However, sharing sensitive biomedical data for research involves complicated issues. Machine learning models such as random forests (RF), though they are commonly used and achieve good performances for prognostic prediction, usually suffer worse performance under multicenter privacy-preserving data mining scenarios compared to a centrally trained version. 
  Methods and materials:  In this study, a multicenter random forest prognosis prediction model is proposed that enables federated clinical data mining from horizontally partitioned datasets. By using a novel data enhancement approach based on a differentially private generative adversarial network customized to clinical prognosis data, the proposed model is able to provide a multicenter RF model with performances on par with-or even better than-centrally trained RF but without the need to aggregate the raw data. Moreover, our model also incorporates an importance ranking step designed for feature selection without sharing patient-level information. 
  Result:  The proposed model was evaluated on colorectal cancer datasets from the US and China. Two groups of datasets with different levels of heterogeneity within the collaborative research network were selected. First, we compare the performance of the distributed random forest model under different privacy parameters with different percentages of enhancement datasets and validate the effectiveness and plausibility of our approach. Then, we compare the discrimination and calibration ability of the proposed multicenter random forest with a centrally trained random forest model and other tree-based classifiers as well as some commonly used machine learning methods. The results show that the proposed model can provide better prediction performance in terms of discrimination and calibration ability than the centrally trained RF model or the other candidate models while following the privacy-preserving rules in both groups. Additionally, good discrimination and calibration ability are shown on the simplified model based on the feature importance ranking in the proposed approach. 
  Conclusion:  The proposed random forest model exhibits ideal prediction capability using multicenter clinical data and overcomes the performance limitation arising from privacy guarantees. It can also provide feature importance ranking across institutions without pooling the data at a central site. This study offers a practical solution for building a prognosis prediction model in the collaborative clinical research network and solves practical issues in real-world applications of medical artificial intelligence. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(19)30855-3  |  
------------------------------------------- 
10.1002/cnm.3303  |   Deep learning methods combined with large datasets have recently shown significant progress in solving several medical tasks. However, collecting and annotating large datasets can be a very cumbersome and expensive task. We tackle these problems with a virtual database approach where training data is generated using computer simulations of related phenomena. Specifically, we concentrate on the following problem: can cardiovascular indices such as aortic elasticity, diastolic and systolic blood pressures, and blood flow from heart be predicted continuously using wearable photoplethysmographic sensors? We simulate the blood flow using a haemodynamic model consisting of the entire human circulation. Repeated evaluation of the simulator allows us to create a database of "virtual subjects" with size that is only limited by available computational resources. Using this database, we train neural networks to predict the cardiac indices from photoplethysmographic signal waveform. We consider two approaches: neural networks based on predefined input features and deep convolutional neural networks taking waveform directly as the input. The performance of the methods is demonstrated using numerical examples, thus carrying out a preliminary assessment of the approaches. The results show improvements in accuracy compared with the previous methods. The improvements are especially significant with indices related to aortic elasticity and maximum blood flow. The proposed approach would provide new means to measure cardiovascular health continuously, for example, with a simple wrist device. 
  |  https://doi.org/10.1002/cnm.3303  |  
------------------------------------------- 
10.1016/j.canlet.2019.10.023  |   Pancreatic cystic lesions (PCLs) are well-known precursors of pancreatic cancer. Their diagnosis can be challenging as their behavior varies from benign to malignant disease. Precise and timely management of malignant pancreatic cysts might prevent transformation to pancreatic cancer. However, the current consensus guidelines, which rely on standard imaging features to predict cyst malignancy potential, are conflicting and unclear. This has led to an increased interest in radiomics, a high-throughput extraction of comprehensible data from standard of care images. Radiomics can be used as a diagnostic and prognostic tool in personalized medicine. It utilizes quantitative image analysis to extract features in conjunction with machine learning and artificial intelligence (AI) methods like support vector machines, random forest, and convolutional neural network for feature selection and classification. Selected features can then serve as imaging biomarkers to predict high-risk PCLs. Radiomics studies conducted heretofore on PCLs have shown promising results. This cost-effective approach would help us to differentiate benign PCLs from malignant ones and potentially guide clinical decision-making leading to better utilization of healthcare resources. In this review, we discuss the process of radiomics, its myriad applications such as diagnosis, prognosis, and prediction of therapy response. We also discuss the outcomes of studies involving radiomic analysis of PCLs and pancreatic cancer, and challenges associated with this novel field along with possible solutions. Although these studies highlight the potential benefit of radiomics in the prevention and optimal treatment of pancreatic cancer, further studies are warranted before incorporating radiomics into the clinical decision support system. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0304-3835(19)30520-8  |  
------------------------------------------- 
10.1007/s11060-019-03343-4  |    Purpose:  Gamma Knife radiosurgery (GKRS) is a non-invasive procedure for the treatment of brain metastases. This study sought to determine whether radiomic features of brain metastases derived from pre-GKRS magnetic resonance imaging (MRI) could be used in conjunction with clinical variables to predict the effectiveness of GKRS in achieving local tumor control. 
  Methods:  We retrospectively analyzed 161 patients with non-small cell lung cancer (576 brain metastases) who underwent GKRS for brain metastases. The database included clinical data and pre-GKRS MRI. Brain metastases were demarcated by experienced neurosurgeons, and radiomic features of each brain metastasis were extracted. Consensus clustering was used for feature selection. Cox proportional hazards models and cause-specific proportional hazards models were used to correlate clinical variables and radiomic features with local control of brain metastases after GKRS. 
  Results:  Multivariate Cox proportional hazards model revealed that higher zone percentage (hazard ratio, HR 0.712; P = .022) was independently associated with superior local tumor control. Similarly, multivariate cause-specific proportional hazards model revealed that higher zone percentage (HR 0.699; P = .014) was independently associated with superior local tumor control. 
  Conclusions:  The zone percentage of brain metastases, a radiomic feature derived from pre-GKRS contrast-enhanced T1-weighted MRIs, was found to be an independent prognostic factor of local tumor control following GKRS in patients with non-small cell lung cancer and brain metastases. Radiomic features indicate the biological basis and characteristics of tumors and could potentially be used as surrogate biomarkers for predicting tumor prognosis following GKRS. 
  |  https://doi.org/10.1007/s11060-019-03343-4  |  
------------------------------------------- 
10.1007/s00247-019-04593-0  |    Background:  The chest radiograph is the most common imaging modality to assess childhood pneumonia. It has been used in epidemiological and vaccine efficacy/effectiveness studies on childhood pneumonia. 
  Objective:  To develop computer-aided diagnosis (CAD4Kids) for chest radiography in children and to evaluate its accuracy in identifying World Health Organization (WHO)-defined chest radiograph primary-endpoint pneumonia compared to a consensus interpretation. 
  Materials and methods:  Chest radiographs were independently evaluated by three radiologists based on WHO criteria. Automatic lung field segmentation was followed by manual inspection and correction, training, feature extraction and classification. Radiographs were filtered with Gaussian derivatives on multiple scales, extracting texture features to classify each pixel in the lung region. To obtain an image score, the 95<sup>th</sup> percentile score of the pixels was used. Training and testing were done in 10-fold cross validation. 
  Results:  The radiologist majority consensus reading of 858 interpretable chest radiographs included 333 (39%) categorised as primary-endpoint pneumonia, 208 (24%) as other infiltrate only and 317 (37%) as no primary-endpoint pneumonia or other infiltrate. Compared to the reference radiologist consensus reading, CAD4Kids had an area under the receiver operator characteristic (ROC) curve of 0.850 (95% confidence interval [CI] 0.823-0.876), with a sensitivity of 76% and specificity of 80% for identifying primary-endpoint pneumonia on chest radiograph. Furthermore, the ROC curve was 0.810 (95% CI 0.772-0.846) for CAD4Kids identifying primary-endpoint pneumonia compared to other infiltrate only. 
  Conclusion:  Further development of the CAD4Kids software and validation in multicentre studies are important for future research on computer-aided diagnosis and artificial intelligence in paediatric radiology. 
  |  https://dx.doi.org/10.1007/s00247-019-04593-0  |  
------------------------------------------- 
10.1038/s41598-020-60214-z  |   The retina decomposes visual stimuli into parallel channels that encode different features of the visual environment. Central to this computation is the synaptic processing in a dense layer of neuropil, the so-called inner plexiform layer (IPL). Here, different types of bipolar cells stratifying at distinct depths relay the excitatory feedforward drive from photoreceptors to amacrine and ganglion cells. Current experimental techniques for studying processing in the IPL do not allow imaging the entire IPL simultaneously in the intact tissue. Here, we extend a two-photon microscope with an electrically tunable lens allowing us to obtain optical vertical slices of the IPL, which provide a complete picture of the response diversity of bipolar cells at a "single glance". The nature of these axial recordings additionally allowed us to isolate and investigate batch effects, i.e. inter-experimental variations resulting in systematic differences in response speed. As a proof of principle, we developed a simple model that disentangles biological from experimental causes of variability and allowed us to recover the characteristic gradient of response speeds across the IPL with higher precision than before. Our new framework will make it possible to study the computations performed in the central synaptic layer of the retina more efficiently. 
  |  http://dx.doi.org/10.1038/s41598-020-60214-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32157103/  |  
------------------------------------------- 
10.1016/j.csbj.2020.02.023  |   Nanotechnology has enabled the discovery of a multitude of novel materials exhibiting unique physicochemical (PChem) properties compared to their bulk analogues. These properties have led to a rapidly increasing range of commercial applications; this, however, may come at a cost, if an association to long-term health and environmental risks is discovered or even just perceived. Many nanomaterials (NMs) have not yet had their potential adverse biological effects fully assessed, due to costs and time constraints associated with the experimental assessment, frequently involving animals. Here, the available NM libraries are analyzed for their suitability for integration with novel nanoinformatics approaches and for the development of NM specific Integrated Approaches to Testing and Assessment (IATA) for human and environmental risk assessment, all within the NanoSolveIT cloud-platform. These established and well-characterized NM libraries (e.g. NanoMILE, NanoSolutions, NANoREG, NanoFASE, caLIBRAte, NanoTEST and the Nanomaterial Registry (&gt;2000 NMs)) contain physicochemical characterization data as well as data for several relevant biological endpoints, assessed in part using harmonized Organisation for Economic Co-operation and Development (OECD) methods and test guidelines. Integration of such extensive NM information sources with the latest nanoinformatics methods will allow NanoSolveIT to model the relationships between NM structure (morphology), properties and their adverse effects and to predict the effects of other NMs for which less data is available. The project specifically addresses the needs of regulatory agencies and industry to effectively and rapidly evaluate the exposure, NM hazard and risk from nanomaterials and nano-enabled products, enabling implementation of computational 'safe-by-design' approaches to facilitate NM commercialization. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32226594/  |  
------------------------------------------- 
10.1136/bmjopen-2019-034301  |    Introduction:  With its legalisation and regulation in Canada in 2018, the proportion of Canadians reporting cannabis use in 2019 increased substantially over the previous year, with half of new users being aged 45+ years. While use in older adults has been low historically, as those born in the 1950s and 1960s continue to age, this demographic will progressively have more liberal attitudes, prior cannabis exposure and higher use rates. However, older adults experience slower metabolism, increased likelihood of polypharmacy, cognitive decline and chronic physical/mental health problems. There is a need to enhance knowledge of the effects of cannabis use in older adults. The following question will be addressed using a scoping review approach: what evidence exists regarding beneficial and harmful effects of medical and non-medical cannabis use in adults &gt;50 years of age? Given that beneficial and harmful effects of cannabis may be mediated by patient-level (eg, age, sex and race) and cannabis-related factors (eg, natural vs synthetic, consumption method), subgroup effects related to these and additional factors will be explored. 
  Methods and analysis:  Methods for scoping reviews outlined by Arksey &amp; O'Malley and the Joanna Briggs Institute will be used. A librarian designed a systematic search of the literature from database inception to June 2019. Using the OVID platform, Ovid MEDLINE will be searched, including Epub Ahead of Print and In-Process and Other Non-Indexed Citations, Embase Classic+Embase, and PsycINFO for reviews, randomised trials, non-randomised trials and observational studies of cannabis use. The Cochrane Library on Wiley will also be searched. Eligibility criteria will be older adult participants, currently using cannabis (medical or non-medical), with studies required to report a cannabis-related health outcome to be eligible. Two reviewers will screen citations and full texts, with support from artificial intelligence. Two reviewers will chart data. Tables/graphics will be used to map evidence and identify evidence gaps. 
  Ethics and dissemination:  This research will enhance awareness of existing evidence addressing the health effects of medical and non-medical cannabis use in older adults. Findings will be disseminated through a peer-reviewed publication, conference presentations and a stakeholder meeting. 
  Trial registration number:  DOI 10.17605/OSF.IO/5JTAQ. 
  |  http://bmjopen.bmj.com/cgi/pmidlookup?view=long&pmid=32114474  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32114474/  |  
------------------------------------------- 
10.1042/BSR20193767  |   Hydroxyoctadecadienoic acids (HODEs) are produced by oxidation and reduction of linoleates. There are several regio- and stereo-isomers of HODE, and their concentrations in vivo are higher than those of other lipids. Although conformational isomers may have different biological activities, comparative analysis of intracellular function of HODE isomers has not yet been performed. We evaluated the transcriptional activity of peroxisome proliferator-activated receptor γ (PPARγ), a therapeutic target for diabetes, and analyzed PPARγ agonist activity of HODE isomers. The lowest scores for docking poses of 12 types of HODE isomers (9-, 10-, 12-, and 13-HODEs) were almost similar in docking simulation of HODEs into PPARγ ligand-binding domain (LBD). Direct binding of HODE isomers to PPARγ LBD was determined by water-ligand observed via gradient spectroscopy (WaterLOGSY) NMR experiments. In contrast, there were differences in PPARγ agonist activities among 9- and 13-HODE stereo-isomers and 12- and 13-HODE enantio-isomers in a dual-luciferase reporter assay. Interestingly, the activity of 9-HODEs was less than that of other regio-isomers, and 9-(E,E)-HODE tended to decrease PPARγ-target gene expression during the maturation of 3T3-L1 cells. In addition, 10- and 12-(Z,E)-HODEs, which we previously proposed as biomarkers for early-stage diabetes, exerted PPARγ agonist activity. These results indicate that all HODE isomers have PPARγ-binding affinity; however, they have different PPARγ agonist activity. Our findings may help to understand the biological function of lipid peroxidation products. 
  |  https://portlandpress.com/bioscirep/article-lookup/doi/10.1042/BSR20193767  |  
------------------------------------------- 
10.3389/fcell.2020.00018  |   Obesity is characterized by low-grade chronic inflammation. As an acute-phase reactant to inflammation and infection, C-reactive protein (CRP) has been found to be the strongest factor associated with obesity. Here we show that chronic elevation of human CRP at baseline level causes the obesity. The obesity phenotype is confirmed by whole-body magnetic resonance imaging (MRI), in which the total fat mass is 6- to 9- fold higher in the CRP rats than the control rats. Univariate linear regression analysis showed different growth rates between the CRP rats and the control rats, and that the difference appears around 11 weeks old, indicating that they developed adult-onset obesity. We also found that chronic elevation of CRP can prime molecular changes broadly in the innate immune system, energy expenditure systems, thyroid hormones, apolipoproteins, and gut flora. Our data established a causal role of CRP elevation in the development of adult-onset obesity. 
  |  https://doi.org/10.3389/fcell.2020.00018  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32154244/  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.138477  |   Rapid identification of marine pathogens is very important in marine ecology. Artificial intelligence combined with Raman spectroscopy is a promising choice for identifying marine pathogens due to its rapidity and efficiency. However, considering the cost of sample collection and the challenging nature of the experimental environment, only limited spectra are typically available to build a classification model, which hinders qualitative analysis. In this paper, we propose a novel method to classify marine pathogens by means of Raman spectroscopy combined with generative adversarial networks (GANs). Three marine strains, namely, Staphylococcus hominis, Vibrio alginolyticus, and Bacillus licheniformis, were cultured. Using Raman spectroscopy, we acquired 100 spectra of each strain, and we fitted them into GAN models for training. After 30,000 training iterations, the spectra generated by G were similar to the actual spectra, and D was used to test the accuracy of the spectra. Our results demonstrate that our method not only improves the accuracy of machine learning classification but also solves the problem of requiring a large amount of training data. Moreover, we have attempted to find potential identifying regions in the Raman spectra that can be used for reference in subsequent related work in this field. Therefore, this method has tremendous potential to be developed as a tool for pathogen identification. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)31990-2  |  
------------------------------------------- 
10.1002/mp.13955  |    Purpose:  We propose a novel domain-specific loss, which is a differentiable loss function based on the dose-volume histogram (DVH), and combine it with an adversarial loss for the training of deep neural networks. In this study, we trained a neural network for generating Pareto optimal dose distributions, and evaluate the effects of the domain-specific loss on the model performance. 
  Methods:  In this study, three loss functions - mean squared error (MSE) loss, DVH loss, and adversarial (ADV) loss - were used to train and compare four instances of the neural network model: (a) MSE, (b) MSE + ADV, (c) MSE + DVH, and (d) MSE + DVH+ADV. The data for 70 prostate patients, including the planning target volume (PTV), and the organs at risk (OAR) were acquired as 96 × 96 × 24 dimension arrays at 5 mm<sup>3</sup> voxel size. The dose influence arrays were calculated for 70 prostate patients, using a 7 equidistant coplanar beam setup. Using a scalarized multicriteria optimization for intensity-modulated radiation therapy, 1200 Pareto surface plans per patient were generated by pseudo-randomizing the PTV and OAR tradeoff weights. With 70 patients, the total number of plans generated was 84 000 plans. We divided the data into 54 training, 6 validation, and 10 testing patients. Each model was trained for a total of 100,000 iterations, with a batch size of 2. All models used the Adam optimizer, with a learning rate of 1 × 10<sup>-3</sup> . 
  Results:  Training for 100 000 iterations took 1.5 days (MSE), 3.5 days (MSE+ADV), 2.3 days (MSE+DVH), and 3.8 days (MSE+DVH+ADV). After training, the prediction time of each model is 0.052 s. Quantitatively, the MSE+DVH+ADV model had the lowest prediction error of 0.038 (conformation), 0.026 (homogeneity), 0.298 (R50), 1.65% (D95), 2.14% (D98), and 2.43% (D99). The MSE model had the worst prediction error of 0.134 (conformation), 0.041 (homogeneity), 0.520 (R50), 3.91% (D95), 4.33% (D98), and 4.60% (D99). For both the mean dose PTV error and the max dose PTV, Body, Bladder and rectum error, the MSE+DVH+ADV outperformed all other models. Regardless of model, all predictions have an average mean and max dose error &lt;2.8% and 4.2%, respectively. 
  Conclusion:  The MSE+DVH+ADV model performed the best in these categories, illustrating the importance of both human and learned domain knowledge. Expert human domain-specific knowledge can be the largest driver in the performance improvement, and adversarial learning can be used to further capture nuanced attributes in the data. The real-time prediction capabilities allow for a physician to quickly navigate the tradeoff space for a patient, and produce a dose distribution as a tangible endpoint for the dosimetrist to use for planning. This is expected to considerably reduce the treatment planning time, allowing for clinicians to focus their efforts on the difficult and demanding cases. 
  |  https://doi.org/10.1002/mp.13955  |  
------------------------------------------- 
10.1016/j.scitotenv.2019.136068  |   The urban heat island is a vastly documented climatological phenomenon, but when it comes to coastal cities, close to desert areas, its analysis becomes extremely challenging, given the high temporal variability and spatial heterogeneity. The strong dependency on the synoptic weather conditions, rather than on city-specific, constant features, hinders the identification of recurrent patterns, leading conventional predicting algorithms to fail. In this paper, an advanced artificial intelligence technique based on long short-term memory (LSTM) model is applied to gain insight and predict the highly fluctuating heat island intensity (UHII) in the city of Sydney, Australia, governed by the dualistic system of cool sea breeze from the ocean and hot western winds from the vast desert biome inlands. Hourly measurements of temperature, collected for a period of 18 years (1999-2017) from 8 different sites in a 50 km radius from the coastline, were used to train (80%) and test (20%) the model. Other inputs included date, time, and previously computed UHII, feedbacked to the model with an optimized time step of six hours. A second set of models integrated wind speed at the reference station to account for the sea breeze effect. The R<sup>2</sup> ranged between 0.770 and 0.932 for the training dataset and between 0.841 and 0.924 for the testing dataset, with the best performance attained right in correspondence of the city hot spots. Unexpectedly, very little benefit (0.06-0.43%) was achieved by including the sea breeze among the input variables. Overall, this study is insightful of a rather rare climatological case at the watershed between maritime and desertic typicality. We proved that accurate UHII predictions can be achieved by learning from long-term air temperature records, provided that an appropriate predicting architecture is utilized. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(19)36064-4  |  
------------------------------------------- 
10.1002/adma.201901958  |   The programmable nature of smart textiles makes them an indispensable part of an emerging new technology field. Smart textile-integrated microelectronic systems (STIMES), which combine microelectronics and technology such as artificial intelligence and augmented or virtual reality, have been intensively explored. A vast range of research activities have been reported. Many promising applications in healthcare, the internet of things (IoT), smart city management, robotics, etc., have been demonstrated around the world. A timely overview and comprehensive review of progress of this field in the last five years are provided. Several main aspects are covered: functional materials, major fabrication processes of smart textile components, functional devices, system architectures and heterogeneous integration, wearable applications in human and nonhuman-related areas, and the safety and security of STIMES. The major types of textile-integrated nonconventional functional devices are discussed in detail: sensors, actuators, displays, antennas, energy harvesters and their hybrids, batteries and supercapacitors, circuit boards, and memory devices. 
  |  https://doi.org/10.1002/adma.201901958  |  
------------------------------------------- 
10.1039/c9nr09146g  |   The development of flexible composites is of great significance in the flexible electronic field. In combination with machine learning technology, the introduction of artificial intelligence to flexible materials design, synthesis, characterization and application research will greatly promote the flexible materials research efficiency. In this study, the back propagation (BP) neural network based on the differential evolution (DE) algorithm was applied to determine the electrical properties of the flexible Ag/poly (amic acid) (PAA) composite structure and to develop flexible materials for its different applications. In the machine learning model, the concentration of PAA, the ion exchange time of AgNO<sub>3</sub>, and the concentration and reduction time of NaBH<sub>4</sub> are set as input parameters, and the product of the sheet resistance of the Ag/PAA film and the processing time are set as output information. To overcome the situation whereby the BP neural network solution process could fall into the local optimum, the initial threshold and the weight of the BP neural network and the data import model are optimized by the DE algorithm. Utilizing 1077 learning samples and 49 predictive samples, a machine learning model with very high accuracy was established and relative errors of predictions less than 1.96% were achieved. In terms of this model, the optimized fabrication conditions of the Ag/PAA composites, which are suitable for strain sensors and electrodes, were predicted. To identify the availability and applicability of the proposed algorithm, a strain gauge sensor, a triboelectric nanogenerator (TENG) and a capacitive pressure sensor array were fabricated successfully using the optimized process parameters. This work shows that machine learning can be used to quickly optimize the process and provide guidance for material and process design, which is of significance for the development of flexible materials and devices. 
  |  https://doi.org/10.1039/c9nr09146g  |  
------------------------------------------- 
10.1002/jmri.27164  |    Background:  Accurate interpretation of hip MRI is time-intensive and difficult, prone to inter- and intrareviewer variability, and lacks a universally accepted grading scale to evaluate morphological abnormalities. 
  Purpose:  To 1) develop and evaluate a deep-learning-based model for binary classification of hip osteoarthritis (OA) morphological abnormalities on MR images, and 2) develop an artificial intelligence (AI)-based assist tool to find if using the model predictions improves interreader agreement in hip grading. 
  Study type:  Retrospective study aimed to evaluate a technical development. 
  Population:  A total of 764 MRI volumes (364 patients) obtained from two studies (242 patients from LASEM [FORCe] and 122 patients from UCSF), split into a 65-25-10% train, validation, test set for network training. 
  Field strength/sequence:  3T MRI, 2D T<sub>2</sub> FSE, PD SPAIR. 
  Assessment:  Automatic binary classification of cartilage lesions, bone marrow edema-like lesions, and subchondral cyst-like lesions using the MRNet, interreader agreement before and after using network predictions. 
  Statistical tests:  Receiver operating characteristic (ROC) curve, area under curve (AUC), specificity and sensitivity, and balanced accuracy. 
  Results:  For cartilage lesions, bone marrow edema-like lesions and subchondral cyst-like lesions the AUCs were: 0.80 (95% confidence interval [CI] 0.65, 0.95), 0.84 (95% CI 0.67, 1.00), and 0.77 (95% CI 0.66, 0.85), respectively. The sensitivity and specificity of the radiologist for binary classification were: 0.79 (95% CI 0.65, 0.93) and 0.80 (95% CI 0.59, 1.02), 0.40 (95% CI -0.02, 0.83) and 0.72 (95% CI 0.59, 0.86), 0.75 (95% CI 0.45, 1.05) and 0.88 (95% CI 0.77, 0.98). The interreader balanced accuracy increased from 53%, 71% and 56% to 60%, 73% and 68% after using the network predictions and saliency maps. 
  Data conclusion:  We have shown that a deep-learning approach achieved high performance in clinical classification tasks on hip MR images, and that using the predictions from the deep-learning model improved the interreader agreement in all pathologies. 
  Level of evidence:  3 TECHNICAL EFFICACY STAGE: 1. 
  |  https://doi.org/10.1002/jmri.27164  |  
------------------------------------------- 
10.1002/biot.201900343  |   Modeling protein structures is critical for understanding protein functions in various biological and biotechnological studies. Among representative protein structure modeling approaches, template-based modeling (TBM) is by far the most reliable and most widely used approach to model protein structures. However, it still remains as a challenge to select appropriate software programs for pairwise alignments and model building, two major steps of the TBM. In this paper, pairwise alignment methods for TBM are first compared with respect to the quality of structure models built using these methods. This comparative study is conducted using comprehensive datasets, which cover 6185 domain sequences from Structural Classification of Proteins extended for soluble proteins, and 259 Protein Data Bank entries (whole protein sequences) from Orientations of Proteins in Membranes database for membrane proteins. Overall, a profile-based method, especially PSI-BLAST, consistently shows high performance across the datasets and model evaluation metrics used. Next, use of two model building programs, MODELLER and SWISS-MODEL, does not seem to significantly affect the quality of protein structure models built except for the Hard group (a group of relatively less homologous proteins) of membrane proteins. The results presented in this study will be useful for more accurate implementation of TBM. 
  |  https://doi.org/10.1002/biot.201900343  |  
------------------------------------------- 
10.1093/humrep/deaa060  |    Study question:  What is the inter-observer agreement among embryologists for decision to freeze blastocysts of borderline morphology and can it be improved with a modified grading system? 
  Summary answer:  The inter-observer agreement among embryologists deciding whether to freeze blastocysts of marginal morphology was low and was not improved by a modified grading system. 
  What is known already:  While previous research on inter-observer variability on the decision of which embryo to transfer from a cohort of blastocysts is good, the impact of grading variability regarding decision to freeze borderline blastocysts has not been investigated. Agreement for inner cell mass (ICM) and trophectoderm (TE) grade is only fair, factors which contribute to the grade that influences decision to freeze. 
  Study design, size, duration:  This was a prospective study involving 18 embryologists working at four different IVF clinics within a single organisation between January 2019 and July 2019. 
  Participants/materials, setting, methods:  All embryologists currently practicing blastocyst grading at a multi-site organisation were invited to participate. The survey was comprised of blastocyst images in three planes and asked (i) the likelihood of freezing and (ii) whether the blastocyst would be frozen based on visual assessment. Blastocysts varied by quality and were categorised as either top (n = 20), borderline (n = 60) or non-viable/degenerate quality (n = 20). A total of 1800 freeze decisions were assessed. To assess the impact of grading criteria on inter-observer agreement for decision to freeze, the survey was taken once when the embryologists used the Gardner criteria and again 6 months after transitioning to a modified Gardner criterion with four grades for ICM and TE. The fourth grade was introduced with the aim to promote higher levels of agreement for the clinical usability decision when the blastocyst was of marginal quality. 
  Main results and the role of chance:  The inter-observer agreement for decision to freeze was near perfect (kappa 1.0) for top and non-viable/degenerate quality blastocysts, and this was not affected by the blastocysts grading criteria used (top quality; P = 0.330 and non-viable/degenerate quality; P = 0.18). In contrast, the cohort of borderline blastocysts received a mixed freeze rate (average 52.7%) during the first survey, indicative of blastocysts that showed uncertain viability and promoting significant disagreement for decision to freeze among the embryologists (kappa 0.304). After transitioning to a modified Gardner criteria with an additional grading tier, the average freeze rate increased (64.8%; P &lt; 0.0001); however, the inter-observer agreement for decision to freeze was unchanged (kappa 0.301). Therefore, significant disagreement for decision to freeze among embryologists is an ongoing issue not resolved by the two grading criteria assessed here. 
  Limitations, reasons for caution:  Blastocyst assessment was performed from time-lapse images in three planes, rather than with a microscope in the laboratory. The inter-observer agreement for decision to freeze may be lower for embryologists working in different clinics with different grading protocols. 
  Wider implications of the findings:  The decision to freeze a blastocyst with borderline morphology is a common clinical issue that has the potential to arise for any patient during blastocyst culture. Disagreement for decision to freeze these blastocysts, and therefore clinical usability in frozen embryo transfer cycles, affects consistency in patient care due to a potential impact on cumulative live birth rates, as well as financial, emotional and time costs associated with the frozen embryo transfer cycles. We demonstrate significant disagreement for decision to freeze borderline blastocysts among embryologists using the same grading scheme within a large multisite organisation, a phenomenon which was not improved with a modified grading system. Decision-making around borderline embryos is an area requiring further research, especially as studies continue to demonstrate the reduced but modest live birth rates for low quality blastocysts (Grade C). These results provide support for emerging technology for embryo assessment, such as artificial intelligence. 
  Study funding/competing interest(s):  None declared. 
  Trial registration number:  Not applicable. 
  |  https://academic.oup.com/humrep/article-lookup/doi/10.1093/humrep/deaa060  |  
------------------------------------------- 
10.1007/s10120-019-00992-2  |    Background:  Magnifying endoscopy with narrow band imaging (M-NBI) has been applied to examine early gastric cancer by observing microvascular architecture and microsurface structure of gastric mucosal lesions. However, the diagnostic efficacy of non-experts in differentiating early gastric cancer from non-cancerous lesions by M-NBI remained far from satisfactory. In this study, we developed a new system based on convolutional neural network (CNN) to analyze gastric mucosal lesions observed by M-NBI. 
  Methods:  A total of 386 images of non-cancerous lesions and 1702 images of early gastric cancer were collected to train and establish a CNN model (Inception-v3). Then a total of 341 endoscopic images (171 non-cancerous lesions and 170 early gastric cancer) were selected to evaluate the diagnostic capabilities of CNN and endoscopists. Primary outcome measures included diagnostic accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. 
  Results:  The sensitivity, specificity, and accuracy of CNN system in the diagnosis of early gastric cancer were 91.18%, 90.64%, and 90.91%, respectively. No significant difference was spotted in the specificity and accuracy of diagnosis between CNN and experts. However, the diagnostic sensitivity of CNN was significantly higher than that of the experts. Furthermore, the diagnostic sensitivity, specificity and accuracy of CNN were significantly higher than those of the non-experts. 
  Conclusions:  Our CNN system showed high accuracy, sensitivity and specificity in the diagnosis of early gastric cancer. It is anticipated that more progress will be made in optimization of the CNN diagnostic system and further development of artificial intelligence in the medical field. 
  |  http://dx.doi.org/10.1007/s10120-019-00992-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31332619/  |  
------------------------------------------- 
10.1038/s41562-019-0762-8  |   When an automated car harms someone, who is blamed by those who hear about it? Here we asked human participants to consider hypothetical cases in which a pedestrian was killed by a car operated under shared control of a primary and a secondary driver and to indicate how blame should be allocated. We find that when only one driver makes an error, that driver is blamed more regardless of whether that driver is a machine or a human. However, when both drivers make errors in cases of human-machine shared-control vehicles, the blame attributed to the machine is reduced. This finding portends a public under-reaction to the malfunctioning artificial intelligence components of automated cars and therefore has a direct policy implication: allowing the de facto standards for shared-control vehicles to be established in courts by the jury system could fail to properly regulate the safety of those vehicles; instead, a top-down scheme (through federal laws) may be called for. 
  |  None  |  
------------------------------------------- 
10.1136/bjophthalmol-2019-314161  |    Background:  Hospital Eye Services (HES) in the UK face an increasing number of optometric referrals driven by progress in retinal imaging. The National Health Service (NHS) published a 10-year strategy (NHS Long-Term Plan) to transform services to meet this challenge. In this study, we implemented a cloud-based referral platform to improve communication between optometrists and ophthalmologists. 
  Methods:  Retrospective cohort study conducted at Moorfields Eye Hospital, Croydon (NHS Foundation Trust, London, UK). Patients classified into the HES referral pathway by contributing optometrists have been included into this study. Main outcome measures was the reduction of unnecessary referrals. 
  Results:  After reviewing the patient's data in a web-based interface 54 (52%) out of 103 attending patients initially classified into the referral pathway did not need a specialist referral. Fourteen (14%) patients needing urgent treatment were identified. Usability was measured in duration for data input and reviewing which was an average of 9.2 min (median: 5.4; IQR: 3.4-8.7) for optometrists and 3.0 min (median: 3.0; IQR: 1.7-3.9) min for ophthalmologists. A variety of diagnosis was covered by this tool with dry age-related macular degeneration (n=34) being most common. 
  Conclusion:  After implementation more than half of the HES referrals have been avoided. This platform offers a digital-first solution that enables rapid-access eye care for patients in community optometrists, facilitates communication between healthcare providers and may serve as a foundation for implementation of artificial intelligence. 
  |  http://bjo.bmj.com/cgi/pmidlookup?view=long&pmid=31320383  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31320383/  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.138595  |   Land subsidence (LS) is a significant problem that can cause loss of life, damage property, and disrupt local economies. The Semnan Plain is an important part of Iran, where LS is a major problem for sustainable development and management. The plain represents the changes occurring in 40% of the country. We introduce a novel-ensemble intelligence approach (called ANN-bagging) that uses bagging as a meta- or ensemble-classifier of an artificial neural network (ANN) to predict LS spatially on the Semnan Plain in Semnan Province, Iran. The ensemble model's goodness-of-fit (to training data) and prediction accuracy (of the validation data) are compared to benchmarks set by ANN-bagging. A total of 96 locations of LS and 12 LS conditioning factors (LSCFs) were collected. Each feature in the LS inventory map (LSIM) was randomly assigned to one of four groups or folds, each comprising 25% of cases. The novel ensemble model was trained using 75% (3 folds) and validated with the remaining 25% (1 fold) in a four-fold cross-validation (CV) system, which is used to control for the effects of the random selection of the training and validation datasets. LSCFs for LS prediction were selected using the information-gain ratio and multi-collinearity test methods. Factor significance was evaluated using a random forest (RF) model. Groundwater drawdown, land use and land cover, elevation, and lithology were the most important LSCFs. Using the k-fold CV approaches, twelve LS susceptibility maps (LSSMs) were prepared as each fold employed all three models (ANN-bagging, ANN, and bagging). The LS susceptibility mapping showed that between 5.7% and 12.6% of the plain had very high LS susceptibility. All three models produced LS susceptibility maps with acceptable prediction accuracies and goodness-of-fits, but the best maps were produced by the ANN-bagging ensemble method. Overall, LS risk was highest in agricultural areas with high groundwater drawdown in the flat lowlands on quaternary sediments (Qcf). Groundwater extraction rates should be monitored and potentially limited in regions of severe or high LS susceptibility. This investigation details a novel methodology that can help environmental planners and policy makers to mitigate LS to help achieve sustainability. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)32111-2  |  
------------------------------------------- 
10.2196/17530  |    Background:  Smoking cessation is a persistent leading public health challenge. Mobile health (mHealth) solutions are emerging to improve smoking cessation treatments. Previous approaches have proposed supporting cessation with tailored motivational messages. Some managed to provide short-term improvements in smoking cessation. Yet, these approaches were either static in terms of personalization or human-based nonscalable solutions. Additionally, long-term effects were neither presented nor assessed in combination with existing psychopharmacological therapies. 
  Objective:  This study aimed to analyze the long-term efficacy of a mobile app supporting psychopharmacological therapy for smoking cessation and complementarily assess the involved innovative technology. 
  Methods:  A 12-month, randomized, open-label, parallel-group trial comparing smoking cessation rates was performed at Virgen del Rocío University Hospital in Seville (Spain). Smokers were randomly allocated to a control group (CG) receiving usual care (psychopharmacological treatment, n=120) or an intervention group (IG) receiving psychopharmacological treatment and using a mobile app providing artificial intelligence-generated and tailored smoking cessation support messages (n=120). The secondary objectives were to analyze health-related quality of life and monitor healthy lifestyle and physical exercise habits. Safety was assessed according to the presence of adverse events related to the pharmacological therapy. Per-protocol and intention-to-treat analyses were performed. Incomplete data and multinomial regression analyses were performed to assess the variables influencing participant cessation probability. The technical solution was assessed according to the precision of the tailored motivational smoking cessation messages and user engagement. Cessation and no cessation subgroups were compared using t tests. A voluntary satisfaction questionnaire was administered at the end of the intervention to all participants who completed the trial. 
  Results:  In the IG, abstinence was 2.75 times higher (adjusted OR 3.45, P=.01) in the per-protocol analysis and 2.15 times higher (adjusted OR 3.13, P=.002) in the intention-to-treat analysis. Lost data analysis and multinomial logistic models showed different patterns in participants who dropped out. Regarding safety, 14 of 120 (11.7%) IG participants and 13 of 120 (10.8%) CG participants had 19 and 23 adverse events, respectively (P=.84). None of the clinical secondary objective measures showed relevant differences between the groups. The system was able to learn and tailor messages for improved effectiveness in supporting smoking cessation but was unable to reduce the time between a message being sent and opened. In either case, there was no relevant difference between the cessation and no cessation subgroups. However, a significant difference was found in system engagement at 6 months (P=.04) but not in all subsequent months. High system appreciation was reported at the end of the study. 
  Conclusions:  The proposed mHealth solution complementing psychopharmacological therapy showed greater efficacy for achieving 1-year tobacco abstinence as compared with psychopharmacological therapy alone. It provides a basis for artificial intelligence-based future approaches. 
  Trial registration:  ClinicalTrials.gov <a href="http://clinicaltrials.gov/show/NCT03553173" title="See in ClinicalTrials.gov">NCT03553173</a>; https://clinicaltrials.gov/ct2/show/NCT03553173. 
  International registered report identifier (irrid):  RR2-10.2196/12464. 
  |  https://mhealth.jmir.org/2020/4/e17530/  |  
------------------------------------------- 
10.1148/radiol.2020200642  |   Background Chest CT is used for diagnosis of 2019 novel coronavirus disease (COVID-19), as an important complement to the reverse-transcription polymerase chain reaction (RT-PCR) tests. Purpose To investigate the diagnostic value and consistency of chest CT as compared with comparison to RT-PCR assay in COVID-19. Methods From January 6 to February 6, 2020, 1014 patients in Wuhan, China who underwent both chest CT and RT-PCR tests were included. With RT-PCR as reference standard, the performance of chest CT in diagnosing COVID-19 was assessed. Besides, for patients with multiple RT-PCR assays, the dynamic conversion of RT-PCR results (negative to positive, positive to negative, respectively) was analyzed as compared with serial chest CT scans for those with time-interval of 4 days or more. Results Of 1014 patients, 59% (601/1014) had positive RT-PCR results, and 88% (888/1014) had positive chest CT scans. The sensitivity of chest CT in suggesting COVID-19 was 97% (95%CI, 95-98%, 580/601 patients) based on positive RT-PCR results. In patients with negative RT-PCR results, 75% (308/413) had positive chest CT findings; of 308, 48% were considered as highly likely cases, with 33% as probable cases. By analysis of serial RT-PCR assays and CT scans, the mean interval time between the initial negative to positive RT-PCR results was 5.1 ± 1.5 days; the initial positive to subsequent negative RT-PCR result was 6.9 ± 2.3 days). 60% to 93% of cases had initial positive CT consistent with COVID-19 prior (or parallel) to the initial positive RT-PCR results. 42% (24/57) cases showed improvement in follow-up chest CT scans before the RT-PCR results turning negative. Conclusion Chest CT has a high sensitivity for diagnosis of COVID-19. Chest CT may be considered as a primary tool for the current COVID-19 detection in epidemic areas. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020200642?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s11517-020-02147-3  |   Histopathological whole slide images of haematoxylin and eosin (H&amp;E)-stained biopsies contain valuable information with relation to cancer disease and its clinical outcomes. Still, there are no highly accurate automated methods to correlate histolopathological images with brain cancer patients' survival, which can help in scheduling patients therapeutic treatment and allocate time for preclinical studies to guide personalized treatments. We now propose a new classifier, namely, DeepSurvNet powered by deep convolutional neural networks, to accurately classify in 4 classes brain cancer patients' survival rate based on histopathological images (class I, 0-6 months; class II, 6-12 months; class III, 12-24 months; and class IV, &gt;24 months survival after diagnosis). After training and testing of DeepSurvNet model on a public brain cancer dataset, The Cancer Genome Atlas, we have generalized it using independent testing on unseen samples. Using DeepSurvNet, we obtained precisions of 0.99 and 0.8 in the testing phases on the mentioned datasets, respectively, which shows DeepSurvNet is a reliable classifier for brain cancer patients' survival rate classification based on histopathological images. Finally, analysis of the frequency of mutations revealed differences in terms of frequency and type of genes associated to each class, supporting the idea of a different genetic fingerprint associated to patient survival. We conclude that DeepSurvNet constitutes a new artificial intelligence tool to assess the survival rate in brain cancer. Graphical abstract A DCNN model was generated to accurately predict survival rates of brain cancer patients (classified in 4 different classes) accurately. After training the model using images from H&amp;E stained tissue biopsies from The Cancer Genome Atlas database (TCGA, left), the model can predict for each patient, based on a histological image (top right), its survival class accurately (bottom right). 
  |  https://dx.doi.org/10.1007/s11517-020-02147-3  |  
------------------------------------------- 
10.2196/17241  |    Background:  Cardiovascular disease (CVD) remains the leading cause of death in the United Arab Emirates (UAE). One of the common CVDs is hypertrophic cardiomyopathy (HCM). Recent studies conducted in heart cells of mice have shown that this condition involves a chemical modification called hydroxymethylation of the DNA of heart cells. 
  Objective:  Objectives of the proposed research are to profile the distribution of 5-hydroxymethylation in the cardiomyocyte (CMC) genome of cadaveric cardiac tissue and cardiac biopsy specimens; to compare the hydroxymethylome of cadaveric CMCs with that of cardiac biopsy specimens from HCM patients and/or cardiac transplant patients (control) undergoing cardiac catheterization; to histologically appraise sarcomere distribution and mitochondrial morphology of CMCs in the presence of HCM; to correlate the mitochondrial genome with the HCM phenotype; and to integrate anatomy with biochemistry and genetics into the instructional design of HCM in the core medical curriculum at Mohammed Bin Rashid University of Medicine and Health Sciences (MBRU). 
  Methods:  Normal and hypertrophic heart specimens will be obtained from 8 whole-body cadavers (2/8, 25% control and 6/8, 75% HCM). Myocardial biopsy specimens will be obtained from cardiothoracic and transplant units at the Cleveland Clinic in Abu Dhabi, UAE. As this is a proof-of-concept study, we plan to recruit 5 patients with HCM, where HCM has been diagnosed according to the guidelines of the 2014 European Society of Cardiology Guidelines. Patients with valvular heart disease, history of myocarditis, regular alcohol consumption, or cardiotoxic chemotherapy will be excluded. The control biopsy specimens will be obtained from patients who had received heart transplants. Three investigational approaches will then be employed: (1) gross anatomical evaluation, (2) histological analysis, and (3) profiling and analysis of the hydroxymethylome. These investigations will be pursued with minor modifications, if required, to the standard protocols and in accordance with institutional policy. The objective associated with the education of health professionals will be addressed through a strategy based on Graham's knowledge translation model. 
  Results:  This study is at the protocol-development stage. The validated questionnaires have been identified in relation to the objectives. The MBRU and the Cleveland Clinic Abu Dhabi Institutional Review Board (IRB) are reviewing this study. Further clarification and information can be obtained from the MBRU IRB. There is funding in place for this study (MBRU-CM-RG2019-08). Currently, we are in the process of standardizing the protocols with respect to the various molecular techniques to be employed during the course of the study. The total duration of the proposed research is 24 months, with a provision for 6 months of a no-cost extension. 
  Conclusions:  The spectrum of CVDs has recently received significant focus from the public health sector in the UAE. HCM is a common familial heart disease, contributing to the sudden increase in the mortality rate of young Emiratis in the UAE. Incorporating artificial intelligence into the identification of epigenetic risk factors associated with HCM will promote accurate diagnosis and lead to the development of improved management plans, hence, positive patient outcomes. Furthermore, integration of these findings into the instructional design of undergraduate, postgraduate, and continuous professional development medical curricula will further contribute to the body of knowledge regarding HCM. 
  International registered report identifier (irrid):  PRR1-10.2196/17241. 
  |  https://www.researchprotocols.org/2020/3/e17241/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32134392/  |  
------------------------------------------- 
10.1016/j.ultsonch.2020.105060  |   This work investigated and compared the dynamic cavitation characteristics between low and high boiling-point phase-shift nanodroplets (NDs) under physiologically relevant flow conditions during focused ultrasound (FUS) exposures at different peak rarefactional pressures. A passive cavitation detection (PCD) system was used to monitor cavitation activity during FUS exposure at various acoustic pressure levels. Root mean square (RMS) amplitudes of broadband noise, spectrograms of the passive cavitation detection signals, and normalized inertial cavitation dose (ICD) values were calculated. Cavitation activity of low-boiling-point perfluoropentane (PFP) NDs and high boiling-point perfluorohexane (PFH) NDs flowing at in vitro mean velocities of 0-15 cm/s were compared in a 4-mm diameter wall-less vessel in a transparent tissue-mimicking phantom. In the static state, both types of phase-shift NDs exhibit a sharp rise in cavitation intensity during initial FUS exposure. Under flow conditions, cavitation activity of the PFH NDs reached the steady state less rapidly compared to PFP NDs under the lower acoustic pressure (1.35 MPa); at the higher acoustic pressure (1.65 MPa), the RMS amplitude increased more sharply during the initial FUS exposure period. In particular, the RMS-time curves of the PFP NDs shifted upward as the mean flow velocity increased from 0 to 15 cm/s; the RMS amplitude of the PFH ND solution increased from 0 to 10 cm/s and decreased at 15 cm/s. Moreover, amplitudes of the echo signal for the low boiling-point PFP NDs were higher compared to the high boiling-point PFH NDs in the lower frequency range, whereas the inverse occurred in the higher frequency range. Both PFP and PFH NDs showed increased cavitation activity in the higher frequency under the flow condition compared to the static state, especially PFH NDs. At 1.65 MPa, normalized ICD values for PFH increased from 0.93 ± 0.03 to 0.96 ± 0.04 and from 0 to 10 cm/s, then decreased to 0.86 ± 0.05 at 15 cm/s. This work contributes to our further understanding of cavitation characteristics of phase-shift NDs under physiologically relevant flow conditions during FUS exposure. In addition, the results provide a reference for selecting suitable phase-shift NDs to enhance the efficiency of cavitation-mediated ultrasonic applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1350-4177(19)30212-3  |  
------------------------------------------- 
10.1038/s41467-019-14198-8  |   Astrocytes, a major cell type found throughout the central nervous system, have general roles in the modulation of synapse formation and synaptic transmission, blood-brain barrier formation, and regulation of blood flow, as well as metabolic support of other brain resident cells. Crucially, emerging evidence shows specific adaptations and astrocyte-encoded functions in regions, such as the spinal cord and cerebellum. To investigate the true extent of astrocyte molecular diversity across forebrain regions, we used single-cell RNA sequencing. Our analysis identifies five transcriptomically distinct astrocyte subtypes in adult mouse cortex and hippocampus. Validation of our data in situ reveals distinct spatial positioning of defined subtypes, reflecting the distribution of morphologically and physiologically distinct astrocyte populations. Our findings are evidence for specialized astrocyte subtypes between and within brain regions. The data are available through an online database (https://holt-sc.glialab.org/), providing a resource on which to base explorations of local astrocyte diversity and function in the brain. 
  |  http://dx.doi.org/10.1038/s41467-019-14198-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32139688/  |  
------------------------------------------- 
10.1186/s12962-020-00210-2  |   Since its inception in 2003, <i>Cost Effectiveness and Resource Allocation</i> journal has come a long way over the past 18 years. Possibly much longer than many of its contemporaries in the blossoming science of health economics might have anticipated. Today, entering 2020 it celebrates the Age of Maturity. We believe that in the third decade of XXI century the interdisciplinary science of health economics, will rejuvenate and come back to us younger than ever from its early historical roots almost a century ago. The spreading of economic globalization in several distinctive ways, either led by multinational business corporations or newly emerged Asian leadership, or both, is likely to make challenges for contemporary health systems far more serious. The fourth industrial revolution (cyber physical systems and artificial intelligence technology) and accelerated innovation in the field of E-Health and digital health, will probably change the workflow in medical and health care, and inevitably transform the labour market in the upcoming decades. So, let us be up to the task. Let us provide academic centres, industry-sponsored pharmaceutical and medical device innovation hubs, and governing authorities alike, with a powerful forum for debate on cost-effective resource allocation in the years to come. 
  |  https://resource-allocation.biomedcentral.com/articles/10.1186/s12962-020-00210-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32265598/  |  
------------------------------------------- 
10.1038/s41746-020-0244-4  |   In recent years, there has been a significant expansion in the development and use of multi-modal sensors and technologies to monitor physical activity, sleep and circadian rhythms. These developments make accurate sleep monitoring at scale a possibility for the first time. Vast amounts of multi-sensor data are being generated with potential applications ranging from large-scale epidemiological research linking sleep patterns to disease, to wellness applications, including the sleep coaching of individuals with chronic conditions. However, in order to realise the full potential of these technologies for individuals, medicine and research, several significant challenges must be overcome. There are important outstanding questions regarding performance evaluation, as well as data storage, curation, processing, integration, modelling and interpretation. Here, we leverage expertise across neuroscience, clinical medicine, bioengineering, electrical engineering, epidemiology, computer science, mHealth and human-computer interaction to discuss the digitisation of sleep from a inter-disciplinary perspective. We introduce the state-of-the-art in sleep-monitoring technologies, and discuss the opportunities and challenges from data acquisition to the eventual application of insights in clinical and consumer settings. Further, we explore the strengths and limitations of current and emerging sensing methods with a particular focus on novel data-driven technologies, such as Artificial Intelligence. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32219183/  |  
------------------------------------------- 
10.1186/s41512-020-00074-3  |    Background:  How to select variables and identify functional forms for continuous variables is a key concern when creating a multivariable model. Ad hoc 'traditional' approaches to variable selection have been in use for at least 50 years. Similarly, methods for determining functional forms for continuous variables were first suggested many years ago. More recently, many alternative approaches to address these two challenges have been proposed, but knowledge of their properties and meaningful comparisons between them are scarce. To define a state of the art and to provide evidence-supported guidance to researchers who have only a basic level of statistical knowledge, many outstanding issues in multivariable modelling remain. Our main aims are to identify and illustrate such gaps in the literature and present them at a moderate technical level to the wide community of practitioners, researchers and students of statistics. 
  Methods:  We briefly discuss general issues in building descriptive regression models, strategies for variable selection, different ways of choosing functional forms for continuous variables and methods for combining the selection of variables and functions. We discuss two examples, taken from the medical literature, to illustrate problems in the practice of modelling. 
  Results:  Our overview revealed that there is not yet enough evidence on which to base recommendations for the selection of variables and functional forms in multivariable analysis. Such evidence may come from comparisons between alternative methods. In particular, we highlight seven important topics that require further investigation and make suggestions for the direction of further research. 
  Conclusions:  Selection of variables and of functional forms are important topics in multivariable analysis. To define a state of the art and to provide evidence-supported guidance to researchers who have only a basic level of statistical knowledge, further comparative research is required. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32266321/  |  
------------------------------------------- 
10.1016/j.media.2019.101632  |   Neurodegenerative diseases are excessively affecting millions of patients, especially elderly people. Early detection and management of these diseases are crucial as the clinical symptoms take years to appear after the onset of neuro-degeneration. This paper proposes an adaptive feature learning framework using multiple templates for early diagnosis. A multi-classification scheme is developed based on multiple brain parcellation atlases with various regions of interest. Different sets of features are extracted and then fused, and a feature selection is applied with an adaptively chosen sparse degree. In addition, both linear discriminative analysis and locally preserving projections are integrated to construct a least square regression model. Finally, we propose a feature space to predict the severity of the disease by the guidance of clinical scores. Our proposed method is validated on both Alzheimer's disease neuroimaging initiative and Parkinson's progression markers initiative databases. Extensive experimental results suggest that the proposed method outperforms the state-of-the-art methods, such as the multi-modal multi-task learning or joint sparse learning. Our method demonstrates that accurate feature learning facilitates the identification of the highly relevant brain regions with significant contribution in the prediction of disease progression. This may pave the way for further medical analysis and diagnosis in practical applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(19)30168-9  |  
------------------------------------------- 
10.1021/jacs.9b10700  |   All-inorganic α-CsPbI<sub>3</sub> perovskite quantum dots (QDs) are attracting great interest as solar cell absorbers due to their appealing light harvesting properties and enhanced stability due to the absence of volatile organic constituents. Moreover, ex situ synthesized QDs significantly reduce the variability of the perovskite layer deposition process. However, the incorporation of α-CsPbI<sub>3</sub> QDs into mesoporous TiO<sub>2</sub> (m-TiO<sub>2</sub>) is highly challenging, but these constitute the best performing electron transport materials in state-of-the-art perovskite solar cells. Herein, the m-TiO<sub>2</sub> surface is engineered using an electron-rich cesium-ion containing methyl acetate solution. As one effect of this treatment, the solid-liquid interfacial tension at the TiO<sub>2</sub> surface is reduced and the wettability is improved, facilitating the migration of the QDs into m-TiO<sub>2</sub>. As a second effect, Cs<sup>+</sup> ions passivate the QD surface and promote the charge transfer at the m-TiO<sub>2</sub>/QD interface, leading to an enhancement of the electron injection rate by a factor of 3. In combination with an ethanol-environment smoothing route that significantly reduces the surface roughness of the m-TiO<sub>2</sub>/QD layer, optimized devices exhibit highly reproducible power conversion efficiencies exceeding 13%. The best cell with an efficiency of 14.32% (reverse scan) reaches a short-circuit current density of 17.77 mA cm<sup>-2</sup>, which is an outstanding value for QD-based perovskite solar cells. 
  |  https://dx.doi.org/10.1021/jacs.9b10700  |  
------------------------------------------- 
10.1177/0962280218819202  |   Genetic association studies using high-throughput genotyping and sequencing technologies have identified a large number of genetic variants associated with complex human diseases. These findings have provided an unprecedented opportunity to identify individuals in the population at high risk for disease who carry causal genetic mutations and hold great promise for early intervention and individualized medicine. While interest is high in building risk prediction models based on recent genetic findings, it is crucial to have appropriate statistical measurements to assess the performance of a genetic risk prediction model. Predictiveness curves were recently proposed as a graphic tool for evaluating a risk prediction model on the basis of a single continuous biomarker. The curve evaluates a risk prediction model for classification performance as well as its usefulness when applied to a population. In this article, we extend the predictiveness curve to measure the collective contribution of multiple genetic variants. We further propose a nonparametric, U-statistics-based measurement, referred to as the U-Index, to quantify the performance of a multi-locus predictiveness curve. In particular, a global U-Index and a partial U-Index can be used in the general population and a subpopulation of particular clinical interest, respectively. Through simulation studies, we demonstrate that the proposed U-Index has advantages over several existing summary statistics under various disease models. We also show that the partial U-Index can have its own uniqueness when rare variants have a substantial contribution to disease risk. Finally, we use the proposed predictiveness curve and its corresponding U-Index to evaluate the performance of a genetic risk prediction model for nicotine dependence. 
  |  http://journals.sagepub.com/doi/full/10.1177/0962280218819202?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/bioinformatics/btz496  |    Motivation:  Protein complexes play critical roles in many aspects of biological functions. Three-dimensional (3D) structures of protein complexes are critical for gaining insights into structural bases of interactions and their roles in the biomolecular pathways that orchestrate key cellular processes. Because of the expense and effort associated with experimental determinations of 3D protein complex structures, computational docking has evolved as a valuable tool to predict 3D structures of biomolecular complexes. Despite recent progress, reliably distinguishing near-native docking conformations from a large number of candidate conformations, the so-called scoring problem, remains a major challenge. 
  Results:  Here we present iScore, a novel approach to scoring docked conformations that combines HADDOCK energy terms with a score obtained using a graph representation of the protein-protein interfaces and a measure of evolutionary conservation. It achieves a scoring performance competitive with, or superior to, that of state-of-the-art scoring functions on two independent datasets: (i) Docking software-specific models and (ii) the CAPRI score set generated by a wide variety of docking approaches (i.e. docking software-non-specific). iScore ranks among the top scoring approaches on the CAPRI score set (13 targets) when compared with the 37 scoring groups in CAPRI. The results demonstrate the utility of combining evolutionary, topological and energetic information for scoring docked conformations. This work represents the first successful demonstration of graph kernels to protein interfaces for effective discrimination of near-native and non-native conformations of protein complexes. 
  Availability and implementation:  The iScore code is freely available from Github: https://github.com/DeepRank/iScore (DOI: 10.5281/zenodo.2630567). And the docking models used are available from SBGrid: https://data.sbgrid.org/dataset/684). 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz496  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31199455/  |  
------------------------------------------- 
10.1016/j.schres.2019.10.055  |    Background:  Identifying variables that influence daily-life fluctuations in auditory verbal hallucinations (AVHs) provides insight into potential mechanisms and targets for intervention. Network analysis, that uses time-series data collected by Experience Sampling Method (ESM), could be used to examine relations between multiple variables over time. 
  Methods:  95 daily voice-hearing individuals filled in a short questionnaire ten times a day for six consecutive days at pseudo-random moments. Using multilevel vector auto-regression, relations between voice-hearing and negative affect, positive affect, uncontrollable thoughts, dissociation, and paranoia were analysed in three types of networks: between-subjects (between persons, undirected), contemporaneous (within persons, undirected), and temporal (within persons, directed) networks. Strength centrality was measured to identify the most interconnected variables in the models. 
  Results:  Voice-hearing co-occurred with all variables, while on a 6-day period voice-hearing was only related to uncontrollable thoughts. Voice-hearing was not predicted by any of the factors, but it did predict uncontrollable thoughts and paranoia. All variables showed large autoregressions, i.e. mainly predicted themselves in this severe voice-hearing sample. Uncontrollable thoughts was the most interconnected factor, though relatively uninfluential. 
  Discussion:  Severe voice-hearing might be mainly related to mental state factors on the short-term. Once activated, voice-hearing appears to maintain itself. It is important to assess possible reactivity of AVH to triggers at the start of therapy; if reactive, therapy should focus on the triggering factor. If not reactive, Cognitive Behavioural interventions could be used first to reduce the negative effects of the voices. Limitations are discussed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0920-9964(19)30490-6  |  
------------------------------------------- 
10.1136/ebmental-2019-300134  |    Background:  Utilisation of routinely collected electronic health records from secondary care offers unprecedented possibilities for medical science research but can also present difficulties. One key issue is that medical information is presented as free-form text and, therefore, requires time commitment from clinicians to manually extract salient information. Natural language processing (NLP) methods can be used to automatically extract clinically relevant information. 
  Objective:  Our aim is to use natural language processing (NLP) to capture real-world data on individuals with depression from the Clinical Record Interactive Search (CRIS) clinical text to foster the use of electronic healthcare data in mental health research. 
  Methods:  We used a combination of methods to extract salient information from electronic health records. First, clinical experts define the information of interest and subsequently build the training and testing corpora for statistical models. Second, we built and fine-tuned the statistical models using active learning procedures. 
  Findings:  Results show a high degree of accuracy in the extraction of drug-related information. Contrastingly, a much lower degree of accuracy is demonstrated in relation to auxiliary variables. In combination with state-of-the-art active learning paradigms, the performance of the model increases considerably. 
  Conclusions:  This study illustrates the feasibility of using the natural language processing models and proposes a research pipeline to be used for accurately extracting information from electronic health records. 
  Clinical implications:  Real-world, individual patient data are an invaluable source of information, which can be used to better personalise treatment. 
  |  https://ebmh.bmj.com/cgi/pmidlookup?view=long&pmid=32046989  |  
------------------------------------------- 
10.1093/bioinformatics/btz734  |    Motivation:  Mitochondria are an essential organelle in most eukaryotes. They not only play an important role in energy metabolism but also take part in many critical cytopathological processes. Abnormal mitochondria can trigger a series of human diseases, such as Parkinson's disease, multifactor disorder and Type-II diabetes. Protein submitochondrial localization enables the understanding of protein function in studying disease pathogenesis and drug design. 
  Results:  We proposed a new method, SubMito-XGBoost, for protein submitochondrial localization prediction. Three steps are included: (i) the g-gap dipeptide composition (g-gap DC), pseudo-amino acid composition (PseAAC), auto-correlation function (ACF) and Bi-gram position-specific scoring matrix (Bi-gram PSSM) are employed to extract protein sequence features, (ii) Synthetic Minority Oversampling Technique (SMOTE) is used to balance samples, and the ReliefF algorithm is applied for feature selection and (iii) the obtained feature vectors are fed into XGBoost to predict protein submitochondrial locations. SubMito-XGBoost has obtained satisfactory prediction results by the leave-one-out-cross-validation (LOOCV) compared with existing methods. The prediction accuracies of the SubMito-XGBoost method on the two training datasets M317 and M983 were 97.7% and 98.9%, which are 2.8-12.5% and 3.8-9.9% higher than other methods, respectively. The prediction accuracy of the independent test set M495 was 94.8%, which is significantly better than the existing studies. The proposed method also achieves satisfactory predictive performance on plant and non-plant protein submitochondrial datasets. SubMito-XGBoost also plays an important role in new drug design for the treatment of related diseases. 
  Availability and implementation:  The source codes and data are publicly available at https://github.com/QUST-AIBBDRC/SubMito-XGBoost/. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz734  |  
------------------------------------------- 
10.1016/j.neuroscience.2019.12.032  |   Hypnosis is a psychological technology proved to be effective in respiratory motion control, which is essential to reduce radiation dose during radiotherapy. This study explored the neural mechanisms and cognitive neuroscience of hypnosis for respiration control by functional magnetic resonance imaging with a within-subject design of 15 healthy volunteers in rest state (RS) and hypnosis state (HS). Temporal fluctuation and signal synchronization of brain activity were employed to investigate the altered physiological performance in hypnosis. The altered correlations between temporal fluctuation and signal synchronization were examined within large scale of intrinsic networks which were identified by seed-wise functional connectivity. As a result, hypnosis was observed with increased activity in the right calcarine, bilateral fusiform gyrus and left middle temporal gyrus, and with decreased activity in the left cerebellum posterior lobe (inferior semilunar lobule part). Compared to RS, enhanced positive correlations were observed between temporal fluctuation and signal synchronization in HS. Most importantly, coupled correlation was observed between temporal fluctuation and global signal synchronization within the identified intrinsic networks (R = 0.3843, p &gt; 0.05 in RS; R = 0.6212, p &lt; 0.005 in HS). The findings provide implications for the neural basis of hypnosis for respiratory motion control and suggest the involvement of emotional processing and regulation of perceptual consciousness in hypnosis. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0306-4522(19)30887-5  |  
------------------------------------------- 
10.3390/s20020569  |   Coping with stress is crucial for a healthy lifestyle. In the past, a great deal of research has been conducted to use socially assistive robots as a therapy to alleviate stress and anxiety related problems. However, building a fully autonomous social robot which can deliver psycho-therapeutic solutions is a very challenging endeavor due to limitations in artificial intelligence (AI). To overcome AI's limitations, researchers have previously introduced crowdsourcing-based teleoperation methods, which summon the crowd's input to control a robot's functions. However, in the context of robotics, such methods have only been used to support the object manipulation, navigational, and training tasks. It is not yet known how to leverage real-time crowdsourcing (RTC) to process complex therapeutic conversational tasks for social robotics. To fill this gap, we developed Crowd of Oz (CoZ), an open-source system that allows Softbank's Pepper robot to support such conversational tasks. To demonstrate the potential implications of this crowd-powered approach, we investigated how effectively, crowd workers recruited in real-time can teleoperate the robot's speech, in situations when the robot needs to act as a life coach. We systematically varied the number of workers who simultaneously handle the speech of the robot (N = 1, 2, 4, 8) and investigated the concomitant effects for enabling RTC for social robotics. Additionally, we present Pavilion, a novel and open-source algorithm for managing the workers' queue so that a required number of workers are engaged or waiting. Based on our findings, we discuss salient parameters that such crowd-powered systems must adhere to, so as to enhance their performance in response latency and dialogue quality. 
  |  http://www.mdpi.com/resolver?pii=s20020569  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31968650/  |  
------------------------------------------- 
10.1038/s41598-020-61247-0  |   Cell Population Data (CPD) provides various blood cell parameters that can be used for differential diagnosis. Data analytics using Machine Learning (ML) have been playing a pivotal role in revolutionizing medical diagnostics. This research presents a novel approach of using ML algorithms for screening hematologic malignancies using CPD. The data collection was done at Konkuk University Medical Center, Seoul. A total of (882 cases: 457 hematologic malignancy and 425 hematologic non-malignancy) were used for analysis. In our study, seven machine learning models, i.e., SGD, SVM, RF, DT, Linear model, Logistic regression, and ANN, were used. In order to measure the performance of our ML models, stratified 10-fold cross validation was performed, and metrics, such as accuracy, precision, recall, and AUC were used. We observed outstanding performance by the ANN model as compared to other ML models. The diagnostic ability of ANN achieved the highest accuracy, precision, recall, and AUC ± Standard Deviation as follows: 82.8%, 82.8%, 84.9%, and 93.5% ± 2.6 respectively. ANN algorithm based on CPD appeared to be an efficient aid for clinical laboratory screening of hematologic malignancies. Our results encourage further work of applying ML to wider field of clinical practice. 
  |  http://dx.doi.org/10.1038/s41598-020-61247-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32179774/  |  
------------------------------------------- 
10.1016/j.acra.2020.02.020  |    Rationale and objectives:  Recently, advanced magnetic resonance imaging has been widely adopted to investigate altered structure and functional activities in patients with diffuse axonal injury (DAI), this patient presumed to be caused by shearing forces and results in significant neurological effects. However, little is known regarding cerebral temporal dynamics and its predictive ability in the clinical dysfunction of DAI. 
  Materials and methods:  In this study, static and dynamic fractional amplitude of low-frequency fluctuation (fALFF), an improved approach to detect the intensity of intrinsic neural activities, and their temporal variability were applied to examine the alteration between DAI patients (n = 24) and healthy controls (n = 26) at the voxel level. Then, the altered functional index was used to explore the clinical relationship and predict dysfunction in DAI patients. 
  Results:  We discovered that, compared to healthy controls, DAI patients showed commonly altered regions of static fALFF, and its variability was mainly located in the left cerebellum posterior lobe. Furthermore, decreased static fALFF values over the left cerebellum posterior lobe and bilateral medial frontal gyrus showed significant correlations with disease duration and Mini-Mental State Examination scores. More important, the increased temporal variability of dynamic fALFF in the left caudate could predict the severity of the Glasgow Coma Scale score in DAI patients. 
  Conclusion:  Overall, these results suggested selective abnormalities in intrinsic neural activities with reduced intensity and increased variability, and this novel predictive marker may be developed as a useful indicator for future connectomics or artificial intelligence analyses. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30106-9  |  
------------------------------------------- 
10.1089/sur.2019.126  |   The definition of sepsis continues to be as dynamic as the management strategies used to treat this. Sepsis-3 has replaced the earlier systemic inflammatory response syndrome (SIRS)-based diagnoses with the rapid Sequential Organ Failure Assessment (SOFA) score assisting in predicting overall prognosis with regards to mortality. Surgeons have an important role in ensuring adequate source control while recognizing the threat of carbapenem-resistance in gram-negative organisms. Rapid diagnostic tests are being used increasingly for the early identification of multi-drug-resistant organisms (MDROs), with a key emphasis on the multidisciplinary alert of results. Novel, higher generation antibiotic agents have been developed for resistance in ESKCAPE (<i>Enterococcus faecium</i>, <i>Staphylococcus aureus</i>, <i>Klebsiella pneumoniae</i>, <i>Acinetobacter baumannii</i>, <i>Pseudomonas aeruginosa</i>, and <i>Enterobacter</i> species) organisms while surgeons have an important role in the prevention of spread. The Study to Optimize Peritoneal Infection Therapy (STOP-IT) trial has challenged the previous paradigm of length of antibiotic treatment whereas biomarkers such as procalcitonin are playing a prominent role in individualizing therapy. Several novel therapies for refractory septic shock, while still investigational, are gaining prominence rapidly (such as vitamin C) whereas others await further clinical trials. Management strategies presented as care bundles continue to be updated by the Surviving Sepsis Campaign, yet still remain controversial in its global adoption. We have broadened our temporal and epidemiologic perspective of sepsis by understanding it both as an acute, time-sensitive, life-threatening illness to a chronic condition that increases the risk of mortality up to five years post-discharge. Artificial intelligence, machine learning, and bedside scoring systems can assist the clinician in predicting post-operative sepsis. The public health role of the surgeon is key. This includes collaboration and multi-disciplinary antibiotic stewardship at a hospital level. It also requires controlling pharmaceutical sales and the unregulated dispensing of antibiotic agents globally through policy initiatives to control emerging resistance through prevention. 
  |  https://www.liebertpub.com/doi/full/10.1089/sur.2019.126?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.bbi.2019.11.019  |   Biological motion is a powerful perceptual cue that can reveal important information about the inner state of an individual. Activation of inflammatory processes likely leads to changes in gait, posture, and mobility patterns, but the specific characteristics of inflammation-related biological motion have not been characterized. The aim of this study was to determine the effect of inflammation on gait and motion in humans. Systemic inflammation was induced in 19 healthy volunteers with an intravenous injection of lipopolysaccharide (2 ng/kg body weight). Biological motion parameters (walking speed, stride length and time, arm, leg, head, and shoulder angles) were assessed during a walking paradigm and the timed-up-and-go test. Cytokine concentrations, body temperature, and sickness symptoms were measured. During inflammation, compared to placebo, participants exhibited shorter, slower, and wider strides, less arm extension, less knee flexion, and a more downward-tilting head while walking. They were also slower and took a shorter first step in the timed-up-and-go test. Higher interleukin-6 concentrations, stronger sickness symptoms, and lower body temperature predicted the inflammation-related alterations in biological motion. These findings show that biological motion contains clear information about the inflammatory status of an individual, and may be used by peers or artificial intelligence to recognize that someone is sick or contagious. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0889-1591(19)30642-7  |  
------------------------------------------- 
10.1016/j.cmpb.2019.105254  |    Background and objective:  Electrocardiogram (ECG) is one of the most important tools for assessing cardiac function and detecting potential heart problems. However, most of the current ECG report records remain on the paper, which makes it difficult to preserve and analyze the data. Moreover, paper records could result in the loss significant data, which brings inconvenience to the subsequent clinical diagnosis or artificial intelligence-assisted heart health diagnosis. Taking digital pictures is an intuitive way of preserving these files and can be done simply using smartphones or any other devices with cameras. However, these real scene ECG images often have some image noise that hinders signal extraction. How to eliminate image noise and extract ECG binary image automatically from the noisy and low-quality real scene images of ECG reports is the first problem to be solved in this paper. Next, QRS recognition is implemented on the extracted binary images to determine key points of ECG signals. 1D digital ECG signal is also extracted for accessing the exact values of the extracted points. In light of these tasks, an automatic digital ECG signal extraction and normal QRS recognition from real scene ECG images is proposed in this paper. 
  Methods:  The normal QRS recognition approach for real scene ECG images in this paper consists of two steps: ECG binary image extraction from ECG images using a new two-layer hierarchical method, and the subsequent QRS recognition based on a novel feature-fusing method. ECG binary image extraction is implemented using sub-channel filters followed by an adaptive filtering algorithm. According to the ratio between pixel and real value of ECG binary image, 1D digital ECG signal is obtained. The normal QRS recognition includes three main steps: establishment of candidate point sets, feature fusion extraction, and QRS recognition. Two datasets are introduced for evaluation including a real scene ECG images dataset and the public Non-Invasive Fetal Electrocardiogram Database (FECG). 
  Results:  Through the experiment on real scene ECG image, the F<sub>1</sub> score for Q, R, S detection is 0.841, 0.992, and 0.891, respectively. The evaluation on the public FECG dataset also proves the robustness of our algorithm, where F<sub>1</sub> score for R is 0.992 (0.996 for thoracic lead) and 0.988 for thoracic S wave. 
  Conclusions:  The proposed method in this article is a promising tool for automatically extracting digital ECG signals and detecting QRS complex in real scene ECG images with normal QRS. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)30139-7  |  
------------------------------------------- 
10.1021/acsami.9b22369  |   Recently, neuromorphic devices have been receiving increasing interest in the field of artificial intelligence (AI). Realization of fundamental synaptic plasticities on hard-ware devices would endow new intensions for neuromorphic devices. Spike-rate-dependent plasticity (SRDP) is one of the most important synaptic learning mechanisms in brain cognitive behaviors. It is thus interesting to mimic the SRDP behaviors on solid-state neuromorphic devices. In the present work, nanogranular phosphorus silicate glass (PSG)-based proton conductive electrolyte-gated oxide neuromorphic transistors have been proposed. The oxide neuromorphic transistors have good transistor performances and frequency-dependent synaptic plasticity behavior. Moreover, the neuromorphic transistor exhibits SRDP activities. Interestingly, by introducing priming synaptic stimuli, the modulation of threshold frequency value distinguishing synaptic potentiation from synaptic depression is realized for the first time on an electrolyte-gated neuromorphic transistor. Such a mechanism can be well understood with interfacial proton gating effects of the nanogranular PSG-based electrolyte. Furthermore, the effects of SRDP learning rules on pattern learning and memory behaviors have been conceptually demonstrated. The proposed neuromorphic transistors have potential applications in neuromorphic engineering. 
  |  https://dx.doi.org/10.1021/acsami.9b22369  |  
------------------------------------------- 
10.1186/s12969-020-0404-8  |    Backgrounds:  In order to provide juvenile idiopathic arthritis (JIA) patients with better pre-conceptional and prenatal counselling, we investigated the obstetrical and neonatal outcomes among women with Asian descent. 
  Methods:  Through the linkage of Taiwan National Health Insurance database and National Birth Registry, we established a population-based birth cohort in Taiwan between 2004 and 2014. In a case control study design, first children born to mothers with JIA are identified and matched with 5 non-JIA controls by maternal age and birth year. Conditional logistic regression was used to calculate odds ratios for maternal and neonatal outcomes crude and with adjustment. 
  Results:  Of the 2,100,143 newborn, 778 (0.037%) were born to JIA mothers. Among them, 549 first-born children were included in this research. Our result suggested that babies born to mothers with JIA were more likely to have low birth body weight, with an adjusted OR of 1.35(95% CI: 1.02 to 1.79) when compared to babies born to mothers without. No differences were observed in other perinatal complications between women with and without JIA including stillbirth, prematurity, or small for gestational age. The rate of adverse obstetrical outcomes such as caesarean delivery, preeclampsia, gestational diabetes, postpartum hemorrhage and mortality were also similar between the two. 
  Conclusions:  Adverse obstetrical and neonatal outcomes were limited among Asian mothers with JIA. Intensive care may not be necessary for JIA mothers and their newborns. 
  |  https://ped-rheum.biomedcentral.com/articles/10.1186/s12969-020-0404-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31973755/  |  
------------------------------------------- 
10.1002/jemt.23429  |   The numbers of diagnosed patients by melanoma are drastic and contribute more deaths annually among young peoples. An approximately 192,310 new cases of skin cancer are diagnosed in 2019, which shows the importance of automated systems for the diagnosis process. Accordingly, this article presents an automated method for skin lesions detection and recognition using pixel-based seed segmented images fusion and multilevel features reduction. The proposed method involves four key steps: (a) mean-based function is implemented and fed input to top-hat and bottom-hat filters which later fused for contrast stretching, (b) seed region growing and graph-cut method-based lesion segmentation and fused both segmented lesions through pixel-based fusion, (c) multilevel features such as histogram oriented gradient (HOG), speeded up robust features (SURF), and color are extracted and simple concatenation is performed, and (d) finally variance precise entropy-based features reduction and classification through SVM via cubic kernel function. Two different experiments are performed for the evaluation of this method. The segmentation performance is evaluated on PH2, ISBI2016, and ISIC2017 with an accuracy of 95.86, 94.79, and 94.92%, respectively. The classification performance is evaluated on PH2 and ISBI2016 dataset with an accuracy of 98.20 and 95.42%, respectively. The results of the proposed automated systems are outstanding as compared to the current techniques reported in state of art, which demonstrate the validity of the proposed method. 
  |  https://doi.org/10.1002/jemt.23429  |  
------------------------------------------- 
10.1016/j.cyto.2020.155051  |   This study aimed to reveal a new dimension of allergy profiles in the general population by using machine learning to explore complex relationships among various cytokines/chemokines and allergic diseases (asthma and atopic dermatitis; AD). We examined the symptoms related to asthma and AD and the plasma levels of 72 cytokines/chemokines obtained from a general population of 161 children at 6 years of age who participated in a pilot birth cohort study of the Japan Environment and Children's Study (JECS). The children whose signs and symptoms fulfilled the criteria of AD, which are mostly based on questionnaire including past symptoms, tended to have higher levels of the two chemokine ligands, CCL17 and CCL27, which are used for diagnosis of AD. On the other hand, another AD-related chemokine CCL22 level in plasma was higher only in children with visible flexural eczema, which is one of AD diagnostic criteria but was judged on the same day of blood examination unlike other criteria. Here, we also developed an innovative method of machine learning for elucidating the complex cytokine/chemokine milieu related to symptoms of allergic diseases by using clustering analysis based on the random forest dissimilarity measure that relies on artificial intelligence (AI) technique. To our surprise, the majority of children showing at least any asthma-related symptoms during the last month were divided by AI into the two clusters, either cluster-2 having elevated levels of IL-33 (related to eosinophil activation) or cluster-3 having elevated levels of CXCL7/NAP2 (related to neutrophil activation), among the total three clusters. Future studies will clarify better approach for allergic diseases by endotype classification. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1043-4666(20)30067-3  |  
------------------------------------------- 
10.1016/j.artmed.2019.101782  |    Objective:  Electronic Medical Records (EMRs) contain temporal and heterogeneous doctor order information that can be used for treatment pattern discovery. Our objective is to identify "right patient", "right drug", "right dose", "right route", and "right time" from doctor order information. 
  Methods:  We propose a fusion framework to extract typical treatment patterns based on multi-view similarity Network Fusion (SNF) method. The multi-view SNF method involves three similarity measures: content-view similarity, sequence-view similarity and duration-view similarity. An EMR dataset and two metrics were utilized to evaluate the performance and to extract typical treatment patterns. 
  Results:  Experimental results on a real-world EMR dataset show that the multi-view similarity network fusion method outperforms all the single-view similarity measures and also outperforms the existing similarity measure methods. Furthermore, we extract and visualize typical treatment patterns by clustering analysis. 
  Conclusion:  The extracted typical treatment patterns by combining doctor order content, sequence, and duration views can provide data-driven guidelines for artificial intelligence in medicine and help clinicians make better decisions in clinical practice. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(18)30418-4  |  
------------------------------------------- 
10.1016/j.enfcli.2019.09.024  |   The issue of older adults' care is becoming a serious concern in Japan, which has a rapidly aging population and a low birthrate. The development of robots is pushed forward as a measure to compensate for the healthcare worker shortage. The purpose of this paper is to consider the potential legal issues of caring healthcare robot (CHR) for older adults' care. A CHR must recognize the speech, face, and presence of older adults, and make judgments and relay information based on acquired information. CHRs fulfill the caring function by being close to patients, to know them deeply, and to look after them. Therefore, communication functions by advanced artificial intelligence based on caring in nursing are essential for CHRs. The ability to maintain and improve the activities of daily living (ADL) of older adults is to facilitate activities through bidirectional information relay. Furthermore, without guarantees on the safety and ascription of responsibilities, the introduction of CHRs in clinical practice will not proceed. As laws differ from country to country, it is necessary to examine Japanese policies and related laws when using CHRs in Japan. Currently, there are no central rules on information security. In Japan, the law is made after a case has occurred; thus, dealing with novel issues as they occur will not have the benefit of legal guidance. Creating a broad legal framework or taking preventive measures at an early stage is needed. Therefore, as a first step, establishing guidelines for the use of CHRs will be valuable. 
  |  http://www.elsevier.es/en/linksolver/ft/pii/S1130-8621(19)30583-2  |  
------------------------------------------- 
10.21037/atm.2019.12.150  |   Optimal acetabular cup orientation is of substantial importance to good long-term function and low complication rates after total hip arthroplasty (THA). The radiographic anteversion (RA) and inclination (RI) angles of the cup are typically studied due to the practicability, simplicity, and ease of interpretation of their measurements. A great number of methods have been developed to date, most of which have been performed on pelvic or hip anteroposterior radiographs. However, there are primarily two influencing factors for these methods: X-ray offset and pelvic rotation. In addition, there are three types of pelvic rotations about the transverse, longitudinal, and anteroposterior axes of the body. Their effects on the RA and RI angles of the cup are interactively correlated with the position and true orientation of the cup. To date, various fitted or analytical models have been established to disclose the correlations between the X-ray offset and pelvic rotation and the RA and RI angles of the cup. Most of these models do not incorporate all the potential influencing parameters. Advanced methods for performing X-ray offset and pelvic rotation corrections are mainly performed on a single pelvic AP radiograph, two synchronized radiographs, or a two-dimensional/three-dimensional (2D-3D) registration system. Some measurement systems, originally developed for evaluating implant migration or wear, could also be used for correcting the X-ray offset and pelvic rotation simultaneously, but some drawbacks still exist with these systems. Above all, the 2D-3D registration technique might be an alternative and powerful tool for accurately measuring cup orientation. In addition to the current methods used for postoperative assessment, navigation systems and augmented reality are also used for the preoperative planning and intraoperative guidance of cup placement. With the continuing development of artificial intelligence and machine learning, these techniques could be incorporated into robot-assisted orthopaedic surgery in the future. 
  |  https://doi.org/10.21037/atm.2019.12.150  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32175423/  |  
------------------------------------------- 
10.1089/brain.2019.0722  |   This work addresses the problem of constructing a unified, topologically optimal connectivity-based brain atlas. The proposed approach aggregates an ensemble partition from individual parcellations without label agreement, providing a balance between sufficiently flexible individual parcellations and intuitive representation of the average topological structure of the connectome. The methods exploit a previously proposed dense connectivity representation, first performing graph-based hierarchical parcellation of individual brains, and subsequently aggregating the individual parcellations into a consensus parcellation. The search for consensus - based on the Hard Ensemble algorithm - approximately minimizes the sum of cluster membership distances, effectively estimating a pseudo-Karcher mean of individual parcellations. Computational stability, graph structure preservation and biological relevance of the simplified representation resulting from the proposed parcellation are assessed on the Human Connectome Project dataset. These aspects are assessed using (1) edge weight distribution divergence with respect to the dense connectome representation, (2) inter-hemispheric symmetry, (3) network characteristics' stability and agreement with respect to individually and anatomically parcellated networks, and (4) performance of the simplified connectome in a biological sex classification task. Ensemble parcellation was found to be highly stable with respect to subject sampling, outperforming anatomical atlases and other connectome-based parcellations in classification as well as preserving global connectome properties. The Hard Ensemble-based parcellation also showed a degree symmetry comparable to anatomical atlases and a high degree of spatial contiguity without using explicit priors. 
  |  https://www.liebertpub.com/doi/full/10.1089/brain.2019.0722?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1177/0886260520913648  |   Sexual violence (SV) is a public health concern for youth. Few longitudinal studies address how experiences of SV and co-occurring depression and anxiety symptoms in early young adulthood may contribute to poorer functioning in young adulthood. Using a sample of 2,416 youth aged 18 to 20 (Time 1), we assessed past year SV and co-occurring depression and anxiety symptoms. One year later, when youth were between the ages of 20 and 22 (Time 2), participants indicated their functioning in four domains: physical health and sleep quality, substance use consequences, psychological functioning, and social functioning. Using latent class analyses, we found six participant classes at Time 1 based on SV experiences and co-occurring depression and anxiety symptoms. Classes were variable by participants' reported degree of SV experiences and co-occurring depression and anxiety symptomology. Longitudinal analyses indicated that youth in a class that experienced high levels of both SV and co-occurring depression and anxiety at Time 1 generally reported the poorest functioning in all key domains at Time 2. However, classes where participants reported greater depression and anxiety symptoms-most often in the presence of, but at times in the absence of, SV-were consistently associated with poorer functioning for physical health, psychological functioning, and social health. Classes where participants reported greater SV-in the presence of, but at times in the absence of, depression and anxiety symptoms-were associated with greater alcohol and marijuana consequences. Findings suggest prevention of SV, accessible counseling for those that have experienced SV, and screening to identify and intervene to address depression and anxiety may all be essential to help prevent poorer functioning in young adulthood. 
  |  http://journals.sagepub.com/doi/full/10.1177/0886260520913648?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/jamia/ocz166  |    Objective:  This article summarizes the preparation, organization, evaluation, and results of Track 2 of the 2018 National NLP Clinical Challenges shared task. Track 2 focused on extraction of adverse drug events (ADEs) from clinical records and evaluated 3 tasks: concept extraction, relation classification, and end-to-end systems. We perform an analysis of the results to identify the state of the art in these tasks, learn from it, and build on it. 
  Materials and methods:  For all tasks, teams were given raw text of narrative discharge summaries, and in all the tasks, participants proposed deep learning-based methods with hand-designed features. In the concept extraction task, participants used sequence labelling models (bidirectional long short-term memory being the most popular), whereas in the relation classification task, they also experimented with instance-based classifiers (namely support vector machines and rules). Ensemble methods were also popular. 
  Results:  A total of 28 teams participated in task 1, with 21 teams in tasks 2 and 3. The best performing systems set a high performance bar with F1 scores of 0.9418 for concept extraction, 0.9630 for relation classification, and 0.8905 for end-to-end. However, the results were much lower for concepts and relations of Reasons and ADEs. These were often missed because local context is insufficient to identify them. 
  Conclusions:  This challenge shows that clinical concept extraction and relation classification systems have a high performance for many concept types, but significant improvement is still required for ADEs and Reasons. Incorporating the larger context or outside knowledge will likely improve the performance of future systems. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz166  |  
------------------------------------------- 
10.1007/s00345-020-03214-y  |    Purpose:  Urological oncologists have difficulty providing optimal personalized care due to rapid alterations in scientific research results, medical advancements, and treatment guidelines. IBM's Watson for Oncology (WFO) is an artificial intelligence clinical decision-support system that assists oncologists with evidence-based treatment recommendations. In the present study, we examined the level of concordance between the treatment recommendations for prostate cancer according to WFO and the actual treatments that the patients received in the department of urology. 
  Methods:  We enrolled 201 patients who received prostate cancer treatment between January 2018 and June 2018. WFO provided treatment recommendations using clinical data in three categories: recommended, for consideration, and not recommended. These were compared with the actual treatments received by patients. Prostate cancer treatments were considered concordant if the received treatments were included in the "recommended" or "for consideration" categories by WFO. 
  Results:  The patients' mean age was 71.2 years. There were 60 (29.9%) and 114 (56.7%) patients with an Eastern Cooperative Oncology Group (ECOG) performance score ≥ 1 and non-organ confined disease (stage III/IV), respectively. The overall prostate cancer treatment concordance rate was 73.6% ("recommended": 53.2%; "for consideration": 20.4%). An ECOG performance score ≥ 1 and older age (≥ 75 years) were significantly associated with discordance (p = 0.001 and p = 0.026, respectively) on multivariate analysis. 
  Conclusion:  In the present study, the treatment recommendations by WFO and the actual received treatments in the department of urology showed a relatively high concordance rate in prostate cancer patients. 
  |  https://dx.doi.org/10.1007/s00345-020-03214-y  |  
------------------------------------------- 
10.3390/s20092467  |   Today, daily life is composed of many computing systems, therefore interacting with them in a natural way makes the communication process more comfortable. Human-Computer Interaction (HCI) has been developed to overcome the communication barriers between humans and computers. One form of HCI is Hand Gesture Recognition (HGR), which predicts the class and the instant of execution of a given movement of the hand. One possible input for these models is surface electromyography (EMG), which records the electrical activity of skeletal muscles. EMG signals contain information about the intention of movement generated by the human brain. This systematic literature review analyses the state-of-the-art of real-time hand gesture recognition models using EMG data and machine learning. We selected and assessed 65 primary studies following the Kitchenham methodology. Based on a common structure of machine learning-based systems, we analyzed the structure of the proposed models and standardized concepts in regard to the types of models, data acquisition, segmentation, preprocessing, feature extraction, classification, postprocessing, real-time processing, types of gestures, and evaluation metrics. Finally, we also identified trends and gaps that could open new directions of work for future research in the area of gesture recognition using EMG. 
  |  http://www.mdpi.com/resolver?pii=s20092467  |  
------------------------------------------- 
10.1111/febs.15314  |   Crystallographic models of biological macromolecules have been ranked using the quality criteria associated with them in the Protein Data Bank (PDB). The outcomes of this quality analysis have been correlated with time and with the journals that published papers based on those models. The results show that the overall quality of PDB structures has substantially improved over the last ten years, but this period of progress was preceded by several years of stagnation or even depression. Moreover, the study shows that the historically observed negative correlation between journal impact and the quality of structural models presented therein seems to disappear as time progresses. 
  |  https://doi.org/10.1111/febs.15314  |  
------------------------------------------- 
10.3390/diagnostics10040196  |   Narrative texts in electronic health records can be efficiently utilized for building decision support systems in the clinic, only if they are correctly interpreted automatically in accordance with a specified standard. This paper tackles the problem of developing an automated method of labeling free-form radiology reports, as a precursor for building query-capable report databases in hospitals. The analyzed dataset consists of 1295 radiology reports concerning the condition of a knee, retrospectively gathered at the Clinical Hospital Centre Rijeka, Croatia. Reports were manually labeled with one or more labels from a set of 10 most commonly occurring clinical conditions. After primary preprocessing of the texts, two sets of text classification methods were compared: (1) traditional classification models-Naive Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), and Random Forests (RF)-coupled with Bag-of-Words (BoW) features (i.e., symbolic text representation) and (2) Convolutional Neural Network (CNN) coupled with dense word vectors (i.e., word embeddings as a semantic text representation) as input features. We resorted to nested 10-fold cross-validation to evaluate the performance of competing methods using accuracy, precision, recall, and F 1 score. The CNN with semantic word representations as input yielded the overall best performance, having a micro-averaged F 1 score of 86 . 7 % . The CNN classifier yielded particularly encouraging results for the most represented conditions: degenerative disease ( 95 . 9 % ), arthrosis ( 93 . 3 % ), and injury ( 89 . 2 % ). As a data-hungry deep learning model, the CNN, however, performed notably worse than the competing models on underrepresented classes with fewer training instances such as multicausal disease or metabolic disease. LR, RF, and SVM performed comparably well, with the obtained micro-averaged F 1 scores of 84 . 6 % , 82 . 2 % , and 82 . 1 % , respectively. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10040196  |  
------------------------------------------- 
10.1002/mp.13978  |    Purpose:  Cone-beam computed tomography (CBCT) scanning is used daily or weekly (i.e., on-treatment CBCT) for accurate patient setup in image-guided radiotherapy. However, inaccuracy of CT numbers prevents CBCT from performing advanced tasks such as dose calculation and treatment planning. Motivated by the promising performance of deep learning in medical imaging, we propose a deep U-net-based approach that synthesizes CT-like images with accurate numbers from planning CT, while keeping the same anatomical structure as on-treatment CBCT. 
  Methods:  We formulated the CT synthesis problem under a deep learning framework, where a deep U-net architecture was used to take advantage of the anatomical structure of on-treatment CBCT and image intensity information of planning CT. U-net was chosen because it exploits both global and local features in the image spatial domain, matching our task to suppress global scattering artifacts and local artifacts such as noise in CBCT. To train the synthetic CT generation U-net (sCTU-net), we include on-treatment CBCT and initial planning CT of 37 patients (30 for training, seven for validation) as the input. Additional replanning CT images acquired on the same day as CBCT after deformable registration are utilized as the corresponding reference. To demonstrate the effectiveness of the proposed sCTU-net, we use another seven independent patient cases (560 slices) for testing. 
  Results:  We quantitatively compared the resulting synthetic CT (sCT) with the original CBCT image using deformed same-day pCT images as reference. The averaged accuracy measured by mean absolute error (MAE) between sCT and reference CT (rCT) on testing data is 18.98 HU, while MAE between CBCT and rCT is 44.38 HU. 
  Conclusions:  The proposed sCTU-net can synthesize CT-quality images with accurate CT numbers from on-treatment CBCT and planning CT. This potentially enables advanced CBCT applications for adaptive treatment planning. 
  |  https://doi.org/10.1002/mp.13978  |  
------------------------------------------- 
10.3390/bs10020055  |   Postpartum Depression (PPD), a condition that affects up to 15% of mothers in high-income countries, reduces attention to the needs of the child and is among the first causes of infanticide. PPD is usually identified using self-report measures and therefore it is possible that mothers are unwilling to report PPD because of a social desirability bias. Previous studies have highlighted the presence of significant differences in the acoustical properties of the vocalizations of infants of depressed and healthy mothers, suggesting that the mothers' behavior can induce changes in infants' vocalizations. In this study, cry episodes of infants (N = 56, 157.4 days ± 8.5, 62% firstborn) of depressed (N = 29) and non-depressed (N = 27) mothers (mean age = 31.1 years ± 3.9) are analyzed to investigate the possibility that a cloud-based machine learning model can identify PPD in mothers from the acoustical properties of their infants' vocalizations. Acoustic features (fundamental frequency, first four formants, and intensity) are first extracted from recordings of crying infants, then cloud-based artificial intelligence models are employed to identify maternal depression versus non-depression from estimated features. The trained model shows that commonly adopted acoustical features can be successfully used to identify postpartum depressed mothers with high accuracy (89.5%). 
  |  http://www.mdpi.com/resolver?pii=bs10020055  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32041121/  |  
------------------------------------------- 
10.3389/fpsyt.2020.00026  |   Bullying and sibling aggression can appear as similar behavior, though the latter is comparatively understudied. Aligned with the Theory of Intergenerational Transmission of Violence, research suggests that exposure to family violence increases an individual's risk for perpetrating violence in their own future relationships. Additionally, Problem Behavior Theory suggests that engaging in one problem behavior (e.g., bullying) increases the likelihood of engaging in other problem behavior (e.g., substance use). In Phase 1, this study of middle school students from the U.S. examined how exposure to family violence predicted membership in latent classes of bullying and sibling aggression perpetration (<i>N</i> = 894, sampled from four middle schools). In Phase 2, we used mixture modeling to understand how latent classes of family violence, sibling aggression, and bullying predict future substance use, mental health outcomes, and deviance behavior later in high school. Results yielded four profiles of peer and sibling aggression: <i>high all</i>, <i>high sibling aggression</i>, <i>high peer aggression</i>, and <i>low all aggression</i>. Youth who reported witnessing more family violence at home were significantly more likely to fall into the <i>sibling aggression</i> only and <i>high all</i> classes, compared to the <i>low all</i> class. Phase 2 results also yielded four classes: a <i>high all</i> class, a <i>sibling aggression and family violence</i> class, a <i>peer aggression</i> class, and a <i>low all</i> class. Individuals in the <i>high all</i> class were more likely to experience several unfavorable outcomes (substance use, depression, delinquency) compared to other classes. This study provides evidence for pathways from witnessing violence, to perpetrating aggression across multiple contexts, to developing other deleterious mental and behavioral health outcomes. These findings highlight the negative impact family violence can have on child development, providing support for a cross-contextual approach for programming aimed at developing relationships skills. 
  |  https://doi.org/10.3389/fpsyt.2020.00026  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32116843/  |  
------------------------------------------- 
10.1007/s11548-019-02114-w  |    Purpose:  Intensive planning and analysis from echocardiography are a crucial step before reconstructive surgeries are applied to malfunctioning mitral valves. Volume visualizations of echocardiographic data are often used in clinical routine. However, they lack a clear visualization of the crucial factors for decision making. 
  Methods:  We build upon patient-specific mitral valve surface models segmented from echocardiography that represent the valve's geometry, but suffer from self-occlusions due to complex 3D shape. We transfer these to 2D maps by unfolding their geometry, resulting in a novel 2D representation that maintains anatomical resemblance to the 3D geometry. It can be visualized together with color mappings and presented to physicians to diagnose the pathology in one gaze without the need for further scene interaction. Furthermore, it facilitates the computation of a Pathology Score, which can be used for diagnosis support. 
  Results:  Quality and effectiveness of the proposed methods were evaluated through a user survey conducted with domain experts. We assessed pathology detection accuracy using 3D valve models in comparison with the novel visualizations. Classification accuracy increased by 5.3% across all tested valves and by 10.0% for prolapsed valves. Further, the participants' understanding of the relation between 3D and 2D views was evaluated. The Pathology Score is found to have potential to support discriminating pathologic valves from normal valves. 
  Conclusions:  In summary, our survey shows that pathology detection can be improved in comparison with simple 3D surface visualizations of the mitral valve. The correspondence between the 2D and 3D representations is comprehensible, and color-coded pathophysiological magnitudes further support the clinical assessment. 
  |  https://dx.doi.org/10.1007/s11548-019-02114-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31955326/  |  
------------------------------------------- 
10.3390/s20041068  |   An entity's existence in an image can be depicted by the activity instantiation vector from a group of neurons (called capsule). Recently, multi-layered capsules, called CapsNet, have proven to be state-of-the-art for image classification tasks. This research utilizes the prowess of this algorithm to detect pneumonia from chest X-ray (CXR) images. Here, an entity in the CXR image can help determine if the patient (whose CXR is used) is suffering from pneumonia or not. A simple model of capsules (also known as Simple CapsNet) has provided results comparable to best Deep Learning models that had been used earlier. Subsequently, a combination of convolutions and capsules is used to obtain two models that outperform all models previously proposed. These models-Integration of convolutions with capsules (ICC) and Ensemble of convolutions with capsules (ECC)-detect pneumonia with a test accuracy of 95.33% and 95.90%, respectively. The latter model is studied in detail to obtain a variant called EnCC, where n = 3, 4, 8, 16. Here, the E4CC model works optimally and gives test accuracy of 96.36%. All these models had been trained, validated, and tested on 5857 images from Mendeley. 
  |  http://www.mdpi.com/resolver?pii=s20041068  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32075339/  |  
------------------------------------------- 
10.3389/fnhum.2019.00457  |   To characterize each cognitive function <i>per se</i> and to understand the brain as an aggregate of those functions, it is vital to relate dozens of these functions to each other. Knowledge about the relationships among cognitive functions is informative not only for basic neuroscientific research but also for clinical applications and developments of brain-inspired artificial intelligence. In the present study, we propose an exhaustive data mining approach to reveal relationships among cognitive functions based on functional brain mapping and network analysis. We began our analysis with 109 pseudo-activation maps (cognitive function maps; CFM) that were reconstructed from a functional magnetic resonance imaging meta-analysis database, each of which corresponds to one of 109 cognitive functions such as 'emotion,' 'attention,' 'episodic memory,' etc. Based on the resting-state functional connectivity between the CFMs, we mapped the cognitive functions onto a two-dimensional space where the relevant functions were located close to each other, which provided a rough picture of the brain as an aggregate of cognitive functions. Then, we conducted so-called conceptual analysis of cognitive functions using clustering of voxels in each CFM connected to the other 108 CFMs with various strengths. As a result, a CFM for each cognitive function was subdivided into several parts, each of which is strongly associated with some CFMs for a subset of the other cognitive functions, which brought in sub-concepts (i.e., sub-functions) of the cognitive function. Moreover, we conducted network analysis for the network whose nodes were parcels derived from whole-brain parcellation based on the whole-brain voxel-to-CFM resting-state functional connectivities. Since each parcel is characterized by associations with the 109 cognitive functions, network analyses using them are expected to inform about relationships between cognitive and network characteristics. Indeed, we found that informational diversities of interaction between parcels and densities of local connectivity were dependent on the kinds of associated functions. In addition, we identified the homogeneous and inhomogeneous network communities about the associated functions. Altogether, we suggested the effectiveness of our approach in which we fused the large-scale meta-analysis of functional brain mapping with the methods of network neuroscience to investigate the relationships among cognitive functions. 
  |  https://doi.org/10.3389/fnhum.2019.00457  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31998102/  |  
------------------------------------------- 
10.1016/j.yebeh.2020.107021  |    Purpose:  The 2017 epilepsy and seizure diagnosis framework emphasizes epilepsy syndromes and the etiology-based approach. We developed a propositional artificial intelligence (AI) system based on the above concepts to support physicians in the diagnosis of epilepsy. 
  Methods:  We analyzed and built ontology knowledge for the classification of seizure patterns, epilepsy, epilepsy syndrome, and etiologies. Protégé ontology tool was applied in this study. In order to enable the system to be close to the inferential thinking of clinical experts, we classified and constructed knowledge of other epilepsy-related knowledge, including comorbidities, epilepsy imitators, epilepsy descriptors, characteristic electroencephalography (EEG) findings, treatments, etc. We used the Ontology Web Language with Description Logic (OWL-DL) and Semantic Web Rule Language (SWRL) to design rules for expressing the relationship between these ontologies. 
  Results:  Dravet syndrome was taken as an illustration for epilepsy syndromes implementation. We designed an interface for the physician to enter the various characteristics of the patients. Clinical data of an 18-year-old boy with epilepsy was applied to the AI system. Through SWRL and reasoning engine Drool's execution, we successfully demonstrate the process of differential diagnosis. 
  Conclusion:  We developed a propositional AI system by using the OWL-DL/SWRL approach to deal with the complexity of current epilepsy diagnosis. The experience of this system, centered on the clinical epilepsy syndromes, paves a path to construct an AI system for further complicated epilepsy diagnosis. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1525-5050(20)30200-6  |  
------------------------------------------- 
10.1021/acs.molpharmaceut.9b00983  |   Ultrasound-induced microbubble sonoporation has been shown to effectively improve drug/gene delivery efficiency by enhancing tissue and cell permeability. However, the microscale size and short duration of ultrasound contrast agents limit their accumulation in target areas. Here, a kind of ultrasound-triggered phase-transitioning and size-changing cationic nanodroplet, perfluoropentane/C<sub>9</sub>F<sub>17</sub>-PAsp(DET)/miR-122/poly(glutamic acid)-<i>g</i>-MeO-poly(ethylene glycol) (PGA-<i>g</i>-mPEG) ternary nanodroplets (PFP-TNDs/miR-122), was developed to deliver microRNA-122 (miR-122) for hepatocellular carcinoma (HCC) treatment. PFP served as an ultrasound-sensitive core for ultrasound-triggered phase transition and size change from the nanoscale to the microscale. Positively charged C<sub>9</sub>F<sub>17</sub>-PAsp(DET) ensured adequate miRNA loading. PGA-<i>g</i>-mPEG, which served as the shell of the nanodroplet, modified the nanodroplets, enhanced their stability in serum, and protected miR-122 from degradation in vivo. The results exhibited that PFP-TNDs/miR-122 has a nanosize diameter (362 ± 15 nm) and remained stable for 24 h. After treatment with PFP-TNDs/miR-122 combined with ultrasound irradiation, the miR-122 expression level was significantly increased by approximately 600-fold in HepG2 cells, 500-fold in SMMC-7721 cells, and 30-fold in human HCC xenografts. Moreover, PFP-TNDs/miR-122 combined with ultrasound radiation effectively suppressed the growth, migration, and invasion of HCC cells, and inhibited tumor proliferation in mice. This study revealed that the biodegradable PFP-TNDs is a promising therapeutic gene carrier with functions of gene protection and effective gene delivery for clinical applications. Furthermore, PFP-TNDs/miR-122 associated with ultrasound irradiation may pave a new way to improve the prognosis of patients with HCC. 
  |  https://dx.doi.org/10.1021/acs.molpharmaceut.9b00983  |  
------------------------------------------- 
10.1007/978-1-0716-0282-9_17  |   Computational methods are a powerful and consolidated tool in the early stage of the drug lead discovery process. Among these techniques, high-throughput molecular docking has proved to be extremely useful in identifying novel bioactive compounds within large chemical libraries. In the docking procedure, the predominant binding mode of each small molecule within a target binding site is assessed, and a docking score reflective of the likelihood of binding is assigned to them. These methods also shed light on how a given hit could be modified in order to improve protein-ligand interactions and are thus able to guide lead optimization. The possibility of reducing time and cost compared to experimental approaches made this technology highly appealing. Due to methodological developments and the increase of computational power, the application of quantum mechanical methods to study macromolecular systems has gained substantial attention in the last decade. A quantum mechanical description of the interactions involved in molecular association of biomolecules may lead to better accuracy compared to molecular mechanics, since there are many physical phenomena that cannot be correctly described within a classical framework, such as covalent bond formation, polarization effects, charge transfer, bond rearrangements, halogen bonding, and others, that require electrons to be explicitly accounted for. Considering the fact that quantum mechanics-based approaches in biomolecular simulation constitute an active and important field of research, we highlight in this work the recent developments of quantum mechanical-based molecular docking and high-throughput docking. 
  |  https://dx.doi.org/10.1007/978-1-0716-0282-9_17  |  
------------------------------------------- 
10.1159/000505429  |   Medical imaging plays a key role in evaluating and monitoring lung diseases such as chronic obstructive pulmonary disease (COPD) and lung cancer. The application of artificial intelligence in medical imaging has transformed medical images into mineable data, by extracting and correlating quantitative imaging features with patients' outcomes and tumor phenotype - a process termed radiomics. While this process has already been widely researched in lung oncology, the evaluation of COPD in this fashion remains in its infancy. Here we outline the main applications of radiomics in lung cancer and briefly review the workflow from image acquisition to the evaluation of model performance. Finally, we discuss the current assessments of COPD and the potential application of radiomics in COPD. 
  |  https://www.karger.com?DOI=10.1159/000505429  |  
------------------------------------------- 
10.1093/ndt/gfz295  |    Background:  All-cause mortality in haemodialysis (HD) is high, reaching 15.6% in the first year according to the European Renal Association. 
  Methods:  A new clinical tool to predict all-cause mortality in HD patients is proposed. It uses a post hoc analysis of data from the prospective cohort study Photo-Graph V3. A total of 35 variables related to patient characteristics, laboratory values and treatments were used as predictors of all-cause mortality. The first step was to compare the results obtained using a logistic regression to those obtained by a Bayesian network. The second step aimed to increase the performance of the best prediction model using synthetic data. Finally, a compromise between performance and ergonomics was proposed by reducing the number of variables to be entered in the prediction tool. 
  Results:  Among the 9010 HD patients included in the Photo-Graph V3 study, 4915 incident patients with known medical status at 2 years were analysed. All-cause mortality at 2 years was 34.1%. The Bayesian network provided the most reliable prediction. The final optimized models that used 14 variables had areas under the receiver operating characteristic curves of 0.78 ± 0.01, sensitivity of 72 ± 2%, specificity of 69 ± 2%, predictive positive value of 70 ± 1% and negative predictive value of 71 ± 2% for the prediction of all-cause mortality. 
  Conclusions:  Using artificial intelligence methods, a new clinical tool to predict all-cause mortality in incident HD patients is proposed. The latter can be used for research purposes before its external validation at: https://www.hed.cc/? a=twoyearsallcausemortalityhemod&amp;n=2-years%20All-cause%20Mortality%20Hemodialysis.neta. 
  |  https://academic.oup.com/ndt/article-lookup/doi/10.1093/ndt/gfz295  |  
------------------------------------------- 
10.1016/j.compbiomed.2020.103629  |   A deep learning (DL) network for 2D-based breast mass segmentation in unenhanced dedicated breast CT images was developed and validated, and its robustness in radiomic feature stability and diagnostic performance compared to manual annotations of multiple radiologists was investigated. 93 mass-like lesions were extensively augmented and used to train the network (n = 58 masses), which was then tested (n = 35 masses) against manual ground truth of a qualified breast radiologist with experience in breast CT imaging using the Conformity coefficient (with a value equal to 1 indicating a perfect performance). Stability and diagnostic power of 672 radiomic descriptors were investigated between the computerized segmentation, and 4 radiologists' annotations for the 35 test set cases. Feature stability and diagnostic performance in the discrimination between benign and malignant cases were quantified using intraclass correlation (ICC) and multivariate analysis of variance (MANOVA), performed for each segmentation case (4 radiologists and DL algorithm). DL-based segmentation resulted in a Conformity of 0.85 ± 0.06 against the annotated ground truth. For the stability analysis, although modest agreement was found among the four annotations performed by radiologists (Conformity 0.78 ± 0.03), over 90% of all radiomic features were found to be stable (ICC&gt;0.75) across multiple segmentations. All MANOVA analyses were statistically significant (p ≤ 0.05), with all dimensions equal to 1, and Wilks' lambda ≤0.35. In conclusion, DL-based mass segmentation in dedicated breast CT images can achieve high segmentation performance, and demonstrated to provide stable radiomic descriptors with comparable discriminative power in the classification of benign and malignant tumors to expert radiologist annotation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(20)30028-7  |  
------------------------------------------- 
10.1161/JAHA.119.014717  |   Background Severe, symptomatic aortic stenosis (AS) is associated with poor prognoses. However, early detection of AS is difficult because of the long asymptomatic period experienced by many patients, during which screening tools are ineffective. The aim of this study was to develop and validate a deep learning-based algorithm, combining a multilayer perceptron and convolutional neural network, for detecting significant AS using ECGs. Methods and Results This retrospective cohort study included adult patients who had undergone both ECG and echocardiography. A deep learning-based algorithm was developed using 39 371 ECGs. Internal validation of the algorithm was performed with 6453 ECGs from one hospital, and external validation was performed with 10 865 ECGs from another hospital. The end point was significant AS (beyond moderate). We used demographic information, features, and 500-Hz, 12-lead ECG raw data as predictive variables. In addition, we identified which region had the most significant effect on the decision-making of the algorithm using a sensitivity map. During internal and external validation, the areas under the receiver operating characteristic curve of the deep learning-based algorithm using 12-lead ECG for detecting significant AS were 0.884 (95% CI, 0.880-0.887) and 0.861 (95% CI, 0.858-0.863), respectively; those using a single-lead ECG signal were 0.845 (95% CI, 0.841-0.848) and 0.821 (95% CI, 0.816-0.825), respectively. The sensitivity map showed the algorithm focused on the T wave of the precordial lead to determine the presence of significant AS. Conclusions The deep learning-based algorithm demonstrated high accuracy for significant AS detection using both 12-lead and single-lead ECGs. 
  |  http://www.ahajournals.org/doi/full/10.1161/JAHA.119.014717?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/s20030879  |   As research in smart homes and activity recognition is increasing, it is of ever increasing importance to have benchmarks systems and data upon which researchers can compare methods. While synthetic data can be useful for certain method developments, real data sets that are open and shared are equally as important. This paper presents the E-care@home system, its installation in a real home setting, and a series of data sets that were collected using the E-care@home system. Our first contribution, the E-care@home system, is a collection of software modules for data collection, labeling, and various reasoning tasks such as activity recognition, person counting, and configuration planning. It supports a heterogeneous set of sensors that can be extended easily and connects collected sensor data to higher-level Artificial Intelligence (AI) reasoning modules. Our second contribution is a series of open data sets which can be used to recognize activities of daily living. In addition to these data sets, we describe the technical infrastructure that we have developed to collect the data and the physical environment. Each data set is annotated with ground-truth information, making it relevant for researchers interested in benchmarking different algorithms for activity recognition. 
  |  http://www.mdpi.com/resolver?pii=s20030879  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32041376/  |  
------------------------------------------- 
10.1155/2020/8989752  |   Compared with traditional imaging, the light field contains more comprehensive image information and higher image quality. However, the available data for light field reconstruction are limited, and the repeated calculation of data seriously affects the accuracy and the real-time performance of multiperspective light field reconstruction. To solve the problems, this paper proposes a multiperspective light field reconstruction method based on transfer reinforcement learning. Firstly, the similarity measurement model is established. According to the similarity threshold of the source domain and the target domain, the reinforcement learning model or the feature transfer learning model is autonomously selected. Secondly, the reinforcement learning model is established. The model uses multiagent (i.e., multiperspective) Q-learning to learn the feature set that is most similar to the target domain and the source domain and feeds it back to the source domain. This model increases the capacity of the source-domain samples and improves the accuracy of light field reconstruction. Finally, the feature transfer learning model is established. The model uses PCA to obtain the maximum embedding space of source-domain and target-domain features and maps similar features to a new space for label data migration. This model solves the problems of multiperspective data redundancy and repeated calculations and improves the real-time performance of maneuvering target recognition. Extensive experiments on PASCAL VOC datasets demonstrate the effectiveness of the proposed algorithm against the existing algorithms. 
  |  https://doi.org/10.1155/2020/8989752  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32076436/  |  
------------------------------------------- 
10.1016/S2468-1253(19)30416-9  |   Pancreatic ductal adenocarcinoma is most frequently detected at an advanced stage. Such late detection restricts treatment options and contributes to a dismal 5-year survival rate of 3-15%. Pancreatic ductal adenocarcinoma is relatively uncommon and screening of the asymptomatic adult population is not feasible or recommended with current modalities. However, screening of individuals in high-risk groups is recommended. Here, we review groups at high risk for pancreatic ductal adenocarcinoma, including individuals with inherited predisposition and patients with pancreatic cystic lesions. We discuss studies aimed at finding ways of identifying pancreatic ductal adenocarcinoma in high-risk groups, such as among individuals with new-onset diabetes mellitus and people attending primary and secondary care practices with symptoms that suggest this cancer. We review early detection biomarkers, explore the potential of using social media for detection, appraise prediction models developed using electronic health records and research data, and examine the application of artificial intelligence to medical imaging for the purposes of early detection. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2468-1253(19)30416-9  |  
------------------------------------------- 
10.1007/s12032-020-01353-1  |   The diagnosis of breast cancer currently relies on radiological and clinical evaluation, confirmed by histopathological examination. However, such approach has some limitations as the suboptimal sensitivity, the long turnaround time for recall tests, the invasiveness of the procedure and the risk that some features of target lesions may remain undetected, making re-biopsy a necessity. Recent technological advances in the field of artificial intelligence hold promise in addressing such medical challenges not only in cancer diagnosis, but also in treatment assessment, and monitoring of disease progression. In the perspective of a truly personalised medicine, based on the early diagnosis and individually tailored treatments, two new technologies, namely radiomics and liquid biopsy, are rising as means to obtain information from diagnosis to molecular profiling and response assessment, without the need of a biopsied tissue sample. Radiomics works through the extraction of quantitative peculiar features of cancer from radiological data, while liquid biopsy gets the whole of the malignancy's biology from something as easy as a blood sample. Both techniques hopefully will identify diagnostic and prognostic information of breast cancer potentially reducing the need for invasive (and often difficult to perform) biopsies and favouring an approach that is as personalised as possible for each patient. Nevertheless, such techniques will not substitute tissue biopsy in the near future, and even in further times they will require the aid of other parameters to be correctly interpreted and acted upon. 
  |  https://dx.doi.org/10.1007/s12032-020-01353-1  |  
------------------------------------------- 
10.1089/cmb.2019.0334  |   Cardiovascular and cerebrovascular diseases, which mainly consist of atherosclerosis (AS), are major causes of death. A great deal of research has been carried out to clarify the molecular mechanisms of AS. However, the etiology of AS remains poorly understood. To screen the potential genes of AS occurrence and development, GSE43292 and GSE57691 were obtained from the Gene Expression Omnibus (GEO) database in this study for bioinformatic analysis. First, GEO2R was used to identify differentially expressed genes (DEGs) and the functional annotation of DEGs was performed by gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis. The Search Tool for the Retrieval of Interacting Genes (STRING) tool was used to construct the protein-protein interaction network and the most important modules and core genes were mined. The results show that a total of 211 DEGs are identified. The functional changes of DEGs are mainly associated with the cellular process, catalytic activity, and protein binding. Eighteen genes were identified as core genes. Bioinformatic analysis showed that the core genes are mainly enriched in numerous processes related to actin. In conclusion, the DEGs and hub genes identified in this study may help us understand the potential etiology of the occurrence and development of AS. 
  |  https://www.liebertpub.com/doi/full/10.1089/cmb.2019.0334?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1371/journal.pone.0226248  |   Depression is a major public health concern in the U.S. and globally. While successful early identification and treatment can lead to many positive health and behavioral outcomes, depression, remains undiagnosed, untreated or undertreated due to several reasons, including denial of the illness as well as cultural and social stigma. With the ubiquity of social media platforms, millions of people are now sharing their online persona by expressing their thoughts, moods, emotions, and even their daily struggles with mental health on social media. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of depressive symptoms from tweets obtained, unobtrusively. Particularly, we examine and exploit multimodal big (social) data to discern depressive behaviors using a wide variety of features including individual-level demographics. By developing a multimodal framework and employing statistical techniques to fuse heterogeneous sets of features obtained through the processing of visual, textual, and user interaction data, we significantly enhance the current state-of-the-art approaches for identifying depressed individuals on Twitter (improving the average F1-Score by 5 percent) as well as facilitate demographic inferences from social media. Besides providing insights into the relationship between demographics and mental health, our research assists in the design of a new breed of demographic-aware health interventions. 
  |  http://dx.plos.org/10.1371/journal.pone.0226248  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32275658/  |  
------------------------------------------- 
10.1038/s41565-020-0655-z  |   Traditional von Neumann computing systems involve separate processing and memory units. However, data movement is costly in terms of time and energy and this problem is aggravated by the recent explosive growth in highly data-centric applications related to artificial intelligence. This calls for a radical departure from the traditional systems and one such non-von Neumann computational approach is in-memory computing. Hereby certain computational tasks are performed in place in the memory itself by exploiting the physical attributes of the memory devices. Both charge-based and resistance-based memory devices are being explored for in-memory computing. In this Review, we provide a broad overview of the key computational primitives enabled by these memory devices as well as their applications spanning scientific computing, signal processing, optimization, machine learning, deep learning and stochastic computing. 
  |  None  |  
------------------------------------------- 
10.1016/j.radcr.2020.04.031  |   The SARS-CoV-2 infection (COVID-19), originally reported in Wuhan, China, has rapidly proliferated throughout several continents and the first case in the United States was reported on January 19, 2020. According to the ACR guidelines issued shortly after this disease was declared a pandemic, radiologists are expected to familiarize themselves with the CT appearance of COVID-19 infection in order to be able to identify specific findings of this entity. This case report discusses the relevant imaging findings of one of the first cases in the midwestern US. It involves a 60-year-old man who presented with fever, dyspnea, and cough for 1 week and subsequently tested positive for COVID-19. The utility of the noncontrast CT chest in the diagnosis of COVID-19 has been controversial, but there are specific imaging findings that have been increasingly associated with this virus in the appropriate clinical context. The stages of imaging findings in COVID-19 are considered along with the implications of fibrosis throughout the stages. Future considerations include using artificial intelligence algorithms to distinguish between community acquired pneumonias and COVID-19 infection. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1930-0433(20)30137-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32313588/  |  
------------------------------------------- 
10.1016/j.neuroimage.2020.116745  |   The 21st century marks the emergence of "big data" with a rapid increase in the availability of data sets with multiple measurements. In neuroscience, brain-imaging datasets are more commonly accompanied by dozens or even hundreds of phenotypic subject descriptors on the behavioral, neural, and genomic level. The complexity of such "big data" repositories offer new opportunities and pose new challenges for systems neuroscience. Canonical correlation analysis (CCA) is a prototypical family of methods that is useful in identifying the links between variable sets from different modalities. Importantly, CCA is well suited to describing relationships across multiple sets of data and so is well suited to the analysis of big neuroscience datasets. Our primer discusses the rationale, promises, and pitfalls of CCA. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(20)30232-9  |  
------------------------------------------- 
10.1002/1878-0261.12659  |   Technology has a pivotal role in the continuous development of radiotherapy. The long road toward modern 'high-tech' radiation oncology has been studded with discoveries and technological innovations that resulted from the interaction of various disciplines. In the last decades, a dramatic technology-driven revolution has hugely improved the capability of accurately and safely delivering complex-shaped dose distributions. This has contributed to many clinical improvements, such as the successful management of lung cancer and oligometastatic disease through stereotactic body radiotherapy. Technology-driven research is an active and lively field with promising potential in several domains, including image guidance, adaptive radiotherapy, integration of artificial intelligence, heavy-particle therapy, and 'flash' ultra-high dose-rate radiotherapy. The evolution toward personalized Oncology will deeply influence technology-driven research, aiming to integrate predictive models and omics analyses into fast and efficient solutions to deliver the best treatment for each single patient. Personalized radiation oncology will need affordable technological solutions for middle-/low-income countries, as these are expected to experience the highest increase of cancer incidence and mortality. Moreover, technology solutions for automation of commissioning, quality assurance, safety tests, image segmentation, and plan optimization will be required. Although a large fraction of cancer patients receive radiotherapy, this is certainly not reflected in the worldwide budget for radiotherapy research. Differently from the pharmaceutical companies-driven research, resources for research in radiotherapy are highly limited to equipment vendors, who can, in turn, initiate a limited number of collaborations with academic research centers. Thus, enhancement of investments in technology-driven radiotherapy research via public funds, national governments, and the European Union would have a crucial societal impact. It would allow for radiotherapy to further strengthen its role as a highly effective and cost-efficient cancer treatment modality, and it could facilitate a rapid and equalitarian large-scale transfer of technology to clinic, with direct impact on patient care. 
  |  https://doi.org/10.1002/1878-0261.12659  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105484  |   A great challenge in osteoporosis clinical assessment is identifying patients at higher risk of hip fracture. Bone Mineral Density (BMD) measured by Dual-Energy X-Ray Absorptiometry (DXA) is the current gold-standard, but its classification accuracy is limited to 65%. DXA-based Finite Element (FE) models have been developed to predict the mechanical failure of the bone. Yet, their contribution has been modest. In this study, supervised machine learning (ML) is applied in conjunction with clinical and computationally driven mechanical attributes. Through this multi-technique approach, we aimed to obtain a predictive model that outperforms BMD and other clinical data alone, as well as to identify the best-learned ML classifier within a group of suitable algorithms. A total number of 137 postmenopausal women (81.4 ± 6.95 years) were included in the study and separated into a fracture group (n = 89) and a control group (n = 48). A semi-automatic and patient-specific DXA-based FE model was used to generate mechanical attributes, describing the geometry, the impact force, bone structure and mechanical response of the bone after a sideways-fall. After preprocessing the whole dataset, 19 attributes were selected as predictors. Support Vector Machine (SVM) with radial basis function (RBF), Logistic Regression, Shallow Neural Networks and Random Forest were tested through a comprehensive validation procedure to compare their predictive performance. Clinical attributes were used alone in another experimental setup for the sake of comparison. SVM was confirmed to generate the best-learned algorithm for both experimental setups, including 19 attributes and only clinical attributes. The first, generated the best-learned model and outperformed BMD by 14pp. The results suggests that this approach could be easily integrated for effective prediction of hip fracture without interrupting the actual clinical workflow. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(20)30182-6  |  
------------------------------------------- 
10.1136/bjophthalmol-2018-313706  |    Background/aims:  To develop a deep learning system (DLS) that can automatically detect malignant melanoma (MM) in the eyelid from histopathological sections with colossal information density. 
  Methods:  Setting: Double institutional study. 
  Study population:  We retrospectively reviewed 225 230 pathological patches (small section cut from pathologist-labelled area from an H&amp;E image), cut from 155 H&amp;E-stained whole-slide images (WSI). 
  Observation procedures:  Labelled gigapixel pathological WSIs were used to train and test a model designed to assign patch-level classification. Using malignant probability from a convolutional neural network, the patches were embedded back into each WSI to generate a visualisation heatmap and leveraged a random forest model to establish a WSI-level diagnosis. 
  Main outcome measure(s):  For classification, the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity and specificity were used to evaluate the efficacy of the DLS in detecting MM. 
  Results:  For patch diagnosis, the model achieved an AUC of 0.989 (95% CI 0.989 to 0.991), with an accuracy, sensitivity and specificity of 94.9%, 94.7% and 95.3%, respectively. We displayed the lesion area on the WSIs as graded by malignant potential. For WSI, the obtained sensitivity, specificity and accuracy were 100%, 96.5% and 98.2%, respectively, with an AUC of 0.998 (95% CI 0.994 to 1.000). 
  Conclusion:  Our DLS, which uses artificial intelligence, can automatically detect MM in histopathological slides and highlight the lesion area on WSIs using a probabilistic heatmap. In addition, our approach has the potential to be applied to the histopathological sections of other tumour types. 
  |  http://bjo.bmj.com/cgi/pmidlookup?view=long&pmid=31302629  |  
------------------------------------------- 
10.1016/j.breast.2020.01.001  |   The deep inferior epigastric perforator (DIEP) is the most commonly used free flap in mastectomy reconstruction. Preoperative imaging techniques are routinely used to detect location, diameter and course of perforators, with direct intervention from the imaging team, who subsequently draw a chart that will help surgeons choosing the best vascular support for the reconstruction. In this work, the feasibility of using a computer software to support the preoperative planning of 40 patients proposed for breast reconstruction with a DIEP flap is evaluated for the first time. Blood vessel centreline extraction and local characterization algorithms are applied to identify perforators and compared with the manual mapping, aiming to reduce the time spent by the imaging team, as well as the inherent subjectivity to the task. Comparing with the measures taken during surgery, the software calibre estimates were worse for vessels smaller than 1.5 mm (P = 6e-4) but better for the remaining ones (P = 2e-3). Regarding vessel location, the vertical component of the software output was significantly different from the manual measure (P = 0.02), nonetheless that was irrelevant during surgery as errors in the order of 2-3 mm do not have impact in the dissection step. Our trials support that a reduction of the time spent is achievable using the automatic tool (about 2 h/case). The introduction of artificial intelligence in clinical practice intends to simplify the work of health professionals and to provide better outcomes to patients. This pilot study paves the way for a success story. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9776(20)30002-3  |  
------------------------------------------- 
10.3390/s20071956  |   Sensor data are gaining increasing global attention due to the advent of Internet of Things (IoT). Reasoning is applied on such sensor data in order to compute prediction. Generating a health warning that is based on prediction of atmospheric pollution, planning timely evacuation of people from vulnerable areas with respect to prediction of natural disasters, etc., are the use cases of sensor data stream where prediction is vital to protect people and assets. Thus, prediction accuracy is of paramount importance to take preventive steps and avert any untoward situation. Uncertainties of sensor data is a severe factor which hampers prediction accuracy. Belief Rule Based Expert System (BRBES), a knowledge-driven approach, is a widely employed prediction algorithm to deal with such uncertainties based on knowledge base and inference engine. In connection with handling uncertainties, it offers higher accuracy than other such knowledge-driven techniques, e.g., fuzzy logic and Bayesian probability theory. Contrarily, Deep Learning is a data-driven technique, which constitutes a part of Artificial Intelligence (AI). By applying analytics on huge amount of data, Deep Learning learns the hidden representation of data. Thus, Deep Learning can infer prediction by reasoning over available data, such as historical data and sensor data streams. Combined application of BRBES and Deep Learning can compute prediction with improved accuracy by addressing sensor data uncertainties while utilizing its discovered data pattern. Hence, this paper proposes a novel predictive model that is based on the integrated approach of BRBES and Deep Learning. The uniqueness of this model lies in the development of a mathematical model to combine Deep Learning with BRBES and capture the nonlinear dependencies among the relevant variables. We optimized BRBES further by applying parameter and structure optimization on it. Air pollution prediction has been taken as use case of our proposed combined approach. This model has been evaluated against two different datasets. One dataset contains synthetic images with a corresponding label of PM<sub>2.5</sub> concentrations. The other one contains real images, PM<sub>2.5</sub> concentrations, and numerical weather data of Shanghai, China. We also distinguished a hazy image between polluted air and fog through our proposed model. Our approach has outperformed only BRBES and only Deep Learning in terms of prediction accuracy. 
  |  http://www.mdpi.com/resolver?pii=s20071956  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32244380/  |  
------------------------------------------- 
10.1021/acsami.9b21068  |   Flexible electronics have gained considerable research concern due to their wide prospect for health monitoring, soft robotics, and artificial intelligence, wherein flexible pressure sensors are necessary components of wearable devices. It is well known that the synergistic functions and multiscale structures of hybrid materials exert tremendous effects on the performance of flexible devices. Herein, inspired by the unique structure of the faceplate of sunflowers, we construct a hierarchical structure by in situ grown vertically aligned molybdenum disulfide (MoS<sub>2</sub>) nanosheets on carbonized silk fabric (MoS<sub>2</sub>/CSilk), which is applied as the sensing material in flexible pressure sensors. The MoS<sub>2</sub>/CSilk sensor displayed high sensitivity and good stability. We demonstrated its applications in monitoring subtle physiology signals, such as pulse wave and voice vibrations. In addition, it served as electrodes in lithium-ion batteries. The MoS<sub>2</sub>/CSilk electrode delivered ultrahigh first-cycle discharge and charge capacities of 2895 and 1594 mA h g<sup>-1</sup>, respectively. The MoS<sub>2</sub>/CSilk electrode exhibited a high capacity of 810 mA h g<sup>-1</sup> with a CE close to 100% even after 300 cycles, suggesting good stability. The excellent overall performances are ascribed to the unique structure of the MoS<sub>2</sub>/CSilk and the synergistic effect of CSilk and MoS<sub>2</sub>. The concept and strategy of this work can be extended to the design and fabrication of other multifunctional devices. 
  |  https://dx.doi.org/10.1021/acsami.9b21068  |  
------------------------------------------- 
10.3348/kjr.2019.0470  |    Objective:  We aimed to develop and validate a deep learning system for fully automated segmentation of abdominal muscle and fat areas on computed tomography (CT) images. 
  Materials and methods:  A fully convolutional network-based segmentation system was developed using a training dataset of 883 CT scans from 467 subjects. Axial CT images obtained at the inferior endplate level of the 3rd lumbar vertebra were used for the analysis. Manually drawn segmentation maps of the skeletal muscle, visceral fat, and subcutaneous fat were created to serve as ground truth data. The performance of the fully convolutional network-based segmentation system was evaluated using the Dice similarity coefficient and cross-sectional area error, for both a separate internal validation dataset (426 CT scans from 308 subjects) and an external validation dataset (171 CT scans from 171 subjects from two outside hospitals). 
  Results:  The mean Dice similarity coefficients for muscle, subcutaneous fat, and visceral fat were high for both the internal (0.96, 0.97, and 0.97, respectively) and external (0.97, 0.97, and 0.97, respectively) validation datasets, while the mean cross-sectional area errors for muscle, subcutaneous fat, and visceral fat were low for both internal (2.1%, 3.8%, and 1.8%, respectively) and external (2.7%, 4.6%, and 2.3%, respectively) validation datasets. 
  Conclusion:  The fully convolutional network-based segmentation system exhibited high performance and accuracy in the automatic segmentation of abdominal muscle and fat on CT images. 
  |  https://www.kjronline.org/DOIx.php?id=10.3348/kjr.2019.0470  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31920032/  |  
------------------------------------------- 
10.1017/S2040174420000227  |   Infant colic is a condition of unknown cause which can result in carer distress and attachment difficulties. Recent studies have implicated the gut microbiota in infant colic, and certain probiotics have demonstrated possible efficacy. We aim to investigate whether the intestinal microbiota composition in infants with colic is associated with cry/fuss time at baseline, persistence of cry/fuss at 4-week follow-up, or child behavior at 2 years of age. Fecal samples from infants with colic (n = 118, 53% male) were analyzed using 16S rRNA sequencing. After examining the alpha and beta diversity of the clinical samples, we performed a differential abundance analysis of the 16S data to look for taxa that associate with baseline and future behavior, while adjusting for potential confounding variables. In addition, we used random forest classifiers to evaluate how well baseline gut microbiota can predict future crying time. Alpha diversity of the fecal microbiota was strongly influenced by birth mode, feed type, and child gender, but did not significantly associate with crying or behavioral outcomes. Several taxa within the microbiota (including Bifidobacterium, Clostridium, Lactobacillus, and Klebsiella) associate with colic severity, and the baseline microbiota composition can predict further crying at 4 weeks with up to 65% accuracy. The combination of machine learning findings with associative relationships demonstrates the potential prognostic utility of the infant fecal microbiota in predicting subsequent infant crying problems. 
  |  https://www.cambridge.org/core/product/identifier/S2040174420000227/type/journal_article  |  
------------------------------------------- 
10.1186/s40662-020-0174-x  |   Corneal biomechanics has been a hot topic for research in contemporary ophthalmology due to its prospective applications in diagnosis, management, and treatment of several clinical conditions, including glaucoma, elective keratorefractive surgery, and different corneal diseases. The clinical biomechanical investigation has become of great importance in the setting of refractive surgery to identify patients at higher risk of developing iatrogenic ectasia after laser vision correction. This review discusses the latest developments in the detection of corneal ectatic diseases. These developments should be considered in conjunction with multimodal corneal and refractive imaging, including Placido-disk based corneal topography, Scheimpflug corneal tomography, anterior segment tomography, spectral-domain optical coherence tomography (SD-OCT), very-high-frequency ultrasound (VHF-US), ocular biometry, and ocular wavefront measurements. The ocular response analyzer (ORA) and the Corvis ST are non-contact tonometry systems that provide a clinical corneal biomechanical assessment. More recently, Brillouin optical microscopy has been demonstrated to provide in vivo biomechanical measurements. The integration of tomographic and biomechanical data into artificial intelligence techniques has demonstrated the ability to increase the accuracy to detect ectatic disease and characterize the inherent susceptibility for biomechanical failure and ectasia progression, which is a severe complication after laser vision correction. 
  |  https://eandv.biomedcentral.com/articles/10.1186/s40662-020-0174-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32042837/  |  
------------------------------------------- 
10.3390/biom10040524  |   Lung cancer is one of the leading causes of death worldwide. Therefore, understanding the factors linked to patient survival is essential. Recently, multi-omics analysis has emerged, allowing for patient groups to be classified according to prognosis and at a more individual level, to support the use of precision medicine. Here, we combined RNA expression and miRNA expression with clinical information, to conduct a multi-omics analysis, using publicly available datasets (the cancer genome atlas (TCGA) focusing on lung adenocarcinoma (LUAD)). We were able to successfully subclass patients according to survival. The classifiers we developed, using inferred labels obtained from patient subtypes showed that a support vector machine (SVM), gave the best classification results, with an accuracy of 0.82 with the test dataset. Using these subtypes, we ranked genes based on RNA expression levels. The top 25 genes were investigated, to elucidate the mechanisms that underlie patient prognosis. Bioinformatics analyses showed that the expression levels of six out of 25 genes (<i>ERO1B</i>, <i>DPY19L1</i>, <i>NCAM1</i>, <i>RET</i>, <i>MARCH1</i>, and <i>SLC7A8</i>) were associated with LUAD patient survival (<i>p</i> &lt; 0.05), and pathway analyses indicated that major cancer signaling was altered in the subtypes. 
  |  http://www.mdpi.com/resolver?pii=biom10040524  |  
------------------------------------------- 
10.3390/brainsci10020086  |   It remains a mystery as to how neurons are connected and thereby enable use to think, and volume reconstruction from series of microscopy sections of brains is a vital technique in determining this connectivity. Image registration is a key component; the aim of image registration is to estimate the deformation field between two images. Current methods choose to directly regress the deformation field; however, this task is very challenging. It is common to trade off computational complexity with precision when designing complex models for deformation field estimation. This approach is very inefficient, leading to a long inference time. In this paper, we suggest that complex models are not necessary and solve this dilemma by proposing a dual-network architecture. We divide the deformation field prediction problem into two relatively simple subproblems and solve each of them on one branch of the proposed dual network. The two subproblems have completely opposite properties, and we fully utilize these properties to simplify the design of the dual network. These simple architectures enable high-speed image registration. The two branches are able to work together and make up for each other's drawbacks, and no loss of accuracy occurs even when simple architectures are involved. Furthermore, we introduce a series of loss functions to enable the joint training of the two networks in an unsupervised manner without introducing costly manual annotations. The experimental results reveal that our method outperforms state-of-the-art methods in fly brain electron microscopy image registration tasks, and further ablation studies enable us to obtain a comprehensive understanding of each component of our network. 
  |  http://www.mdpi.com/resolver?pii=brainsci10020086  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32045982/  |  
------------------------------------------- 
10.3390/mi11040341  |   Resistive random access memory (RRAM), which is considered as one of the most promising next-generation non-volatile memory (NVM) devices and a representative of memristor technologies, demonstrated great potential in acting as an artificial synapse in the industry of neuromorphic systems and artificial intelligence (AI), due its advantages such as fast operation speed, low power consumption, and high device density. Graphene and related materials (GRMs), especially graphene oxide (GO), acting as active materials for RRAM devices, are considered as a promising alternative to other materials including metal oxides and perovskite materials. Herein, an overview of GRM-based RRAM devices is provided, with discussion about the properties of GRMs, main operation mechanisms for resistive switching (RS) behavior, figure of merit (FoM) summary, and prospect extension of GRM-based RRAM devices. With excellent physical and chemical advantages like intrinsic Young's modulus (1.0 TPa), good tensile strength (130 GPa), excellent carrier mobility (2.0 × 10<sup>5</sup> cm<sup>2</sup>∙V<sup>-1</sup>∙s<sup>-1</sup>), and high thermal (5000 Wm<sup>-1</sup>∙K<sup>-1</sup>) and superior electrical conductivity (1.0 × 10<sup>6</sup> S∙m<sup>-1</sup>), GRMs can act as electrodes and resistive switching media in RRAM devices. In addition, the GRM-based interface between electrode and dielectric can have an effect on atomic diffusion limitation in dielectric and surface effect suppression. Immense amounts of concrete research indicate that GRMs might play a significant role in promoting the large-scale commercialization possibility of RRAM devices. 
  |  http://www.mdpi.com/resolver?pii=mi11040341  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105381  |    Introduction:  Being able to predict functional outcomes after a stroke is highly desirable for clinicians. This allows clinicians to set reasonable goals with patients and relatives, and to reach shared after-care decisions for recovery or rehabilitation. The aim of this study was to apply various machine learning (ML) methods for 90-day stroke outcome predictions, using a nationwide disease registry. 
  Methods:  This study used the Taiwan Stroke Registry (TSR) which has prospectively collected data from stroke patients since 2006. Three known ML models (support vector machine, random forest, and artificial neural network), and a hybrid artificial neural network were implemented and evaluated by 10-time repeated hold-out with 10-fold cross-validation. 
  Results:  ML techniques present over 0.94 AUC in both ischemic and hemorrhagic stroke using preadmission and inpatient data. By adding follow-up data, the prediction ability improved to 0.97 AUC. We screened 206 clinical variables to identify 17 important features from the ischemic stroke dataset and 22 features from the hemorrhagic stroke dataset without losing much performance. Error analysis revealed that most prediction errors come from more severe stroke patients. 
  Conclusion:  The study showed that ML techniques trained from large, cross-reginal registry datasets were able to predict functional outcome after stroke with high accuracy. The follow-up data is important which can further improve the predictive models' performance. With similar performances among different ML techniques, the algorithm's characteristics and performance on severe stroke patients will be the primary focus when we further develop inference models and artificial intelligence tools for potential medical. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31436-1  |  
------------------------------------------- 
10.1093/bib/bbaa038  |   As an important reversible lipid modification, S-palmitoylation mainly occurs at specific cysteine residues in proteins, participates in regulating various biological processes and is associated with human diseases. Besides experimental assays, computational prediction of S-palmitoylation sites can efficiently generate helpful candidates for further experimental consideration. Here, we reviewed the current progress in the development of S-palmitoylation site predictors, as well as training data sets, informative features and algorithms used in these tools. Then, we compiled a benchmark data set containing 3098 known S-palmitoylation sites identified from small- or large-scale experiments, and developed a new method named data quality discrimination (DQD) to distinguish data quality weights (DQWs) between the two types of the sites. Besides DQD and our previous methods, we encoded sequence similarity values into images, constructed a deep learning framework of convolutional neural networks (CNNs) and developed a novel algorithm of graphic presentation system (GPS) 6.0. We further integrated nine additional types of sequence-based and structural features, implemented parallel CNNs (pCNNs) and designed a new predictor called GPS-Palm. Compared with other existing tools, GPS-Palm showed a &gt;31.3% improvement of the area under the curve (AUC) value (0.855 versus 0.651) for general prediction of S-palmitoylation sites. We also produced two species-specific predictors, with corresponding AUC values of 0.900 and 0.897 for predicting human- and mouse-specific sites, respectively. GPS-Palm is free for academic research at http://gpspalm.biocuckoo.cn/. 
  |  https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbaa038  |  
------------------------------------------- 
10.1002/adma.201908419  |   Infrared (IR) photodetectors are a key optoelectronic device and have thus attracted considerable research attention in recent years. Photosensitivity is an increasingly important device performance parameter for nanoscale photodetectors and image sensors, as it determines the ultimate imaging quality and contrast. However, photosensitivities of state-of-the-art low-dimensional nanostructure-based IR detectors are considerably low, limiting their practical applications. Herein, a biomimetic IR detection amplification (IRDA) system that boosts photosensitivity by several orders of magnitude by introducting nanowire field effect transistors (FETs), resulting in a peak photosensitivity of 7.6 × 10<sup>4</sup> under an illumination of 1342 nm, is presented. Consequently, high-contrast imaging of IR light is obtained on the flexible IRDA arrays. The image information can be then trained and recognized by an artificial neural network for higher image-recognition efficiency. This work provides a new perspective for developing high-performance IR imaging systems, and is expected to undoubtedly enlighten future work on artificial intelligence and biorobotic systems. 
  |  https://doi.org/10.1002/adma.201908419  |  
------------------------------------------- 
10.3340/jkns.2019.0084  |    Objective:  To generate synthetic spine magnetic resonance (MR) images from spine computed tomography (CT) using generative adversarial networks (GANs), as well as to determine the similarities between synthesized and real MR images. 
  Methods:  GANs were trained to transform spine CT image slices into spine magnetic resonance T2 weighted (MRT2) axial image slices by combining adversarial loss and voxel-wise loss. Experiments were performed using 280 pairs of lumbar spine CT scans and MRT2 images. The MRT2 images were then synthesized from 15 other spine CT scans. To evaluate whether the synthetic MR images were realistic, two radiologists, two spine surgeons, and two residents blindly classified the real and synthetic MRT2 images. Two experienced radiologists then evaluated the similarities between subdivisions of the real and synthetic MRT2 images. Quantitative analysis of the synthetic MRT2 images was performed using the mean absolute error (MAE) and peak signal-to-noise ratio (PSNR). 
  Results:  The mean overall similarity of the synthetic MRT2 images evaluated by radiologists was 80.2%. In the blind classification of the real MRT2 images, the failure rate ranged from 0% to 40%. The MAE value of each image ranged from 13.75 to 34.24 pixels (mean, 21.19 pixels), and the PSNR of each image ranged from 61.96 to 68.16 dB (mean, 64.92 dB). 
  Conclusion:  This was the first study to apply GANs to synthesize spine MR images from CT images. Despite the small dataset of 280 pairs, the synthetic MR images were relatively well implemented. Synthesis of medical images using GANs is a new paradigm of artificial intelligence application in medical imaging. We expect that synthesis of MR images from spine CT images using GANs will improve the diagnostic usefulness of CT. To better inform the clinical applications of this technique, further studies are needed involving a large dataset, a variety of pathologies, and other MR sequence of the lumbar spine. 
  |  http://jkns.or.kr/journal/view.php?doi=10.3340/jkns.2019.0084  |  
------------------------------------------- 
10.1016/j.neuropsychologia.2020.107343  |   Developmental dyslexia is known to involve dysfunctions in multiple brain regions; however, a clear understanding of the brain networks behind this disorder is still lacking. The present study examined the functional network connectivity in Chinese dyslexic children with resting-state electroencephalography (EEG) recordings. EEG data were recorded from 27 dyslexic children and 40 age-matched controls, and a minimum spanning tree (MST) analysis was performed to examine the network connectivity in the delta, theta, alpha, and beta frequency bands. The results show that, compared to age-matched controls, Chinese dyslexic children had global network deficiencies in the beta band, and the network topology was more path-like. Moderate correlations are observed between MST degree metric and rapid automatized naming and morphological awareness tests. These observations, together with the findings in alphabetic languages, show that brain network deficiency is a common neural underpinning of dyslexia across writing systems. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0028-3932(20)30013-0  |  
------------------------------------------- 
10.1038/s41559-020-1144-3  |    |  http://dx.doi.org/10.1038/s41559-020-1144-3  |  
------------------------------------------- 
10.1111/gfs.12458  |   Grazing plays an important role in milk production in most regions of the world. In this review, some challenges to the grazing cow are discussed together with opportunities for future improvement. We focus on daily feed intake, efficiency of pasture utilization, output of milk per head, environmental impact of grazing and the nutritional quality to humans of milk produced from dairy cows in contrasting production systems. Challenges are discussed in the context of a trend towards increased size of individual herds and include limited and variable levels of daily herbage consumption, lower levels of milk output per cow, excessive excretion of nitrogenous compounds and requirements for minimal periods of grazing regardless of production system. A major challenge is to engage more farmers in making appropriate adjustments to their grazing management. In relation to product quality, the main challenge is to demonstrate enhanced nutritional/processing benefits of milk from grazed cows. Opportunities include more accurate diet formulations, supplementation of grazed pasture to match macro- and micronutrient supply with animal requirement and plant breeding. The application of robotics and artificial intelligence to pasture management will assist in matching daily supply to animal requirement. Wider consumer recognition of the perceived enhanced nutritional value of milk from grazed cows, together with greater appreciation of the animal health, welfare and behavioural benefits of grazing should contribute to the future sustainability of demand for milk from dairy cows on pasture. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32109974/  |  
------------------------------------------- 
10.1007/s00261-020-02504-8  |   Prostate MRI has seen increasing interest in recent years and has led to the development of new MRI techniques and sequences to improve prostate cancer (PCa) diagnosis which are reviewed in this article. Numerous studies have focused on improving image quality (segmented DWI) and faster acquisition (compressed sensing, k-t-SENSE, PROPELLER). An increasing number of studies have developed new quantitative and computer-aided diagnosis methods including artificial intelligence (PROSTATEx challenge) that mitigate the subjective nature of mpMRI interpretation. MR fingerprinting allows rapid, simultaneous generation of quantitative maps of multiple physical properties (T1, T2), where PCa are characterized by lower T1 and T2 values. New techniques like luminal water imaging (LWI), restriction spectrum imaging (RSI), VERDICT and hybrid multi-dimensional MRI (HM-MRI) have been developed for microstructure imaging, which provide information similar to histology. The distinct MR properties of tissue components and their change with the presence of cancer is used to diagnose prostate cancer. LWI is a T2-based imaging technique where long T2-component corresponding to luminal water is reduced in PCa. RSI and VERDICT are diffusion-based techniques where PCa is characterized by increased signal from intra-cellular restricted water and increased intracellular volume fraction, respectively, due to increased cellularity. VERDICT also reveal loss of extracellular-extravascular space in PCa due to loss of glandular structure. HM-MRI measures volumes of prostate tissue components, where PCa has reduced lumen and stromal and increased epithelium volume similar to results shown in histology. Similarly, molecular imaging using hyperpolarized <sup>13</sup>C imaging has been utilized. 
  |  https://dx.doi.org/10.1007/s00261-020-02504-8  |  
------------------------------------------- 
10.3390/nano10040664  |   The reasonable design pattern of flexible pressure sensors with excellent performance and prominent features including high sensitivity and a relatively wide workable linear range has attracted significant attention owing to their potential application in the advanced wearable electronics and artificial intelligence fields. Herein, nano carbon black from kerosene soot, an atmospheric pollutant generated during the insufficient burning of hydrocarbon fuels, was utilized as the conductive material with a bottom interdigitated textile electrode screen printed using silver paste to construct a piezoresistive pressure sensor with prominent performance. Owing to the distinct loose porous structure, the lumpy surface roughness of the fabric electrodes, and the softness of polydimethylsiloxane, the piezoresistive pressure sensor exhibited superior detection performance, including high sensitivity (31.63 kPa<sup>-1</sup> within the range of 0-2 kPa), a relatively large feasible range (0-15 kPa), a low detection limit (2.26 pa), and a rapid response time (15 ms). Thus, these sensors act as outstanding candidates for detecting the human physiological signal and large-scale limb movement, showing their broad range of application prospects in the advanced wearable electronics field. 
  |  http://www.mdpi.com/resolver?pii=nano10040664  |  
------------------------------------------- 
10.1016/j.joms.2019.08.005  |    Purpose:  Three-dimensional (3D) autotransplantation is a technique for surgical transposition of a tooth to another site within one patient, using 3D printed replicas of the donor tooth. In this study, we evaluated intraoperative experiences during 3D autotransplantation of teeth. 
  Materials and methods:  A multicenter prospective clinical study was implemented. All procedures were performed using preoperative cone-beam computed tomography imaging and computer-assisted design with computer-assisted manufacturing resulting in a 3D replica of the donor tooth. 
  Results:  The 100 autotransplantation procedures (79 patients) included canines, premolars, molars, and 1 supernumerary tooth. In 82% of the procedures, the transplantation was performed with an extra-alveolar time of less than 1 minute and an immediate good fit of the donor tooth. In 14%, the extra-alveolar time was 1 to 3 minutes or multiple fitting attempts were necessary. In 4%, the extra-alveolar time exceeded 3 minutes. Difficulties during the procedures were caused by movement artifacts on the preoperative cone-beam computed tomography imaging, a long interval between the imaging and the procedure, or insufficient bone volume at the recipient site. 
  Conclusions:  The use of a 3D printed donor tooth replica during autotransplantation procedures minimized the extra-alveolar time and intraoperative fitting attempts of transplants. This facilitated a quick and reliable treatment and enabled more difficult procedures. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0278-2391(19)31003-1  |  
------------------------------------------- 
10.1371/journal.pone.0230853  |   Variation of information in the firing rate of neural population, as reflected in different frequency bands of electroencephalographic (EEG) time series, provides direct evidence for change in neural responses of the brain to hypnotic suggestibility. However, realization of an effective biomarker for spiking behaviour of neural population proves to be an elusive subject matter with its impact evident in highly contrasting results in the literature. In this article, we took an information-theoretic stance on analysis of the EEG time series of the brain activity during hypnotic suggestions, thereby capturing the variability in pattern of brain neural activity in terms of its information content. For this purpose, we utilized differential entropy (DE, i.e., the average information content in a continuous time series) of theta, alpha, and beta frequency bands of fourteen-channel EEG time series recordings that pertain to the brain neural responses of twelve carefully selected high and low hypnotically suggestible individuals. Our results show that the higher hypnotic suggestibility is associated with a significantly lower variability in information content of theta, alpha, and beta frequencies. Moreover, they indicate that such a lower variability is accompanied by a significantly higher functional connectivity (FC, a measure of spatiotemporal synchronization) in the parietal and the parieto-occipital regions in the case of theta and alpha frequency bands and a non-significantly lower FC in the central region's beta frequency band. Our results contribute to the field in two ways. First, they identify the applicability of DE as a unifying measure to reproduce the similar observations that are separately reported through adaptation of different hypnotic biomarkers in the literature. Second, they extend these previous findings that were based on neutral hypnosis (i.e., a hypnotic procedure that involves no specific suggestions other than those for becoming hypnotized) to the case of hypnotic suggestions, thereby identifying their presence as a potential signature of hypnotic experience. 
  |  http://dx.plos.org/10.1371/journal.pone.0230853  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32271781/  |  
------------------------------------------- 
10.1093/jamia/ocz075  |    Objective:  This article describes an ensembling system to automatically extract adverse drug events and drug related entities from clinical narratives, which was developed for the 2018 n2c2 Shared Task Track 2. 
  Materials and methods:  We designed a neural model to tackle both nested (entities embedded in other entities) and polysemous entities (entities annotated with multiple semantic types) based on MIMIC III discharge summaries. To better represent rare and unknown words in entities, we further tokenized the MIMIC III data set by splitting the words into finer-grained subwords. We finally combined all the models to boost the performance. Additionally, we implemented a featured-based conditional random field model and created an ensemble to combine its predictions with those of the neural model. 
  Results:  Our method achieved 92.78% lenient micro F1-score, with 95.99% lenient precision, and 89.79% lenient recall, respectively. Experimental results showed that combining the predictions of either multiple models, or of a single model with different settings can improve performance. 
  Discussion:  Analysis of the development set showed that our neural models can detect more informative text regions than feature-based conditional random field models. Furthermore, most entity types significantly benefit from subword representation, which also allows us to extract sparse entities, especially nested entities. 
  Conclusion:  The overall results have demonstrated that the ensemble method can accurately recognize entities, including nested and polysemous entities. Additionally, our method can recognize sparse entities by reconsidering the clinical narratives at a finer-grained subword level, rather than at the word level. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz075  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31197355/  |  
------------------------------------------- 
10.1021/acsnano.9b07165  |   Fabrication of human-like intelligent tactile sensors is an intriguing challenge for developing human-machine interfaces. As inspired by somatosensory signal generation and neuroplasticity-based signal processing, intelligent neuromorphic tactile sensors with learning and memory based on the principle of a triboelectric nanogenerator are demonstrated. The tactile sensors can actively produce signals with various amplitudes on the basis of the history of pressure stimulations because of their capacity to mimic neuromorphic functions of synaptic potentiation and memory. The time over which these tactile sensors can retain the memorized information is alterable, enabling cascaded devices to have a multilevel forgetting process and to memorize a rich amount of information. Furthermore, smart fingers by using the tactile sensors are constructed to record a rich amount of information related to the fingers' current actions and previous actions. This intelligent active tactile sensor can be used as a functional element for artificial intelligence. 
  |  https://dx.doi.org/10.1021/acsnano.9b07165  |  
------------------------------------------- 
10.1016/j.ab.2019.113477  |   Proteases are a type of enzymes, which perform the process of proteolysis. Proteolysis normally refers to protein and peptide degradation which is crucial for the survival, growth and wellbeing of a cell. Moreover, proteases have a strong association with therapeutics and drug development. The proteases are classified into five different types according to their nature and physiochemical characteristics. Mostly the methods used to differentiate protease from other proteins and identify their class requires a clinical test which is usually time-consuming and operator dependent. Herein, we report a classifier named iProtease-PseAAC (2L) for identifying proteases and their classes. The predictor is developed employing the flow of 5-step rule, initiating from the collection of benchmark dataset and terminating at the development of predictor. Rigorous verification and validation tests are performed and metrics are collected to calculate the authenticity of the trained model. The self-consistency validation gives the 98.32% accuracy, for cross-validation the accuracy is 90.71% and jackknife gives 96.07% accuracy. The average accuracy for level-2 i.e. protease classification is 95.77%. Based on the above-mentioned results, it is concluded that iProtease-PseAAC (2L) has the great ability to identify the proteases and their classes using a given protein sequence. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0003-2697(19)30126-5  |  
------------------------------------------- 
10.1007/s00702-019-02067-z  |   Sleep disturbances and chronic pain are considered public health concerns. They are frequently associated, and the direction of its relationship and possible mechanisms underlying it are frequently debated. The exploration of the sleep-pain association is of great clinical interest to explore in order to steer potential therapeutic avenues, accommodate the patient's experience, and adapt the common practice of health professionals. In this review, the direction between sleep-pain in adult and pediatric populations will be discussed. Moreover, the possible mechanisms contributing to this relationship as endogenous pain modulation, inflammation, affect, mood and other states, the role of different endogenous substances (dopamine, orexin, melatonin, vitamin D) as well as other lesser known such as cyclic alternating pattern among others, will be explored. Finally, directions for future studies on this area will be discussed, opening up to the addition of tools such as brain imaging (e.g., fMRI), electrophysiology and non-invasive brain stimulation techniques. Such resources paired with artificial intelligence are key to personalized medicine management for patients facing pain and sleep interacting conditions. 
  |  https://doi.org/10.1007/s00702-019-02067-z  |  
------------------------------------------- 
10.3389/fncom.2020.00017  |   Image registration and segmentation are the two most studied problems in medical image analysis. Deep learning algorithms have recently gained a lot of attention due to their success and state-of-the-art results in variety of problems and communities. In this paper, we propose a novel, efficient, and multi-task algorithm that addresses the problems of image registration and brain tumor segmentation jointly. Our method exploits the dependencies between these tasks through a natural coupling of their interdependencies during inference. In particular, the similarity constraints are relaxed within the tumor regions using an efficient and relatively simple formulation. We evaluated the performance of our formulation both quantitatively and qualitatively for registration and segmentation problems on two publicly available datasets (BraTS 2018 and OASIS 3), reporting competitive results with other recent state-of-the-art methods. Moreover, our proposed framework reports significant amelioration (<i>p</i> &lt; 0.005) for the registration performance inside the tumor locations, providing a generic method that does not need any predefined conditions (e.g., absence of abnormalities) about the volumes to be registered. Our implementation is publicly available online at https://github.com/TheoEst/joint_registration_tumor_segmentation. 
  |  https://doi.org/10.3389/fncom.2020.00017  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32265680/  |  
------------------------------------------- 
10.1038/s41598-020-58076-6  |   Lifelog photo review is considered to enhance the recall of personal events. While a sizable body of research has explored the neural basis of autobiographical memory (AM), there is limited neural evidence on the retrieval-based enhancement effect on event memory among older adults in the real-world environment. This study examined the neural processes of AM as was modulated by retrieval practice through lifelog photo review in older adults. In the experiment, blood-oxygen-level dependent response during subjects' recall of recent events was recorded, where events were cued by photos that may or may not have been exposed to a priori retrieval practice (training). Subjects remembered more episodic details under the trained relative to non-trained condition. Importantly, the neural correlates of AM was exhibited by (1) dissociable cortical areas related to recollection and familiarity, and (2) a positive correlation between the amount of recollected episodic details and cortical activation within several lateral temporal and parietal regions. Further analysis of the brain activation pattern at a few regions of interest within the core remember network showed a training_condition × event_detail interaction effect, suggesting that the boosting effect of retrieval practice depended on the level of recollected event details. 
  |  http://dx.doi.org/10.1038/s41598-020-58076-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31996715/  |  
------------------------------------------- 
10.1155/2020/1628357  |   Societal determinants of health are of recognized importance for understanding the causal association of society and health of an individual. Iron deficiency anemia (IDA) is a challenging public health problem across the globe instigating from a broader sociocultural background. It is more prevalent among pregnant women, children under the age of five years, and adolescent girls. Adolescent girls are vulnerable to develop IDA because of additional nutritional demand of the body needed for growth spurt, blood loss due to onset of menarche, malnourishment, and poor dietary iron intake. In this study, we explore the societal determinants of anemia among adolescent girls in Azad Jammu and Kashmir (AJK), Pakistan. A cross-sectional study was conducted in the Muzaffarabad division of AJK on randomly selected 626 adolescent girls. The data were collected using a pretested self-administered interview schedule comprising mainly closed-ended questions with a few open-ended questions. Descriptive statistics was computed for describing the data, and bivariate regression and logistic regression were used to determine the association of anemia with its societal determinants. Multiple linear regression is used to determine the relationship of different determinants (independent variables) with the hemoglobin level (dependent variable) of the respondents. The prevalence of anemia among adolescent girls is 47.9%, of which 47.7% have mild anemia, 51.7% have moderate anemia, and 5.7% have severe anemia, which reveals that anemia is a severe public health problem among adolescent girls in the study area. The findings aver that anemia occurrence was significantly associated with the respondent's and her parental education, economic well-being, prevalence of communicable diseases, menstrual disorder, exercise habits, meals regularity, and type of sewerage system. 
  |  https://doi.org/10.1155/2020/1628357  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32047664/  |  
------------------------------------------- 
10.1016/j.bandl.2019.104740  |   It is controversial as to how age of acquisition (AoA) and proficiency level of the second language influence the similarities and differences between the first (L1) and the second (L2) language brain networks. In this functional MRI study, we used representational similarity analysis to quantify the degree of neural similarity between L1 and L2 during sentence comprehension tasks in 26 adult Chinese-English bilinguals, who learned English as L2 at different ages and had different proficiency levels. We found that although L1 and L2 processing activated similar brain regions, greater neural pattern dissimilarity between L1 and L2 was associated with earlier AoA in the left inferior and middle frontal gyri after the effect of proficiency level was controlled. On the other hand, the association between proficiency level and the neural pattern dissimilarity between L1 and L2 was not significant when the effect of AoA was partialled out. The results suggest that the activity pattern of L2 is more distinct from that of L1 in bilingual individuals who acquired L2 earlier and that the contribution of AoA to the neural pattern dissimilarity is greater than that of proficiency level. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0093-934X(19)30356-6  |  
------------------------------------------- 
10.3390/s20040978  |   Waste collection has become a major issue all over the world, especially when it is offered as a service for businesses; unlike public waste collection where the parameters are relatively homogeneous. This industry can greatly benefit from new sensing technologies and advances in artificial intelligence that have been achieved over the last few years. However, in most situations waste management systems are based on obsolete technologies, with a low level of interoperability and thus offering static processes. The most advanced solutions are generally limited to statistical, non-predictive approaches and have a limited view of reality, making them weakly effective in dealing with day-to-day business issues (overflowing containers, poor quality of service, etc.). This paper presents a case study currently being developed in Luxembourg with a company offering a business waste collection service, which has a significant amount of constraints to consider. Our main objective is to investigate the use of multiple waste data sources to derive useful indicators for improving collection processes. We start with company-owned historical data and then investigate GPS information from tracking devices positioned on collection trucks. Furthermore, we analyze data collected from ultrasonic sensors deployed on almost 50 different containers to measure fill levels. We describe the deployment steps and show that this approach, combined with anomaly detection and prediction techniques, has the potential to change the way this business operates. We also discuss the interest of the different datasets presented and multi-objective optimization issues. To the best of our knowledge, this article is the first major work dedicated to the world of professional waste collection. 
  |  http://www.mdpi.com/resolver?pii=s20040978  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32059411/  |  
------------------------------------------- 
10.1161/CIRCULATIONAHA.119.044966  |   Mr. M is a 55 year-old man who suffers an acute myocardial infarction (MI) and undergoes coronary stenting. Following hospitalization, he completes cardiac rehabilitation. Thereafter, he is approached about joining a digital smartwatch study to help monitor his health behaviors. He enrolls with enthusiasm, and, feeling empowered, creates a profile on PatientsLikeMe to share lessons from his medical journey. There he reads about an over-the-counter vitamin and downloads a coupon for his local supermarket. Determined to remain accountable for his health, he starts exercising with a fitness trainer and provides her with heart rate data from his smartwatch. He also downloads a mobile nutrition application she recommends. 
  |  http://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.119.044966?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41598-020-60595-1  |   Identification of AD (Alzheimer's disease)-related genes obtained from blood samples is crucial for early AD diagnosis. We used three public datasets, ADNI, AddNeuroMed1 (ANM1), and ANM2, for this study. Five feature selection methods and five classifiers were used to curate AD-related genes and discriminate AD patients, respectively. In the internal validation (five-fold cross-validation within each dataset), the best average values of the area under the curve (AUC) were 0.657, 0.874, and 0.804 for ADNI, ANMI, and ANM2, respectively. In the external validation (training and test sets from different datasets), the best AUCs were 0.697 (training: ADNI to testing: ANM1), 0.764 (ADNI to ANM2), 0.619 (ANM1 to ADNI), 0.79 (ANM1 to ANM2), 0.655 (ANM2 to ADNI), and 0.859 (ANM2 to ANM1), respectively. These results suggest that although the classification performance of ADNI is relatively lower than that of ANM1 and ANM2, classifiers trained using blood gene expression can be used to classify AD for other data sets. In addition, pathway analysis showed that AD-related genes were enriched with inflammation, mitochondria, and Wnt signaling pathways. Our study suggests that blood gene expression data are useful in predicting the AD classification. 
  |  http://dx.doi.org/10.1038/s41598-020-60595-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32103140/  |  
------------------------------------------- 
10.5306/wjco.v11.i2.53  |   The tumor objective response rate (ORR) is an important parameter to demonstrate the efficacy of a treatment in oncology. The ORR is valuable for clinical decision making in routine practice and a significant end-point for reporting the results of clinical trials. World Health Organization and Response Evaluation Criteria in Solid Tumors (RECIST) are anatomic response criteria developed mainly for cytotoxic chemotherapy. These criteria are based on the visual assessment of tumor size in morphological images provided by computed tomography (CT) or magnetic resonance imaging. Anatomic response criteria may not be optimal for biologic agents, some disease sites, and some regional therapies. Consequently, modifications of RECIST, Choi criteria and Morphologic response criteria were developed based on the concept of the evaluation of viable tumors. Despite its limitations, RECIST v1.1 is validated in prospective studies, is widely accepted by regulatory agencies and has recently shown good performance for targeted cancer agents. Finally, some alternatives of RECIST were developed as immune-specific response criteria for checkpoint inhibitors. Immune RECIST criteria are based essentially on defining true progressive disease after a confirmatory imaging. Some graphical methods may be useful to show longitudinal change in the tumor burden over time. Tumor tissue is a tridimensional heterogenous mass, and tumor shrinkage is not always symmetrical; thus, metabolic response assessments using positron emission tomography (PET) or PET/CT may reflect the viability of cancer cells or functional changes evolving after anticancer treatments. The metabolic response can show the benefit of a treatment earlier than anatomic shrinkage, possibly preventing delays in drug approval. Computer-assisted automated volumetric assessments, quantitative multimodality imaging in radiology, new tracers in nuclear medicine and finally artificial intelligence have great potential in future evaluations. 
  |  http://www.wjgnet.com/2218-4333/full/v11/i2/53.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32133275/  |  
------------------------------------------- 
10.1080/0284186X.2020.1736332  |   <b>Background:</b> The current standard for evaluating axillary nodal burden in clinically node negative breast cancer is sentinel lymph node biopsy (SLNB). However, the accuracy of SLNB to detect nodal stage N2-3 remains debatable. Nomograms can help the decision-making process between axillary treatment options. The aim of this study was to create a new model to predict the nodal stage N2-3 after a positive SLNB using machine learning methods that are rarely seen in nomogram development.<b>Material and methods:</b> Primary breast cancer patients who underwent SLNB and axillary lymph node dissection (ALND) between 2012 and 2017 formed cohorts for nomogram development (training cohort, <i>N</i> = 460) and for nomogram validation (validation cohort, <i>N</i> = 70). A machine learning method known as the gradient boosted trees model (XGBoost) was used to determine the variables associated with nodal stage N2-3 and to create a predictive model. Multivariate logistic regression analysis was used for comparison.<b>Results:</b> The best combination of variables associated with nodal stage N2-3 in XGBoost modeling included tumor size, histological type, multifocality, lymphovascular invasion, percentage of ER positive cells, number of positive sentinel lymph nodes (SLN) and number of positive SLNs multiplied by tumor size. Indicating discrimination, AUC values for the training cohort and the validation cohort were 0.80 (95%CI 0.71-0.89) and 0.80 (95%CI 0.65-0.92) in the XGBoost model and 0.85 (95%CI 0.77-0.93) and 0.75 (95%CI 0.58-0.89) in the logistic regression model, respectively.<b>Conclusions:</b> This machine learning model was able to maintain its discrimination in the validation cohort better than the logistic regression model. This indicates advantages in employing modern artificial intelligence techniques into nomogram development. The nomogram could be used to help identify nodal stage N2-3 in early breast cancer and to select appropriate treatments for patients. 
  |  http://www.tandfonline.com/doi/full/10.1080/0284186X.2020.1736332  |  
------------------------------------------- 
10.1093/gigascience/giaa010  |    Background:  Microbiome biomarker discovery for patient diagnosis, prognosis, and risk evaluation is attracting broad interest. Selected groups of microbial features provide signatures that characterize host disease states such as cancer or cardio-metabolic diseases. Yet, the current predictive models stemming from machine learning still behave as black boxes and seldom generalize well. Their interpretation is challenging for physicians and biologists, which makes them difficult to trust and use routinely in the physician-patient decision-making process. Novel methods that provide interpretability and biological insight are needed. Here, we introduce "predomics", an original machine learning approach inspired by microbial ecosystem interactions that is tailored for metagenomics data. It discovers accurate predictive signatures and provides unprecedented interpretability. The decision provided by the predictive model is based on a simple, yet powerful score computed by adding, subtracting, or dividing cumulative abundance of microbiome measurements. 
  Results:  Tested on &gt;100 datasets, we demonstrate that predomics models are simple and highly interpretable. Even with such simplicity, they are at least as accurate as state-of-the-art methods. The family of best models, discovered during the learning process, offers the ability to distil biological information and to decipher the predictability signatures of the studied condition. In a proof-of-concept experiment, we successfully predicted body corpulence and metabolic improvement after bariatric surgery using pre-surgery microbiome data. 
  Conclusions:  Predomics is a new algorithm that helps in providing reliable and trustworthy diagnostic decisions in the microbiome field. Predomics is in accord with societal and legal requirements that plead for an explainable artificial intelligence approach in the medical field. 
  |  https://academic.oup.com/gigascience/article-lookup/doi/10.1093/gigascience/giaa010  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32150601/  |  
------------------------------------------- 
10.21037/qims.2019.12.12  |    Background:  Recently, the paradigm of computed tomography (CT) reconstruction has shifted as the deep learning technique evolves. In this study, we proposed a new convolutional neural network (called ADAPTIVE-NET) to perform CT image reconstruction directly from a sinogram by integrating the analytical domain transformation knowledge. 
  Methods:  In the proposed ADAPTIVE-NET, a specific network layer with constant weights was customized to transform the sinogram onto the CT image domain via analytical back-projection. With this new framework, feature extractions were performed simultaneously on both the sinogram domain and the CT image domain. The Mayo low dose CT (LDCT) data was used to validate the new network. In particular, the new network was compared with the previously proposed residual encoder-decoder (RED)-CNN network. For each network, the mean square error (MSE) loss with and without VGG-based perceptual loss was compared. Furthermore, to evaluate the image quality with certain metrics, the noise correlation was quantified via the noise power spectrum (NPS) on the reconstructed LDCT for each method. 
  Results:  CT images that have clinically relevant dimensions of 512×512 can be easily reconstructed from a sinogram on a single graphics processing unit (GPU) with moderate memory size (e.g., 11 GB) by ADAPTIVE-NET. With the same MSE loss function, the new network is able to generate better results than the RED-CNN. Moreover, the new network is able to reconstruct natural looking CT images with enhanced image quality if jointly using the VGG loss. 
  Conclusions:  The newly proposed end-to-end supervised ADAPTIVE-NET is able to reconstruct high-quality LDCT images directly from a sinogram. 
  |  https://doi.org/10.21037/qims.2019.12.12  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32190567/  |  
------------------------------------------- 
10.4103/sjg.SJG_636_19  |    |  http://www.saudijgastro.com/article.asp?issn=1319-3767;year=2020;volume=26;issue=1;spage=1;epage=3;aulast=Almadi  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32098934/  |  
------------------------------------------- 
10.1002/jmri.26872  |    Background:  Super-resolution is an emerging method for enhancing MRI resolution; however, its impact on image quality is still unknown. 
  Purpose:  To evaluate MRI super-resolution using quantitative and qualitative metrics of cartilage morphometry, osteophyte detection, and global image blurring. 
  Study type:  Retrospective. 
  Population:  In all, 176 MRI studies of subjects at varying stages of osteoarthritis. 
  Field strength/sequence:  Original-resolution 3D double-echo steady-state (DESS) and DESS with 3× thicker slices retrospectively enhanced using super-resolution and tricubic interpolation (TCI) at 3T. 
  Assessment:  A quantitative comparison of femoral cartilage morphometry was performed for the original-resolution DESS, the super-resolution, and the TCI scans in 17 subjects. A reader study by three musculoskeletal radiologists assessed cartilage image quality, overall image sharpness, and osteophytes incidence in all three sets of scans. A referenceless blurring metric evaluated blurring in all three image dimensions for the three sets of scans. 
  Statistical tests:  Mann-Whitney U-tests compared Dice coefficients (DC) of segmentation accuracy for the DESS, super-resolution, and TCI images, along with the image quality readings and blurring metrics. Sensitivity, specificity, and diagnostic odds ratio (DOR) with 95% confidence intervals compared osteophyte detection for the super-resolution and TCI images, with the original-resolution as a reference. 
  Results:  DC for the original-resolution (90.2 ± 1.7%) and super-resolution (89.6 ± 2.0%) were significantly higher (P &lt; 0.001) than TCI (86.3 ± 5.6%). Segmentation overlap of super-resolution with the original-resolution (DC = 97.6 ± 0.7%) was significantly higher (P &lt; 0.0001) than TCI overlap (DC = 95.0 ± 1.1%). Cartilage image quality for sharpness and contrast levels, and the through-plane quantitative blur factor for super-resolution images, was significantly (P &lt; 0.001) better than TCI. Super-resolution osteophyte detection sensitivity of 80% (76-82%), specificity of 93% (92-94%), and DOR of 32 (22-46) was significantly higher (P &lt; 0.001) than TCI sensitivity of 73% (69-76%), specificity of 90% (89-91%), and DOR of 17 (13-22). 
  Data conclusion:  Super-resolution appears to consistently outperform naïve interpolation and may improve image quality without biasing quantitative biomarkers. 
  Level of evidence:  2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:768-779. 
  |  https://doi.org/10.1002/jmri.26872  |  
------------------------------------------- 
10.1007/s11547-020-01174-2  |    Purpose:  To test the technical reproducibility of acquisition and scanners of CT image-based radiomics model for early recurrent hepatocellular carcinoma (HCC). 
  Methods:  We included primary HCC patient undergone curative therapies, using early recurrence as endpoint. Four datasets were constructed: 109 images from hospital #1 for training (set 1: 1-mm image slice thickness), 47 images from hospital #1 for internal validation (sets 2 and 3: 1-mm and 10-mm image slice thicknesses, respectively), and 47 images from hospital #2 for external validation (set 4: vastly different from training dataset). A radiomics model was constructed. Radiomics technical reproducibility was measured by overfitting and calibration deviation in external validation dataset. The influence of slice thickness on reproducibility was evaluated in two internal validation datasets. 
  Results:  Compared with set 1, the model in set 2 indicated favorable prediction efficiency (the area under the curve 0.79 vs. 0.80, P = 0.47) and good calibration (unreliability statistic U: P = 0.33). However, in set 4, significant overfitting (0.63 vs. 0.80, P &lt; 0.01) and calibration deviation (U: P &lt; 0.01) were observed. Similar poor performance was also observed in set 3 (0.56 vs. 0.80, P = 0.02; U: P &lt; 0.01). 
  Conclusions:  CT-based radiomics has poor reproducibility between centers. Image heterogeneity, such as slice thickness, can be a significant influencing factor. 
  |  https://dx.doi.org/10.1007/s11547-020-01174-2  |  
------------------------------------------- 
10.1002/trc2.12013  |    Introduction:  The search for drugs to treat Alzheimer's disease (AD) has failed to yield effective therapies. Here we report the first genome-wide search for biomarkers associated with therapeutic response in AD. Blarcamesine (ANAVEX2-73), a selective sigma-1 receptor (SIGMAR1) agonist, was studied in a 57-week Phase 2a trial (<a href="http://clinicaltrials.gov/show/NCT02244541" title="See in ClinicalTrials.gov">NCT02244541</a>). The study was extended for a further 208 weeks (<a href="http://clinicaltrials.gov/show/NCT02756858" title="See in ClinicalTrials.gov">NCT02756858</a>) after meeting its primary safety endpoint. 
  Methods:  Safety, clinical features, pharmacokinetic, and efficacy, measured by changes in the Mini-Mental State Examination (MMSE) and the Alzheimer's Disease Cooperative Study-Activities of Daily Living scale (ADCS-ADL), were recorded. Whole exome and transcriptome sequences were obtained for 21 patients. The relationship between all available patient data and efficacy outcome measures was analyzed with unsupervised formal concept analysis (FCA), integrated in the Knowledge Extraction and Management (KEM) environment. 
  Results:  Biomarkers with a significant impact on clinical outcomes were identified at week 57: mean plasma concentration of blarcamesine (slope MMSE:<i>P</i> &lt; .041), genomic variants SIGMAR1 p.Gln2Pro (ΔMMSE:<i>P</i> &lt; .039; ΔADCS-ADL:<i>P</i> &lt; .063) and COMT p.Leu146fs (ΔMMSE:<i>P</i> &lt; .039; ΔADCS-ADL:<i>P</i> &lt; .063), and baseline MMSE score (slope MMSE:<i>P</i> &lt; .015). Their combined impact on drug response was confirmed at week 148 with linear mixed effect models. 
  Discussion:  Confirmatory Phase 2b/3 clinical studies of these patient selection markers are ongoing. This FCA/KEM analysis is a template for the identification of patient selection markers in early therapeutic development for neurologic disorders. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32318621/  |  
------------------------------------------- 
10.2196/15931  |    Background:  The detection of dyskalemias-hypokalemia and hyperkalemia-currently depends on laboratory tests. Since cardiac tissue is very sensitive to dyskalemia, electrocardiography (ECG) may be able to uncover clinically important dyskalemias before laboratory results. 
  Objective:  Our study aimed to develop a deep-learning model, ECG12Net, to detect dyskalemias based on ECG presentations and to evaluate the logic and performance of this model. 
  Methods:  Spanning from May 2011 to December 2016, 66,321 ECG records with corresponding serum potassium (K<sup>+</sup>) concentrations were obtained from 40,180 patients admitted to the emergency department. ECG12Net is an 82-layer convolutional neural network that estimates serum K<sup>+</sup> concentration. Six clinicians-three emergency physicians and three cardiologists-participated in human-machine competition. Sensitivity, specificity, and balance accuracy were used to evaluate the performance of ECG12Net with that of these physicians. 
  Results:  In a human-machine competition including 300 ECGs of different serum K+ concentrations, the area under the curve for detecting hypokalemia and hyperkalemia with ECG12Net was 0.926 and 0.958, respectively, which was significantly better than that of our best clinicians. Moreover, in detecting hypokalemia and hyperkalemia, the sensitivities were 96.7% and 83.3%, respectively, and the specificities were 93.3% and 97.8%, respectively. In a test set including 13,222 ECGs, ECG12Net had a similar performance in terms of sensitivity for severe hypokalemia (95.6%) and severe hyperkalemia (84.5%), with a mean absolute error of 0.531. The specificities for detecting hypokalemia and hyperkalemia were 81.6% and 96.0%, respectively. 
  Conclusions:  A deep-learning model based on a 12-lead ECG may help physicians promptly recognize severe dyskalemias and thereby potentially reduce cardiac events. 
  |  https://medinform.jmir.org/2020/3/e15931/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32134388/  |  
------------------------------------------- 
10.1002/pros.23934  |    Introduction:  The 2019 Coffey-Holden Prostate Cancer Academy (CHPCA) Meeting, "Prostate Cancer Research: The Next Generation," was held 20 to 23 June, 2019, in Los Angeles, California. 
  Methods:  The CHPCA Meeting is an annual conference held by the Prostate Cancer Foundation, that is uniquely structured to stimulate intense discussion surrounding topics most critical to accelerating prostate cancer research and the discovery of new life-extending treatments for patients. The 7th Annual CHPCA Meeting was attended by 86 investigators and concentrated on many of the most promising new treatment opportunities and next-generation research technologies. 
  Results:  The topics of focus at the meeting included: new treatment strategies and novel agents for targeted therapies and precision medicine, new treatment strategies that may synergize with checkpoint immunotherapy, next-generation technologies that visualize tumor microenvironment (TME) and molecular pathology in situ, multi-omics and tumor heterogeneity using single cells, 3D and TME models, and the role of extracellular vesicles in cancer and their potential as biomarkers. 
  Discussion:  This meeting report provides a comprehensive summary of the talks and discussions held at the 2019 CHPCA Meeting, for the purpose of globally disseminating this knowledge and ultimately accelerating new treatments and diagnostics for patients with prostate cancer. 
  |  https://doi.org/10.1002/pros.23934  |  
------------------------------------------- 
10.1016/S1470-2045(19)30738-7  |    Background:  An increasing volume of prostate biopsies and a worldwide shortage of urological pathologists puts a strain on pathology departments. Additionally, the high intra-observer and inter-observer variability in grading can result in overtreatment and undertreatment of prostate cancer. To alleviate these problems, we aimed to develop an artificial intelligence (AI) system with clinically acceptable accuracy for prostate cancer detection, localisation, and Gleason grading. 
  Methods:  We digitised 6682 slides from needle core biopsies from 976 randomly selected participants aged 50-69 in the Swedish prospective and population-based STHLM3 diagnostic study done between May 28, 2012, and Dec 30, 2014 (ISRCTN84445406), and another 271 from 93 men from outside the study. The resulting images were used to train deep neural networks for assessment of prostate biopsies. The networks were evaluated by predicting the presence, extent, and Gleason grade of malignant tissue for an independent test dataset comprising 1631 biopsies from 246 men from STHLM3 and an external validation dataset of 330 biopsies from 73 men. We also evaluated grading performance on 87 biopsies individually graded by 23 experienced urological pathologists from the International Society of Urological Pathology. We assessed discriminatory performance by receiver operating characteristics and tumour extent predictions by correlating predicted cancer length against measurements by the reporting pathologist. We quantified the concordance between grades assigned by the AI system and the expert urological pathologists using Cohen's kappa. 
  Findings:  The AI achieved an area under the receiver operating characteristics curve of 0·997 (95% CI 0·994-0·999) for distinguishing between benign (n=910) and malignant (n=721) biopsy cores on the independent test dataset and 0·986 (0·972-0·996) on the external validation dataset (benign n=108, malignant n=222). The correlation between cancer length predicted by the AI and assigned by the reporting pathologist was 0·96 (95% CI 0·95-0·97) for the independent test dataset and 0·87 (0·84-0·90) for the external validation dataset. For assigning Gleason grades, the AI achieved a mean pairwise kappa of 0·62, which was within the range of the corresponding values for the expert pathologists (0·60-0·73). 
  Interpretation:  An AI system can be trained to detect and grade cancer in prostate needle biopsy samples at a ranking comparable to that of international experts in prostate pathology. Clinical application could reduce pathology workload by reducing the assessment of benign biopsies and by automating the task of measuring cancer length in positive biopsy cores. An AI system with expert-level grading performance might contribute a second opinion, aid in standardising grading, and provide pathology expertise in parts of the world where it does not exist. 
  Funding:  Swedish Research Council, Swedish Cancer Society, Swedish eScience Research Center, EIT Health. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1470-2045(19)30738-7  |  
------------------------------------------- 
10.1002/lary.28539  |    Objectives/hypothesis:  To develop a deep-learning-based computer-aided diagnosis system for distinguishing laryngeal neoplasms (benign, precancerous lesions, and cancer) and improve the clinician-based accuracy of diagnostic assessments of laryngoscopy findings. 
  Study design:  Retrospective study. 
  Methods:  A total of 24,667 laryngoscopy images (normal, vocal nodule, polyps, leukoplakia and malignancy) were collected to develop and test a convolutional neural network (CNN)-based classifier. A comparison between the proposed CNN-based classifier and the clinical visual assessments (CVAs) by 12 otolaryngologists was conducted. 
  Results:  In the independent testing dataset, an overall accuracy of 96.24% was achieved; for leukoplakia, benign, malignancy, normal, and vocal nodule, the sensitivity and specificity were 92.8% vs. 98.9%, 97% vs. 99.7%, 89% vs. 99.3%, 99.0% vs. 99.4%, and 97.2% vs. 99.1%, respectively. Furthermore, when compared with CVAs on the randomly selected test dataset, the CNN-based classifier outperformed physicians for most laryngeal conditions, with striking improvements in the ability to distinguish nodules (98% vs. 45%, P &lt; .001), polyps (91% vs. 86%, P &lt; .001), leukoplakia (91% vs. 65%, P &lt; .001), and malignancy (90% vs. 54%, P &lt; .001). 
  Conclusions:  The CNN-based classifier can provide a valuable reference for the diagnosis of laryngeal neoplasms during laryngoscopy, especially for distinguishing benign, precancerous, and cancer lesions. 
  Level of evidence:  NA Laryngoscope, 2020. 
  |  https://doi.org/10.1002/lary.28539  |  
------------------------------------------- 
10.1002/cncy.22236  |    Background:  The effective detection and monitoring of potentially malignant oral lesions (PMOL) are critical to identifying early-stage cancer and improving outcomes. In the current study, the authors described cytopathology tools, including machine learning algorithms, clinical algorithms, and test reports developed to assist pathologists and clinicians with PMOL evaluation. 
  Methods:  Data were acquired from a multisite clinical validation study of 999 subjects with PMOLs and oral squamous cell carcinoma (OSCC) using a cytology-on-a-chip approach. A machine learning model was trained to recognize and quantify the distributions of 4 cell phenotypes. A least absolute shrinkage and selection operator (lasso) logistic regression model was trained to distinguish PMOLs and cancer across a spectrum of histopathologic diagnoses ranging from benign, to increasing grades of oral epithelial dysplasia (OED), to OSCC using demographics, lesion characteristics, and cell phenotypes. Cytopathology software was developed to assist pathologists in reviewing brush cytology test results, including high-content cell analyses, data visualization tools, and results reporting. 
  Results:  Cell phenotypes were determined accurately through an automated cytological assay and machine learning approach (99.3% accuracy). Significant differences in cell phenotype distributions across diagnostic categories were found in 3 phenotypes (type 1 ["mature squamous"], type 2 ["small round"], and type 3 ["leukocytes"]). The clinical algorithms resulted in acceptable performance characteristics (area under the curve of 0.81 for benign vs mild dysplasia and 0.95 for benign vs malignancy). 
  Conclusions:  These new cytopathology tools represent a practical solution for rapid PMOL assessment, with the potential to facilitate screening and longitudinal monitoring in primary, secondary, and tertiary clinical care settings. 
  |  https://doi.org/10.1002/cncy.22236  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32032477/  |  
------------------------------------------- 
10.1021/acsnano.9b09549  |   Developing sophisticated device architectures is of great significance to go beyond Moore's law with versatility toward human-machine interaction and artificial intelligence. Tribotronics/tribo-iontronics offer a direct way to controlling the transport properties of semiconductor devices by mechanical actions, which fundamentally relies on how to enhance the tribotronic gating effect through device engineering. Here, we propose a universal method to enhance the tribotronic properties through electric double layer (EDL) capacitive coupling. By preparing an ion gel layer on top of tribotronic graphene transistor, we demonstrate a dual-mode field effect transistor (<i>i.e.</i>, a tribotronic transistor with capacitively coupled ion gel and an ion-gel-gated graphene transistor with a second tribotronic gate). The resulted tribotronic gating performances are greatly improved by twice for the on-state current and four times for the on/off ratio (the first mode). It can also be utilized as a multiparameter distance sensor with drain current increased by ∼600 μA and threshold voltage shifted by ∼0.8 V under a mechanical displacement of 0.25 mm (the second mode). The proposed methodology of EDL capacitive coupling offers a facile and efficient way to designing more sophisticated tribotronic devices with superior performance and multifunctional sensations. 
  |  https://dx.doi.org/10.1021/acsnano.9b09549  |  
------------------------------------------- 
10.1007/s10554-019-01761-z  |   The incidence of heart failure (HF) increases in patients with chronic kidney disease (CKD). Factors that could predict patients with CKD who are at high risk for developing HF should be identified. We analysed clinical parameters and stress/rest myocardial perfusion imaging (MPI) findings derived from 499 patients with CKD by the Japanese Assessment of Cardiac Events and Survival Study by Quantitative Gated SPECT 3 (J-ACCESS 3) to clarify predictors of new-onset HF. Forty-one patients with congestive HF in the J-ACCESS 3 database were followed up for three years. Multivariable Cox hazards models selected haemoglobin (hazard ratio [HR] 0.809; 95% confidence interval [CI] 0.679-0.964), summed stress score (HR 1.082; 95% CI 1.016-1.151) and left ventricular ejection fraction (HR 0.970; 95% CI 0.949-0.992) as independent predictors of new-onset HF. Haemoglobin combined with summed stress scores and ejection fraction had the greatest incremental prognostic value over any one or more combined factors (global χ<sup>2</sup>, 29.9). Anaemia, stress-induced myocardial ischaemia, and left ventricular contraction are independent predictors of risk of new-onset HF in patients with CKD. Stress/rest MPI provides additional information with which to identify patients with CKD at greater risk of new-onset HF. 
  |  https://doi.org/10.1007/s10554-019-01761-z  |  
------------------------------------------- 
10.1073/pnas.1909872117  |   We apply to the random-field Ising model at zero temperature ([Formula: see text]) the perturbative loop expansion around the Bethe solution. A comparison with the standard ϵ expansion is made, highlighting the key differences that make the expansion around the Bethe solution much more appropriate to correctly describe strongly disordered systems, especially those controlled by a [Formula: see text] renormalization group (RG) fixed point. The latter loop expansion produces an effective theory with cubic vertices. We compute the one-loop corrections due to cubic vertices, finding additional terms that are absent in the ϵ expansion. However, these additional terms are subdominant with respect to the standard, supersymmetric ones; therefore, dimensional reduction is still valid at this order of the loop expansion. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=31953263  |  
------------------------------------------- 
10.3389/fped.2020.00001  |   Grading hydronephrosis severity relies on subjective interpretation of renal ultrasound images. Deep learning is a data-driven algorithmic approach to classifying data, including images, presenting a promising option for grading hydronephrosis. The current study explored the potential of deep convolutional neural networks (CNN), a type of deep learning algorithm, to grade hydronephrosis ultrasound images according to the 5-point Society for Fetal Urology (SFU) classification system, and discusses its potential applications in developing decision and teaching aids for clinical practice. We developed a five-layer CNN to grade 2,420 sagittal hydronephrosis ultrasound images [191 SFU 0 (8%), 407 SFU I (17%), 666 SFU II (28%), 833 SFU III (34%), and 323 SFU IV (13%)], from 673 patients ranging from 0 to 116.29 months old (<i>M</i> <sub>age</sub> = 16.53, <i>SD</i> = 17.80). Five-way (all grades) and two-way classification problems [i.e., II vs. III, and low (0-II) vs. high (III-IV)] were explored. The CNN classified 94% (95% CI, 93-95%) of the images correctly or within one grade of the provided label in the five-way classification problem. Fifty-one percent of these images (95% CI, 49-53%) were correctly predicted, with an average weighted F1 score of 0.49 (95% CI, 0.47-0.51). The CNN achieved an average accuracy of 78% (95% CI, 75-82%) with an average weighted F1 of 0.78 (95% CI, 0.74-0.82) when classifying low vs. high grades, and an average accuracy of 71% (95% CI, 68-74%) with an average weighted F1 score of 0.71 (95% CI, 0.68-0.75) when discriminating between grades II vs. III. Our model performs well above chance level, and classifies almost all images either correctly or within one grade of the provided label. We have demonstrated the applicability of a CNN approach to hydronephrosis ultrasound image classification. Further investigation into a deep learning-based clinical adjunct for hydronephrosis is warranted. 
  |  https://doi.org/10.3389/fped.2020.00001  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32064241/  |  
------------------------------------------- 
10.1002/jmri.27060  |   Penile cancer is one of the male-specific cancers. Accurate pretreatment staging is crucial due to a plethora of treatment options currently available. The 8<sup>th</sup> edition American Joint Committee on Cancer-Tumor Node and Metastasis (AJCC-TNM) revised the staging for penile cancers, with invasion of corpora cavernosa upstaged from T2 to T3 and invasion of urethra downstaged from T3 to being not separately relevant. With this revision, MRI is more relevant in local staging because MRI is accurate in identifying invasion of corpora cavernosa, while the accuracy is lower for detection of urethral involvement. The recent European Urology Association (EAU) guidelines recommend MRI to exclude invasion of the corpora cavernosa, especially if penis preservation is planned. Identification of satellite lesions and measurement of residual-penile-length help in surgical planning. When nonsurgical treatment modalities of the primary tumor are being considered, accurate local staging helps in decision-making regarding upfront inguinal lymph node dissection as against surveillance. MRI helps in detection and extent of inguinal and pelvic lymphadenopathy and is superior to clinical palpation, which continues to be the current approach recommended by National Comprehensive Cancer Network (NCCN) treatment guidelines. MRI helps the detection of "bulky" lymph nodes that warrant neoadjuvant chemotherapy and potentially identify extranodal extension. However, tumor involvement in small lymph nodes and differentiation of reactive vs. malignant lymphadenopathy in large lymph nodes continue to be challenging and the utilization of alternative contrast agents (superparamagnetic iron oxide), positron emission tomography (PET)-MRI along with texture analysis is promising. In locally recurrent tumors, MRI is invaluable in identification of deep invasion, which forms the basis of treatment. Multiparametric MRI, especially diffusion-weighted-imaging, may allow for quantitative noninvasive assessment of tumor grade and histologic subtyping to avoid biopsy undersampling. Further research is required for incorporation of MRI with deep learning and artificial intelligence algorithms for effective staging in penile cancer. LEVEL OF EVIDENCE: 5 TECHNICAL EFFICACY: Stage 3. 
  |  https://doi.org/10.1002/jmri.27060  |  
------------------------------------------- 
10.3389/fbioe.2020.00088  |    Background:  DNA methylation plays essential roles in tumor occurrence and stemness maintenance. Tumor-repopulating cells (TRCs) are cancer stem cell (CSC)-like cells with highly tumorigenic and self-renewing abilities, which were selected from tumor cells in soft three-dimensional (3D) fibrin gels. 
  Methods:  Here, we presented a genome-wide map of methylated cytosines for time-series samples in TRC selection, in a 3D culture using whole-genome bisulfite sequencing (WGBS). 
  Results:  A comparative analysis revealed that the methylation degrees of many differentially methylated genes (DMGs) were increased by the mechanical environment and changed from 2D rigid to 3D soft. DMGs were significantly enriched in stemness-related terms. In 1-day, TRCs had the highest non-CG methylation rate indicating its strong stemness. We found that genes with continuously increasing or decreasing methylation like <i>CREB5/ADAMTS6/LMX1A</i> may also affect the TRC screening process. Furthermore, results showed that stage-specific/common CSCs markers were biased toward changing their methylation in non-CG (CHG and CHH, where H corresponds to A, T, or C) methylation and enriched in gene body region. 
  Conclusions:  WGBS provides DNA methylome in TRC screening. It was confirmed that non-CG DNA methylation plays an important role in TRC selection, which indicates that it is more sensitive to mechanical microenvironments and affects TRCs by regulating the expression of stemness genes in tumor cells. 
  |  https://doi.org/10.3389/fbioe.2020.00088  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32258002/  |  
------------------------------------------- 
10.1021/acsnano.9b08952  |   Recently, wearable and flexible pressure sensors have sparked tremendous research interest, and considerable applications including human activity monitoring, biomedical research, and artificial intelligence interaction are reported. However, the large-scale preparation of low-cost, high-sensitivity piezoresistive sensors still face huge challenges. Inspired by the specific structures and excellent metal conductivity of a family of two-dimensional (2D) transition-metal carbides and nitrides (MXene) and the high-performance sensing effect of human skin including randomly distributed microstructural receptors, we fabricate a highly sensitive MXene-based piezoresistive sensor with bioinspired microspinous microstructures formed by a simple abrasive paper stencil printing process. The obtained piezoresistive sensor shows high sensitivity (151.4 kPa<sup>-1</sup>), relatively short response time (&lt;130 ms), subtle pressure detection limit of 4.4 Pa, and excellent cycle stability over 10,000 cycles. The mechanism of the high sensitivity of the sensor is dynamically revealed from the structural perspective by means of <i>in situ</i> electron microscopy experiment and finite element simulation. Bioinspired microspinous microstructures can effectively improve the sensitivity of the pressure sensor and the limit of the detectable subtle pressure. In practice, the sensor shows great performance in monitoring human physiological signals, detecting quantitatively pressure distributions, and remote monitoring of intelligent robot motion in real time. 
  |  https://dx.doi.org/10.1021/acsnano.9b08952  |  
------------------------------------------- 
10.1016/j.media.2019.101630  |   Fusing multi-modality data is crucial for accurate identification of brain disorder as different modalities can provide complementary perspectives of complex neurodegenerative disease. However, there are at least four common issues associated with the existing fusion methods. First, many existing fusion methods simply concatenate features from each modality without considering the correlations among different modalities. Second, most existing methods often make prediction based on a single classifier, which might not be able to address the heterogeneity of the Alzheimer's disease (AD) progression. Third, many existing methods often employ feature selection (or reduction) and classifier training in two independent steps, without considering the fact that the two pipelined steps are highly related to each other. Forth, there are missing neuroimaging data for some of the participants (e.g., missing PET data), due to the participants' "no-show" or dropout. In this paper, to address the above issues, we propose an early AD diagnosis framework via novel multi-modality latent space inducing ensemble SVM classifier. Specifically, we first project the neuroimaging data from different modalities into a latent space, and then map the learned latent representations into the label space to learn multiple diversified classifiers. Finally, we obtain the more reliable classification results by using an ensemble strategy. More importantly, we present a Complete Multi-modality Latent Space (CMLS) learning model for complete multi-modality data and also an Incomplete Multi-modality Latent Space (IMLS) learning model for incomplete multi-modality data. Extensive experiments using the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset have demonstrated that our proposed models outperform other state-of-the-art methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(19)30166-5  |  
------------------------------------------- 
10.1186/s12911-020-1078-3  |    Background:  Drug label, or packaging insert play a significant role in all the operations from production through drug distribution channels to the end consumer. Image of the label also called Display Panel or label could be used to identify illegal, illicit, unapproved and potentially dangerous drugs. Due to the time-consuming process and high labor cost of investigation, an artificial intelligence-based deep learning model is necessary for fast and accurate identification of the drugs. 
  Methods:  In addition to image-based identification technology, we take advantages of rich text information on the pharmaceutical package insert of drug label images. In this study, we developed the Drug Label Identification through Image and Text embedding model (DLI-IT) to model text-based patterns of historical data for detection of suspicious drugs. In DLI-IT, we first trained a Connectionist Text Proposal Network (CTPN) to crop the raw image into sub-images based on the text. The texts from the cropped sub-images are recognized independently through the Tesseract OCR Engine and combined as one document for each raw image. Finally, we applied universal sentence embedding to transform these documents into vectors and find the most similar reference images to the test image through the cosine similarity. 
  Results:  We trained the DLI-IT model on 1749 opioid and 2365 non-opioid drug label images. The model was then tested on 300 external opioid drug label images, the result demonstrated our model achieves up-to 88% of the precision in drug label identification, which outperforms previous image-based or text-based identification method by up-to 35% improvement. 
  Conclusion:  To conclude, by combining Image and Text embedding analysis under deep learning framework, our DLI-IT approach achieved a competitive performance in advancing drug label identification. 
  |  https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1078-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32293428/  |  
------------------------------------------- 
10.1111/gtc.12746  |   Translesion synthesis (TLS) polymerases mediate DNA damage bypass during replication. The TLS polymerase Rev1 has two important functions in the TLS pathway, including dCMP transferase activity and acting as a scaffolding protein for other TLS polymerases at the C-terminus. Because of the former activity, Rev1 bypasses apurinic/apyrimidinic sites by incorporating dCMP, whereas the latter activity mediates assembly of multipolymerase complexes at the DNA lesions. We generated rev1 mutants lacking each of these two activities in Oryzias latipes (medaka) fish and analyzed cytotoxicity and mutagenicity in response to the alkylating agent diethylnitrosamine (DENA). Mutant lacking the C-terminus was highly sensitive to DENA cytotoxicity, whereas mutant with reduced dCMP transferase activity was slightly sensitive to DENA cytotoxicity, but exhibited a higher tumorigenic rate than wild-type fish. There was no significant difference in the frequency of DENA-induced mutations between mutant with reduced dCMP transferase activity and wild-type cultured cell. However, loss of heterozygosity (LOH) occurred frequently in cells with reduced dCMP transferase activity. LOH is a common genetic event in many cancer types and plays an important role on carcinogenesis. To our knowledge, this is the first report to identify the involvement of the catalytic activity of Rev1 in suppression of LOH. 
  |  https://doi.org/10.1111/gtc.12746  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31917895/  |  
------------------------------------------- 
10.3390/s20051462  |   The development of innovative solutions that allow the aging population to remain healthier and independent longer is essential to alleviate the burden that this increasing segment of the population supposes for the long term sustainability of the public health systems. It has been claimed that promoting physical activity could prevent functional decline. However, given the vulnerability of this population, the activity prescription requires to be tailored to the individual's physical condition. We propose mobile Senior Fitness Test (m-SFT), a novel m-health system, that allows the health practitioner to determine the elderly physical condition by implementing a smartphone-based version of the senior fitness test (SFT). The technical reliability of m-SFT has been tested by carrying out a comparative study in seven volunteers (53-61 years) between the original SFT and the proposed m-health system obtaining high agreement (intra-class correlation coefficient (ICC) between 0.93 and 0.99). The system usability has been evaluated by 34 independent health experts (mean = 36.64 years; standard deviation = 6.26 years) by means of the System Usability Scale (SUS) obtaining an average SUS score of 84.4 out of 100. Both results point out that m-SFT is a reliable and easy to use m-health system for the evaluation of the elderly physical condition, also useful in intervention programs to follow-up the patient's evolution. 
  |  http://www.mdpi.com/resolver?pii=s20051462  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32155931/  |  
------------------------------------------- 
10.1021/acs.jctc.9b01255  |   The complete active space self-consistent field (CASSCF) method is the principal approach employed for studying strongly correlated systems. However, exact CASSCF can only be performed on small active spaces of ∼20 electrons in ∼20 orbitals due to exponential growth in the computational cost. We show that employing the Adaptive Sampling Configuration Interaction (ASCI) method as an approximate Full CI solver in the active space allows CASSCF-like calculations within chemical accuracy (&lt;1 kcal/mol for relative energies) in active spaces with more than ∼50 active electrons in ∼50 active orbitals, significantly increasing the sizes of systems amenable to accurate multiconfigurational treatment. The main challenge with using any selected CI-based approximate CASSCF is the orbital optimization problem; they tend to exhibit large numbers of local minima in orbital space due to their lack of invariance to active-active rotations (in addition to the local minima that exist in exact CASSCF). We highlight methods that can avoid spurious local extrema as a practical solution to the orbital optimization problem. We employ ASCI-SCF to demonstrate a lack of polyradical character in moderately sized periacenes with up to 52 correlated electrons and compare against heat-bath CI on an iron porphyrin system with more than 40 correlated electrons. 
  |  None  |  
------------------------------------------- 
10.1016/j.ejmp.2020.01.004  |    Purpose:  Anti-scatter grids suppress the scatter substantially thus improving image contrast in radiography. However, its active use in cone-beam CT for the purpose of improving contrast-to-noise ratio (CNR) has not been successful mainly due to the increased noise related to Poisson statistics of photons. This paper proposes a sparse-view scanning approach to address the above issue. 
  Method:  Compared to the conventional cone-beam CT imaging framework, the proposed method reduces the number of projections and increases exposure in each projection to enhance image quality without an additional cost of radiation dose to patients. For image reconstruction from sparse-view data, an adaptive-steepest-descent projection-onto-convex-sets (ASD POCS) algorithm regularized by total-variation (TV) minimization was adopted. Contrast and CNR with various scattering conditions were evaluated in projection domain by a simulation study using GATE. Then we evaluated contrast, resolution, and image uniformity in CT image domain with Catphan phantom. A head phantom with soft-tissue structures was also employed for demonstrating a realistic application. A virtual grid-based estimation and reduction of scatter has also been implemented for comparison with the real anti-scatter grid. 
  Results:  In the projection domain evaluation, contrast and CNR enhancement was observed when using an anti-scatter grid compared to the virtual grid. In the CT image domain, the proposed method produced substantially higher contrast and CNR of the low-contrast structures with much improved image uniformity. 
  Conclusion:  We have shown that the proposed method can provide high-quality CBCT images particularly with an increased contrast of soft-tissue at a neutral dose for image-guidance. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1120-1797(20)30004-1  |  
------------------------------------------- 
10.1186/s12931-020-1285-6  |    Background:  Ventilator-associated pneumonia (VAP) is a significant cause of mortality in the intensive care unit. Early diagnosis of VAP is important to provide appropriate treatment and reduce mortality. Developing a noninvasive and highly accurate diagnostic method is important. The invention of electronic sensors has been applied to analyze the volatile organic compounds in breath to detect VAP using a machine learning technique. However, the process of building an algorithm is usually unclear and prevents physicians from applying the artificial intelligence technique in clinical practice. Clear processes of model building and assessing accuracy are warranted. The objective of this study was to develop a breath test for VAP with a standardized protocol for a machine learning technique. 
  Methods:  We conducted a case-control study. This study enrolled subjects in an intensive care unit of a hospital in southern Taiwan from February 2017 to June 2019. We recruited patients with VAP as the case group and ventilated patients without pneumonia as the control group. We collected exhaled breath and analyzed the electric resistance changes of 32 sensor arrays of an electronic nose. We split the data into a set for training algorithms and a set for testing. We applied eight machine learning algorithms to build prediction models, improving model performance and providing an estimated diagnostic accuracy. 
  Results:  A total of 33 cases and 26 controls were used in the final analysis. Using eight machine learning algorithms, the mean accuracy in the testing set was 0.81 ± 0.04, the sensitivity was 0.79 ± 0.08, the specificity was 0.83 ± 0.00, the positive predictive value was 0.85 ± 0.02, the negative predictive value was 0.77 ± 0.06, and the area under the receiver operator characteristic curves was 0.85 ± 0.04. The mean kappa value in the testing set was 0.62 ± 0.08, which suggested good agreement. 
  Conclusions:  There was good accuracy in detecting VAP by sensor array and machine learning techniques. Artificial intelligence has the potential to assist the physician in making a clinical diagnosis. Clear protocols for data processing and the modeling procedure needed to increase generalizability. 
  |  https://respiratory-research.biomedcentral.com/articles/10.1186/s12931-020-1285-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32033607/  |  
------------------------------------------- 
10.1007/s00784-020-03245-0  |    Objective:  Ex-vivo evaluation of the detectability of vertical root fractures (VRFs) using digital subtraction radiography (DSR) and conventional digital periapical radiography (CDPR); investigation of the effect of root canal filling, x-ray angulation, and thickness of the VRF in the diagnostic accuracy. 
  Materials and methods:  Sixty root canals were mechanically prepared and radiographed either with a gutta-percha root canal filling or without, at 0<sup>o</sup> and ± 10<sup>o</sup>. VRFs were introduced with a universal testing machine. The width and angulation of the fracture line with the radiographic beam were calculated. DSR was performed comparing radiographs obtained prior to and after the VRF induction. Five examiners evaluated the resultant images and analysis was performed using receiver operator characteristic (ROC) statistics and binary logistic regression tests. 
  Results:  No significant differences in sensitivity, specificity, and the areas under the ROC curves (AUC) between the CDPR and DSR were detected (p &gt; 0.05), except for root canal filled teeth where the AUC for DSR was higher (p &lt; 0.05). Using DSR, a VRF was 1.3 times more likely to be diagnosed [95% confidence intervals (CI): 1.045-1.59; p = 0.018]. A correct diagnosis was 2.399 times more likely to occur in non-filled teeth regardless of the radiographic technique (95% CI 1.940-2.965; p = 0). The regression coefficients were positive for width and negative for angle. 
  Conclusions:  DSR showed a better diagnostic accuracy of VRFs compared with CDPR, in single root canal filled teeth. The angulation, the width, and the presence of a root canal filling affected the diagnostic potential. 
  Clinical relevance:  DSR is a cost- and time-effective imaging technique that could contribute in early diagnosis of VRFs. 
  |  https://dx.doi.org/10.1007/s00784-020-03245-0  |  
------------------------------------------- 
10.3390/brainsci10020084  |   Alzheimer's disease (AD) may cause damage to the memory cells permanently, which results in the form of dementia. The diagnosis of Alzheimer's disease at an early stage is a problematic task for researchers. For this, machine learning and deep convolutional neural network (CNN) based approaches are readily available to solve various problems related to brain image data analysis. In clinical research, magnetic resonance imaging (MRI) is used to diagnose AD. For accurate classification of dementia stages, we need highly discriminative features obtained from MRI images. Recently advanced deep CNN-based models successfully proved their accuracy. However, due to a smaller number of image samples available in the datasets, there exist problems of over-fitting hindering the performance of deep learning approaches. In this research, we developed a Siamese convolutional neural network (SCNN) model inspired by VGG-16 (also called Oxford Net) to classify dementia stages. In our approach, we extend the insufficient and imbalanced data by using augmentation approaches. Experiments are performed on a publicly available dataset open access series of imaging studies (OASIS), by using the proposed approach, an excellent test accuracy of 99.05% is achieved for the classification of dementia stages. We compared our model with the state-of-the-art models and discovered that the proposed model outperformed the state-of-the-art models in terms of performance, efficiency, and accuracy. 
  |  http://www.mdpi.com/resolver?pii=brainsci10020084  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32033462/  |  
------------------------------------------- 
10.1016/S2213-8587(19)30411-5  |   Although the prevalence of all stages of diabetic retinopathy has been declining since 1980 in populations with improved diabetes control, the crude prevalence of visual impairment and blindness caused by diabetic retinopathy worldwide increased between 1990 and 2015, largely because of the increasing prevalence of type 2 diabetes, particularly in low-income and middle-income countries. Screening for diabetic retinopathy is essential to detect referable cases that need timely full ophthalmic examination and treatment to avoid permanent visual loss. In the past few years, personalised screening intervals that take into account several risk factors have been proposed, with good cost-effectiveness ratios. However, resources for nationwide screening programmes are scarce in many countries. New technologies, such as scanning confocal ophthalmology with ultrawide field imaging and handheld mobile devices, teleophthalmology for remote grading, and artificial intelligence for automated detection and classification of diabetic retinopathy, are changing screening strategies and improving cost-effectiveness. Additionally, emerging evidence suggests that retinal imaging could be useful for identifying individuals at risk of cardiovascular disease or cognitive impairment, which could expand the role of diabetic retinopathy screening beyond the prevention of sight-threatening disease. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2213-8587(19)30411-5  |  
------------------------------------------- 
10.1093/database/baaa005  |   Drosophila melanogaster is a well-established model organism that is widely used in genetic studies. This species enjoys the availability of a wide range of research tools, well-annotated reference databases and highly similar gene circuitry to other insects. To facilitate molecular mechanism studies in Drosophila, we present the Predicted Drosophila Interactome Resource (PDIR), a database of high-quality predicted functional gene interactions. These interactions were inferred from evidence in 10 public databases providing information for functional gene interactions from diverse perspectives. The current version of PDIR includes 102 835 putative functional associations with balanced sensitivity and specificity, which are expected to cover 22.56% of all Drosophila protein interactions. This set of functional interactions is a good reference for hypothesis formulation in molecular mechanism studies. At the same time, these interactions also serve as a high-quality reference interactome for gene set linkage analysis (GSLA), which is a web tool for the interpretation of the potential functional impacts of a set of changed genes observed in transcriptomics analyses. In a case study, we show that the PDIR/GSLA system was able to produce a more comprehensive and concise interpretation of the collective functional impact of multiple simultaneously changed genes compared with the widely used gene set annotation tools, including PANTHER and David. PDIR and its associated GSLA service can be accessed at http://drosophila.biomedtzc.cn. 
  |  https://academic.oup.com/database/article-lookup/doi/10.1093/database/baaa005  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32103267/  |  
------------------------------------------- 
10.1089/tmj.2020.0008  |   <b><i>Background:</i></b> <i>The introduction of artificial intelligence (AI) in medicine has raised significant ethical, economic, and scientific controversies.</i> <b><i>Introduction:</i></b> <i>Because an explicit goal of AI is to perform processes previously reserved for human clinicians and other health care personnel, there is justified concern about the impact on patient safety, efficacy, equity, and liability.</i> <b><i>Discussion:</i></b> <i>Systems for computer-assisted and fully automated detection, triage, and diagnosis of diabetic retinopathy (DR) from retinal images show great variation in design, level of autonomy, and intended use. Moreover, the degree to which these systems have been evaluated and validated is heterogeneous. We use the term DR AI system as a general term for any system that interprets retinal images with at least some degree of autonomy from a human grader. We put forth these standardized descriptors to form a means to categorize systems for computer-assisted and fully automated detection, triage, and diagnosis of DR. The components of the categorization system include level of device autonomy, intended use, level of evidence for diagnostic accuracy, and system design.</i> <b><i>Conclusion:</i></b> <i>There is currently minimal empirical basis to assert that certain combinations of autonomy, accuracy, or intended use are better or more appropriate than any other. Therefore, at the current stage of development of this document, we have been descriptive rather than prescriptive, and we treat the different categorizations as independent and organized along multiple axes.</i> 
  |  https://www.liebertpub.com/doi/full/10.1089/tmj.2020.0008?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/ijms21062017  |   The pursuit for effective strategies inhibiting the amyloidogenic process in neurodegenerative disorders, such as Alzheimer's disease (AD), remains one of the main unsolved issues, and only a few drugs have demonstrated to delay the degeneration of the cognitive system. Moreover, most therapies induce severe side effects and are not effective at all stages of the illness. The need to find novel and reliable drugs appears therefore of primary importance. In this context, natural compounds have shown interesting beneficial effects on the onset and progression of neurodegenerative diseases, exhibiting a great inhibitory activity on the formation of amyloid aggregates and proving to be effective in many preclinical and clinical studies. However, their inhibitory mechanism is still unclear. In this work, ensemble docking and molecular dynamics simulations on S-shaped Aβ<sub>42</sub> fibrils have been carried out to evaluate the influence of several natural compounds on amyloid conformational behaviour. A deep understanding of the interaction mechanisms between natural compounds and Aβ aggregates may play a key role to pave the way for design, discovery and optimization strategies toward an efficient destabilization of toxic amyloid assemblies. 
  |  http://www.mdpi.com/resolver?pii=ijms21062017  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32188076/  |  
------------------------------------------- 
10.1007/s11427-019-1638-6  |   Spatial chromatin structure plays fundamental roles in many vital biological processes including DNA replication, transcription, damage and repair. However, the current understanding of the secondary structure of chromatin formed by local nucleosome-nucleosome interactions remains controversial, especially for the existence and conformation of 30 nm structure. Since chromatin structure influences the fragment length distribution (FLD) of ionizing radiation-induced DNA strand breaks, a 3D chromatin model fitting FLD patterns can help to distinguish different models of chromatin structure. Here, we developed a novel "30-C" model combining 30 nm chromatin structure models with Hi-C data, which measured the spatial contact frequency between different loci in the genome. We first reconstructed the 3D coordinates of the 25 kb bins from Hi-C heatmaps. Within the 25 kb bins, lower level chromatin structures supported by recent studies were filled. Simulated FLD patterns based on the 30-C model were compared to published FLD patterns induced by heavy ion radiation to validate the models. Importantly, the 30-C model predicted that the most probable chromatin fiber structure for human interphase fibroblasts in vivo was 45% zig-zag 30 nm fibers and 55% 10 nm fibers. 
  |  https://dx.doi.org/10.1007/s11427-019-1638-6  |  
------------------------------------------- 
10.1007/s11948-020-00210-8  |   The Internet has been identified in human enhancement scholarship as a powerful cognitive enhancement technology. It offers instant access to almost any type of information, along with the ability to share that information with others. The aim of this paper is to critically assess the enhancement potential of the Internet. We argue that unconditional access to information does not lead to cognitive enhancement. The Internet is not a simple, uniform technology, either in its composition, or in its use. We will look into why the Internet as an informational resource currently fails to enhance cognition. We analyze some of the phenomena that emerge from vast, continual fluxes of information-information overload, misinformation and persuasive design-and show how they could negatively impact users' cognition. Methods for mitigating these negative impacts are then advanced: individual empowerment, better collaborative systems for sorting and categorizing information, and the use of artificial intelligence assistants that could guide users through the informational space of today's Internet. 
  |  https://dx.doi.org/10.1007/s11948-020-00210-8  |  
------------------------------------------- 
10.1016/j.neunet.2020.03.025  |   The accuracy of deep learning (e.g., convolutional neural networks) for an image classification task critically relies on the amount of labeled training data. Aiming to solve an image classification task on a new domain that lacks labeled data but gains access to cheaply available unlabeled data, unsupervised domain adaptation is a promising technique to boost the performance without incurring extra labeling cost, by assuming images from different domains share some invariant characteristics. In this paper, we propose a new unsupervised domain adaptation method named Domain-Adversarial Residual-Transfer (DART) learning of deep neural networks to tackle cross-domain image classification tasks. In contrast to the existing unsupervised domain adaption approaches, the proposed DART not only learns domain-invariant features via adversarial training, but also achieves robust domain-adaptive classification via a residual-transfer strategy, all in an end-to-end training framework. We evaluate the performance of the proposed method for cross-domain image classification tasks on several well-known benchmark data sets, in which our method clearly outperforms the state-of-the-art approaches. 
  |  None  |  
------------------------------------------- 
10.1038/s41433-019-0667-9  |   Glaucoma presents considerable challenges in providing clinically and cost-effective care pathways. While UK population screening is not seen as justifiable, arrangements for case finding have historically been considered relatively ineffective. Detection challenges include an undetected disease burden, whether from populations failing to access services or difficulties in delivering effective case-finding strategies, and a high false positive rate from referrals via traditional case finding pathways. The enhanced General Ophthalmic Service (GOS) in Scotland and locally commissioned glaucoma referral filtering services (GRFS) elsewhere have undoubtedly reduced false positive referrals, and there is emerging evidence of effectiveness of these pathways. At the same time, it is recognised that implementing GRFS does not intrinsically reduce the burden of undetected glaucoma and late presentation, and obvious challenges remain. In terms of diagnosis and monitoring, considerable growth in capacity remains essential, and non-medical health care professional (HCP) co-management and virtual clinics continue to be important solutions in offering requisite capacity. National guidelines, commissioning recommendations, and the Common Clinical Competency Framework have clarified requirements for such services, including recommendations on training and accreditation of HCPs. At the same time, the nature of consultant-delivered care and expectations on the glaucoma specialist's role has evolved alongside these developments. Despite progress in recent decades, given projected capacity requirements, further care pathways innovations appear mandated. While the timeline for implementing potential artificial intelligence innovations in streamlining care pathways is far from established, the glaucoma burden presents an expectation that such developments will need to be at the vanguard of future developments. 
 青光眼在提供临床和成本效益的护理途径方面面临着相当大的挑战。虽然英国的人群筛查被认为是不合理的, 但在历史上, 个体诊断也一直认为是相对无效的。早期诊断方面的挑战包括了未被发现的疾病负担, 无论是无法获得服务的人群筛查还是难以提供有效的病例发现策略, 以及通过传统的转诊途径所发现的病例诊断的高假阳性率。不断提升的苏格兰普通眼科服务 (GOS) 和其他地方委托的青光眼转诊过滤服务 (GRFS) 无疑减少了假阳性转诊率, 而且新的证据表明这些途径是有效的。同时, 我们认识到实施GRFS并不能从本质上减轻未被发现的青光眼和晚期青光眼的负担, 而且仍存在明显的挑战。在早期诊断和监测方面, 监护能力的大幅增长仍至关重要, 非医疗保健专业人员 (HCP) 联合管理和虚拟诊所仍然是提供必要能力的重要解决办法。国家指导方针、委托建议和共同临床能力框架阐明了对此类服务的要求, 包括对HCP培训和认证的建议。同时, 在青光眼专家在咨询以及对其期待的承担的角色也伴随着这些发展而变化。尽管近几十年来取得了进展, 但考虑到预期的能力需求, 进一步的护理路径创新似乎已迫在眉睫。虽然在简化青光眼的护理路径方面实施潜在的人工智能创新的时间还远未确定, 但对青光眼早期诊断与监控的巨大负担也预示着它将成为未来发展的先锋 
  |  http://dx.doi.org/10.1038/s41433-019-0667-9  |  
------------------------------------------- 
10.3390/cancers12030603  |   Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance, etc. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients. The improvement of biomedical translational research and the application of advanced statistical analysis and machine learning methods are the driving forces to improve cancer prognosis prediction. Recent years, there is a significant increase of computational power and rapid advancement in the technology of artificial intelligence, particularly in deep learning. In addition, the cost reduction in large scale next-generation sequencing, and the availability of such data through open source databases (e.g., TCGA and GEO databases) offer us opportunities to possibly build more powerful and accurate models to predict cancer prognosis more accurately. In this review, we reviewed the most recent published works that used deep learning to build models for cancer prognosis prediction. Deep learning has been suggested to be a more generic model, requires less data engineering, and achieves more accurate prediction when working with large amounts of data. The application of deep learning in cancer prognosis has been shown to be equivalent or better than current approaches, such as Cox-PH. With the burst of multi-omics data, including genomics data, transcriptomics data and clinical information in cancer studies, we believe that deep learning would potentially improve cancer prognosis. 
  |  http://www.mdpi.com/resolver?pii=cancers12030603  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32150991/  |  
------------------------------------------- 
10.3390/ma13051205  |   Concrete filled steel tubes (CFSTs) show advantageous applications in the field of construction, especially for a high axial load capacity. The challenge in using such structure lies in the selection of many parameters constituting CFST, which necessitates defining complex relationships between the components and the corresponding properties. The axial capacity (P<sub>u</sub>) of CFST is among the most important mechanical properties. In this study, the possibility of using a feedforward neural network (FNN) to predict P<sub>u</sub> was investigated. Furthermore, an evolutionary optimization algorithm, namely invasive weed optimization (IWO), was used for tuning and optimizing the FNN weights and biases to construct a hybrid FNN-IWO model and improve its prediction performance. The results showed that the FNN-IWO algorithm is an excellent predictor of P<sub>u</sub>, with a value of R<sup>2</sup> of up to 0.979. The advantage of FNN-IWO was also pointed out with the gains in accuracy of 47.9%, 49.2%, and 6.5% for root mean square error (RMSE), mean absolute error (MAE), and R<sup>2</sup>, respectively, compared with simulation using the single FNN. Finally, the performance in predicting the P<sub>u</sub> in the function of structural parameters such as depth/width ratio, thickness of steel tube, yield stress of steel, concrete compressive strength, and slenderness ratio was investigated and discussed. 
  |  http://www.mdpi.com/resolver?pii=ma13051205  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32156033/  |  
------------------------------------------- 
10.1186/s12885-020-6694-x  |    Background:  As the number of PET/CT scanners increases and FDG PET/CT becomes a common imaging modality for oncology, the demands for automated detection systems on artificial intelligence (AI) to prevent human oversight and misdiagnosis are rapidly growing. We aimed to develop a convolutional neural network (CNN)-based system that can classify whole-body FDG PET as 1) benign, 2) malignant or 3) equivocal. 
  Methods:  This retrospective study investigated 3485 sequential patients with malignant or suspected malignant disease, who underwent whole-body FDG PET/CT at our institute. All the cases were classified into the 3 categories by a nuclear medicine physician. A residual network (ResNet)-based CNN architecture was built for classifying patients into the 3 categories. In addition, we performed a region-based analysis of CNN (head-and-neck, chest, abdomen, and pelvic region). 
  Results:  There were 1280 (37%), 1450 (42%), and 755 (22%) patients classified as benign, malignant and equivocal, respectively. In the patient-based analysis, CNN predicted benign, malignant and equivocal images with 99.4, 99.4, and 87.5% accuracy, respectively. In region-based analysis, the prediction was correct with the probability of 97.3% (head-and-neck), 96.6% (chest), 92.8% (abdomen) and 99.6% (pelvic region), respectively. 
  Conclusion:  The CNN-based system reliably classified FDG PET images into 3 categories, indicating that it could be helpful for physicians as a double-checking system to prevent oversight and misdiagnosis. 
  |  https://bmccancer.biomedcentral.com/articles/10.1186/s12885-020-6694-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32183748/  |  
------------------------------------------- 
10.3390/s20030903  |   Wireless acoustic sensor networks are nowadays an essential tool for noise pollution monitoring and managing in cities. The increased computing capacity of the nodes that create the network is allowing the addition of processing algorithms and artificial intelligence that provide more information about the sound sources and environment, e.g., detect sound events or calculate loudness. Several models to predict sound pressure levels in cities are available, mainly road, railway and aerial traffic noise. However, these models are mostly based in auxiliary data, e.g., vehicles flow or street geometry, and predict equivalent levels for a temporal long-term. Therefore, forecasting of temporal short-term sound levels could be a helpful tool for urban planners and managers. In this work, a Long Short-Term Memory (LSTM) deep neural network technique is proposed to model temporal behavior of sound levels at a certain location, both sound pressure level and loudness level, in order to predict near-time future values. The proposed technique can be trained for and integrated in every node of a sensor network to provide novel functionalities, e.g., a method of early warning against noise pollution and of backup in case of node or network malfunction. To validate this approach, one-minute period equivalent sound levels, captured in a two-month measurement campaign by a node of a deployed network of acoustic sensors, have been used to train it and to obtain different forecasting models. Assessments of the developed LSTM models and Auto regressive integrated moving average models were performed to predict sound levels for several time periods, from 1 to 60 min. Comparison of the results show that the LSTM models outperform the statistics-based models. In general, the LSTM models achieve a prediction of values with a mean square error less than 4.3 dB for sound pressure level and less than 2 phons for loudness. Moreover, the goodness of fit of the LSTM models and the behavior pattern of the data in terms of prediction of sound levels are satisfactory. 
  |  http://www.mdpi.com/resolver?pii=s20030903  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046231/  |  
------------------------------------------- 
10.1021/acs.jmedchem.9b01721  |   p300 and CREB-binding protein (CBP) are ubiquitously expressed pleiotropic lysine acetyltransferases and play a key role as transcriptional co-activators that are essential for a multitude of cellular processes. Despite great importance, there is a lack of highly selective, potent, druglike p300/CBP inhibitors. Through the artificial-intelligence-assisted drug discovery pipeline and further optimization, we reported the discovery of novel, highly selective, potent small-molecule inhibitors of p300/CBP histone acetyltransferases (HAT) with desired druglike properties, exemplified by <b>B026</b>. Our data demonstrated that <b>B026</b>, with half maximal inhibitory concentration (IC<sub>50</sub>) values of 1.8 nM to p300 and 9.5 nM to CBP enzyme inhibitory activity, is the most potent, selective p300/CBP HAT inhibitor. Moreover, <b>B026</b> achieves significant and dose-dependent tumor growth inhibition in an animal model of human cancer, suggesting that <b>B026</b> is a highly promising p300/CBP HAT inhibitor and warrants extensive preclinical investigation as a potential clinical development candidate. 
  |  https://dx.doi.org/10.1021/acs.jmedchem.9b01721  |  
------------------------------------------- 
PMID:32189957  |   This commentary describes the changes taking place in dentistry and speculates on improvements that could happen soon. Advances in health care will have an impact on the integration and delivery of oral care; conversely, there is growing acceptance that oral health impacts systemic health. Technological innovations are changing the face of medical care and are quickly becoming integrated into dentistry. Advances in novel antimicrobials, genomics, robotics and artificial intelligence are transforming our ability to diagnose and manage disease. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32189957/  |  
------------------------------------------- 
10.1016/j.mrfmmm.2020.111687  |   Methylene tetrahydrofolate reductase (MTHFR) is a flavoprotein, involved in one-carbon pathway and is responsible for folate and homocysteine metabolism. Regulation of MTHFR is pivotal for maintaining the cellular concentrations of methionine and SAM (S-adenosyl methionine) which are essential for the synthesis of nucleotides and amino acids, respectively. Therefore, mutations in MTHFR leads to its dysfunction resulting in conditions like homocystinuria, cardiovascular diseases, and neural tube defects in infants. Among these conditions, homocystinuria has been highly explored, as it manifests ocular disorders, cognitive disorders and skeletal abnormalities. Hence, in this study, we intend to explore the mutational landscape of human MTHFR isoform-1 (h.MTHFR-1) to decipher the most pathogenic variants pertaining to homocystinuria. Thus, a multilevel stringent prioritization of non-synonymous mutations in h.MTHFR-1 by integrative machine learning approaches was implemented to delineate highly deleterious variants based on its pathogenicity, impact on structural stability and functionality. Subsequently, extended molecular dynamics simulations and molecular docking studies were also integrated in order to prioritize the mutations that perturbs structural stability and functionality of h.MTHFR-1. In addition, displacement of Loop (Arg157-Tyr174) and helix α9 (His263-Ser272) involved in open/closed conformation of substrate binding domain were also probed to confirm the functional loss. On juxtaposed analysis, it was inferred that among 126 missense mutations screened, along with known pathogenic mutations (H127 T, A222 V, T227 M, F257 V and G387D) predicted that W500C, P254S and D585 N variants could be potentially driving homocystinuria. Thus, uncovering the prospects for inclusion of these mutations in diagnostic panels based on further experimental validations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0027-5107(19)30104-6  |  
------------------------------------------- 
10.1172/JCI131048  |   Smooth muscle cell (SMC) proliferation has been thought to limit the progression of thoracic aortic aneurysm and dissection (TAAD) because loss of medial cells associates with advanced disease. We investigated effects of SMC proliferation in the aortic media by conditional disruption of Tsc1, which hyperactivates mTOR complex 1. Consequent SMC hyperplasia led to progressive medial degeneration and TAAD. In addition to diminished contractile and synthetic functions, fate-mapped SMCs displayed increased proteolysis, endocytosis, phagocytosis, and lysosomal clearance of extracellular matrix and apoptotic cells. SMCs acquired a limited repertoire of macrophage markers and functions via biogenesis of degradative organelles through an mTOR/β-catenin/MITF-dependent pathway, but were distinguishable from conventional macrophages by an absence of hematopoietic lineage markers and certain immune effectors even in the context of hyperlipidemia. Similar mTOR activation and induction of a degradative SMC phenotype in a model of mild TAAD due to Fbn1 mutation greatly worsened disease with near-uniform lethality. The finding of increased lysosomal markers in medial SMCs from clinical TAAD specimens with hyperplasia and matrix degradation further supports the concept that proliferation of degradative SMCs within the media causes aortic disease, thus identifying mTOR-dependent phenotypic modulation as a therapeutic target for combating TAAD. 
  |  https://doi.org/10.1172/JCI131048  |  
------------------------------------------- 
10.17116/kurort20209701113  |    Introduction:  Bronchial asthma (BA) is a relevant social and medical problem in our country and around the world. Currently, phenotypes of the disease are distinguished. One of the original decisions in phenotypes distinguishing is the use of cluster analysis. However, the concept of BA phenotypes at the health resort period of rehabilitation has not yet been formed. 
  Aim:  To determine the BA phenotypes upon admission of patients to a health resort medical rehabilitation (HRMR) using cluster analysis and to offer personalized rehabilitation programs. 
  Material and methods:  518 patients with asthma who underwent HRMR on the southern coast of Crimea were examined. Each patient received clinical, functional and laboratory examination. HRMR included correction of long-term treatment according to the severity of asthma, climate therapy, respiratory therapy, educational programs, and physiotherapeutic procedures. We applied cluster analysis in order to identify BA phenotypes. Description statistics methods were used for phenotype-cluster characterization, comparative analysis - for determination of reliable phenotypic characteristics and relation of the effectiveness of HRMR and phenotypes. 
  Results:  A model of three phenotype-clusters was developed. The first cluster included patients with BA of moderate severity, uncontrolled course, frequent exacerbations, history of atopy, a tendency to obesity, moderately reduced external respiration function, fixed airway obstruction, high adherence to long-term therapy with medium doses of inhaled glucocorticoids (ICS) combined with long-acting β-2 agonists (LABA). Cluster 2 included patients with mild asthma, controlled or partially controlled course of the disease, with rare short exacerbations, late onset, preserved external respiration function and exercise tolerance, but low adherence to long-term therapy with medium and low doses of ICS. Cluster 3 included patients with moderate to severe BA, uncontrolled course, with early onset, frequent and prolonged exacerbations, severe symptoms, significantly reduced external respiration function with fixed obstruction, decreased exercise tolerance, but low adherence to long-term therapy (4th stage) with high doses of ICS in combination with LABA and long-acting anticholinergics. A close relationship was found between phenotypes-clusters and the achieved effects: a significant increase in the control of BA and a high efficiency of rehabilitation in patients of the 1st and especially 3rd clusters and low in the 2nd cluster. The optimal rehabilitation programs for each of the selected cluster phenotypes were determined. 
  Conclusion:  The cluster model developed with the help of artificial intelligence has demonstrated high prognostic value in the determination of the effectiveness and change of control over the course of asthma as a result of HRMR. Personalized HRMR programs are suggested. 
 Введение. Бронхиальная астма (БА) является актуальной социальной и медицинской проблемой в нашей стране и во всем мире. В настоящее время выделяют фенотипы заболевания. Одним из оригинальных решений выделения фенотипов явилось применение кластерного анализа. Однако до сих пор не сформированы представления о фенотипах БА при санаторно-курортном этапе реабилитации. Цель исследования - выделить фенотипы БА при поступлении пациентов на санаторно-курортную медицинскую реабилитацию (СКМР) с помощью кластерного анализа и предложить персонифицированные программы реабилитации. Материал и методы. Обследовали 518 пациентов с БА, прошедших СКМР на Южном берегу Крыма. Всем больным было проведено клиническое, функциональное, лабораторное обследование. СКМР включала: коррекцию базисной терапии соответственно тяжести БА, климатотерапию, респираторную терапию, образовательные программы, физиотерапевтические процедуры. Для идентификации фенотипов применяли кластерный анализ. Использовали методы описательной статистики для характеристики фенотипов-кластеров, сравнительный анализ - для выявления достоверных фенотипических характеристик и зависимости эффективности СКМР от фенотипов. Результаты. Была разработана модель из трех фенотипов-кластеров. В 1-й кластер вошли пациенты со среднетяжелой БА, неконтролируемым течением, частыми обострениями, атопией в анамнезе, склонностью к ожирению, умеренно сниженной функцией внешнего дыхания, сформированной фиксированной обструкцией дыхательных путей, высокой приверженностью к базисной терапии средними дозами ингаляционных глюкокортикоидов (ИГКС) в сочетании с длительно действующими β-2-агонистами (ДДБА). Во 2-й кластер включили пациентов с легкой степенью тяжести БА, контролируемой или частично контролируемой, редкими непродолжительными обострениями, поздним дебютом, сохранной функцией внешнего дыхания и толерантностью к физической нагрузке, но низкой приверженностью к базисной терапии средними и низкими дозами ИГКС. В 3-й кластер вошли пациенты с тяжелой и среднетяжелой БА, неконтролируемой, с ранним дебютом, частыми и длительными обострениями, выраженными симптомами, значительно сниженной функцией внешнего дыхания с фиксированной обструкцией, сниженной толерантностью к физической нагрузке, но низкой приверженностью к базисной терапии (4-я степень) высокими дозами ИГКС в сочетании с ДДБА и длительного действия антихолинергическими препаратами. Обнаружена тесная связь между фенотипами-кластерами и достигнутыми эффектами: значительным повышением контроля течения БА и высокой эффективностью реабилитации у пациентов 1-го и особенно 3-го кластеров и низкой - во 2-м кластере. Определены оптимальные программы реабилитации для каждого из выделенных фенотипов-кластеров. Заключение. Разработанная с помощью искусственного интеллекта кластерная модель продемонстрировала высокую прогностическую способность для определения эффективности и изменения контроля течения БА в результате СКМР. Предложены персонализированные программы СКМР. 
  |  None  |  
------------------------------------------- 
10.1111/1753-6405.12972  |    Introduction:  A Melanoma Screening Summit was held in Brisbane, Australia, to review evidence regarding current approaches for early detection of melanomas and explore new opportunities. 
  Results:  Formal population-based melanoma screening is not carried out in Australia, but there is evidence of considerable opportunistic screening as well as early detection. Biopsy rates are rising and most melanomas are now diagnosed when in situ. Based on evidence review and expert opinion, the Summit attendees concluded that there is currently insufficient information in terms of comparative benefits, harms and costs to support change from opportunistic to systematic screening. Assessment of gains in precision and cost-effectiveness of integrating total body imaging, artificial intelligence algorithms and genetic risk information is required, as well as better understanding of clinical and molecular features of thin fatal melanomas. 
  Conclusions:  Research is needed to understand how to further optimise early detection of melanoma in Australia. Integrating risk-based population stratification and more precise diagnostic tests is likely to improve the balance of benefits and harms of opportunistic screening, pending assessment of cost-effectiveness. Implications for public health: The Summit Group identified that the personal and financial costs to the community of detecting and treating melanoma are rising, and this may be mitigated by developing and implementing a more systematic process for diagnosing melanoma. 
  |  https://doi.org/10.1111/1753-6405.12972  |  
------------------------------------------- 
10.1016/j.acra.2020.01.016  |    Rationale and objectives:  The relative competitiveness of radiology and the number of first-choice applicants to diagnostic radiology have steadily declined over the past decade. The purpose of this study was to identify factors contributing to the declining interest in diagnostic radiology as a career and to explore factors affecting specialty choice. 
  Materials and methods:  A retrospective survey was distributed to resident physicians at a single academic center between July and August 2017. Participants identified factors affecting career choice and evaluated level of agreement with statements regarding radiology using 5-point Likert scales. Higher scores indicated stronger agreement. 
  Results:  One hundred and fifty-two resident physicians from Canada participated (21.5% response rate): 20 radiology and 132 nonradiology. Of the total, 27% were registered in postgraduate year (PGY) 1, 23% in PGY 2, 15% in PGY 3, 19% in PGY 4, and 16% in PGY 5, or above. Sixty-one percent of the respondents self-reported as female, 34% as male, and 5% as other/unknown. Of those in radiology, 40% self-reported as female, 55% as male, and 5% as other/unknown, compared to 64% female, 31% male, and 5% other/unknown in other specialties. Regardless of specialty, positive clinical/mentoring experiences strongly affected career choice. Radiology residents were attracted to diverse pathology (M = 4.5) and positive staff/resident interactions (M = 4.4). Nonradiology residents were deterred by lack of patient contact (M = 3.9) and dark work environment (M = 3.6). Resident physicians who had applied to radiology were more likely to report positive mentorship during medical school, disagree that technology will replace radiologists, and desire a higher income specialty (Wald = 56.6, p &lt; 0.001). More recent graduates showed a higher level of concern regarding the potential negative impact of technology and outsourcing on the profession (F (3, 189) = 2.6, p = 0.05). Several trainees (21%) considered radiology, but lacked mentorship (52%) and identified job market concerns (29%). 
  Conclusion:  More recent graduates are relatively more concerned about technology replacing radiologists, and radiology applicants have less concern about artificial intelligence replacing radiologists. As positive interactions with radiologists and mentorship are key influencers, our results advocate for early training exposure and reinforcement regarding the positive outlook of the profession. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30043-X  |  
------------------------------------------- 
10.1038/s41598-020-60902-w  |   Humans may have evolved a need to connect with nature, and nature provides substantial cultural and social values to humans. However, quantifying the connection between humans and nature at a global scale remains challenging. We lack answers to fundamental questions: how do humans experience nature in different contexts (daily routines, fun activities, weddings, honeymoons, other celebrations, and vacations) and how do nature experiences differ across countries? We answer these questions by coupling social media and artificial intelligence using 31,534 social media photographs across 185 countries. We find that nature was more likely to appear in photographs taken during a fun activity, honeymoon, or vacation compared to photographs of daily routines. More importantly, the proportion of photographs with nature taken during fun activities is associated with national life satisfaction scores. This study provides global evidence of the biophilia hypothesis by showing a connection between humans and nature that contributes to life satisfaction and highlights how nature serves as background to many of our positive memories. 
  |  http://dx.doi.org/10.1038/s41598-020-60902-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32139774/  |  
------------------------------------------- 
10.3390/brainsci10040220  |   The need for automatic detection and classification of high-frequency oscillations (HFOs) as biomarkers of the epileptogenic tissue is strongly felt in the clinical field. In this context, the employment of artificial intelligence methods could be the missing piece to achieve this goal. This work proposed a double-step procedure based on machine learning algorithms and tested it on an intracranial electroencephalogram (iEEG) dataset available online. The first step aimed to define the optimal length for signal segmentation, allowing for an optimal discrimination of segments with HFO relative to those without. In this case, binary classifiers have been tested on a set of energy features. The second step aimed to classify these segments into ripples, fast ripples and fast ripples occurring during ripples. Results suggest that LDA applied to 10 ms segmentation could provide the highest sensitivity (0.874) and 0.776 specificity for the discrimination of HFOs from no-HFO segments. Regarding the three-class classification, non-linear methods provided the highest values (around 90%) in terms of specificity and sensitivity, significantly different to the other three employed algorithms. Therefore, this machine-learning-based procedure could help clinicians to automatically reduce the quantity of irrelevant data. 
  |  http://www.mdpi.com/resolver?pii=brainsci10040220  |  
------------------------------------------- 
10.3390/cancers12030578  |   Cancer pathology reflects disease progression (or regression) and associated molecular characteristics, and provides rich phenotypic information that is predictive of cancer grade and has potential implications in treatment planning and prognosis. According to the remarkable performance of computational approaches in the digital pathology domain, we hypothesized that machine learning can help to distinguish low-grade gliomas (LGG) from high-grade gliomas (HGG) by exploiting the rich phenotypic information that reflects the microvascular proliferation level, mitotic activity, presence of necrosis, and nuclear atypia present in digital pathology images. A set of 735 whole-slide digital pathology images of glioma patients (median age: 49.65 years, male: 427, female: 308, median survival: 761.26 days) were obtained from TCGA. Sub-images that contained a viable tumor area, showing sufficient histologic characteristics, and that did not have any staining artifact were extracted. Several clinical measures and imaging features, including conventional (intensity, morphology) and advanced textures features (gray-level co-occurrence matrix and gray-level run-length matrix), extracted from the sub-images were further used for training the support vector machine model with linear configuration. We sought to evaluate the combined effect of conventional imaging, clinical, and texture features by assessing the predictive value of each feature type and their combinations through a predictive classifier. The texture features were successfully validated on the glioma patients in 10-fold cross-validation (accuracy = 75.12%, AUC = 0.652). The addition of texture features to clinical and conventional imaging features improved grade prediction compared to the models trained on clinical and conventional imaging features alone (<i>p</i> = 0.045 and <i>p</i> = 0.032 for conventional imaging features and texture features, respectively). The integration of imaging, texture, and clinical features yielded a significant improvement in accuracy, supporting the synergistic value of these features in the predictive model. The findings suggest that the texture features, when combined with conventional imaging and clinical markers, may provide an objective, accurate, and integrated prediction of glioma grades. The proposed digital pathology imaging-based marker may help to (i) stratify patients into clinical trials, (ii) select patients for targeted therapies, and (iii) personalize treatment planning on an individual person basis. 
  |  http://www.mdpi.com/resolver?pii=cancers12030578  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32131409/  |  
------------------------------------------- 
10.1016/j.remn.2019.10.003  |    Background:  Recently, evidence has accumulated that demonstrates the potential for future applications of radiomics in many clinical settings, including thoracic oncology. Methodological reasons for the immaturity of image mining (radiomics and artificial intelligence-based) studies have been identified. However, data on the influence of the composition of the research team on the quality of investigations in radiomics are lacking. 
  Aim:  This review aims to evaluate the interdisciplinarity within studies on radiomics in thoracic oncology in order to assess its influence on the quality of research (QUADAS-2 score) in the image mining field. 
  Methods:  We considered for inclusion radiomics investigations with objectives relating to clinical practice in thoracic oncology. Subsequently, we interviewed the corresponding authors. The field of expertise and/or educational degree was then used to assess interdisciplinarity. Subsequently, all studies were evaluated applying the QUADAS-2 score and assigned to a research phase from 0 to IV. 
  Results:  Overall, 27 studies were included. The study quality according to the QUADAS-2 score was low (score ≤5) in 8, moderate (=6) in 12, and high (≥7) in 7 papers. An interdisciplinary team (at least 3 different expertise categories) was involved in half of the papers without any type of validation and in all papers with independent validation. Clinicians were not involved in phase 0 studies while they contributed to all papers classified as phase I and to 4/5 papers classified as phase II with independent validation. 
  Conclusions:  The composition of the research team influences the quality of investigations in radiomics. Also, growth in interdisciplinarity appears to reflect research development from the early phase to a more mature, clinically oriented stage of investigation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2253-654X(19)30301-4  |  
------------------------------------------- 
10.3389/fonc.2020.00207  |   Metabolic reprogramming is prevalent in cancer, largely due to its altered chemical environments such as the distinct intracellular concentrations of O<sub>2</sub>, H<sub>2</sub>O<sub>2</sub> and H<sup>+</sup>, compared to those in normal tissue cells. The reprogrammed metabolisms are believed to play essential roles in cancer formation and progression. However, it is highly challenging to elucidate how individual normal metabolisms are altered in a cancer-promoting environment; hence for many metabolisms, our knowledge about how they are changed is limited. We present a novel method, CaMeRe (CAncer MEtabolic REprogramming), for identifying metabolic pathways in cancer tissues. Based on the specified starting and ending compounds, along with gene expression data of given cancer tissue samples, CaMeRe identifies metabolic pathways connecting the two compounds via collection of compatible enzymes, which are most consistent with the provided gene-expression data. In addition, cancer-specific knowledge, such as the expression level of bottleneck enzymes in the pathways, is incorporated into the search process, to enable accurate inference of cancer-specific metabolic pathways. We have applied this tool to predict the altered sugar-energy metabolism in cancer, referred to as the Warburg effect, and found the prediction result is highly accurate by checking the appearance and ranking of those key pathways in the results of CaMeRe. Computational evaluation indicates that the tool is fast and capable of handling large metabolic network inference in cancer tissues. Hence, we believe that CaMeRe offers a powerful tool to cancer researchers for their discovery of reprogrammed metabolisms in cancer. The URL of CaMeRe is http://csbl.bmb.uga.edu/CaMeRe/. 
  |  https://doi.org/10.3389/fonc.2020.00207  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32161720/  |  
------------------------------------------- 
10.3390/ijerph17082686  |   The purpose of the present study was to investigate the association between different levels of health-related physical fitness measurements and obesity status in Taiwanese adults. A cross-sectional study was conducted by reviewing the National Physical Fitness Survey in Taiwan (HPFSIT) database. Responses from 60,056 participants, aged 23-64 years from the database were collected in the present study. Data from a standardized structured questionnaire and health-related physical fitness tests were analyzed. The quartiles of each physical fitness measurement were used for unconditional logistic regression analyses. Our results indicated clear trends in the association between cardiorespiratory fitness and overweight/obesity. Overweight and obesity were associated with a 10% to 60% increased risk of low levels of cardiorespiratory fitness in men and a 10% to almost 30% increased risk in women. However, the association between muscle strength/endurance and obesity status as well as flexibility and obesity status needs further investigation. 
  |  http://www.mdpi.com/resolver?pii=ijerph17082686  |  
------------------------------------------- 
10.1016/j.gie.2020.04.039  |    Background and aims:  Deep learning is an innovative algorithm based on neural networks. Wireless capsule endoscopy (WCE) is considered the criterion standard for detecting small-bowel diseases. Manual examination of WCE is time consuming and can benefit from automatic detection using artificial intelligence (AI). We aimed to perform a systematic review of current literature pertaining to deep learning implementation in WCE. 
  Methods:  We conducted a search in PubMed for all original publications on the subject of deep learning applications in WCE published between January 1, 2016, and December 15, 2019. Evaluation of the risk of bias was performed using tailored Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2). Pooled sensitivity and specificity were calculated. Summary receiver operating characteristic curves were plotted. 
  Results:  Of the 45 studies retrieved, 19 studies were included. All studies were retrospective. Deep learning applications for WCE included detection of ulcers, polyps, celiac disease, bleeding, and hookworm. Detection accuracy was above 90% for most studies and diseases. The pooled sensitivity and specificity for ulcer detection were 0.95 (95% CI, 0.89-0.98) and 0.94 (95% CI, 0.90-0.96), respectively. The pooled sensitivity and specificity for bleeding or bleeding source were 0.98 (95% CI, 0.96-0.99) and 0.99 (95% CI, 0.97-0.99), respectively. 
  Conclusions:  Deep learning has achieved excellent performance for the detection of a range of diseases in WCE. Notwithstanding, current research is based on retrospective studies with a high risk of bias. Thus, future prospective multicenter studies are necessary in order for this technology to be implemented in the clinical use of WCE. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)34193-6  |  
------------------------------------------- 
10.2196/13244  |    Background:  There has recently been exponential growth in the development and use of health apps on mobile phones. As with most mobile apps, however, the majority of users abandon them quickly and after minimal use. One of the most critical factors for the success of a health app is how to support users' commitment to their health. Despite increased interest from researchers in mobile health, few studies have examined the measurement of user engagement with health apps. 
  Objective:  User engagement is a multidimensional, complex phenomenon. The aim of this study was to understand the concept of user engagement and, in particular, to demonstrate the applicability of a user engagement scale (UES) to mobile health apps. 
  Methods:  To determine the measurability of user engagement in a mobile health context, a UES was employed, which is a psychometric tool to measure user engagement with a digital system. This was adapted to Ada, developed by Ada Health, an artificial intelligence-powered personalized health guide that helps people understand their health. A principal component analysis (PCA) with varimax rotation was conducted on 30 items. In addition, sum scores as means of each subscale were calculated. 
  Results:  Survey data from 73 Ada users were analyzed. PCA was determined to be suitable, as verified by the sampling adequacy of Kaiser-Meyer-Olkin=0.858, a significant Bartlett test of sphericity (χ<sup>2</sup><sub>300</sub>=1127.1; P&lt;.001), and communalities mostly within the 0.7 range. Although 5 items had to be removed because of low factor loadings, the results of the remaining 25 items revealed 4 attributes: perceived usability, aesthetic appeal, reward, and focused attention. Ada users showed the highest engagement level with perceived usability, with a value of 294, followed by aesthetic appeal, reward, and focused attention. 
  Conclusions:  Although the UES was deployed in German and adapted to another digital domain, PCA yielded consistent subscales and a 4-factor structure. This indicates that user engagement with health apps can be assessed with the German version of the UES. These results can benefit related mobile health app engagement research and may be of importance to marketers and app developers. 
  |  https://mhealth.jmir.org/2020/1/e13244/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31899454/  |  
------------------------------------------- 
10.1111/ajt.15850  |   The Banff Digital Pathology Working Group (DPWG) was formed in the time leading up to and during the joint American Society for Histocompatibility and Immunogenetics/Banff Meeting, September 23-27, 2019, held in Pittsburgh, Pennsylvania. At the meeting, the 14th Banff Conference, presentations directly and peripherally related to the topic of "digital pathology" were presented; and discussions before, during, and after the meeting have resulted in a list of issues to address for the DPWG. Included are practice standardization, integrative approaches for study classification, scoring of histologic parameters (eg, interstitial fibrosis and tubular atrophy and inflammation), algorithm classification, and precision diagnosis (eg, molecular pathways and therapeutics). Since the meeting, a survey with international participation of mostly pathologists (81%) was conducted, showing that whole slide imaging is available at the majority of centers (71%) but that artificial intelligence (AI)/machine learning was only used in ≈12% of centers, with a wide variety of programs/algorithms employed. Digitalization is not just an end in itself. It also is a necessary precondition for AI and other approaches. Discussions at the meeting and the survey highlight the unmet need for a Banff DPWG and point the way toward future contributions that can be made. 
  |  https://doi.org/10.1111/ajt.15850  |  
------------------------------------------- 
10.1371/journal.pone.0230084  |   The European-Commission-funded project 'Citclops' (Citizens' observatory for coast and ocean optical monitoring) developed methods, tools and sensors, which can be used by citizens to monitor natural waters, with a strong focus on long-term data series related to environmental sciences. The new sensors, based on optical technologies, respond to a number of scientific, technical and societal objectives, ranging from more precise monitoring of key environmental descriptors of the aquatic environment (water colour, transparency and fluorescence) to an improved management of data collected with citizen participation. The sensors were tested, calibrated, integrated on several platforms, scientifically validated and demonstrated in the field. The new methods and tools were tested in a citizen-science context. The general conclusion is that citizens are valuable contributors in quality and quantity to the objective of collecting, integrating and analysing fragmented and diverse environmental data. An integration of these data into data-analysis tools has a large potential to support authoritative monitoring and decision-making. In this paper, the project's objectives, results, technical achievements and lessons learned are presented. 
  |  http://dx.plos.org/10.1371/journal.pone.0230084  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32214341/  |  
------------------------------------------- 
10.1016/j.gie.2019.09.034  |    Background and aims:  Diagnosing esophageal squamous cell carcinoma (SCC) depends on individual physician expertise and may be subject to interobserver variability. Therefore, we developed a computerized image-analysis system to detect and differentiate esophageal SCC. 
  Methods:  A total of 9591 nonmagnified endoscopy (non-ME) and 7844 ME images of pathologically confirmed superficial esophageal SCCs and 1692 non-ME and 3435 ME images from noncancerous lesions or normal esophagus were used as training image data. Validation was performed using 255 non-ME white-light images, 268 non-ME narrow-band images/blue-laser images, and 204 ME narrow-band images/blue-laser images from 135 patients. The same validation test data were diagnosed by 15 board-certified specialists (experienced endoscopists). 
  Results:  Regarding diagnosis by non-ME with narrow-band imaging/blue-laser imaging, the sensitivity, specificity, and accuracy were 100%, 63%, and 77%, respectively, for the artificial intelligence (AI) system and 92%, 69%, and 78%, respectively, for the experienced endoscopists. Regarding diagnosis by non-ME with white-light imaging, the sensitivity, specificity, and accuracy were 90%, 76%, and 81%, respectively, for the AI system and 87%, 67%, and 75%, respectively, for the experienced endoscopists. Regarding diagnosis by ME, the sensitivity, specificity, and accuracy were 98%, 56%, and 77%, respectively, for the AI system and 83%, 70%, and 76%, respectively, for the experienced endoscopists. There was no significant difference in the diagnostic performance between the AI system and the experienced endoscopists. 
  Conclusions:  Our AI system showed high sensitivity for detecting SCC by non-ME and high accuracy for differentiating SCC from noncancerous lesions by ME. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(19)32301-6  |  
------------------------------------------- 
10.1136/heartjnl-2019-316452  |    Objectives:  We aimed to investigate the effects of meteorological factors and air pollutants on out-of-hospital cardiac arrest (OHCA) according to seasonal variations because the roles of these factors remain controversial to date. 
  Methods:  A total of 38 928 OHCAs of cardiac origin that occurred within eight metropolitan areas between 2012 and 2016 were identified from the Korean nationwide emergency medical service database. A time series multilevel approach based on Poisson analysis following a Granger causality test was used to analyse the influence of air pollution and 13 meteorological variables on OHCA occurrence. 
  Results:  Particulate matter (PM) ≤2.5 µm (PM<sub>2.5</sub>), average temperature, daily temperature range and humidity were significantly associated with a higher daily OHCA risk (PM<sub>2.5</sub>: 1.59%; 95% CI: 1.51% to 1.66% per 10µg/m<sup>3</sup>, average temperature 0.73%, 95% CI: 0.63% to 0.84% per 1°C, daily temperature range: 1.05%, 95% CI: 0.63% to 1.48% per 1°C, humidity -0.48, 95% CI: -0.40 to -0.56 per 1%) on lag day 1. In terms of the impact of these four risk factors in different seasons, average temperature and daily temperature range were highly associated with OHCA in the summer and winter, respectively. However, only PM<sub>2.5</sub> elevation (to varying extents) was an independent and consistent OHCA risk factor irrespective of the season. 
  Conclusions:  PM<sub>2.5</sub>, average temperature, daily temperature range and humidity were independently associated with OHCA occurrence in a season-dependent manner. Importantly, PM<sub>2.5</sub> was the only independent risk factor for OHCA occurrence irrespective of seasonal changes. 
  |  http://heart.bmj.com/cgi/pmidlookup?view=long&pmid=32341139  |  
------------------------------------------- 
10.1038/s41598-020-62327-x  |   The hand explores the environment for obtaining tactile information that can be fruitfully integrated with other functions, such as vision, audition, and movement. In theory, somatosensory signals gathered by the hand are accurately mapped in the world-centered (allocentric) reference frame such that the multi-modal information signals, whether visual-tactile or motor-tactile, are perfectly aligned. However, an accumulating body of evidence indicates that the perceived tactile orientation or direction is inaccurate; yielding a surprisingly large perceptual bias. To investigate such perceptual bias, this study presented tactile motion stimuli to healthy adult participants in a variety of finger and head postures, and requested the participants to report the perceived direction of motion mapped on a video screen placed on the frontoparallel plane in front of the eyes. Experimental results showed that the perceptual bias could be divided into systematic and nonsystematic biases. Systematic bias, defined as the mean difference between the perceived and veridical directions, correlated linearly with the relative posture between the finger and the head. By contrast, nonsystematic bias, defined as minor difference in bias for different stimulus directions, was highly individualized, phase-locked to stimulus orientation presented on the skin. Overall, the present findings on systematic bias indicate that the transformation bias among the reference frames is dominated by the finger-to-head posture. Moreover, the highly individualized nature of nonsystematic bias reflects how information is obtained by the orientation-selective units in the S1 cortex. 
  |  http://dx.doi.org/10.1038/s41598-020-62327-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32218502/  |  
------------------------------------------- 
10.1002/mp.13953  |    Purpose:  Various dose calculation algorithms are available for radiation therapy for cancer patients. However, these algorithms are faced with the tradeoff between efficiency and accuracy. The fast algorithms are generally less accurate, while the accurate dose engines are often time consuming. In this work, we try to resolve this dilemma by exploring deep learning (DL) for dose calculation. 
  Methods:  We developed a new radiotherapy dose calculation engine based on a modified Hierarchically Densely Connected U-net (HD U-net) model and tested its feasibility with prostate intensity-modulated radiation therapy (IMRT) cases. Mapping from an IMRT fluence map domain to a three-dimensional (3D) dose domain requires a deep neural network of complicated architecture and a huge training dataset. To solve this problem, we first project the fluence maps to the dose domain using a broad beam ray-tracing (RT) algorithm, and then we use the HD U-net to map the RT dose distribution into an accurate dose distribution calculated using a collapsed cone convolution/superposition (CS) algorithm. The model is trained on 70 patients with fivefold cross validation, and tested on a separate 8 patients. 
  Results:  It takes about 1 s to compute a 3D dose distribution for a typical 7-field prostate IMRT plan, which can be further reduced to achieve real-time dose calculation by optimizing the network. The average Gamma passing rate between DL and CS dose distributions for the 8 test patients are 98.5% (±1.6%) at 1 mm/1% and 99.9% (±0.1%) at 2 mm/2%. For comparison of various clinical evaluation criteria (dose-volume points) for IMRT plans between two dose distributions, the average difference for dose criteria is less than 0.25 Gy while for volume criteria is &lt;0.16%, showing that the DL dose distributions are clinically identical to the CS dose distributions. 
  Conclusions:  We have shown the feasibility of using DL for calculating radiotherapy dose distribution with high accuracy and efficiency. 
  |  https://doi.org/10.1002/mp.13953  |  
------------------------------------------- 
10.1097/WNO.0000000000000888  |    Background:  Cerebrovascular disease (CeVD), including stroke, is a leading cause of death globally. The retina is an extension of the cerebrum, sharing embryological and vascular pathways. The association between different retinal signs and CeVD has been extensively evaluated. In this review, we summarize recent studies which have examined this association. 
  Evidence acquisition:  We searched 6 databases through July 2019 for studies evaluating the link between retinal vascular signs and diseases with CeVD. CeVD was classified into 2 groups: clinical CeVD (including clinical stroke, silent cerebral infarction, cerebral hemorrhage, and stroke mortality), and sub-clinical CeVD (including MRI-defined lacunar infarct and white matter lesions [WMLs]). Retinal vascular signs were classified into 3 groups: classic hypertensive retinopathy (including retinal microaneurysms, retinal microhemorrhage, focal/generalized arteriolar narrowing, cotton-wool spots, and arteriovenous nicking), clinical retinal diseases (including diabetic retinopathy [DR], age-related macular degeneration [AMD], retinal vein occlusion, retinal artery occlusion [RAO], and retinal emboli), and retinal vascular imaging measures (including retinal vessel diameter and geometry). We also examined emerging retinal vascular imaging measures and the use of artificial intelligence (AI) deep learning (DL) techniques. 
  Results:  Hypertensive retinopathy signs were consistently associated with clinical CeVD and subclinical CeVD subtypes including subclinical cerebral large artery infarction, lacunar infarction, and WMLs. Some clinical retinal diseases such as DR, retinal arterial and venous occlusion, and transient monocular vision loss are consistently associated with clinical CeVD. There is an increased risk of recurrent stroke immediately after RAO. Less consistent associations are seen with AMD. Retinal vascular imaging using computer assisted, semi-automated software to measure retinal vascular caliber and other parameters (tortuosity, fractal dimension, and branching angle) has shown strong associations to clinical and subclinical CeVD. Other new retinal vascular imaging techniques (dynamic retinal vessel analysis, adaptive optics, and optical coherence tomography angiography) are emerging technologies in this field. Application of AI-DL is expected to detect subclinical retinal changes and discrete retinal features in predicting systemic conditions including CeVD. 
  Conclusions:  There is extensive and increasing evidence that a range of retinal vascular signs and disease are closely linked to CeVD, including subclinical and clinical CeVD. New technology including AI-DL will allow further translation to clinical utilization. 
  |  http://dx.doi.org/10.1097/WNO.0000000000000888  |  
------------------------------------------- 
10.1002/adma.201902684  |   Mechanically and visually imperceptible sensor sheets integrated with lightweight wireless loggers are employed in ultimate flexible hybrid electronics (FHE) to reduce vital stress/nervousness and monitor natural biosignal responses. The key technologies and applications for conceptual sensor system fabrication are reported, as exemplified by the use of a stretchable sensor sheet completely conforming to an individual's body surface to realize a low-noise wireless monitoring system (&lt;1 µV) that can be attached to the human forehead for recording electroencephalograms. The above system can discriminate between Alzheimer's disease and the healthy state, thus offering a rapid in-home brain diagnosis possibility. Moreover, the introduction of metal nanowires to improve the transparency of the biocompatible sensor sheet allows one to wirelessly acquire electrocorticograms of nonhuman primates and simultaneously offers optogenetic stimulation such as toward-the-brain-machine interface under free movement. Also discussed are effective methods of improving electrical reliability, biocompatibility, miniaturization, etc., for metal nanowire based tracks and exploring the use of an organic amplifier as an important component to realize a flexible active probe with a high signal-to-noise ratio. Overall, ultimate FHE technologies are demonstrated to achieve efficient closed-loop systems for healthcare management, medical diagnostics, and preclinical studies in neuroscience and neuroengineering. 
  |  https://doi.org/10.1002/adma.201902684  |  
------------------------------------------- 
10.1016/j.amjcard.2019.12.048  |   This study compared the survival and the risk of heart failure (HF), chronic obstructive pulmonary disease (COPD), diabetes mellitus (DM), hypoglycemia, and renal failure (RF) hospitalizations in geriatric patients exposed to carvedilol or metoprolol. Data sources were Danish administrative registers. Patients aged ≥65 and having HF, COPD, and DM were followed for 1 year from the first β-blocker prescription redemption. Patients' characteristics were used to 1:1 propensity score match carvedilol and metoprolol users. A Cox regression model was used to compute the hazard ratio (HR) of study outcomes. For statistically significant associations, a conditional inference tree was used to assess predictors most associated with the outcome. In total, 1,424 patients were included. No statistically significant differences were observed for survival (HR 0.86; 95% confidence interval [CI] 0.67 to 1.11, p = 0.240) between carvedilol/metoprolol users. The same applied to COPD (HR 0.88; 95% CI 0.75 to 1.05, p = 0.177), DM (HR 0.95; 95% CI 0.82 to 1.10, p = 0.485), hypoglycemia (HR 0.88; 95% CI 0.47 to 1.67, p = 0.707), and RF (HR 1.25; 95% CI 0.93 to 1.69, p = 0.142) hospitalizations. Carvedilol users had a 38% higher hazard then metoprolol users of HF hospitalization during the follow-up period (HR 1.38; 95% CI 1.19 to 1.60, p &lt;0.001). Artificial intelligence identified carvedilol exposure as the most important predictor for HF hospitalization. In conclusion, we found an increased risk of HF hospitalization for carvedilol users with this triad of diseases but no statistically significant differences in survival or risk of COPD, DM, hypoglycemia, and RF hospitalizations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9149(20)30016-3  |  
------------------------------------------- 
10.1186/s12859-020-3380-6  |    Background:  The analysis and comparison of RNA m<sup>6</sup>A methylation profiles have become increasingly important for understanding the post-transcriptional regulations of gene expression. However, current m<sup>6</sup>A profiles in public databases are not readily intercomparable, where heterogeneous profiles from the same experimental report but different cell types showed unwanted high correlations. 
  Results:  Several normalizing or correcting methods were tested to remove such laboratory bias. And m6Acorr, an effective pipeline for correcting m<sup>6</sup>A profiles, was presented on the basis of quantile normalization and empirical Bayes batch regression method. m6Acorr could efficiently correct laboratory bias in the simulated dataset and real m<sup>6</sup>A profiles in public databases. The preservation of biological signals was examined after correction, and m6Acorr was found to better preserve differential methylation signals, m<sup>6</sup>A regulated targets, and m<sup>6</sup>A-related biological features than alternative methods. Finally, the m6Acorr server was established. This server could eliminate the potential laboratory bias in m<sup>6</sup>A methylation profiles and perform profile-profile comparisons and functional analysis of hyper- (hypo-) methylated genes based on corrected methylation profiles. 
  Conclusion:  m6Acorr was established to correct the existing laboratory bias in RNA m<sup>6</sup>A methylation profiles and perform profile comparisons on the corrected datasets. The m6Acorr server is available at http://www.rnanut.net/m6Acorr. A stand-alone version with the correction function is also available in GitHub at https://github.com/emersON106/m6Acorr. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3380-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31996134/  |  
------------------------------------------- 
10.1002/advs.201902864  |   Emerging memory devices, that can provide programmable information recording with tunable resistive switching under external stimuli, hold great potential for applications in data storage, logic circuits, and artificial synapses. Realization of multifunctional manipulation within individual memory devices is particularly important in the More-than-Moore era, yet remains a challenge. Here, both rewritable and nonerasable memory are demonstrated in a single stimuli-responsive polymer diode, based on a nanohole-nanowrinkle bi-interfacial structure. Such synergic nanostructure is constructed from interfacing a nanowrinkled bottom graphene electrode and top polymer matrix with nanoholes; and it can be easily prepared by spin coating, which is a low-cost and high-yield production method. Furthermore, the resulting device, with ternary and low-power operation under varied external stimuli, can enable both reversible and irreversible biomimetic pressure recognition memories using a device-to-system framework. This work offers both a general guideline to fabricate multifunctional memory devices via interfacial nanostructure engineering and a smart information storage basis for future artificial intelligence. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32328417/  |  
------------------------------------------- 
10.1093/jamia/ocz101  |    Objective:  Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. 
  Materials and methods:  We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction. We additionally developed separate models for intra- and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. 
  Results:  Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-to-end relation extraction, respectively (Tracks 2 and 3). Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data- and corpus-driven processing techniques. 
  Conclusions:  We proposed a relation extraction system to identify relations between drugs and medication-related entities. The proposed approach is independent of external syntactic tools. Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz101  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31390003/  |  
------------------------------------------- 
10.1007/s42058-020-00031-5  |   COVID-19 has become a public health emergency due to its rapid transmission. The appearance of pneumonia is one of the major clues for the diagnosis, progress and therapeutic evaluation. More and more literatures about imaging manifestations and related research have been reported. In order to know about the progress and prospective on imaging of COVID-19, this review focus on interpreting the CT findings, stating the potential pathological basis, proposing the challenge of patients with underlying diseases, differentiating with other diseases and suggesting the future research and clinical directions, which would be helpful for the radiologists in the clinical practice and research. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32292880/  |  
------------------------------------------- 
10.1007/s11606-019-05512-7  |    Background:  Emergency departments (ED) are becoming increasingly overwhelmed, increasing poor outcomes. Triage scores aim to optimize the waiting time and prioritize the resource usage. Artificial intelligence (AI) algorithms offer advantages for creating predictive clinical applications. 
  Objective:  Evaluate a state-of-the-art machine learning model for predicting mortality at the triage level and, by validating this automatic tool, improve the categorization of patients in the ED. 
  Design:  An institutional review board (IRB) approval was granted for this retrospective study. Information of consecutive adult patients (ages 18-100) admitted at the emergency department (ED) of one hospital were retrieved (January 1, 2012-December 31, 2018). Features included the following: demographics, admission date, arrival mode, referral code, chief complaint, previous ED visits, previous hospitalizations, comorbidities, home medications, vital signs, and Emergency Severity Index (ESI). The following outcomes were evaluated: early mortality (up to 2 days post ED registration) and short-term mortality (2-30 days post ED registration). A gradient boosting model was trained on data from years 2012-2017 and examined on data from the final year (2018). The area under the curve (AUC) for mortality prediction was used as an outcome metric. Single-variable analysis was conducted to develop a nine-point triage score for early mortality. 
  Key results:  Overall, 799,522 ED visits were available for analysis. The early and short-term mortality rates were 0.6% and 2.5%, respectively. Models trained on the full set of features yielded an AUC of 0.962 for early mortality and 0.923 for short-term mortality. A model that utilized the nine features with the highest single-variable AUC scores (age, arrival mode, chief complaint, five primary vital signs, and ESI) yielded an AUC of 0.962 for early mortality. 
  Conclusion:  The gradient boosting model shows high predictive ability for screening patients at risk of early mortality utilizing data available at the time of triage in the ED. 
  |  https://dx.doi.org/10.1007/s11606-019-05512-7  |  
------------------------------------------- 
10.1080/09168451.2019.1677451  |   Specific conditions, such as exposure to cold, can induce the production of brown-like adipocytes in white adipose tissue. These adipocytes express high levels of uncoupling protein 1 (UCP1) and energy expended by generating heat. Thus, these are a potential target for the prevention or treatment of obesity. The present study involved a comprehensive analysis of the adipose tissue to understand the relationship between long non-coding RNA (lncRNA) 2310069B03Rik and UCP1. Cold exposure increased both <i>lncRNA 2310069B03Rik</i> and <i>Ucp1</i> expression in inguinal white adipose tissue (iWAT). However, overexpression of lncRNA 2310069B03Rik suppressed the <i>Ucp1</i> mRNA expression and the promoter activity of UCP1 in the iWAT primary adipocytes. In addition, compared to the early induction of <i>Ucp1</i> expression by cold stimulation, the induction of lncRNA 2310069B03Rik expression was later. These results suggest that lncRNA 2310069B03Rik functions as a suppression factor of <i>Ucp1</i> expression. 
  |  http://www.tandfonline.com/doi/full/10.1080/09168451.2019.1677451  |  
------------------------------------------- 
10.1371/journal.pone.0219413  |   Seed dormancy and germination are the two important traits related to plant survival, reproduction and crop yield. To understand the regulatory mechanisms of these traits, it is crucial to clarify which genes or pathways participate in the regulation of these processes. However, little information is available on seed dormancy and germination in peanut. In this study, seeds of the variety Luhua No.14, which undergoes nondeep dormancy, were selected, and their transcriptional changes at three different developmental stages, the freshly harvested seed (FS), the after-ripening seed (DS) and the newly germinated seed (GS) stages, were investigated by comparative transcriptomic analysis. The results showed that genes with increased transcription in the DS vs FS comparison were overrepresented for oxidative phosphorylation, the glycolysis pathway and the tricarboxylic acid (TCA) cycle, suggesting that after a period of dry storage, the intermediates stored in the dry seeds were rapidly mobilized by glycolysis, the TCA cycle, the glyoxylate cycle, etc.; the electron transport chain accompanied by respiration was reactivated to provide ATP for the mobilization of other reserves and for seed germination. In the GS vs DS pairwise comparison, dozens of the upregulated genes were related to plant hormone biosynthesis and signal transduction, including the majority of components involved in the auxin signal pathway, brassinosteroid biosynthesis and signal transduction as well as some GA and ABA signal transduction genes. During seed germination, the expression of some EXPANSIN and XYLOGLUCAN ENDOTRANSGLYCOSYLASE genes was also significantly enhanced. To investigate the effects of different hormones during seed germination, the contents and differential distribution of ABA, GAs, BRs and IAA in the cotyledons, hypocotyls and radicles, and plumules of three seed sections at different developmental stages were also investigated. Combined with previous data in other species, it was suggested that the coordination of multiple hormone signal transduction nets plays a key role in radicle protrusion and seed germination. 
  |  http://dx.plos.org/10.1371/journal.pone.0219413  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31899920/  |  
------------------------------------------- 
10.1136/injuryprev-2020-043642  |    |  http://ip.bmj.com/cgi/pmidlookup?view=long&pmid=31964722  |  
------------------------------------------- 
10.1016/j.jinf.2019.11.017  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0163-4453(19)30368-8  |  
------------------------------------------- 
10.1002/minf.201900151  |   Ligand enrichment assessment based on benchmarking data sets has become a necessity for the rational selection of the best-suited approach for prospective data mining of drug-like molecules. Up to now, a variety of benchmarking data sets had been generated and frequently used. Among them, MUBD-HDACs from our prior research efforts was regarded as one of five state-of-the-art benchmarks in 2017 by Frontiers in Pharmacology. This benchmarking set was generated by one of our unique de-biasing algorithms. It also rendered quite a few other cases of successful applications in recent years, thus is expected to have more impact in modern drug discovery. To make our algorithm amenable to more users, we developed a Python GUI application called MUBD-DecoyMaker 2.0. Moreover, it has two new additional functional modules, i. e. "Detect 2D Bias" and "Quality Control". This new GUI version had been proved to be easy to use while generate benchmarking data sets of the same quality. MUBD-DecoyMaker 2.0 is freely available at https://github.com/jwxia2014/MUBD-DecoyMaker2.0, along with its manual and testcase. 
  |  None  |  
------------------------------------------- 
10.1016/j.drugalcdep.2019.107721  |    Objective:  Exposure to violent victimization is associated with higher rates of mental health and substance use disorders (SUD). Some youth who experience multiple victimizations and associated characteristics (i.e. poly-victims) are at heightened risk for long term problems. Thus, we conducted the first study to examine how heterogeneity in experiences of victimization vary in terms of latency to illicit drug use following treatment completion. We also examined if victimization profiles vary across gender and if comorbid conditions (e.g., posttraumatic stress disorder and major depressive disorder) differentially predict latentcy to illicit drug use across groups. 
  Methods:  Adolescents and young adults (N = 5956; M<sub>age</sub> = 17.5 years; 64.0% male) with SUDs in treatment for illicit drug use completed a battery of measures at baseline. At 3-, 6- and 12-month follow-ups, they reported on the number of days before they used any illicit drug following their last assessment. 
  Results:  Continuous time survival mixture modeling revealed that, as hypothesized, females who experienced high rates of all victimization and related characteristics had a higher hazard for latency to first illicit drug use as compared to females in the low victimization group. This was not the case for males; rather, those who experienced high rates of sexual abuse were quickest to return to illicit drug use. Finally, comorbid conditions led to a higher hazard rate, but only for certain profiles across females. 
  Discussion:  Findings emphasize the necessity for professionals to more fully integrate poly-victimization research and theory into their clinical practices and research. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0376-8716(19)30498-3  |  
------------------------------------------- 
10.1039/d0nr00672f  |   Bilayer graphene possesses new degrees of freedom for modulating the electronic band structure, which makes it a tempting solution for overcoming the intrinsic absence of sizeable bandgaps in graphene and designing next-generation devices for post-silicon electronics. By twisting bilayer graphene, interlayer hybridized and twist angle-dependent van Hove singularities in the electronic band structure are generated and expected to facilitate the vertical tunneling transport between bilayer graphene. Herein, based on the ab initio quantum transport simulations, we designed a novel all-metallic vertical quantum transport architecture with the twisted bilayer graphene as the transport channel region and Au electrodes as the source/drain contacts to investigate the twist angle-dependent vertical transport properties. Enhancement in the ION/IOFF ratio by 2 orders of magnitude can be achieved by simply twisting the bilayer graphene. Compared to the traditional gate voltage modulation, which tunes the Fermi energy level alone, the current strategy shifts the Fermi energy level of the channel region away from the Dirac cone, moves the Fermi level and the van Hove singularities towards each other and promotes the vertical quantum transport due to the interlayer electronic hybridization. This dual modulation strategy of this novel mechanical gating device thus provides a potential new solution for designing novel vertical transistors. 
  |  https://doi.org/10.1039/d0nr00672f  |  
------------------------------------------- 
10.1007/s11136-020-02512-7  |    Purpose:  The number of patients with depressive disordered globally increased and affects people of all ages and countries and has a significant and negative impact on the quality of life (QoL). Depression if left untreated may lead to severe consequences. However, there are several types of effective treatments, but often patients need support from health staff to find suitable treatments. This study aims to examine the global trend of the publications as well as the development of interventions for depressing treatment. 
  Methods:  We download and analyzed 15,976 scientific research from the Web of Science from 1990 to 2018. A text mining based on Latent Dirichlet and terms' co-occurrence in titles and abstracts to identify hidden research topics and research landscapes. 
  Results:  We found that the number of papers related to non-pharmacological treatment (such as cognitive-behavioral, mindfulness, or family and social support) to improve the QoL of patients with depression has increased. The number of papers on this serious health issue in low-middle income countries (LMICs) was not as high as in high-income countries (HICs). 
  Conclusion:  It is necessary to increase support of the treatment of depression in LMICs as well as applied non-pharmacological interventions to patients with depression. 
  |  https://doi.org/10.1007/s11136-020-02512-7  |  
------------------------------------------- 
10.1016/j.neunet.2019.11.013  |   Medical prediction is always collectively determined based on bioimages collected from different sources or various clinical characterizations described from multiple physiological features. Notably, learning intrinsic structures from multiple heterogeneous features is significant but challenging in multi-view disease understanding. Different from existing methods that separately deal with each single view, this paper proposes a discriminative Margin-Sensitive Autoencoder (MSAE) framework for automated Alzheimer's disease (AD) diagnosis and accurate protein fold recognition. Generally, our MSAE aims to collaboratively explore the complementary properties of multi-view bioimage features in a semantic-sensitive encoder-decoder paradigm, where the discriminative semantic space is explicitly constructed in a margin-scalable regression model. Specifically, we develop a semantic-sensitive autoencoder, where an encoder projects multi-view visual features into the common semantic-aware latent space, and a decoder is exerted as an additional constraint to reconstruct the respective visual features. In particular, the importance of different views is adaptively weighted by self-adjusting learning scheme, such that their underlying correlations and complementary characteristics across multiple views are simultaneously preserved into the latent common representations. Moreover, a flexible semantic space is formulated by a margin-scalable support vector machine to improve the discriminability of the learning model. Importantly, correntropy induced metric is exploited as a robust regularization measurement to better control outliers for effective classification. A half-quadratic minimization and alternating learning strategy are devised to optimize the resulting framework such that each subproblem exists a closed-form solution in each iterative minimization phase. Extensive experimental results performed on the Alzheimer's Disease Neuroimaging Initiative (ADNI) datasets show that our MSAE can achieve superior performances for both binary and multi-class classification in AD diagnosis, and evaluations on protein folds demonstrate that our method can achieve very encouraging performance on protein structure recognition, outperforming the state-of-the-art methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30353-3  |  
------------------------------------------- 
10.3390/s20030782  |   Advancement in science and technology is playing an increasingly important role in solving difficult cases at present. Thermal cameras can help the police crack difficult cases by capturing the heat trace on the ground left by perpetrators, which cannot be spotted by the naked eye. Therefore, the purpose of this study is to establish a thermalfoot model using thermal imaging system to estimate the departure time. To this end, in the current work, we use a thermal camera to acquire the thermal sequence left on the floor, and convert it into the heat signal via image processing algorithm. We establish the model of thermalfoot print as we observe that the residual temperature would exponentially decrease with the departure time according to Newton's Law of Cooling. The correlation coefficients of 107 thermalfoot models derived from the corresponding 107 heat signals are basically above 0.99. In a validation experiment, a residual analysis is conducted and the residuals between estimated departure time points and ground-truth times are almost within a certain range from -150 s to +150 s. The reverse accuracy of the thermalfoot model for estimating departure time at one-third, one-half, two-thirds, three-fourths, four-fifths, and five-sixths capture time points are 71.96%, 50.47%, 42.06%, 31.78%, 21.70%, and 11.21%, respectively. The results of comparison experiments with two subjective evaluation methods (subjective 1: we directly estimate the departure time according to obtained local curves; subjective 2: we utilize auxiliary means such as a ruler to estimate the departure time based on obtained local curves) further demonstrate the effectiveness of thermalfoot model for detecting the departure time inversely. Experimental results also demonstrated that the thermalfoot model has good performance on the departure time reversal within a short time window someone leaves, whereas it is probably only approximately 15% to accurately determine the departure time via thermalfoot model within a long time window someone leaves. The influence of outliers, ROI (Region of Interest) selection, ROI size, different capture time points and environment temperature on the performance of thermalfoot model on departure time reversal can be explored in the future work. Overall, the thermalfoot model can help the police solve crimes to some extent, which in turn brings more guarantees for people's health, social security, and stability. 
  |  http://www.mdpi.com/resolver?pii=s20030782  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023963/  |  
------------------------------------------- 
10.1016/j.annpat.2020.02.023  |   Breast cancers occurring in the context of a hereditary mutation of a predisposition gene represent 5 to 10% of all breast cancers, 20 to 25% of which being due to a mutation in the BRCA1 or BRCA2 genes. Authorization to market PARP inhibitors for breast cancer patients with hereditary BRCA1 and 2 mutations has recently been obtained. Given the annual frequency of breast cancer, morphological identification could facilitate the patient care process to limit the search for BRCA1 and 2 mutations to patients whose tumors have very specific characteristics. However, only a few morphological features have been recognized and differ depending on the mutated genes. Breast cancer occurring as part of a mutation in the BRCA1 gene is in 85% of cases of high-grade non-specific type invasive carcinomas with very limited contours, contain numerous lymphocytes in the stroma and are of triple-negative phenotype. Carcinomas associated with mutations in the BRCA2 genes and genes more recently recognized as associated with a risk of development of breast cancer (CHECK2, BMPR1A, BRIP1, PALB2, MUTYH) are most often non-specific invasive carcinomas, although other histological types are possible, grade III, luminal B phenotype. Breast cancer occurring in the context of a constitutional mutation of TP53 occurs in women under 35 years old are of non-specific histological type and with an amplification of HER2 in two thirds of the cases. Those associated with a PTEN mutation are readily of the apocrine type. Finally, very rarely, certain lobular-type breast cancers can occur in the context of a constitutional mutation of the CDH1 gene, which codes for the protein E-cadherin. The morphological and phenotypic characteristics may suggest to the pathologist a carcinoma of the breast occurring in a context of hereditary mutation. However, at the present time the only situations where a morphological sorting makes it possible to accelerate the genetic analysis are those of an invasive carcinoma of non-specific type of triple-negative phenotype in a woman of less than 50 years or that of a diagnosis of HER2 breast cancer amplified in a woman under 31 years of age (Chompret criteria). Family background and personal history are of great importance in the genetic counseling indication decision trees. Unfortunately, to date, no quality antibody has been developed against BRCA1 and 2 to help the pathologist identify hereditary cases. The immunohistochemical analysis of RAD51 could facilitate the identification of tumors possibly sensitive to PARP inhibitors. Progress to identify hereditary cancers is expected thanks to the development of artificial intelligence algorithms from digitized histological slides. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0242-6498(20)30059-6  |  
------------------------------------------- 
10.1093/ehjci/jeaa045  |    Aims :  Cardiac resynchronization therapy (CRT) is a disease-modifying therapy in patients with chronic heart failure (CHF). Current guidelines ascribe CRT eligibility on three parameters only: left ventricular ejection fraction (LVEF), QRS duration, and New York Heart Association (NYHA) functional class. However, one-third of CHF patients does not benefit from CRT. This study evaluated whether 123I-meta-iodobenzylguanidine (123I-mIBG) assessed cardiac sympathetic activity could optimize CRT patient selection. 
  Methods and results :  A total of 78 stable CHF subjects (age 66.8 ± 9.6 years, 73% male, LVEF 25.2 ± 6.7%, QRS duration 153 ± 23 ms, NYHA 2.2 ± 0.7) referred for CRT implantation were enrolled. Subjects underwent 123I-mIBG scintigraphy prior to implantation. Early and late heart-to-mediastinum (H/M) ratio and 123I-mIBG washout were calculated. CRT response was defined as either an increase of LVEF to &gt;35%, any improvement in LVEF of &gt;10%, QRS shortening to &lt;150 ms, or improvement in NYHA class of &gt;1 class. In 33 patients LVEF increased to &gt;35%, QRS decreased &lt;150 ms in 36 patients, and NYHA class decreased in 33 patients. Late H/M ratio and hypertension were independent predictors of LVEF improvement to &gt;35% (P = 0.0014 and P = 0.0149, respectively). In addition, early H/M ratio, LVEF, and absence of diabetes mellitus (DM) were independent predictors for LVEF improvement by &gt;10%. No independent predictors were found for QRS shortening to &lt;150 ms or improvement in NYHA class. 
  Conclusion :  Early and late H/M ratio were independent predictors of CRT response when improvement of LVEF was used as measure of response. Therefore, cardiac 123I-mIBG scintigraphy may be used as a tool to optimize selection of subjects that might benefit from CRT. 
  |  https://academic.oup.com/ehjcimaging/article-lookup/doi/10.1093/ehjci/jeaa045  |  
------------------------------------------- 
10.3389/fonc.2020.00077  |   <b>Objective:</b> To investigate whether pre-treatment CT-derived radiomic features could be applied for prediction of clinical response to neoadjuvant chemotherapy (NACT) in locally advanced cervical cancer (LACC). <b>Patients and Methods:</b> Two hundred and seventy-seven LACC patients treated with NACT followed by surgery/radiotherapy were included in this multi-institution retrospective study. One thousand and ninety-four radiomic features were extracted from venous contrast enhanced and non-enhanced CT imaging for each patient. Five combined methods of feature selection were used to reduce dimension of features. Radiomics signature was constructed by Random Forest (RF) method in a primary cohort of 221 patients. A combined model incorporating radiomics signature with clinical factors was developed using multivariable logistic regression. Prediction performance was then tested in a validation cohort of 56 patients. <b>Results:</b> Radiomics signature containing pre- and post-contrast imaging features can adequately distinguish chemotherapeutic responders from non-responders in both primary and validation cohorts [AUCs: 0.773 (95% CI, 0.701-0.845) and 0.816 (95% CI, 0.690-0.942), respectively] and remain relatively stable across centers. The combined model has a better predictive performance with an AUC of 0.803 (95% CI, 0.734-0.872) in the primary set and an AUC of 0.821 (95% CI, 0.697-0.946) in the validation set, compared to radiomics signature alone. Both models showed good discrimination, calibration. <b>Conclusion:</b> Newly developed radiomic model provided an easy-to-use predictor of chemotherapeutic response with improved predictive ability, which might facilitate optimal treatment strategies tailored for individual LACC patients. 
  |  https://doi.org/10.3389/fonc.2020.00077  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32117732/  |  
------------------------------------------- 
10.3390/s20051539  |   Prostate cancer is the most commonly diagnosed cancer in North American men; however, prognosis is relatively good given early diagnosis. This motivates the need for fast and reliable prostate cancer sensing. Diffusion weighted imaging (DWI) has gained traction in recent years as a fast non-invasive approach to cancer sensing. The most commonly used DWI sensing modality currently is apparent diffusion coefficient (ADC) imaging, with the recently introduced computed high-b value diffusion weighted imaging (CHB-DWI) showing considerable promise for cancer sensing. In this study, we investigate the efficacy of ADC and CHB-DWI sensing modalities when applied to zone-level prostate cancer sensing by introducing several radiomics driven zone-level prostate cancer sensing strategies geared around hand-engineered radiomic sequences from DWI sensing (which we term as Zone-X sensing strategies). Furthermore, we also propose Zone-DR, a discovery radiomics approach based on zone-level deep radiomic sequencer discovery that discover radiomic sequences directly for radiomics driven sensing. Experimental results using 12,466 pathology-verified zones obtained through the different DWI sensing modalities of 101 patients showed that: (i) the introduced Zone-X and Zone-DR radiomics driven sensing strategies significantly outperformed the traditional clinical heuristics driven strategy in terms of AUC, (ii) the introduced Zone-DR and Zone-SVM strategies achieved the highest sensitivity and specificity, respectively for ADC amongst the tested radiomics driven strategies, (iii) the introduced Zone-DR and Zone-LR strategies achieved the highest sensitivities for CHB-DWI amongst the tested radiomics driven strategies, and (iv) the introduced Zone-DR, Zone-LR, and Zone-SVM strategies achieved the highest specificities for CHB-DWI amongst the tested radiomics driven strategies. Furthermore, the results showed that the trade-off between sensitivity and specificity can be optimized based on the particular clinical scenario we wish to employ radiomic driven DWI prostate cancer sensing strategies for, such as clinical screening versus surgical planning. Finally, we investigate the critical regions within sensing data that led to a given radiomic sequence generated by a Zone-DR sequencer using an explainability method to get a deeper understanding on the biomarkers important for zone-level cancer sensing. 
  |  http://www.mdpi.com/resolver?pii=s20051539  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32164378/  |  
------------------------------------------- 
10.3390/jcm9041193  |   Recent advances in surgical, immunosuppressive and monitoring protocols have led to the significant improvement of overall one-year kidney allograft outcomes. Nonetheless, there has not been a significant change in long-term kidney allograft outcomes. In fact, chronic and acute antibody-mediated rejection (ABMR) and non-immunological complications following kidney transplantation, including multiple incidences of primary kidney disease, as well as complications such as cardiovascular diseases, infections, and malignancy are the major factors that have contributed to the failure of kidney allografts. The use of molecular techniques to enhance histological diagnostics and noninvasive surveillance are what the latest studies in the field of clinical kidney transplant seem to mainly focus upon. Increasingly innovative approaches are being used to discover immunosuppressive methods to overcome critical sensitization, prevent the development of anti-human leukocyte antigen (HLA) antibodies, treat chronic active ABMR, and reduce non-immunological complications following kidney transplantation, such as the recurrence of primary kidney disease and other complications, such as cardiovascular diseases, infections, and malignancy. In the present era of utilizing electronic health records (EHRs), it is strongly believed that big data and artificial intelligence will reshape the research done on kidney transplantation in the near future. In addition, the utilization of telemedicine is increasing, providing benefits such as reaching out to kidney transplant patients in remote areas and helping to make scarce healthcare resources more accessible for kidney transplantation. In this article, we discuss the recent research developments in kidney transplants that may affect long-term allografts, as well as the survival of the patient. The latest developments in living kidney donation are also explored. 
  |  http://www.mdpi.com/resolver?pii=jcm9041193  |  
------------------------------------------- 
10.1016/j.gaitpost.2020.04.005  |    Introduction:  Injury prevention programs for athletes are still limited by a lack of understanding of specific risk factors that can influence injuries within different sports. The majority of studies on volleyball have not considered the movement patterns when moving in different directions or in planned and unplanned block jump-landings. 
  Methods:  This study investigated all planes mechanics between the lead and trail limb when moving in dominant and non-dominant directions, for both planned and unplanned jump-landings in thirteen semi-professional female volleyball players. Ankle, knee and hip joint kinematics, kinetics and joint stiffness were recorded. 
  Results:  Our results showed statistically significant differences between the lead limb and the trail limb in the hip flexion angles, moments and velocity; in the knee flexion angles, moments, stiffness, power and energy absorption and in the ankle dorsiflexion, power and energy absorption, showing a tendency where the lead limb has a higher injury risk than the trail limb. When considering planned versus unplanned situations, there were statistically significant differences in knee flexion angles, moments, power and energy absorption; and hip contact angle, flexion angular velocity and energy absorption, with musculoskeletal adaptations in the planned situations. 
  Discussion:  It appears that the role of the limb, either lead or trail, is more important than the limb dominance when performing directional jump-landings, with the lead limb having a higher implication on possible overuse injuries than the trail limb. Furthermore, planned movements showed a difference in strategy indicating greater implications to possible overuse injuries than in the unplanned situations which may be associated with more conscious thought about the movements. 
  Conclusion:  Coaches should consider unilateral coordination training in both landing directions for the lead and trail limb, and should adapt training to replicate the competition environment, using unplanned situations to minimize asymmetries to might reduce injury risks. 
  |  None  |  
------------------------------------------- 
10.1186/s12967-020-02287-y  |    Background:  The performance of previously published glomerular filtration rate (GFR) estimation equations degrades when directly used in Chinese population. We incorporated more independent variables and using complicated non-linear modeling technology (artificial neural network, ANN) to develop a more accurate GFR estimation model for Chinese population. 
  Methods:  The enrolled participants came from the Third Affiliated Hospital of Sun Yat-sen University, China from Jan 2012 to Jun 2016. Participants with age &lt; 18, unstable kidney function, taking trimethoprim or cimetidine, or receiving dialysis were excluded. Among the finally enrolled 1952 participants, 1075 participants (55.07%) from Jan 2012 to Dec 2014 were assigned as the development data whereas 877 participants (44.93%) from Jan 2015 to Jun 2016 as the internal validation data. We in total developed 3 GFR estimation models: a 4-variable revised CKD-EPI (chronic kidney disease epidemiology collaboration) equation (standardized serum creatinine and cystatin C, age and gender), a 9-variable revised CKD-EPI equation (additional auxiliary variables: body mass index, blood urea nitrogen, albumin, uric acid and hemoglobin), and a 9-variable ANN model. 
  Results:  Compared with the 4-variable equation, the 9-variable equation could not achieve superior performance in the internal validation data (mean of difference: 5.00 [3.82, 6.54] vs 4.67 [3.55, 5.90], P = 0.5; interquartile range (IQR) of difference: 18.91 [17.43, 20.48] vs 20.11 [18.46, 21.80], P = 0.05; P30: 76.6% [73.7%, 79.5%] vs 75.8% [72.9%, 78.6%], P = 0.4), but the 9-variable ANN model significantly improve bias and P30 accuracy (mean of difference: 2.77 [1.82, 4.10], P = 0.007; IQR: 19.33 [17.77, 21.17], P = 0.3; P30: 80.0% [77.4%, 82.7%], P &lt; 0.001). 
  Conclusions:  It is suggested that using complicated non-linear models like ANN could fully utilize the predictive ability of the independent variables, and then finally achieve a superior GFR estimation model. 
  |  https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02287-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32156297/  |  
------------------------------------------- 
10.1063/1.5119857  |   The daily Colombian coffee price is a chaotic signal that has emerged from a complex economic system. This work proposes to identify its dynamics by means of two models: a single multiscroll Chua system and the coupling of two of these systems. Models are fine-tuned through an artificial bee colony optimization algorithm. Results show that this approach can reconstruct the price signal in terms of several statistics and points out a way for its long-term forecasting. 
  |  https://dx.doi.org/10.1063/1.5119857  |  
------------------------------------------- 
10.3390/molecules25082000  |   Neurodegenerative diseases represent a significant unmet medical need in our aging society. There are no effective treatments for most of these diseases, and we know comparatively little regarding pathogenic mechanisms. Among the challenges faced by those involved in developing therapeutic drugs for neurodegenerative diseases, the syndromes are often complex, and small animal models do not fully recapitulate the unique features of the human nervous system. Human induced pluripotent stem cells (iPSCs) are a novel technology that ideally would permit us to generate neuronal cells from individual patients, thereby eliminating the problem of species-specificity inherent when using animal models. Specific phenotypes of iPSC-derived cells may permit researchers to identify sub-types and to distinguish among unique clusters and groups. Recently, iPSCs were used for drug screening and testing for neurologic disorders including Alzheimer's disease (AD), amyotrophic lateral sclerosis (ALS), spinocerebellar atrophy (SCA), and Zika virus infection. However, there remain many challenges still ahead, including how one might effectively recapitulate sporadic disease phenotypes and the selection of ideal phenotypes and for large-scale drug screening. Fortunately, quite a few novel strategies have been developed that might be combined with an iPSC-based model to solve these challenges, including organoid technology, single-cell RNA sequencing, genome editing, and deep learning artificial intelligence. Here, we will review current applications and potential future directions for iPSC-based neurodegenerative disease models for critical drug screening. 
  |  http://www.mdpi.com/resolver?pii=molecules25082000  |  
------------------------------------------- 
10.1021/acsnano.0c00384  |   Fast and inexpensive characterization of materials properties is a key element to discover novel functional materials. In this work, we suggest an approach employing three classes of Bayesian machine learning (ML) models to correlate electronic absorption spectra of nanoaggregates with the strength of intermolecular electronic couplings in organic conducting and semiconducting materials. As a specific model system, we consider poly(3,4-ethylenedioxythiophene) (PEDOT) polystyrene sulfonate, a cornerstone material for organic electronic applications, and so analyze the couplings between charged dimers of closely packed PEDOT oligomers that are at the heart of the material's unrivaled conductivity. We demonstrate that ML algorithms can identify correlations between the coupling strengths and the electronic absorption spectra. We also show that ML models can be trained to be transferable across a broad range of spectral resolutions and that the electronic couplings can be predicted from the simulated spectra with an 88% accuracy when ML models are used as classifiers. Although the ML models employed in this study were trained on data generated by a multiscale computational workflow, they were able to leverage experimental data. 
  |  https://dx.doi.org/10.1021/acsnano.0c00384  |  
------------------------------------------- 
10.12927/hcq.2020.26144  |   Across Canada, healthcare leaders are exploring the potential of artificial intelligence and advanced analytics to transform the healthcare system. This report shares a summary of the current state of healthcare analytics across major hospitals and public healthcare agencies in Canada. We present information on the current level of investment, data governance maturity, analytics talent and tools and models being leveraged across the nation. The findings point to an opportunity for enhanced collaboration in advanced analytics and the adoption of nascent artificial intelligence technologies in healthcare. The recommendations will help drive adoption in Canada, ultimately improving the patient experience and promoting better health outcomes for Canadians. 
  |  https://www.longwoods.com/content/26144  |  
------------------------------------------- 
10.1186/s40249-020-0622-9  |    Background:  Crowdsourcing is used increasingly in health and medical research. Crowdsourcing is the process of aggregating crowd wisdom to solve a problem. The purpose of this systematic review is to summarize quantitative evidence on crowdsourcing to improve health. 
  Methods:  We followed Cochrane systematic review guidance and systematically searched seven databases up to September 4th 2019. Studies were included if they reported on crowdsourcing and related to health or medicine. Studies were excluded if recruitment was the only use of crowdsourcing. We determined the level of evidence associated with review findings using the GRADE approach. 
  Results:  We screened 3508 citations, accessed 362 articles, and included 188 studies. Ninety-six studies examined effectiveness, 127 examined feasibility, and 37 examined cost. The most common purposes were to evaluate surgical skills (17 studies), to create sexual health messages (seven studies), and to provide layperson cardio-pulmonary resuscitation (CPR) out-of-hospital (six studies). Seventeen observational studies used crowdsourcing to evaluate surgical skills, finding that crowdsourcing evaluation was as effective as expert evaluation (low quality). Four studies used a challenge contest to solicit human immunodeficiency virus (HIV) testing promotion materials and increase HIV testing rates (moderate quality), and two of the four studies found this approach saved money. Three studies suggested that an interactive technology system increased rates of layperson initiated CPR out-of-hospital (moderate quality). However, studies analyzing crowdsourcing to evaluate surgical skills and layperson-initiated CPR were only from high-income countries. Five studies examined crowdsourcing to inform artificial intelligence projects, most often related to annotation of medical data. Crowdsourcing was evaluated using different outcomes, limiting the extent to which studies could be pooled. 
  Conclusions:  Crowdsourcing has been used to improve health in many settings. Although crowdsourcing is effective at improving behavioral outcomes, more research is needed to understand effects on clinical outcomes and costs. More research is needed on crowdsourcing as a tool to develop artificial intelligence systems in medicine. 
  Trial registration:  PROSPERO: CRD42017052835. December 27, 2016. 
  |  https://idpjournal.biomedcentral.com/articles/10.1186/s40249-020-0622-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31959234/  |  
------------------------------------------- 
10.1242/jeb.206342  |   Field studies on freely behaving animals commonly require tagging and often are focused on single species. Weakly electric fish generate a species- and individual-specific electric organ discharge (EOD) and therefore provide a unique opportunity for individual tracking without tagging. Here, we present and test tracking algorithms based on recordings with submerged electrode arrays. Harmonic structures extracted from power spectra provide fish identity. Localization of fish based on weighted averages of their EOD amplitudes is found to be more robust than fitting a dipole model. We apply these techniques to monitor a community of three species, <i>Apteronotus rostratus</i>, <i>Eigenmannia humboldtii</i> and <i>Sternopygus dariensis</i>, in their natural habitat in Darién, Panama. We found consistent upstream movements after sunset followed by downstream movements in the second half of the night. Extrapolations of these movements and estimates of fish density obtained from additional transect data suggest that some fish cover at least several hundreds of meters of the stream per night. Most fish, including <i>E</i> <i>. humboldtii</i>, were traversing the electrode array solitarily. From <i>in situ</i> measurements of the decay of the EOD amplitude with distance of individual animals, we estimated that fish can detect conspecifics at distances of up to 2 m. Our recordings also emphasize the complexity of natural electrosensory scenes resulting from the interactions of the EODs of different species. Electrode arrays thus provide an unprecedented window into the so-far hidden nocturnal activities of multispecies communities of weakly electric fish at an unmatched level of detail. 
  |  http://jeb.biologists.org/cgi/pmidlookup?view=long&pmid=31937524  |  
------------------------------------------- 
10.3390/ijerph17041125  |   Excessive alcohol use in the US contributes to over 88,000 deaths per year and costs over $250 billion annually. While previous studies have shown that excessive alcohol use can be detected from general patterns of social media engagement, we characterized how drinking-specific language varies across regions and cultures in the US. From a database of 38 billion public tweets, we selected those mentioning "drunk", found the words and phrases distinctive of drinking posts, and then clustered these into topics and sets of semantically related words. We identified geolocated "drunk" tweets and correlated their language with the prevalence of self-reported excessive alcohol consumption (Behavioral Risk Factor Surveillance System; BRFSS). We then identified linguistic markers associated with excessive drinking in different regions and cultural communities as identified by the American Community Project. "Drunk" tweet frequency (of the 3.3 million geolocated "drunk" tweets) correlated with excessive alcohol consumption at both the county and state levels (<i>r</i> = 0.26 and 0.45, respectively, <i>p</i> &lt; 0.01). Topic analyses revealed that excessive alcohol consumption was most correlated with references to drinking with friends (<i>r</i> = 0.20), family (<i>r</i> = 0.15), and driving under the influence (<i>r</i> = 0.14). Using the American Community Project classification, we found a number of cultural markers of drinking: religious communities had a high frequency of anti-drunk driving tweets, Hispanic centers discussed family members drinking, and college towns discussed sexual behavior. This study shows that Twitter can be used to explore the specific sociocultural contexts in which excessive alcohol use occurs within particular regions and communities. These findings can inform more targeted public health messaging and help to better understand cultural determinants of substance abuse. 
  |  http://www.mdpi.com/resolver?pii=ijerph17041125  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32053866/  |  
------------------------------------------- 
10.1021/acsami.9b20662  |   Nowadays, the integration of easy production, simple structure, high sensitivity, and multifunctionality is the developing tendency for flexible sensors. Herein we report a facile manufacture of a highly flexible, sensitive, and multifunctional dual-mode sensor with an ultrasimple structure by directly attaching magnetic iron rubber (IR) onto the surface of carbon aerogel (CA) derived from melamine foam. The dual-mode CA/IR sensor exhibits high sensitivities of 5.6 kPa<sup>-1</sup> and 1.6·10<sup>-3</sup> Oe<sup>-1</sup>, respectively, toward pressure and magnetic field in a wide frequency ranging from 0.1 to 10 Hz, which are higher than those of the existing flexible pressure/magnetism sensors. The multifunctionality of the dual-mode CA/IR sensor is demonstrated by monitoring blood pulse, human breath, balloon volume, and thoracic volume via pressure and magnetism sensing or their combination. Due to its simple structure and high sensitivities, the dual-mode sensor is employed as the building block to create a direction-recognizable sensor for identifying the directions of pressure and magnetic field for the awareness of surrounding barriers that are of practical importance in sophisticated situations such as autonomous artificial intelligence, autodriving and robotics, and so on. 
  |  https://dx.doi.org/10.1021/acsami.9b20662  |  
------------------------------------------- 
10.1016/j.actbio.2020.02.037  |   The advancement of glass science has played a pivotal role in enhancing the quality and length of human life. However, with an ever-increasing demand for glasses in a variety of healthcare applications - especially with controlled degradation rates - it is becoming difficult to design new glass compositions using conventional approaches. For example, it is difficult, if not impossible, to design new gene-activation bioactive glasses, with controlled release of functional ions tailored for specific patient states, using trial-and-error based approaches. Notwithstanding, it is possible to design new glasses with controlled release of functional ions by using artificial intelligence-based methods, for example, supervised machine learning (ML). In this paper, we present an ensemble ML model for reliable prediction of time- and composition-dependent dissolution behavior of a wide variety of oxide glasses relevant for various biomedical applications. A comprehensive database, comprising of over 1300 data-records consolidated from original glass dissolution experiments, has been used for training and subsequent testing of prediction performance of the ML model. Results demonstrate that the ensemble ML model can predict chemical degradation behavior of glasses in aqueous solutions over a wide range of pH relevant for their usage in a human body where the environment can be highly acidic (for example, pH = 3), for example, due to secretion of citric acid by osteoclasts, or highly alkaline (pH ≈10) due to the release of alkali cations from bioactive glasses. Outcomes of this study can be leveraged to design glasses with controlled dissolution behavior in various biological environments. STATEMENT OF SIGNIFICANCE: In this paper, we present an ensemble machine learning (ML) model for prediction of dissolution behavior of a wide variety of oxide glasses relevant for various biomedical applications. The results demonstrate that the ML model can predict the chemical degradation behavior of glasses in aqueous solutions over a wide range of pH relevant for their usage in a human body where the environment can be highly acidic (for example, pH = 3), for example, due to secretion of citric acid by osteoclasts, or highly alkaline (pH ≈10) due to the release of alkali cations from bioactive glasses. Outcomes of this study can be leveraged to design new biomedical glasses with controlled (desired) dissolution behavior in various biological environments. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1742-7061(20)30125-2  |  
------------------------------------------- 
10.1007/s00415-020-09742-2  |   Axonal variants of Guillain-Barré syndrome (GBS) mainly include acute motor axonal neuropathy, acute motor and sensory axonal neuropathy, and pharyngeal-cervical-brachial weakness. Molecular mimicry of human gangliosides by a pathogen's lipooligosaccharides is a well-established mechanism for Campylobacter jejuni-associated GBS. New triggers of the axonal variants of GBS (axonal GBS), such as Zika virus, hepatitis viruses, intravenous administration of ganglioside, vaccination, and surgery, are being identified. However, the pathogenetic mechanisms of axonal GBS related to antecedent bacterial or viral infections other than Campylobacter jejuni remain unknown. Currently, autoantibody classification and serial electrophysiology are cardinal approaches to differentiate axonal GBS from the prototype of GBS, acute inflammatory demyelinating polyneuropathy. Newly developed technologies, including metabolite analysis, peripheral nerve ultrasound, and feature selection via artificial intelligence are facilitating more accurate diagnosis of axonal GBS. Nevertheless, some key issues, such as genetic susceptibilities, remain unanswered and moreover, current therapies bear limitations. Although several therapies have shown considerable benefits to experimental animals, randomized controlled trials are still needed to validate their efficacy. 
  |  https://dx.doi.org/10.1007/s00415-020-09742-2  |  
------------------------------------------- 
10.3390/s20082263  |   In view of the inability of Global Navigation Satellite System (GNSS) to provide accurate indoor positioning services and the growing demand for location-based services, indoor positioning has become one of the most attractive research areas. Moreover, with the improvement of the smartphone hardware level, the rapid development of deep learning applications on mobile terminals has been promoted. Therefore, this paper borrows relevant ideas to transform indoor positioning problems into problems that can be solved by artificial intelligence algorithms. First, this article reviews the current mainstream pedestrian dead reckoning (PDR) optimization and improvement methods, and based on this, uses the micro-electromechanical systems (MEMS) sensor on a smartphone to achieve better step detection, stride length estimation, and heading estimation modules. In the real environment, an indoor continuous positioning system based on a smartphone is implemented. Then, in order to solve the problem that the PDR algorithm has accumulated errors for a long time, a calibration method is proposed without the need to deploy any additional equipment. An indoor turning point feature detection model based on deep neural network is designed, and the accuracy of turning point detection is 98%. Then, the particle filter algorithm is used to fuse the detected turning point and the PDR positioning result, thereby realizing lightweight cumulative error calibration. In two different experimental environments, the performance of the proposed algorithm and the commonly used localization algorithm are compared through a large number of experiments. In a small-scale indoor office environment, the average positioning accuracy of the algorithm is 0.14 m, and the error less than 1 m is 100%. In a large-scale conference hall environment, the average positioning accuracy of the algorithm is 1.29 m, and 65% of the positioning errors are less than 1.50 m which verifies the effectiveness of the proposed algorithm. The simple and lightweight indoor positioning design scheme proposed in this article is not only easy to popularize, but also provides new ideas for subsequent scientific research in the field of indoor positioning. 
  |  http://www.mdpi.com/resolver?pii=s20082263  |  
------------------------------------------- 
10.1016/j.jbi.2020.103419  |   This work deals with negation detection in the context of clinical texts. Negation detection is a key for decision support systems since negated events (detection of absence of some events) help ascertain current medical conditions. For artificial intelligence, negation detection is a valuable point as it can revert the meaning of a part of a text and, accordingly, influence other tasks such as medical dosage adjustment, the detection of adverse drug reactions or hospital acquired diseases. We focus on negated medical events such as disorders, findings and allergies. From Natural Language Processing (NLP) background, we refer to them as negated medical entities. A novelty of this work is that we approached this task as Named Entity Recognition (NER) with the restriction that just negated medical entities must be recognized (in an attempt to help distinguish them from non-negated ones). Our study is driven with Electronic Health Records (EHRs) written in Spanish. A challenge to cope with is the lexical variability (alternative medical forms, abbreviations, etc.). To this end, we employed an approach based on deep learning. Specifically, the system combines character embeddings to cope with out-of-vocabulary (OOV) words, Long Short-Term Memory (LSTM) networks to model contextual representations and it makes use of Conditional Random Fields (CRF) to classify each medical entity as either negated or not given the contextual dense representation. Moreover, we explored both embeddings created from words and embeddings created from lemmas. The best results were obtained with the lemmatized embeddings. Apparently, this approach reinforced the capability of the LSTMs to cope with the high lexical variability. The f-measure for exact-match was 65.1 and 82.4 for the partial-match. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1532-0464(20)30047-2  |  
------------------------------------------- 
10.1097/PRS.0000000000006650  |    Background:  A recent artificial intelligence-based investigation has shown the impacts of orthognathic surgery on the patient's facial appearance and apparent age. However, appearance and age perception as reported by patients and surgical professionals have not been addressed in the same cohort to date. 
  Methods:  FACE-Q facial appraisal (appearance and age) and quality-of-life scale scores obtained before and after orthognathic surgery, in addition to three-dimensional photographs of 70 patients with skeletal class III deformity, were collected for a comparative cross-sectional study. Seven blinded plastic surgeons rated all photographs for apparent facial aesthetic and age scales. The FACE-Q data from 57 matched normal individuals were adopted for the comparative analyses. The correlation between the FACE-Q and the professional-based scales was tested. 
  Results:  Pre-orthognathic surgery versus post-orthognathic surgery comparisons showed significant differences (p &lt; 0.001) for all FACE-Q scales and panel assessments, with higher (FACE-Q scales and professional-based aesthetic parameters) and lower (FACE-Q patient-perceived age scale and professional-based age parameter) values for post-orthognathic surgery measurements. Patients had significantly (p &lt; 0.001) higher (patient-perceived age scale) and lower (facial appraisal and quality-of-life scales) FACE-Q values than normal individuals for pre-orthognathic surgery but not for post-orthognathic surgery measurements. The FACE-Q facial appearance overall scale had significant correlations (p &lt; 0.001) with the panel assessment for the parameters "beautiful" and "attractive" but not for the "pleasant" parameter. No significant correlations were observed for facial age scales. 
  Conclusion:  This study contributes to the orthognathic surgery literature by revealing that orthognathic surgery positively impacts the perception of apparent facial age and improves facial appearance and quality of life. 
  Clinical question/level of evidence:  Therapeutic, IV. 
  |  http://Insights.ovid.com/pubmed?pmid=32221228  |  
------------------------------------------- 
10.1016/j.cmpb.2019.105278  |    Background and objective:  Mobility of subject (MoS) and muscle contraction force variation (MCFV) have been shown to individually degrade the performance of multiple degrees of freedom electromyogram (EMG) pattern recognition (PR) based prostheses control systems. Though these factors (MoS-MCFV) co-exist simultaneously in the practical use of the prosthesis, their combined impact on PR-based system has rarely been studied especially in the context of amputees who are the target users of the device. 
  Methods:  To address this problem, this study systematically investigated the co-existing impact of MoS-MCFV on the performance of PR-based movement intent classifier, using EMG recordings acquired from eight participants who performed multiple classes of targeted limb movements across static and non-static scenarios with three distinct muscle contraction force levels. Then, a robust feature extraction method that is invariant to the combined effect of MoS-MCFV, namely, invariant time-domain descriptor (invTDD), was proposed to optimally characterize the multi-class EMG signal patterns in the presence of both factors. 
  Results:  Experimental results consistently showed that the proposed invTDD method could significantly mitigate the co-existing impact of MoS-MCFV on PR-based movement-intent classifier with error reduction in the range of 7.50%~17.97% (p&lt;0.05), compared to the commonly applied methods. Further evaluation using 2-dimentional principal component analysis (PCA) technique, revealed that the proposed invTDD method has obvious class-separability in the PCA feature space, with a significantly lower standard error (0.91%) compared to the existing methods. 
  Conclusion:  This study offers compelling insight on how to develop accurately robust multiple degrees of freedom control scheme for multifunctional prostheses that would be clinically viable. Also, the study may spur positive advancement in other application areas of medical robotics that adopts myoelectric control schemes such as the electric wheelchair and human-computer-interaction systems. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31550-0  |  
------------------------------------------- 
10.1111/jvh.13249  |   The Republic of Korea Armed Forces has implemented the hepatitis A virus (HAV) vaccination programme with a single-dose administration schedule in new recruits since 2013. A single-dose administration was selected for economic feasibility. We analysed the effectiveness of the single-dose HAV vaccination in a young and healthy population. To measure the effectiveness of the programme, we observed the incidence of HAV between the vaccinated and unvaccinated groups. A comparison between the two groups during the vaccine introduction period (2013-2016) revealed a lower incidence rate of infection in the vaccinated group (3 cases/603 550 person-years) than in the unvaccinated group (21 cases/1 020 450 person-years). The effectiveness of single-dose HAV vaccination was found to be 75.85%. 
  |  https://doi.org/10.1111/jvh.13249  |  
------------------------------------------- 
10.3390/ijerph17061907  |   Physical fitness is a powerful indicator of health. Sleep condition plays an essential role in maintaining quality of life and is an important marker that predicts physical fitness. This study aimed to determine the relationship between sleep conditions (sleep quality, sleep duration, bedtime) and multiple physical fitness indicators (body mass index (BMI), flexibility, abdominal muscle strength and endurance, cardiopulmonary endurance) in a well-characterized population of Taiwanese adults aged 23 to 65. The applied data were obtained from the National Physical Fitness Examination Survey 2014 conducted in Taiwan. We assessed the association of the sleep conditions with physical fitness performances in Taiwanese adults by using the multivariate adaptive regression spline (MARS) method with a total of 69,559 samples. The results show that sleep duration, sleep quality, and bedtime were statistically significant influence factors on physical fitness performances with different degrees. Gender was an important factor that affects the effects of daily sleep conditions on performances of physical fitness. Sleep duration was the most important factor as it was simultaneously correlated with BMI, sit-ups, and sit-and-reach indicators in both genders. Bedtime and sleep quality were only associated with sit-ups performance in both genders. 
  |  http://www.mdpi.com/resolver?pii=ijerph17061907  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32183445/  |  
------------------------------------------- 
10.1371/journal.pone.0227240  |   This study examined and compared outcomes of deep learning (DL) in identifying swept-source optical coherence tomography (OCT) images without myopic macular lesions [i.e., no high myopia (nHM) vs. high myopia (HM)], and OCT images with myopic macular lesions [e.g., myopic choroidal neovascularization (mCNV) and retinoschisis (RS)]. A total of 910 SS-OCT images were included in the study as follows and analyzed by k-fold cross-validation (k = 5) using DL's renowned model, Visual Geometry Group-16: nHM, 146 images; HM, 531 images; mCNV, 122 images; and RS, 111 images (n = 910). The binary classification of OCT images with or without myopic macular lesions; the binary classification of HM images and images with myopic macular lesions (i.e., mCNV and RS images); and the ternary classification of HM, mCNV, and RS images were examined. Additionally, sensitivity, specificity, and the area under the curve (AUC) for the binary classifications as well as the correct answer rate for ternary classification were examined. The classification results of OCT images with or without myopic macular lesions were as follows: AUC, 0.970; sensitivity, 90.6%; specificity, 94.2%. The classification results of HM images and images with myopic macular lesions were as follows: AUC, 1.000; sensitivity, 100.0%; specificity, 100.0%. The correct answer rate in the ternary classification of HM images, mCNV images, and RS images were as follows: HM images, 96.5%; mCNV images, 77.9%; and RS, 67.6% with mean, 88.9%.Using noninvasive, easy-to-obtain swept-source OCT images, the DL model was able to classify OCT images without myopic macular lesions and OCT images with myopic macular lesions such as mCNV and RS with high accuracy. The study results suggest the possibility of conducting highly accurate screening of ocular diseases using artificial intelligence, which may improve the prevention of blindness and reduce workloads for ophthalmologists. 
  |  http://dx.plos.org/10.1371/journal.pone.0227240  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32298265/  |  
------------------------------------------- 
10.1007/s10916-019-1520-1  |   There has been an increasing attention to the study of stress. Particularly, college students often experience high levels of stress that are linked to several negative outcomes concerning academic functioning, physical, and mental health. In this paper, we introduce the EuStress Solution, that aims to create an Information System to monitor and assess, continuously and in real-time, the stress levels of the students in order to predict burnout. The Information System will use a measuring instrument based on wearable device and machine learning techniques to collect and process stress-related data from the students without their explicit interaction. In the present study, we focus on heart rate and heart rate variability indices, by comparing baseline and stress condition. We performed different statistical tests in order to develop a complex and intelligent model. Results showed the neural network had the better model fit. 
  |  https://dx.doi.org/10.1007/s10916-019-1520-1  |  
------------------------------------------- 
10.3390/foods9040492  |   The food industry has recently faced rapid and constant changes due to the current industrial revolution, Industry 4.0, which has also profoundly altered the dynamics of the industry overall. Due to the emerging digitalisation, manufacturing models are changing through the use of smart technologies, such as robotics, Artificial Intelligence (AI), Internet of Things (IoT), machine learning, etc. They are experiencing a new phase of automation that enables innovative and more efficient processes, products and services. The introduction of these novel business models demands new professional skills requirements in the workforce of the food industry. In this work, we introduce an industry-driven proactive strategy to achieve a successful digital transformation in the food sector. For that purpose, we focus on defining the current and near-future key skills and competencies demanded by each of the professional profiles related to the food industry. To achieve this, we generated an automated database of current and future professions and competencies and skills. This database can be used as a fundamental roadmap guiding the sector through future changes caused by Industry 4.0. The interest shown by the local sectorial cluster and related entities reinforce the idea. This research will be a key tool for both academics and policy-makers to provide well-developed and better-oriented continuous training programs in order to reduce the skill mismatch between the workforce and the jobs. 
  |  http://www.mdpi.com/resolver?pii=foods9040492  |  
------------------------------------------- 
10.1111/vox.12885  |    Background:  Blood donors with a relatively low haemoglobin (Hb) level at their previous donation attempt have an increased risk of Hb deferral at the subsequent donation attempt. The aim of this study was to investigate whether the interventions prolongation of donation interval and/or a dietary advice decrease the Hb deferral rate. 
  Methods:  11 897 whole blood donors with Hb levels from below to 0·2 mmol/l above the cut-off level for donation received either no intervention, a prolongation of the donation interval to six or twelve months, a dietary advice, or both. Deferral rates for low Hb levels at the subsequent donation attempt were compared in the different intervention groups. Additionally, the effects of the interventions on Hb deferral risk and donor return for a subsequent donation attempt were analysed using generalized estimating equations. 
  Results:  The Hb deferral rate was substantially lower in the group that received a prolongation of the donation interval to six months than in the Control Group (12·9% vs. 6·3% in men and 20·4% vs. 13·4% in women). However, the additional benefit of twelve over 6-month interval prolongation was small, and no benefit of a dietary advice showed up. On the other hand, receiving a dietary advice increased the likelihood of donor return for a subsequent donation attempt. 
  Conclusion:  Implementation of a protocol for the prolongation of donation intervals to six months for donors with Hb levels from below to slightly above the cut-off level for donation may reduce the deferral rate for low Hb levels while keeping donor lapse at a minimum. 
  |  https://doi.org/10.1111/vox.12885  |  
------------------------------------------- 
10.1016/j.jad.2020.01.175  |    Background:  Electroencephalography (EEG) has revealed increased beta activity in patients with comorbid major depressive disorder (MDD) and anxiety symptoms. Negative emotions and high beta activity could be decreased by a high beta down-training neurofeedback (NFB) protocol. The present study utilized three objective parameters - trainability, independence, and interpretability - to validate the effects of high beta down-training sessions. 
  Methods:  EEG data were collected from 23 patients with comorbid MDD and anxiety symptoms during high beta down-training sessions. Participants received five weeks of training, two sessions per week, to down-train high beta amplitude (20-32 Hz) at EEG sites P3 and P4. Three efficacy parameters were examined by comparing pre-training and post-training EEG. 
  Results:  The trainability index revealed the learning curves of reduced high beta activity at P3 and P4, confirming training effects across and within sessions. The independence index revealed only beta band activity decreased. The interpretability index revealed the decreased high beta activity was positively correlated with decreased severity of depression, especially for cognitive depression. 
  Limitations:  With only ten sessions in this study, it is unknown whether the NFB training caused extended and stable learning effects. Additionally, combining high beta down-training protocol with enhancing another target band could better ensure the desired changes in brain activity. Finally, the effect of medication on EEG cannot be excluded in present study. 
  Conclusions:  The trainability, independence and interpretability of the high beta down-training NFB protocol were confirmed, supporting the protocol's use in future research and clinical applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0165-0327(19)32770-3  |  
------------------------------------------- 
10.1093/cercor/bhaa017  |   Adaptive behavior requires the comparison of outcome predictions with actual outcomes (e.g., performance feedback). This process of performance monitoring is computed by a distributed brain network comprising the medial prefrontal cortex (mPFC) and the anterior insular cortex (AIC). Despite being consistently co-activated during different tasks, the precise neuronal computations of each region and their interactions remain elusive. In order to assess the neural mechanism by which the AIC processes performance feedback, we recorded AIC electrophysiological activity in humans. We found that the AIC beta oscillations amplitude is modulated by the probability of performance feedback valence (positive or negative) given the context (task and condition difficulty). Furthermore, the valence of feedback was encoded by delta waves phase-modulating the power of beta oscillations. Finally, connectivity and causal analysis showed that beta oscillations relay feedback information signals to the mPFC. These results reveal that structured oscillatory activity in the anterior insula encodes performance feedback information, thus coordinating brain circuits related to reward-based learning. 
  |  https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhaa017  |  
------------------------------------------- 
10.1007/s10916-019-1468-1  |   With the rapid development of technologies such as artificial intelligence, blockchain, cloud computing, and big data, Medical Cyber Physical Systems (MCPS) are increasingly demanding data security, while cloud storage solves the storage problem of complex medical data. However, it is difficult to realize data security sharing. The decentralization feature of blockchain is helpful to solve the problem that the secure authentication process is highly dependent on the trusted third party and implement data security transmission. In this paper, the blockchain technology is used to describe the security requirements in authentication process, and a network model of MCPS based on blockchain is proposed. Through analysis of medical data storage architecture, it can ensure that data can't be tampered and untrackable. In the security authentication phase, bilinear mapping and intractable problems can be used to solve the security threat in the authentication process of medical data providers and users. It can avoid the credibility problem of the trusted third party, and also can realize the ?thyc=10?&gt;two-way authentication between the hospital and blockchain node. Then, BAN logic is used to analyze security protocols, and formal analysis and comparison of security protocols are also made. The results show that the MCPS based on blockchain not only realizes medical treatment data sharing, but also meet the various security requirements in the security authentication phase. In addition, the storage and computing overhead costs is ideal. Therefore, the proposed scheme is more suitable for secure sharing of medical big data. 
  |  https://dx.doi.org/10.1007/s10916-019-1468-1  |  
------------------------------------------- 
10.3389/fphys.2019.01551  |   <b>Background:</b> Rectal cancer is a disease characterized with tumor heterogeneity. The combination of surgery, radiotherapy, and chemotherapy can reduce the risk of local recurrence. However, there is a significant difference in the response to radiotherapy among rectal cancer patients even they have the same tumor stage. Despite rapid advances in knowledge of cellular functions affecting radiosensitivity, there is still a lack of predictive factors for local recurrence and normal tissue damage. The tumor protein DNp73 is thought as a biomarker in colorectal cancer, but its clinical significance is still not sufficiently investigated, mainly due to the limitation of human-based pathology analysis. In this study, we investigated the predictive value of DNp73 in patients with rectal adenocarcinoma using image-based network analysis. <b>Methods:</b> The fuzzy weighted recurrence network of time series was extended to handle multi-channel image data, and applied to the analysis of immunohistochemistry images of DNp73 expression obtained from a cohort of 25 rectal cancer patients who underwent radiotherapy before surgery. Two mathematical weighted network properties, which are the clustering coefficient and characteristic path length, were computed for the image-based networks of the primary tumor (obtained after operation) and biopsy (obtained before operation) of each cancer patient. <b>Results:</b> The ratios of two weighted recurrence network properties of the primary tumors to biopsies reveal the correlation of DNp73 expression and long survival time, and discover the non-effective radiotherapy to a cohort of rectal cancer patients who had short survival time. <b>Conclusion:</b> Our work contributes to the elucidation of the predictive value of DNp73 expression in rectal cancer patients who were given preoperative radiotherapy. Mathematical properties of fuzzy weighted recurrence networks of immunohistochemistry images are not only able to show the predictive factor of DNp73 expression in the patients, but also reveal the identification of non-effective application of radiotherapy to those who had poor overall survival outcome. 
  |  https://doi.org/10.3389/fphys.2019.01551  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31969833/  |  
------------------------------------------- 
10.1136/lupus-2020-000389  |   Lupus nephritis (LN) is a severe manifestation of SLE, characterised by subendothelial and/or subepithelial immune complex depositions in the afflicted kidney, resulting in extensive injury and nephron loss during the acute phase and eventually chronic irreversible damage and renal function impairment if not treated effectively. The therapeutic management of LN has improved during the last decades, but the imperative need for consensual outcome measures remains. In order to design trials with success potentiality, it is important to define clinically important short-term and long-term targets of therapeutic and non-therapeutic intervention. While it is known that early response to treatment is coupled with favourable renal outcomes, early predictors of renal function impairment are lacking. The information gleaned from kidney biopsies may provide important insights in this direction. Alas, baseline clinical and histopathological information has not been shown to be informative. By contrast, accumulating evidence of pronounced discrepancies between clinical and histopathological outcomes after the initial phase of immunosuppression has prompted investigations of the potential usefulness of per-protocol repeat kidney biopsies as an integral part of treatment evaluation, including patients showing adequate clinical response. This approach appears to have merit. Hopefully, clinical, molecular or genetic markers that reliably reflect kidney histopathology and portend the long-term prognosis will be identified. Novel non-invasive imaging methods and employment of the evolving artificial intelligence in pattern recognition may also be helpful towards these goals. The molecular and cellular characterisation of SLE and LN will hopefully result in novel therapeutic modalities, maybe new taxonomy perspectives, and ultimately personalised management. 
  |  https://lupus.bmj.com/cgi/pmidlookup?view=long&pmid=32153796  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32153796/  |  
------------------------------------------- 
10.1038/s41598-020-62870-7  |   Affective communication, communicating with emotion, during face-to-face communication is critical for social interaction. Advances in artificial intelligence have made it essential to develop affective human-virtual agent communication. A person's belief during human-virtual agent interaction that the agent is a computer program affects social-cognitive processes. Whether this belief interferes with affective communication is an open question. We hypothesized that the imitation of a positive emotional expression by a virtual agent induces a positive emotion, regardless of the belief. To test this hypothesis, we conducted an fMRI study with 39 healthy volunteers, who were made to believe that a virtual agent was either a person or a computer. They were instructed to smile, and immediately afterwards, the virtual agent displayed a positive, negative, or neutral expression. The participants reported a positive emotion only when their smile was imitated by the agent's positive expression regardless of their belief. This imitation activated the participants' medial prefrontal cortex and precuneus, which are involved in anthropomorphism and contingency, respectively. These results suggest that a positive congruent response by a virtual agent can overcome the effect of believing that the agent is a computer program and thus contribute to achieving affective human-virtual agent communication. 
  |  http://dx.doi.org/10.1038/s41598-020-62870-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32249796/  |  
------------------------------------------- 
10.1016/j.dib.2020.105436  |   Sensor data generated by intelligent systems, such as autonomous robots, smart buildings and other systems based on artificial intelligence, represent valuable sources of knowledge in today's data-driven society, since they contain information about the situations these systems face during their operation. These data are usually multivariate time series since modern technologies enable the simultaneous acquisition of multiple signals during long periods of time. In this paper we present a dataset containing sensor traces of six data acquisition campaigns performed by autonomous aquatic drones involved in water monitoring. A total of 5.6 h of navigation are available, with data coming from both lakes and rivers, and from different locations in Italy and Spain. The monitored variables concern both the internal state of the drone (e.g., battery voltage, GPS position and signals to propellers) and the state of the water (e.g., temperature, dissolved oxygen and electrical conductivity). Data were collected in the context of the EU-funded Horizon 2020 project INTCATCH (http://www.intcatch.eu) which aims to develop a new paradigm for monitoring water quality of catchments. The aquatic drones used for data acquisition are Platypus Lutra boats. Both autonomous and manual drive is used in different parts of the navigation. The dataset is analyzed in the paper "Time series segmentation for state-model generation of autonomous aquatic drones: A systematic framework" [1] by means of recent time series clustering/segmentation techniques to extract data-driven models of the situations faced by the drones in the data acquisition campaigns. These data have strong potential for reuse in other kinds of data analysis and evaluation of machine learning methods on real-world datasets [2]. Moreover, we consider this dataset valuable also for the variety of situations faced by the drone, from which machine learning techniques can learn behavioral patterns or detect anomalous activities. We also provide manual labeling for some known states of the drones, such as, drone inside/outside the water, upstream/downstream navigation, manual/autonomous drive, and drone turning, that represent a ground truth for validation purposes. Finally, the real-world nature of the dataset makes it more challenging for machine learning methods because it contains noisy samples collected while the drone was exposed to atmospheric agents and uncertain water flow conditions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3409(20)30330-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32258287/  |  
------------------------------------------- 
10.2196/15767  |    Background:  Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder with unknown etiology. Early diagnosis and intervention are the keys to improving outcomes for patients with ASD. Structural MRI (sMRI) has been widely used in clinic to facilitate the diagnosis of brain diseases such as brain tumors. However, sMRI is less frequently investigated in neurological and psychiatric disorders such as ASD due to subtle, if any, anatomical changes of the brain. In recent years, more and more evidence has suggested that ASD is associated with anatomical changes of the brain. 
  Objective:  The aim of this study was to investigate the possibility of identifying structural patterns in the ASD patients' brain as potential biomarkers in the diagnosis and evaluation of ASD in clinic. 
  Methods:  We developed a novel two-level histogram-based morphometry (HBM) classification framework in which an algorithm based on a 3D version of histogram of oriented gradients (HOG) was used to extract features from sMRI data. We applied this framework to distinguish ASD patients from healthy controls using four datasets from the second edition of the Autism Brain Imaging Data Exchange (ABIDE II) including sites ETH Zürich (ETH), NYU Langone Medical Center: Sample 1 (NYU), Oregon Health and Science University (OHSU), and Stanford University (SU). We used stratified 10-fold cross-validation method to evaluate the model performance, and optimized the parameters for 3D HOG and selected the best algorithms for each level of the HBM framework. We applied the Naive Bayes approach to identify the predictive ASD-related brain regions based on classification contributions of each HOG feature. 
  Results:  Based on the 3D HOG feature extraction method, our proposed HBM framework achieved &gt;0.75 AUC on each dataset, with the best AUC of 0.849 on the ETH site. We compared the 3D HOG algorithm with the original 2D HOG algorithm and improved &gt;4% AUC on each dataset, with the best improvement of 10% on the SU site. Comparison of the 3D HOG algorithm with the scale-invariant feature transform (SIFT) algorithm showed &gt;14% AUC improvement on each dataset. Furthermore, we identified ASD-related brain regions based on the sMRI images. Some of these regions (e.g., frontal gyrus, temporal gyrus, ingulate gyrus, postcentral gyrus, precuneus, caudate and hippocampus) are known to be implicated in ASD in prior neuroimaging literatures. We also identified less well-known regions that may play unrecognized roles in ASD and be worth further investigation. 
  Conclusions:  Our research suggested it was possible to identify neuroimaging biomarkers that can distinguish ASD patients from healthy controls based on sMRI brain images. As a cost-effective and non-invasive tool for investigating brain structural changes, sMRI is also more amenable to populations for whom compliance is a challenge as it can be completed under sedation. Therefore, our tool could be useful in the diagnosis and evaluation of ASD in clinic. We also demonstrated the potentials of applying data-driven artificial intelligence technology in the clinical settings of neurological and psychiatric disorders that usually harbor in the brain subtle anatomical changes often invisible to human eyes. 
  |  https://doi.org/10.2196/15767  |  
------------------------------------------- 
10.1016/j.neuroimage.2020.116563  |   The human hippocampus is vulnerable to a range of degenerative conditions and as such, accurate in vivo measurement of the hippocampus and hippocampal substructures via neuroimaging is of great interest for understanding mechanisms of disease as well as for use as a biomarker in clinical trials of novel therapeutics. Although total hippocampal volume can be measured relatively reliably, it is critical to understand how this reliability is affected by acquisition on different scanners, as multiple scanning platforms would likely be utilized in large-scale clinical trials. This is particularly true for hippocampal subregional measurements, which have only relatively recently been measurable through common image processing platforms such as FreeSurfer. Accurate segmentation of these subregions is challenging due to their small size, magnetic resonance imaging (MRI) signal loss in medial temporal regions of the brain, and lack of contrast for delineation from standard neuroimaging procedures. Here, we assess the test-retest reliability of the FreeSurfer automated hippocampal subfield segmentation procedure using two Siemens model scanners (a Siemens Trio and Prisma<sup>fit</sup> Trio upgrade). T1-weighted images were acquired for 11 generally healthy younger participants (two scans on the Trio and one scan on the Prisma<sup>fit</sup>). Each scan was processed through the standard cross-sectional stream and the recently released longitudinal pipeline in FreeSurfer v6.0 for hippocampal segmentation. Test-retest reliability of the volumetric measures was examined for individual subfields as well as percent volume difference and Dice overlap among scans and intra-class correlation coefficients (ICC). Reliability was high in the molecular layer, dentate gyrus, and whole hippocampus with the inclusion of three time points with mean volume differences among scans less than 3%, overlap greater than 80%, and ICC &gt;0.95. The parasubiculum and hippocampal fissure showed the least improvement in reliability with mean volume difference greater than 5%, overlap less than 70%, and ICC scores ranging from 0.78 to 0.89. Other subregions, including the CA regions, were stable in their mean volume difference and overlap (&lt;5% difference and &gt;75% respectively) and showed improvement in reliability with the inclusion of three scans (ICC ​&gt; ​0.9). Reliability was generally higher within scanner (Trio-Trio), however, Trio-Prisma<sup>fit</sup> reliability was also high and did not exhibit an obvious bias. These results suggest that the FreeSurfer automated segmentation procedure is a reliable method to measure total as well as hippocampal subregional volumes and may be useful in clinical applications including as an endpoint for future clinical trials of conditions affecting the hippocampus. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(20)30050-1  |  
------------------------------------------- 
10.1021/acs.jpclett.9b03875  |   Two-dimensional van der Waals heterostructure materials, particularly transition metal dichalcogenides (TMDC), have proved to be excellent photoabsorbers for solar radiation, but performance for such electrocatalysis processes as water splitting to form H<sub>2</sub> and O<sub>2</sub> is not adequate. We propose that dramatically improved performance may be achieved by combining two independent TMDC while optimizing such descriptors as rotational angle, bond length, distance between layers, and the ratio of the bandgaps of two component materials. In this paper we apply the least absolute shrinkage and selection operator (LASSO) process of artificial intelligence incorporating these descriptors together with quantum mechanics (density functional theory) to predict novel structures with predicted superior performance. Our predicted best system is MoTe<sub>2</sub>/WTe<sub>2</sub> with a rotation of 300°, which is predicted to have an overpotential of 0.03 V for HER and 0.17 V for OER, dramatically improved over current electrocatalysts for water splitting. 
  |  https://dx.doi.org/10.1021/acs.jpclett.9b03875  |  
------------------------------------------- 
10.1097/RLI.0000000000000640  |    Objectives:  The aim of the study was to implement a deep-learning tool to produce synthetic double inversion recovery (synthDIR) images and compare their diagnostic performance to conventional sequences in patients with multiple sclerosis (MS). 
  Materials and methods:  For this retrospective analysis, 100 MS patients (65 female, 37 [22-68] years) were randomly selected from a prospective observational cohort between 2014 and 2016. In a subset of 50 patients, an artificial neural network (DiamondGAN) was trained to generate a synthetic DIR (synthDIR) from standard acquisitions (T1, T2, and fluid-attenuated inversion recovery [FLAIR]). With the resulting network, synthDIR was generated for the remaining 50 subjects. These images as well as conventionally acquired DIR (trueDIR) and FLAIR images were assessed for MS lesions by 2 independent readers, blinded to the source of the DIR image. Lesion counts in the different modalities were compared using a Wilcoxon signed-rank test, and interrater analysis was performed. Contrast-to-noise ratios were compared for objective image quality. 
  Results:  Utilization of synthDIR allowed to detect significantly more lesions compared with the use of FLAIR images (31.4 ± 20.7 vs 22.8 ± 12.7, P &lt; 0.001). This improvement was mainly attributable to an improved depiction of juxtacortical lesions (12.3 ± 10.8 vs 7.2 ± 5.6, P &lt; 0.001). Interrater reliability was excellent in FLAIR 0.92 (95% confidence interval [CI], 0.85-0.95), synthDIR 0.93 (95% CI, 0.87-0.96), and trueDIR 0.95 (95% CI, 0.85-0.98).Contrast-to-noise ratio in synthDIR exceeded that of FLAIR (22.0 ± 6.4 vs 16.7 ± 3.6, P = 0.009); no significant difference was seen in comparison to trueDIR (22.0 ± 6.4 vs 22.4 ± 7.9, P = 0.87). 
  Conclusions:  Computationally generated DIR images improve lesion depiction compared with the use of standard modalities. This method demonstrates how artificial intelligence can help improving imaging in specific pathologies. 
  |  http://dx.doi.org/10.1097/RLI.0000000000000640  |  
------------------------------------------- 
10.1088/1361-6560/ab79c3  |   Accurate segmentation of organs-at-risk (OARs) is necessary for adaptive head and neck (H&amp;N) cancer treatment planning but manual delineation is tedious, slow, and inconsistent. A Self-Channel-and-Spatial-Attention neural network (SCSA-Net) is developed for H&amp;N OARs segmentation on CT images. To simultaneously ease the training and improve the segmentation performance, the proposed SCSA-Net utilizes the self-attention ability of the network. Spatial and channel-wise attention learning mechanisms are both employed to adaptively force the network to emphasize on the meaningful features and weaken the irrelevant features simultaneously. The proposed network was first evaluated on a public dataset, which includes 48 patients, then on a separate serial CT dataset, which contains ten patients who received weekly diagnostic fan-beam CT scans. On the second dataset, the accuracy of using SCSA-Net to track the parotid and submandibular gland volume changes during radiotherapy treatment was quantified. Dice similarity coefficient (DSC), positive predictive value (PPV), sensitivity (SEN), average surface distance (ASD), and 95%maximum surface distance (95SD) were calculated on the brainstem, optic chiasm, optic nerves, mandible, parotid glands, and submandibular glands to evaluate the proposed SCSA-Net. The proposed SCSA-Net consistently outperforms the state-of-the-art methods on the public dataset. Specifically, compared with the Res-Net and SE-Net, which is constructed by the Squeeze-and-Excitation block equipped Residual blocks, the DSC of the optic nerves and submandibular glands is improved by 0.06, 0.03 and 0.05, 0.04 by the SCSA-Net. Moreover, the proposed method achieves statistically significant improvements in terms of DSC on all and 8 of 9 OARs over Res-Net and SE-Net, respectively. The trained network was able to achieve good segmentation results on the serial dataset, but the results were further improved after fine-tuning of the model using the simulation CT images. For the parotids and submandibular glands, the volume changes of individual patients are highly consistent between the automated and manual segmentation (Pearson's Correlation 0.97-0.99). The proposed SCSA-Net is computationally efficient to perform segmentation (~2 seconds/CT). 
  |  https://doi.org/10.1088/1361-6560/ab79c3  |  
------------------------------------------- 
10.1007/s00774-019-01055-3  |    Introduction:  Currently, osteoarthritis (OA) receives global increasing attention because it associates severe joint pain and serious disability. Stem cells intra-articular injection therapy showed a potential therapeutic superiority to reduce OA development and to improve treating outputs. However, the long-term effect of stem cells intra-articular injection on the cartilage regeneration remains unclear. Recently, miR-140-5p was confirmed as a critical positive regulator in chondrogenesis. We hypothesized that hUC-MSCs overexpressing miR-140-5p have better therapeutic effect on osteoarthritis. 
  Materials and methods:  To enhance stem cell chondrogenic differentiation, we have transfected human umbilical cord mesenchymal stem cells (hUC-MSCs) with miR-140-5p mimics and miR-140-5p lentivirus to overexpress miR-140-5p in a short term or a long term accordingly. Thereafter, MSCs proliferation, chondrogenic genes expression and extracellular matrix were assessed. Destabilization of the medial meniscus (DMM) surgery was performed on the knee joints of SD rats as an OA model, and then intra-articular injection of hUC-MSCs or hUC-MSCs transfected with miR-140-5p lentivirus was carried to evaluate the cartilage healing effect with histological staining and OARSI scores. The localization of hUC-MSCs after intra-articular injection was further confirmed by immunohistochemical staining. 
  Results:  Significant induction of chondrogenic differentiation in the miR-140-5p-hUC-MSCs (140-MSCs), while its proliferation was not influenced. Interestingly, intra-articular injection of 140-MSCs significantly enhanced articular cartilage self-repairing in comparison to normal hUC-MSCs. Moreover, we noticed that intra-articular injection of high 140-MSCs numbers reinforces cells assembling on the impaired cartilage surface and subsequently differentiated into chondrocytes. 
  Conclusions:  In conclusion, these results indicate therapeutic superiority of hUC-MSCs overexpressing miR-140-5p to treat OA using intra-articular injection. 
  |  https://dx.doi.org/10.1007/s00774-019-01055-3  |  
------------------------------------------- 
10.1007/s00125-019-05023-4  |    Aims/hypothesis:  Corneal confocal microscopy is a rapid non-invasive ophthalmic imaging technique that identifies peripheral and central neurodegenerative disease. Quantification of corneal sub-basal nerve plexus morphology, however, requires either time-consuming manual annotation or a less-sensitive automated image analysis approach. We aimed to develop and validate an artificial intelligence-based, deep learning algorithm for the quantification of nerve fibre properties relevant to the diagnosis of diabetic neuropathy and to compare it with a validated automated analysis program, ACCMetrics. 
  Methods:  Our deep learning algorithm, which employs a convolutional neural network with data augmentation, was developed for the automated quantification of the corneal sub-basal nerve plexus for the diagnosis of diabetic neuropathy. The algorithm was trained using a high-end graphics processor unit on 1698 corneal confocal microscopy images; for external validation, it was further tested on 2137 images. The algorithm was developed to identify total nerve fibre length, branch points, tail points, number and length of nerve segments, and fractal numbers. Sensitivity analyses were undertaken to determine the AUC for ACCMetrics and our algorithm for the diagnosis of diabetic neuropathy. 
  Results:  The intraclass correlation coefficients for our algorithm were superior to those for ACCMetrics for total corneal nerve fibre length (0.933 vs 0.825), mean length per segment (0.656 vs 0.325), number of branch points (0.891 vs 0.570), number of tail points (0.623 vs 0.257), number of nerve segments (0.878 vs 0.504) and fractals (0.927 vs 0.758). In addition, our proposed algorithm achieved an AUC of 0.83, specificity of 0.87 and sensitivity of 0.68 for the classification of participants without (n = 90) and with (n = 132) neuropathy (defined by the Toronto criteria). 
  Conclusions/interpretation:  These results demonstrated that our deep learning algorithm provides rapid and excellent localisation performance for the quantification of corneal nerve biomarkers. This model has potential for adoption into clinical screening programmes for diabetic neuropathy. 
  Data availability:  The publicly shared cornea nerve dataset (dataset 1) is available at http://bioimlab.dei.unipd.it/Corneal%20Nerve%20Tortuosity%20Data%20Set.htm and http://bioimlab.dei.unipd.it/Corneal%20Nerve%20Data%20Set.htm. 
  |  https://doi.org/10.1007/s00125-019-05023-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31720728/  |  
------------------------------------------- 
10.1016/j.gie.2020.04.048  |    Background and aims:  The abrupt outbreak of COVID-19 and its rapid spread over many health care systems in the world led to personal protective equipment (PPE) shortening, which cannot be faced only by the reduction in their consumption nor by the expensive and time-requiring implementation of their production. It is thus necessary to promote PPE rational use, highlighting possible differences in terms of efficacy among them and promoting an effective technique to reuse them. 
  Methods:  A literature search was performed on PubMed, Scopus, Cochrane database, and Google Scholar and from 25 top cited papers, 15 were selected for relevance and impact. 
  Results:  Most studies on prior respiratory virus epidemic to date suggest surgical masks not to be inferior compared with N95 respirators in terms of protective efficacy among health care workers. The use of N95 respirators should be then limited in favor of high-risk situations. Concerning respirators reuse, highly energetic short-wave ultraviolet germicidal irradiation (UVGI) at 254 nm was proficiently applied to determine N95 respirators decontamination from viral respiratory agents, but it requires careful consideration of the type of respirator and of the biological target. 
  Conclusions:  Rational use and successful reuse of respirators can help facing PPE shortening during a pandemic. Further evidences testing UVGI and other decontamination techniques are an unmet need. The definitive answer to pandemic issues can be found in artificial intelligence and deep learning: these groundbreaking modalities could help in identifying high-risk patients and in suggesting appropriate types and use of PPE. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)34247-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32353457/  |  
------------------------------------------- 
10.3390/s20071962  |   Wireless sensor network and industrial internet of things have been a growing area of research which is exploited in various fields such as smart home, smart industries, smart transportation, and so on. There is a need of a mechanism which can easily tackle the problems of nonlinear delay integro-differential equations for large-scale applications of Internet of Things. In this paper, Haar wavelet collocation technique is developed for the solution of nonlinear delay integro-differential equations for wireless sensor network and industrial Internet of Things. The method is applied to nonlinear delay Volterra, delay Fredholm and delay Volterra-Fredholm integro-differential equations which are based on the use of Haar wavelets. Some examples are given to show the computational efficiency of the proposed technique. The approximate solutions are compared with the exact solution. The maximum absolute and mean square roots errors for distant number of collocation points are also calculated. The results show that Haar method is efficient for solving these equations for industrial Internet of Things. The results are compared with existing methods from the literature. The results exhibit that the method is simple, precise and efficient. 
  |  http://www.mdpi.com/resolver?pii=s20071962  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32244450/  |  
------------------------------------------- 
10.1016/j.neunet.2020.04.014  |   It was recently found that dendrites are not just a passive channel. They can perform mixed computation of analog and digital signals, and therefore can be abstracted as information processors. Moreover, dendrites possess a feedback mechanism. Motivated by these computational and feedback characteristics, this article proposes a new variant of neural-like P systems, dendrite P (DeP) systems, where neurons simulate the computational function of dendrites and perform a firing-storing process instead of the storing-firing process in spiking neural P (SNP) systems. Moreover, the behavior of the neurons is characterized by dendrite rules that are abstracted by two characteristics of dendrites. Different from the usual firing rules in SNP systems, the firing of a dendrite rule is controlled by the states of the corresponding source neurons. Therefore, DeP systems can provide a collaborative control capability for neurons. We discuss the computational power of DeP systems. In particular, it is proven that DeP systems are Turing-universal number generating/accepting devices. Moreover, we construct a small universal DeP system consisting of 115 neurons for computing functions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30134-9  |  
------------------------------------------- 
10.2217/cer-2019-0117  |   <b>Aim:</b> Evaluate the cost-effectiveness of ocriplasmin in symptomatic vitreomacular adhesion (VMA) with or without full-thickness macular hole ≤400 μm versus standard of care. <b>Methods:</b> A state-transition model simulated a cohort through disease health states; assignment of utilities to health states reflected the distribution of visual acuity. Efficacy of ocriplasmin was derived from logistic regression models using Ocriplasmin for Treatment for Symptomatic Vitreomacular Adhesion Including Macular Hole trial data. Model inputs were extracted from Phase III trials and published literature. The analysis was conducted from a US Medicare perspective. <b>Results:</b> Lifetime incremental cost-effectiveness ratio was US$4887 per quality-adjusted life year gained in the total population, US$4255 and US$10,167 in VMA subgroups without and with full-thickness macular hole, respectively. <b>Conclusion:</b> Ocriplasmin was cost effective compared with standard of care in symptomatic VMA. 
  |  https://www.futuremedicine.com/doi/10.2217/cer-2019-0117?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  
------------------------------------------- 
10.1007/s00234-019-02330-w  |    Purpose:  To analyze the implementation of deep learning software for the detection and worklist prioritization of acute intracranial hemorrhage on non-contrast head CT (NCCT) in various clinical settings at an academic medical center. 
  Methods:  Urgent NCCT scans were reviewed by the Aidoc (Tel Aviv, Israel) neural network software. All cases flagged by the software as positive for acute intracranial hemorrhage on the neuroradiology worklist were prospectively included in this assessment. The scans were classified regarding presence and type of hemorrhage, whether these were initial or follow-up scans, and patient visit location, including trauma/emergency, inpatient, and outpatient departments. 
  Results:  During the 2 months of enrollment, 373 NCCT scans were flagged by the Aidoc software for possible intracranial hemorrhage out of 2011 scans analyzed (18.5%). Among the flagged cases, 275 (72.4%) were positive; 290 (77.7%) were inpatient cases, 75 (20.1%) were trauma/emergency cases, and eight (2.1%) were outpatient cases, and 229 of 373 (62.5%) were follow-up cases, of which 219 (95.6%) inpatient cases. Among the 144 new cases flagged for hemorrhage, 66 (44.4%) were positive, of which 39 (58.2%) were trauma/emergency cases. The overall sensitivity, specificity, positive predictive value, negative predictive value, and accuracy were 88.7%, 94.2% and 73.7%, 97.7%, and 93.4%, respectively. The accuracy of the intracranial hemorrhage detection was significantly higher for emergency cases than for inpatient cases (96.5% versus 89.4%). 
  Conclusion:  This study reveals that the performance of the deep learning software for acute intracranial hemorrhage detection varies depending upon the patient visit location. Furthermore, a substantial portion of flagged cases were follow-up exams, the majority of which were inpatient exams. These findings can help optimize the artificial intelligence-driven clincical workflow. 
  |  https://dx.doi.org/10.1007/s00234-019-02330-w  |  
------------------------------------------- 
10.1093/bioinformatics/btz895  |    Motivation:  The precise targeting of antibodies and other protein therapeutics is required for their proper function and the elimination of deleterious off-target effects. Often the molecular structure of a therapeutic target is unknown and randomized methods are used to design antibodies without a model that relates antibody sequence to desired properties. 
  Results:  Here, we present Ens-Grad, a machine learning method that can design complementarity determining regions of human Immunoglobulin G antibodies with target affinities that are superior to candidates derived from phage display panning experiments. We also demonstrate that machine learning can improve target specificity by the modular composition of models from different experimental campaigns, enabling a new integrative approach to improving target specificity. Our results suggest a new path for the discovery of therapeutic molecules by demonstrating that predictive and differentiable models of antibody binding can be learned from high-throughput experimental data without the need for target structural data. 
  Availability and implementation:  Sequencing data of the phage panning experiment are deposited at NIH's Sequence Read Archive (SRA) under the accession number SRP158510. We make our code available at https://github.com/gifford-lab/antibody-2019. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz895  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31778140/  |  
------------------------------------------- 
10.1093/bib/bby121  |   Genome duplication with hybridization, or allopolyploidization, occurs in animals, fungi and plants, and is especially common in crop plants. There is an increasing interest in the study of allopolyploids because of advances in polyploid genome assembly; however, the high level of sequence similarity in duplicated gene copies (homeologs) poses many challenges. Here we compared standard RNA-seq expression quantification approaches used currently for diploid species against subgenome-classification approaches which maps reads to each subgenome separately. We examined mapping error using our previous and new RNA-seq data in which a subgenome is experimentally added (synthetic allotetraploid Arabidopsis kamchatica) or reduced (allohexaploid wheat Triticum aestivum versus extracted allotetraploid) as ground truth. The error rates in the two species were very similar. The standard approaches showed higher error rates (&gt;10% using pseudo-alignment with Kallisto) while subgenome-classification approaches showed much lower error rates (&lt;1% using EAGLE-RC, &lt;2% using HomeoRoq). Although downstream analysis may partly mitigate mapping errors, the difference in methods was substantial in hexaploid wheat, where Kallisto appeared to have systematic differences relative to other methods. Only approximately half of the differentially expressed homeologs detected using Kallisto overlapped with those by any other method in wheat. In general, disagreement in low-expression genes was responsible for most of the discordance between methods, which is consistent with known biases in Kallisto. We also observed that there exist uncertainties in genome sequences and annotation which can affect each method differently. Overall, subgenome-classification approaches tend to perform better than standard approaches with EAGLE-RC having the highest precision. 
  |  https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bby121  |  
------------------------------------------- 
10.1073/pnas.1913003117  |   Genome-scale technologies have enabled mapping of the complex molecular networks that govern cellular behavior. An emerging theme in the analyses of these networks is that cells use many layers of regulatory feedback to constantly assess and precisely react to their environment. The importance of complex feedback in controlling the real-time response to external stimuli has led to a need for the next generation of cell-based technologies that enable both the collection and analysis of high-throughput temporal data. Toward this end, we have developed a microfluidic platform capable of monitoring temporal gene expression from over 2,000 promoters. By coupling the "Dynomics" platform with deep neural network (DNN) and associated explainable artificial intelligence (XAI) algorithms, we show how machine learning can be harnessed to assess patterns in transcriptional data on a genome scale and identify which genes contribute to these patterns. Furthermore, we demonstrate the utility of the Dynomics platform as a field-deployable real-time biosensor through prediction of the presence of heavy metals in urban water and mine spill samples, based on the the dynamic transcription profiles of 1,807 unique <i>Escherichia coli</i> promoters. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=31974311  |  
------------------------------------------- 
10.1089/cmb.2019.0505  |   By using next-generation sequencing technologies, it is possible to quickly and inexpensively generate large numbers of relatively short reads from both the nuclear and mitochondrial DNA (mtDNA) contained in a biological sample. Unfortunately, assembling such whole-genome sequencing (WGS) data with standard de novo assemblers often fails to generate high-quality mitochondrial genome sequences due to the large difference in copy number (and hence sequencing depth) between the mitochondrial and nuclear genomes. Assembly of complete mitochondrial genome sequences is further complicated by the fact that many de novo assemblers are not designed for circular genomes and by the presence of repeats in the mitochondrial genomes of some species. In this article, we describe the Statistical Mitogenome Assembly with RepeaTs (SMART) pipeline for automated assembly of mitochondrial genomes from WGS data. SMART uses an efficient coverage-based filter to first select a subset of reads enriched in mtDNA sequences. Contigs produced by an initial assembly step are filtered using the Basic Local Alignment Search Tool searches against a comprehensive mitochondrial genome database and are used as "baits" for an alignment-based filter that produces the set of reads used in a second de novo assembly and scaffolding step. In the presence of repeats, the possible paths through the assembly graph are evaluated using a maximum likelihood model. Additionally, the assembly process is repeated for a user-specified number of times on resampled subsets of reads to select for annotation of the reconstructed sequences with highest bootstrap support. Experiments on WGS data sets from a variety of species show that the SMART pipeline produces complete circular mitochondrial genome sequences with a higher success rate than current state-of-the-art tools, particularly for low-coverage WGS data sets. 
  |  https://www.liebertpub.com/doi/full/10.1089/cmb.2019.0505?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.oraloncology.2020.104622  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1368-8375(20)30058-0  |  
------------------------------------------- 
10.1098/rstb.2019.0329  |   A few billion years have passed since the first life forms appeared. Since then, life has continued to forge complex associations between the different emergent levels of interconnection it forms. The advances of recent decades in molecular chemistry and theoretical biology, which have embraced complex systems approaches, now make it possible to conceptualize the questions of the origins of life and its increasing complexity from three complementary notions of closure: processes closure, autocatalytic closure and constraints closure. Developed in the wake of the second-order cybernetics, this triple closure approach, that relies on graph theory and complex networks science, sketch a paradigm where it is possible to go up the physical levels of organization of matter, from physics to biology and society, without resorting to strong reductionism. The phenomenon of life is conceived as the contingent complexification of the organization of matter, until the emergence of life forms, defined as a network of auto-catalytic process networks, organized in a multi-level manner. This approach of living systems, initiated by Maturana &amp; Varela and Kauffman, inevitably leads to a reflection on the nature of cognition; and in the face of the deep changes that affected humanity as a complex systems, on the nature of cultural evolution. Faced with the major challenges that humanity will have to address in the decades to come, this new paradigm invites us to change our conception of causality by shifting our attention from state change to process change and to abandon a widespread notion of 'local' causality in favour of complex systems thinking. It also highlights the importance of a better understanding of the influence of social networks, recommendation systems and artificial intelligence on our future collective dynamics and social cognition processes. This article is part of the theme issue 'Unifying the essential concepts of biological networks: biological insights and philosophical foundations'. 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0329?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1021/acs.langmuir.9b03773  |   Robotics is a frontal interdisciplinary subject across the fields of mechanical engineering, chemical and materials engineering, artificial intelligence, and nanotechnology. Robotic devices with a variety of frameworks, functionalities, and actuation modes have been developed and employed in the manufacture of advanced materials and devices with improved efficiency and automation. In recent years, soft robots have attracted a significant amount of interest among scientific researchers and technological engineers because they can offer the desired safety, adaptability, sensibility, and dexterity that conventional robotics cannot deliver. To date, emulating living creatures in nature has been a promising approach to design soft robots. For living creatures, both body deformation and their surface characteristic are essential for them to function in dynamic ecological environments. Body deformation offers athletic ability while surface characteristics provide extraordinary adaptable interactions with the environment. In this article, we discuss the recent progress of emulating the body deformation of living creatures such as shrinking/expanding, bending, and twisting and programmable deformations based on the manipulation of shape-changing behaviors of liquid-crystal polymeric materials (LCPs) and the interfacial technologies to build up various microstructures similar to the interface of living creatures. We further review the pioneering work that integrates interfacial engineering and the shape-changing modulation of LCPs to develop biomimetic soft robotic devices. We also provide an outlook for opportunities and challenges in the design and fabrication of advanced biomimetic soft robots based on the synergetic combination of interfacial engineering and shape-changing modulation. 
  |  https://dx.doi.org/10.1021/acs.langmuir.9b03773  |  
------------------------------------------- 
10.1002/mp.14106  |    Purpose:  Four-dimensional (4D) computed tomography (CT) imaging is an essential part of current 4D radiotherapy treatment planning workflows, but clinical 4D CT images are often affected by artifacts. The artifacts are mainly caused by breathing irregularity during data acquisition, which leads to projection data coverage issues for currently available commercial 4D CT protocols. It was proposed to improve projection data coverage by online respiratory signal analysis and signal-guided CT tube control, but related work was always theoretical and presented as pure in silico studies. The present work demonstrates a first CT prototype implementation along with respective phantom measurements for the recently introduced intelligent 4D CT (i4DCT) sequence scanning concept (https://doi.org/10.1002/mp.13632). 
  Methods:  Intelligent 4D CT was implemented on the Siemens SOMATOM go platform. Four-dimensional CT measurements were performed using the CIRS motion phantom. Motion curves were programmed to systematically vary from regular to very irregular, covering typical irregular patterns that are known to result in image artifacts using standard 4D CT imaging protocols. Corresponding measurements were performed using i4DCT and routine spiral 4D CT with similar imaging parameters (e.g., mAs setting and gantry rotation time, retrospective ten-phase reconstruction) to allow for a direct comparison of the image data. 
  Results:  Following technological implementation of i4DCT on the clinical CT scanner platform, 4D CT motion artifacts were significantly reduced for all investigated levels of breathing irregularity when compared to routine spiral 4D CT scanning. 
  Conclusions:  The present study confirms feasibility of fully automated respiratory signal-guided 4D CT scanning by means of a first implementation of i4DCT on a CT scanner. The measurements thereby support the conclusions of respective in silico studies and demonstrate that respiratory signal-guided 4D CT (here: i4DCT) is ready for integration into clinical CT scanners. 
  |  https://doi.org/10.1002/mp.14106  |  
------------------------------------------- 
10.3390/pathogens9050324  |   New coronavirus (SARS-CoV-2) treatments and vaccines are under development to combat COVID-19. Several approaches are being used by scientists for investigation, including (1) various small molecule approaches targeting RNA polymerase, 3C-like protease, and RNA endonuclease; and (2) exploration of antibodies obtained from convalescent plasma from patients who have recovered from COVID-19. The coronavirus genome is highly prone to mutations that lead to genetic drift and escape from immune recognition; thus, it is imperative that sub-strains with different mutations are also accounted for during vaccine development. As the disease has grown to become a pandemic, B-cell and T-cell epitopes predicted from SARS coronavirus have been reported. Using the epitope information along with variants of the virus, we have found several variants which might cause drifts. Among such variants, 23403A&gt;G variant (p.D614G) in spike protein B-cell epitope is observed frequently in European countries, such as the Netherlands, Switzerland, and France, but seldom observed in China. 
  |  http://www.mdpi.com/resolver?pii=pathogens9050324  |  
------------------------------------------- 
10.1136/gutjnl-2019-320065  |   IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research. 
  |  http://gut.bmj.com/cgi/pmidlookup?view=long&pmid=32111636  |  
------------------------------------------- 
10.12659/MSM.923836  |   BACKGROUND This study aimed to compare multiple quantitative evaluation indices of levels of theoretical knowledge and clinical practice skills in training medical interns in cardiovascular imaging based on the use of the blended teaching (BT) online artificial intelligence (AI) case resource network platform (CRNP), including time and frequency indices and effectiveness of the CRNP. MATERIAL AND METHODS The study included 110 medical interns who were divided into the routine teaching (RT) group (n=55) and the blended teaching (BT) group (n=55). The two were assessed using the mini-clinical evaluation exercise (mini-CEX) that assessed clinical skills, attitudes, and behaviors and using an objective written questionnaire. The following four indices were compared between the RT and BT groups: the X-ray score (XS), the computed tomography angiography (CTA) score (CS), the cardiac magnetic resonance imaging (CMRI) score (MS), and the average score (AS). Seven assessment indicators included: the imaging description (ID), the qualitative diagnosis (QD), the differential diagnosis (DD), examination preparation (EP), interview skill (IS), position display (PD), and human care (HC). Indicators of CRNP use included: number of times (TN), average duration (AD), single maximum duration (SMD), and total duration (TD). RESULTS AS significantly correlated with AD (rAD=0.761) and TD (rTD=0.754), and showed moderate correlation with TN (rTN=0.595), but weak correlation with SMD (rSMD=0.404). CONCLUSIONS Levels of theoretical knowledge and clinical practice skills during medical intern training in cardiovascular imaging based on BT using the CRNP teaching technology improved theoretical knowledge and practical skills. 
  |  https://www.medscimonit.com/download/index/idArt/923836  |  
------------------------------------------- 
10.1371/journal.pone.0229978  |   Concerned about potentially increased risk of neurodegenerative disease, several health professionals and policy makers have proposed limiting or banning youth participation in American-style tackle football. Given the large affected population (over 1 million boys play high school football annually), careful estimation of the long-term health effects of playing football is necessary for developing effective public health policy. Unfortunately, existing attempts to estimate these effects tend not to generalize to current participants because they either studied a much older cohort or, more seriously, failed to account for potential confounding. We leverage data from a nationally representative cohort of American men who were in grades 7-12 in the 1994-95 school year to estimate the effect of playing football in adolescent on depression in early adulthood. We control for several potential confounders related to subjects' health, behavior, educational experience, family background, and family health history through matching and regression adjustment. We found no evidence of even a small harmful effect of football participation on scores on a version of the Center for Epidemiological Studies Depression scale (CES-D) nor did we find evidence of adverse associations with several secondary outcomes including anxiety disorder diagnosis or alcohol dependence in early adulthood. For men who were in grades 7-12 in the 1994-95 school year, participating or intending to participate in school football does not appear to be a major risk factor for early adulthood depression. 
  |  http://dx.plos.org/10.1371/journal.pone.0229978  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32155206/  |  
------------------------------------------- 
10.1001/jamaophthalmol.2020.0507  |    Importance:  Evaluating corneal morphologic characteristics with corneal tomographic scans before refractive surgery is necessary to exclude patients with at-risk corneas and keratoconus. In previous studies, researchers performed screening with machine learning methods based on specific corneal parameters. To date, a deep learning algorithm has not been used in combination with corneal tomographic scans. 
  Objective:  To examine the use of a deep learning model in the screening of candidates for refractive surgery. 
  Design, setting, and participants:  A diagnostic, cross-sectional study was conducted at the Zhongshan Ophthalmic Center, Guangzhou, China, with examination dates extending from July 18, 2016, to March 29, 2019. The investigation was performed from July 2, 2018, to June 28, 2019. Participants included 1385 patients; 6465 corneal tomographic images were used to generate the artificial intelligence (AI) model. The Pentacam HR system was used for data collection. 
  Interventions:  The deidentified images were analyzed by ophthalmologists and the AI model. 
  Main outcomes and measures:  The performance of the AI classification system. 
  Results:  A classification system centered on the AI model Pentacam InceptionResNetV2 Screening System (PIRSS) was developed for screening potential candidates for refractive surgery. The model achieved an overall detection accuracy of 94.7% (95% CI, 93.3%-95.8%) on the validation data set. Moreover, on the independent test data set, the PIRSS model achieved an overall detection accuracy of 95% (95% CI, 88.8%-97.8%), which was comparable with that of senior ophthalmologists who are refractive surgeons (92.8%; 95% CI, 91.2%-94.4%) (P = .72). In distinguishing corneas with contraindications for refractive surgery, the PIRSS model performed better than the classifiers (95% vs 81%; P &lt; .001) in the Pentacam HR system on an Asian patient database. 
  Conclusions and relevance:  PIRSS appears to be useful in classifying images to provide corneal information and preliminarily identify at-risk corneas. PIRSS may provide guidance to refractive surgeons in screening candidates for refractive surgery as well as for generalized clinical application for Asian patients, but its use needs to be confirmed in other populations. 
  |  https://jamanetwork.com/journals/jamaophthalmology/fullarticle/10.1001/jamaophthalmol.2020.0507  |  
------------------------------------------- 
10.1016/j.gie.2020.02.033  |    Background and aims:  We perform a meta-analysis of all published studies to determine the diagnostic accuracy of AI on histology prediction and detection of colorectal polyps. 
  Method:  We searched Embase, PubMed, Medline, Web of Science and Cochrane library databases to identify studies using AI for colorectal polyp histology prediction and detection. The quality of the included studies was measured by the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool. We used a bivariate meta-analysis following a random effects model to summarize the data and plotted hierarchical summary receiver-operating characteristic (HSROC) curves. The area under the HSROC curve (AUC) served as an indicator of the diagnostic accuracy and during head-to-head comparison. 
  Result:  A total of 7,680 images of colorectal polyps from 18 studies were included in the analysis of histology prediction. The accuracy of the AI (AUC) was 0.96 (95% CI, 0.95-0.98), with corresponding pooled sensitivity of 92.3% (95% CI, 88.8%-94.9%) and specificity of 89.8% (95% CI, 85.3%-93.0%). The AUC of AI using narrow-band imaging (NBI) was significantly higher than non-NBI (0.98 vs 0.84, p&lt;0.01). The performance of AI was superior to nonexpert endoscopists (0.97 vs 0.90, p&lt;0.01). For characterization of diminutive polyps using deep learning model with non-magnifying NBI, the pooled negative predictive value was 95.1% (95% CI, 87.7%-98.1%). For polyp detection, the pooled AUC was 0.90 (95% CI, 0.67-1.00) with sensitivity of 95.0% (95% CI, 91.0%-97.0%) and specificity of 88.0% (95% CI, 58.0%-99.0%). 
  Conclusion:  AI was accurate in histology prediction and detection of colorectal polyps, including diminutive polyps. The performance of AI was better under NBI and was superior to non-expert endoscopists. Despite the difference in AI models and study designs, the AI performances are rather consistent, which could serve a reference for future AI studies. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)30209-1  |  
------------------------------------------- 
10.3390/s20061796  |   Health monitoring and its related technologies is an attractive research area. The electrocardiogram (ECG) has always been a popular measurement scheme to assess and diagnose cardiovascular diseases (CVDs). The number of ECG monitoring systems in the literature is expanding exponentially. Hence, it is very hard for researchers and healthcare experts to choose, compare, and evaluate systems that serve their needs and fulfill the monitoring requirements. This accentuates the need for a verified reference guiding the design, classification, and analysis of ECG monitoring systems, serving both researchers and professionals in the field. In this paper, we propose a comprehensive, expert-verified taxonomy of ECG monitoring systems and conduct an extensive, systematic review of the literature. This provides evidence-based support for critically understanding ECG monitoring systems' components, contexts, features, and challenges. Hence, a generic architectural model for ECG monitoring systems is proposed, an extensive analysis of ECG monitoring systems' value chain is conducted, and a thorough review of the relevant literature, classified against the experts' taxonomy, is presented, highlighting challenges and current trends. Finally, we identify key challenges and emphasize the importance of smart monitoring systems that leverage new technologies, including deep learning, artificial intelligence (AI), Big Data and Internet of Things (IoT), to provide efficient, cost-aware, and fully connected monitoring systems. 
  |  http://www.mdpi.com/resolver?pii=s20061796  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32213969/  |  
------------------------------------------- 
10.1371/journal.pone.0228534  |   The core element of machine learning is a flexible, universal function approximator that can be trained and fit into the data. One of the main challenges in modern machine learning is to understand the role of nonlinearity and complexity in these universal function approximators. In this research, we focus on nonlinear complex systems, and show their capability in representation and learning of different functions. Complex nonlinear dynamics and chaos naturally yield an almost infinite diversity of dynamical behaviors and functions. Physical, biological and engineered systems can utilize this diversity to implement adaptive, robust behaviors and operations. A nonlinear dynamical system can be considered as an embodiment of a collection of different possible behaviors or functions, from which different behaviors or functions can be chosen as a response to different conditions or problems. This process of selection can be manual in the sense that one can manually pick and choose the right function through directly setting parameters. Alternatively, we can automate the process and allow the system itself learn how to do it. This creates an approach to machine learning, wherein the nonlinear dynamics represents and embodies different possible functions, and it learns through training how to pick the right function from this function space. We report on how we utilized nonlinear dynamics and chaos to design and fabricate nonlinear dynamics based, morphable hardware in silicon as a physical embodiment for different possible functions. We demonstrate how this flexible, morphable hardware learns through learning and searching algorithms such as genetic algorithm to implement different desired functions. In this approach, we combine two powerful natural and biological phenomenon, Darwinian evolution and nonlinear dynamics and chaos, as a dynamics-oriented approach to designing intelligent, adaptive systems with applications. Nonlinear dynamics embodies different functions at the hardware level, while an evolutionary method is utilized in order to find the parameters to implement the right function. 
  |  http://dx.plos.org/10.1371/journal.pone.0228534  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32126089/  |  
------------------------------------------- 
10.1002/1873-3468.13782  |   Identifying disease-related metabolites is of great significance for the diagnosis, prevention, and treatment of disease. In this study, we propose a novel computational model of multiple-network logistic matrix factorization (MN-LMF) for predicting metabolite-disease interactions, which is especially relevant for new diseases and new metabolites. First, MN-LMF builds disease (or metabolite) similarity network by integrating heterogeneous omics data. Second, it combines these similarities with known metabolite-disease interaction networks, using modified logistic matrix factorization to predict potential metabolite-disease interactions. Experimental results show that MN-LMF accurately predicts metabolite-disease interactions, and outperforms other state-of-the-art methods. Moreover, case studies also demonstrated the effectiveness of the model to infer unknown metabolite-disease interactions for novel diseases without any known associations. 
  |  https://doi.org/10.1002/1873-3468.13782  |  
------------------------------------------- 
10.1073/pnas.1914598117  |   There is a significant conceptual gap between legal and mathematical thinking around data privacy. The effect is uncertainty as to which technical offerings meet legal standards. This uncertainty is exacerbated by a litany of successful privacy attacks demonstrating that traditional statistical disclosure limitation techniques often fall short of the privacy envisioned by regulators. We define "predicate singling out," a type of privacy attack intended to capture the concept of singling out appearing in the General Data Protection Regulation (GDPR). An adversary predicate singles out a dataset x using the output of a data-release mechanism [Formula: see text] if it finds a predicate p matching exactly one row in x with probability much better than a statistical baseline. A data-release mechanism that precludes such attacks is "secure against predicate singling out" (<i>PSO secure</i>). We argue that PSO security is a mathematical concept with legal consequences. Any data-release mechanism that purports to "render anonymous" personal data under the GDPR must prevent singling out and, hence, must be PSO secure. We analyze the properties of PSO security, showing that it fails to compose. Namely, a combination of more than logarithmically many exact counts, each individually PSO secure, facilitates predicate singling out. Finally, we ask whether differential privacy and k-anonymity are PSO secure. Leveraging a connection to statistical generalization, we show that differential privacy implies PSO security. However, and in contrast with current legal guidance, k-anonymity does not: There exists a simple predicate singling out attack under mild assumptions on the k-anonymizer and the data distribution. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=32234789  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32234789/  |  
------------------------------------------- 
10.3390/ijerph17082749  |   Shallow landslides damage buildings and other infrastructure, disrupt agriculture practices, and can cause social upheaval and loss of life. As a result, many scientists study the phenomenon, and some of them have focused on producing landslide susceptibility maps that can be used by land-use managers to reduce injury and damage. This paper contributes to this effort by comparing the power and effectiveness of five machine learning, benchmark algorithms-Logistic Model Tree, Logistic Regression, Naïve Bayes Tree, Artificial Neural Network, and Support Vector Machine-in creating a reliable shallow landslide susceptibility map for Bijar City in Kurdistan province, Iran. Twenty conditioning factors were applied to 111 shallow landslides and tested using the One-R attribute evaluation (ORAE) technique for modeling and validation processes. The performance of the models was assessed by statistical-based indexes including sensitivity, specificity, accuracy, mean absolute error (MAE), root mean square error (RMSE), and area under the receiver operatic characteristic curve (AUC). Results indicate that all the five machine learning models performed well for shallow landslide susceptibility assessment, but the Logistic Model Tree model (AUC = 0.932) had the highest goodness-of-fit and prediction accuracy, followed by the Logistic Regression (AUC = 0.932), Naïve Bayes Tree (AUC = 0.864), ANN (AUC = 0.860), and Support Vector Machine (AUC = 0.834) models. Therefore, we recommend the use of the Logistic Model Tree model in shallow landslide mapping programs in semi-arid regions to help decision makers, planners, land-use managers, and government agencies mitigate the hazard and risk. 
  |  http://www.mdpi.com/resolver?pii=ijerph17082749  |  
------------------------------------------- 
10.1001/jamaophthalmol.2019.5413  |    Importance:  Although the central visual field (VF) in end-stage glaucoma may substantially vary among patients, structure-function studies and quality-of-life assessments are impeded by the lack of appropriate characterization of end-stage VF loss. 
  Objective:  To provide a quantitative characterization and classification of central VF loss in end-stage glaucoma. 
  Design, setting, and participants:  This retrospective cohort study collected data from 5 US glaucoma services from June 1, 1999, through October 1, 2014. A total of 2912 reliable 10-2 VFs of 1103 eyes from 1010 patients measured after end-stage 24-2 VFs with a mean deviation (MD) of -22 dB or less were included in the analysis. Data were analyzed from March 28, 2018, through May 23, 2019. 
  Main outcomes and measures:  Central VF patterns were determined by an artificial intelligence algorithm termed archetypal analysis. Longitudinal analyses were performed to investigate whether the development of central VF defect mostly affects specific vulnerability zones. 
  Results:  Among the 1103 patients with the most recent VFs, mean (SD) age was 70.4 (14.3) years; mean (SD) 10-2 MD, -21.5 (5.6) dB. Fourteen central VF patterns were determined, including the most common temporal sparing patterns (304 [27.5%]), followed by mostly nasal loss (280 [25.4%]), hemifield loss (169 [15.3%]), central island (120 [10.9%]), total loss (91 [8.3%]), nearly intact field (56 [5.1%]), inferonasal quadrant sparing (42 [3.8%]), and nearly total loss (41 [3.7%]). Location-specific median total deviation analyses partitioned the central VF into a more vulnerable superonasal zone and a less vulnerable inferotemporal zone. At 1-year and 2-year follow-up, new defects mostly occurred in the more vulnerable zone. Initial encroachments on an intact central VF at follow-up were more likely to be from nasal loss (11 [18.4%]; P &lt; .001). One of the nasal loss patterns had a substantial chance at 2-year follow-up (8 [11.0%]; P = .004) to shift to total loss, whereas others did not. 
  Conclusions and relevance:  In this study, central VF loss in end-stage glaucoma was found to exhibit characteristic patterns that might be associated with different subtypes. Initial central VF loss is likely to be nasal loss, and 1 specific type of nasal loss is likely to develop into total loss. 
  |  https://jamanetwork.com/journals/jamaophthalmology/fullarticle/10.1001/jamaophthalmol.2019.5413  |  
------------------------------------------- 
10.1093/bioinformatics/btz571  |    Motivation:  A biochemical reaction, bio-event, depicts the relationships between participating entities. Current text mining research has been focusing on identifying bio-events from scientific literature. However, rare efforts have been dedicated to normalize bio-events extracted from scientific literature with the entries in the curated reaction databases, which could disambiguate the events and further support interconnecting events into biologically meaningful and complete networks. 
  Results:  In this paper, we propose BioNorm, a novel method of normalizing bio-events extracted from scientific literature to entries in the bio-molecular reaction database, e.g. IntAct. BioNorm considers event normalization as a paraphrase identification problem. It represents an entry as a natural language statement by combining multiple types of information contained in it. Then, it predicts the semantic similarity between the natural language statement and the statements mentioning events in scientific literature using a long short-term memory recurrent neural network (LSTM). An event will be normalized to the entry if the two statements are paraphrase. To the best of our knowledge, this is the first attempt of event normalization in the biomedical text mining. The experiments have been conducted using the molecular interaction data from IntAct. The results demonstrate that the method could achieve F-score of 0.87 in normalizing event-containing statements. 
  Availability and implementation:  The source code is available at the gitlab repository https://gitlab.com/BioAI/leen and BioASQvec Plus is available on figshare https://figshare.com/s/45896c31d10c3f6d857a. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz571  |  
------------------------------------------- 
10.1016/j.gie.2019.08.018  |    Background and aims:  We developed a system for computer-assisted diagnosis (CAD) for real-time automated diagnosis of precancerous lesions and early esophageal squamous cell carcinomas (ESCCs) to assist the diagnosis of esophageal cancer. 
  Methods:  A total of 6473 narrow-band imaging (NBI) images, including precancerous lesions, early ESCCs, and noncancerous lesions, were used to train the CAD system. We validated the CAD system using both endoscopic images and video datasets. The receiver operating characteristic curve of the CAD system was generated based on image datasets. An artificial intelligence probability heat map was generated for each input of endoscopic images. The yellow color indicated high possibility of cancerous lesion, and the blue color indicated noncancerous lesions on the probability heat map. When the CAD system detected any precancerous lesion or early ESCCs, the lesion of interest was masked with color. 
  Results:  The image datasets contained 1480 malignant NBI images from 59 consecutive cancerous cases (sensitivity, 98.04%) and 5191 noncancerous NBI images from 2004 cases (specificity, 95.03%). The area under curve was 0.989. The video datasets of precancerous lesions or early ESCCs included 27 nonmagnifying videos (per-frame sensitivity 60.8%, per-lesion sensitivity, 100%) and 20 magnifying videos (per-frame sensitivity 96.1%, per-lesion sensitivity, 100%). Unaltered full-range normal esophagus videos included 33 videos (per-frame specificity 99.9%, per-case specificity, 90.9%). 
  Conclusions:  A deep learning model demonstrated high sensitivity and specificity for both endoscopic images and video datasets. The real-time CAD system has a promising potential in the near future to assist endoscopists in diagnosing precancerous lesions and ESCCs. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(19)32187-X  |  
------------------------------------------- 
10.1017/S1352465819000651  |    Background:  It has been proposed that both positive and negative metacognitive beliefs sustain engagement in post-event processing (PEP). However, it is unknown: (1) whether individuals with social anxiety disorder (SAD) actually derive the benefits from PEP that they expect; (2) if this is not the case, how their positive beliefs are maintained; and (3) if they are aware of the counterproductive effects of PEP, why they still perform PEP. 
  Aims:  To explore the phenomenology of the processes involved in PEP from the perspective of SADs, in order to address the research questions above. 
  Method:  Twenty-one participants suffering from SAD received individual semi-structured interviews. Transcripts were analysed using thematic analysis. 
  Results:  Analysis revealed three main themes: (1) 'Only, safe and useful way to improve myself': SADs feel the need to improve their social performance, and they believe that PEP is the only, safe, and private way to do so, which is an underlying motive for them to do PEP; (2) 'It hurts more than helps me': however, through PEP, they do not seem to obtain the benefit that they expect, or only find a variety of counterproductive outcomes; (3) 'Better safe than sorry': they sometimes find makeshift solutions to improve their social performance during PEP, which may maintain their PEP as a form of intermittent reinforcement. They weigh up such costs and benefits, and choose to perform PEP while feeling conflicted about PEP. 
  Conclusions:  The results suggest that: (1) SADs rarely obtain the benefits from PEP that they expect; (2) their positive metacognitive beliefs are maintained by solutions they sometimes find during PEP; and (3) SADs choose to perform PEP while feeling conflicted; while PEP ironically maintains and exacerbates negative self-beliefs/images, it is the only safe and useful way to improve their social performance. These findings support and expand on the theories of PEP. 
  |  https://www.cambridge.org/core/product/identifier/S1352465819000651/type/journal_article  |  
------------------------------------------- 
10.1002/psp4.12491  |   Artificial intelligence, in particular machine learning (ML), has emerged as a key promising pillar to overcome the high failure rate in drug development. Here, we present a primer on the ML algorithms most commonly used in drug discovery and development. We also list possible data sources, describe good practices for ML model development and validation, and share a reproducible example. A companion article will summarize applications of ML in drug discovery, drug development, and postapproval phase. 
  |  https://doi.org/10.1002/psp4.12491  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31905263/  |  
------------------------------------------- 
10.1038/s41598-019-57083-6  |   Severely burned and non-burned trauma patients are at risk for acute kidney injury (AKI). The study objective was to assess the theoretical performance of artificial intelligence (AI)/machine learning (ML) algorithms to augment AKI recognition using the novel biomarker, neutrophil gelatinase associated lipocalin (NGAL), combined with contemporary biomarkers such as N-terminal pro B-type natriuretic peptide (NT-proBNP), urine output (UOP), and plasma creatinine. Machine learning approaches including logistic regression (LR), k-nearest neighbor (k-NN), support vector machine (SVM), random forest (RF), and deep neural networks (DNN) were used in this study. The AI/ML algorithm helped predict AKI 61.8 (32.5) hours faster than the Kidney Disease and Improving Global Disease Outcomes (KDIGO) criteria for burn and non-burned trauma patients. NGAL was analytically superior to traditional AKI biomarkers such as creatinine and UOP. With ML, the AKI predictive capability of NGAL was further enhanced when combined with NT-proBNP or creatinine. The use of AI/ML could be employed with NGAL to accelerate detection of AKI in at-risk burn and non-burned trauma patients. 
  |  http://dx.doi.org/10.1038/s41598-019-57083-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31937795/  |  
------------------------------------------- 
10.2196/15748  |    Background:  Existing health informatics curriculum requirements mostly use a competency-based approach rather than a skill-based one. 
  Objective:  The main objective of this study was to assess the current skills training requirements in graduate health informatics curricula to evaluate graduate students' confidence in specific health informatics skills. 
  Methods:  A quantitative cross-sectional observational study was developed to evaluate published health informatics curriculum requirements and to determine the comprehensive health informatics skill sets required in a research university in New York, United States. In addition, a questionnaire to assess students' confidence about specific health informatics skills was developed and sent to all enrolled and graduated Master of Science students in a health informatics program. 
  Results:  The evaluation was performed in a graduate health informatics program, and analysis of the students' self-assessments questionnaire showed that 79.4% (81/102) of participants were not confident (not at all confident or slightly confident) about developing an artificial intelligence app, 58.8% (60/102) were not confident about designing and developing databases, and 54.9% (56/102) were not confident about evaluating privacy and security infrastructure. Less than one-third of students (24/105, 23.5%) were confident (extremely confident and very confident) that they could evaluate the use of data capture technologies and develop mobile health informatics apps (10/102, 9.8%). 
  Conclusions:  Health informatics programs should consider specialized tracks that include specific skills to meet the complex health care delivery and market demand, and specific training components should be defined for different specialties. There is a need to determine new competencies and skill sets that promote inductive and deductive reasoning from diverse and various data platforms and to develop a comprehensive curriculum framework for health informatics skills training. 
  |  https://medinform.jmir.org/2020/1/e15748/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31961328/  |  
------------------------------------------- 
10.1016/j.neunet.2020.02.001  |   In recent years, deep learning achieves remarkable results in the field of artificial intelligence. However, the training process of deep neural networks may cause the leakage of individual privacy. Given the model and some background information of the target individual, the adversary can maliciously infer the sensitive feature of the target individual. Therefore, it is imperative to preserve the sensitive information in the training data. Differential privacy is a state-of-the-art paradigm for providing the privacy guarantee of datasets, which protects the private and sensitive information from the attack of adversaries significantly. However, the existing privacy-preserving models based on differential privacy are less than satisfactory since traditional approaches always inject the same amount of noise into parameters to preserve the sensitive information, which may impact the trade-off between the model utility and the privacy guarantee of training data. In this paper, we present a general differentially private deep neural networks learning framework based on relevance analysis, which aims to bridge the gap between private and non-private models while providing an effective privacy guarantee of sensitive information. The proposed model perturbs gradients according to the relevance between neurons in different layers and the model output. Specifically, during the process of backward propagation, more noise is added to gradients of neurons that have less relevance to the model output, and vice-versa. Experiments on five real datasets demonstrate that our mechanism not only bridges the gap between private and non-private models, but also prevents the disclosure of sensitive information effectively. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30046-0  |  
------------------------------------------- 
10.1016/j.crad.2019.12.023  |   Pancreatic ductal adenocarcinoma (PDAC) is one of the most aggressive tumours. PDAC has a poor prognosis; therefore, it is necessary to perform further risk stratification. Identifying prognostic factors before treatment might help to implement suitable and personalised treatment for individuals and avoid side effects. Conventional staging systems and tumour biomarkers are fundamental to establish prognosis; however, they have obvious limitations. Novel imaging biomarkers extracted from advanced imaging techniques offer opportunities to evaluate underlying tumour physiological characteristics, such as mutational status, cellular composition, local microenvironment, tumour metabolism, and biological behaviour. Thus, imaging biomarkers might help the decision making of oncologists and surgeons. The present review discusses the functions of imaging biomarkers for prognostic prediction in patients with PDAC and their potential value for further translation in clinical practice. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0009-9260(20)30017-9  |  
------------------------------------------- 
10.3758/s13428-020-01357-9  |   Vocabulary size seems to be affected by multiple factors, including those that belong to the properties of the words themselves and those that relate to the characteristics of the individuals assessing the words. In this study, we present results from a crowdsourced lexical decision megastudy in which more than 150,000 native speakers from around 20 Spanish-speaking countries performed a lexical decision task to 70 target word items selected from a list of about 45,000 Spanish words. We examined how demographic characteristics such as age, education level, and multilingualism affected participants' vocabulary size. Also, we explored how common factors related to words like frequency, length, and orthographic neighbourhood influenced the knowledge of a particular item. Results indicated important contributions of age to overall vocabulary size, with vocabulary size increasing in a logarithmic fashion with this factor. Furthermore, a contrast between monolingual and bilingual communities within Spain revealed no significant vocabulary size differences between the communities. Additionally, we replicated the standard effects of the words' properties and their interactions, accurately accounting for the estimated knowledge of a particular word. These results highlight the value of crowdsourced approaches to uncover effects that are traditionally masked by small-sampled in-lab factorial experimental designs. 
  |  https://dx.doi.org/10.3758/s13428-020-01357-9  |  
------------------------------------------- 
10.3390/diagnostics10020110  |   During image segmentation tasks in computer vision, achieving high accuracy performance while requiring fewer computations and faster inference is a big challenge. This is especially important in medical imaging tasks but one metric is usually compromised for the other. To address this problem, this paper presents an extremely fast, small and computationally effective deep neural network called Stripped-Down UNet (SD-UNet), designed for the segmentation of biomedical data on devices with limited computational resources. By making use of depthwise separable convolutions in the entire network, we design a lightweight deep convolutional neural network architecture inspired by the widely adapted U-Net model. In order to recover the expected performance degradation in the process, we introduce a weight standardization algorithm with the group normalization method. We demonstrate that SD-UNet has three major advantages including: (i) smaller model size (23x smaller than U-Net); (ii) 8x fewer parameters; and (iii) faster inference time with a computational complexity lower than 8M floating point operations (FLOPs). Experiments on the benchmark dataset of the Internatioanl Symposium on Biomedical Imaging (ISBI) challenge for segmentation of neuronal structures in electron microscopic (EM) stacks and the Medical Segmentation Decathlon (MSD) challenge brain tumor segmentation (BRATs) dataset show that the proposed model achieves comparable and sometimes better results compared to the current state-of-the-art. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10020110  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32085469/  |  
------------------------------------------- 
10.1002/mp.14098  |   Accurate segmentation of the hippocampus for hippocampal avoidance whole brain radiotherapy currently requires high resolution MRI in addition to neuroanatomic expertise for manual segmentation. Removing the need for MR images to identify the hippocampus would reduce planning complexity, the need for a treatment planning MR imaging session, potential uncertainties associated with MRI-CT image registration, and cost. Three-dimensional deep convolutional network models have the potential to automate hippocampal segmentation. In this study, we demonstrate that deep learning models, utilizing 3D convolutional neural networks, can accurately delineate the hippocampus using only high-resolution non-contrast CT images. 
  Purpose:  To investigate the inter-observer accuracy and reliability of hippocampal segmentation by experts using MRI -fusion and an automated deep learning model using CT alone. 
  Methods:  Retrospectively, 390 Gamma Knife patients with high resolution CT and MR images were collected. Following the RTOG 0933 guidelines, images were rigidly fused, and a neuroanatomic expert contoured the hippocampus on the MR, then transferred the contours to CT. Using a calculated cranial centroid, the image volumes were cropped to 200 x 200 x 35 voxels, which were used to train four models, including our proposed Attention-Gated 3D ResNet (AG-3D ResNet). These models were then compared with results from a nested 10-fold validation. From the predicted test set volumes, we calculated the 100% Hausdorff distance (HD). Acceptability was assessed using the RTOG 0933 protocol criteria, contours were considered passing with HD ≤ 7 mm. 
  Results:  The bilateral hippocampus passing rate across all 90 models trained in the nested cross-fold validation was 80.2% for AG-3D ResNet, which performs with a comparable pass rate (p = 0.3345) to physicians during centralized review for the RTOG 0933 Phase II clinical trial. 
  Conclusion:  Our proposed AG-3D ResNet's segmentation of the hippocampus from non-contrast CT images alone are comparable to those obtained by participating physicians from the RTOG 0933 Phase II clinical trial. 
  |  https://doi.org/10.1002/mp.14098  |  
------------------------------------------- 
10.3390/s20041074  |   Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed. 
  |  http://www.mdpi.com/resolver?pii=s20041074  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32079124/  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.137007  |   The purpose of the present paper is improving the accuracy of existing formulas for the longitudinal dispersion coefficient (LDC) prediction based on a novel and simple meta-heuristic optimization method called Whale Optimization Algorithm (WOA). Although several existing formulas calculate LDC in the rivers based on the hydraulic and hydrodynamic specifications, most of them have significant errors in confronting extensive field data. In this study, comprehensive field data, including the geometrical and hydraulic properties of different rivers in the world, were adopted to build a reliable model. Statistical error measures were used to evaluate and compare the results with other studies. Furthermore, the Subset Selection of Maximum Dissimilarity (SSMD) method was utilized for a reputable selection of data for training and testing the WOA model. Subset selection is a critical factor in artificial intelligence (AI) computations. Finally, an integrated model based on the SSMD method and WOA technique has been proposed to develop the high accuracy formulas for the prediction of LDC. According to the results, the developed formulas are competitive or superior to the previous formulas for LDC estimation. Results also indicated that the WOA algorithm could be applied to improve the performance of the predictive equations in other fields of studies by finding the optimum values of coefficients. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)30517-9  |  
------------------------------------------- 
10.1016/j.artmed.2020.101810  |   Modern computer technology sheds light on new ways of innovating Traditional Chinese Medicine (TCM). One method that gets increasing attention is the quantitative research method, which makes use of data mining and artificial intelligence technology as well as the mathematical principles in the research on rationales, academic viewpoints of famous doctors of TCM, dialectical treatment by TCM, clinical technology of TCM, the patterns of TCM prescriptions, clinical curative effects of TCM and other aspects. This paper reviews the methods, means, progress and achievements of quantitative research on TCM. In the core database of the Web of Science, "Traditional Chinese Medicine", "Computational Science" and "Mathematical Computational Biology" are selected as the main retrieval fields, and the retrieval time interval from 1999 to 2019 is used to collect relevant literature. It is found that researchers from China Academy of Chinese Medical Sciences, Zhejiang University, Chinese Academy of Sciences and other institutes have opened up new methods of research on TCM since 2009, with quantitative methods and knowledge presentation models. The adopted tools mainly consist of text mining, knowledge discovery, technologies of the TCM database, data mining and drug discovery through TCM calculation, etc. In the future, research on quantitative models of TCM will focus on solving the heterogeneity and incompleteness of big data of TCM, establishing standardized treatment systems, and promoting the development of modernization and internationalization of TCM. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(19)30629-3  |  
------------------------------------------- 
10.1007/s10827-020-00742-9  |   Grid cells in the entorhinal cortex, together with head direction, place, speed and border cells, are major contributors to the organization of spatial representations in the brain. In this work we introduce a novel theoretical and algorithmic framework able to explain the optimality of hexagonal grid-like response patterns. We show that this pattern is a result of minimal variance encoding of neurons together with maximal robustness to neurons' noise and minimal number of encoding neurons. The novelty lies in the formulation of the encoding problem considering neurons as an overcomplete basis (a frame) where the position information is encoded. Through the modern Frame Theory language, specifically that of tight and equiangular frames, we provide new insights about the optimality of hexagonal grid receptive fields. The proposed model is based on the well-accepted and tested hypothesis of Hebbian learning, providing a simplified cortical-based framework that does not require the presence of velocity-driven oscillations (oscillatory model) or translational symmetries in the synaptic connections (attractor model). We moreover demonstrate that the proposed encoding mechanism naturally explains axis alignment of neighbor grid cells and maps shifts, rotations and scaling of the stimuli onto the shape of grid cells' receptive fields, giving a straightforward explanation of the experimental evidence of grid cells remapping under transformations of environmental cues. 
  |  https://doi.org/10.1007/s10827-020-00742-9  |  
------------------------------------------- 
10.1007/s12350-020-02081-9  |    Aims:  <sup>123</sup>I-labeled meta-iodobenzylguanidine (MIBG) has used a planar image to measure the heart-to-mediastinum ratio (HMR). However, planar images are not available from IQ-SPECT with SMARTZOOM collimator due to its multi-focal collimation. Since we created the planar-equivalent (IQ-planar) images by adding all slices of the IQ-SPECT coronal image. The aim of this study was to demonstrate the utility of the new method for calculating HMR. 
  Methods:  The planar image and transverse images of IQ-SPECT with attenuation and scatter corrections (ACSC) and without ACSC (NC) were obtained. Multi-planar reconstruction and ray-summation processing were applied to create IQ-planar images with NC and ACSC. Linear regression between the measured HMR from the planar image and the mathematically calculated HMR was used to calibrate HMR to standardized values. 
  Results:  Scatterplots and linear regression lines between planar and IQ-planar HMRs before and after cross-calibration showed systematic differences in both NC and ACSC conditions. The IQ-planar HMR with NC and ACSC was significantly higher compared with that of the conventional planar image. However, the IQ-planar HMR with NC and ACSC after cross-calibration was similar to the standardized HMR calculated by planar image. 
  Conclusion:  The IQ-planar HMR using the new ray-summation processing method could be used along with the conventional planar HMR. 
  |  https://dx.doi.org/10.1007/s12350-020-02081-9  |  
------------------------------------------- 
10.5858/arpa.2019-0435-OA  |    Context.—:  Mitotic count is an important histologic criterion for grading and prognostication in phyllodes tumors (PTs). Counting mitoses is a routine practice for pathologists evaluating neoplasms, but different microscopes, variable field selection, and areas have led to possible misclassification. 
  Objective.—:  To determine whether 10 high-power fields (HPFs) or whole slide mitotic counts correlated better with PT clinicopathologic parameters using digital pathology (DP). We also aimed to find out whether this study might serve as a basis for an artificial intelligence (AI) protocol to count mitosis. 
  Design.—:  Representative slides were chosen from 93 cases of PTs diagnosed between 2014 and 2015. The slides were scanned and viewed with DP. Mitotic counting was conducted on the whole slide image, before choosing 10 HPFs and demarcating the tumor area in DP. Values of mitoses per millimeter squared were used to compare results between 10 HPFs and the whole slide. Correlations with clinicopathologic parameters were conducted. 
  Results.—:  Both whole slide counting of mitoses and 10 HPFs had similar statistically significant correlation coefficients with grade, stromal atypia, and stromal hypercellularity. Neither whole slide mitotic counts nor mitoses per 10 HPFs showed statistically significant correlations with patient age and tumor size. 
  Conclusions.—:  Accurate mitosis counting in breast PTs is important for grading. Exploring machine learning on digital whole slides may influence approaches to training, testing, and validation of a future AI algorithm. 
  |  http://www.archivesofpathology.org/doi/10.5858/arpa.2019-0435-OA?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3233/JAD-191089  |    Background:  A digital version of the clock drawing test (dCDT) provides new latency and graphomotor behavioral measurements. These variables have yet to be validated with external neuropsychological domains in non-demented adults. 
  Objective:  The current investigation reports on cognitive constructs associated with selected dCDT latency and graphomotor variables and compares performances between individuals with mild cognitive impairment (MCI) and non-MCI peers. 
  Methods:  202 non-demented older adults (age 68.79 ± 6.18, 46% female, education years 16.02 ± 2.70) completed the dCDT and a comprehensive neuropsychological protocol. dCDT variables of interest included: total completion time (TCT), pre-first hand latency (PFHL), post-clock face latency (PCFL), and clock face area (CFA). We also explored variables of percent time drawing (i.e., 'ink time') versus percent time not drawing (i.e., 'think time'). Neuropsychological domains of interest included processing speed, working memory, language, and declarative memory. 
  Results:  Adjusting for age and premorbid cognitive reserve metrics, command TCT positively correlated with multiple cognitive domains; PFHL and PCFL negatively associated with worse performance on working memory and processing speed tests. For Copy, TCT, PCFL, and PFHL negatively correlated with processing speed, and CFA negatively correlated with language. Between-group analyses show MCI participants generated slower command TCT, produced smaller CFA, and required more command 'think' (% Think) than 'ink' (% Ink) time. 
  Conclusion:  Command dCDT variables of interest were primarily processing speed and working memory dependent. MCI participants showed dCDT differences relative to non-MCI peers, suggesting the dCDT may assist with classification. Results document cognitive construct validation to digital metrics of clock drawing. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/JAD-191089  |  
------------------------------------------- 
10.1371/journal.pcbi.1007698  |   Humans are able to track multiple objects at any given time in their daily activities-for example, we can drive a car while monitoring obstacles, pedestrians, and other vehicles. Several past studies have examined how humans track targets simultaneously and what underlying behavioral and neural mechanisms they use. At the same time, computer-vision researchers have proposed different algorithms to track multiple targets automatically. These algorithms are useful for video surveillance, team-sport analysis, video analysis, video summarization, and human-computer interaction. Although there are several efficient biologically inspired algorithms in artificial intelligence, the human multiple-target tracking (MTT) ability is rarely imitated in computer-vision algorithms. In this paper, we review MTT studies in neuroscience and biologically inspired MTT methods in computer vision and discuss the ways in which they can be seen as complementary. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007698  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32271746/  |  
------------------------------------------- 
10.1186/s12859-020-3463-4  |    Background:  Microarray technology provides the expression level of many genes. Nowadays, an important issue is to select a small number of informative differentially expressed genes that provide biological knowledge and may be key elements for a disease. With the increasing volume of data generated by modern biomedical studies, software is required for effective identification of differentially expressed genes. Here, we describe an R package, called ORdensity, that implements a recent methodology (Irigoien and Arenas, 2018) developed in order to identify differentially expressed genes. The benefits of parallel implementation are discussed. 
  Results:  ORdensity gives the user the list of genes identified as differentially expressed genes in an easy and comprehensible way. The experimentation carried out in an off-the-self computer with the parallel execution enabled shows an improvement in run-time. This implementation may also lead to an important use of memory load. Results previously obtained with simulated and real data indicated that the procedure implemented in the package is robust and suitable for differentially expressed genes identification. 
  Conclusions:  The new package, ORdensity, offers a friendly and easy way to identify differentially expressed genes, which is very useful for users not familiar with programming. 
  Availability:  https://github.com/rsait/ORdensity. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3463-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32264950/  |  
------------------------------------------- 
10.1080/09537104.2020.1742310  |   Thrombin generation (TG) is a better determinant of the overall function of the hemostatic system than routinely used clotting time-based assays and can be studied more in detail by thrombin dynamics analysis. Platelet poor plasma is often used to measure TG, however, measuring the contribution of the platelets is also important as patients with a low platelet count or with dysfunctional platelets have an increased risk of developing bleeding. In this study, platelet rich plasma (PRP) was collected from 117 healthy individuals. PRP was measured undiluted and diluted to a varying platelet concentration of 10*10<sup>9</sup>/L to 400*10<sup>9</sup>/L. Prothrombin conversion and thrombin inactivation were calculated from the data obtained by the TG parameters and coagulation factor levels (antithrombin, α<sub>2</sub>Macroglobulin (α<sub>2</sub>M) and fibrinogen). Reference ranges of TG and thrombin dynamics in PRP of 117 healthy individuals were established. Peak, velocity index and the maximum rate of prothrombin conversion increased linearly with platelet count, but endogenous thrombin potential reached a maximum at 150*10<sup>9</sup>/L as seen in a subset population (n = 20). More extensive analysis revealed that a platelet count below 50*10<sup>9</sup>/L did not affect TG parameters (except for the ETP). Correlation analysis indicated that the platelet count mainly affected the rate of prothrombin conversion. Inhibition of thrombin by antithrombin and α<sub>2</sub>M increased with increasing TG, but the ratio of inhibition by antithrombin or α<sub>2</sub>M remained the same independently of the total thrombin formed. In conclusion, TG and thrombin dynamics were assessed in PRP of healthy donors to provide reference values for future TG studies in PRP. Increasing the platelet count mainly affected the rate of prothrombin conversion and TG, rather than the total amount of thrombin formed. 
  |  http://www.tandfonline.com/doi/full/10.1080/09537104.2020.1742310  |  
------------------------------------------- 
10.3390/ijerph17082637  |   Multiple sclerosis (MS) is an inflammatory neurological disease characterized by autoimmune-mediated demyelination of the central nervous system. Genetic and environmental factors may contribute to the development of MS. This has not been confirmed yet. Dental amalgam has long been controversial in MS due to its mercury content but the toxicological implications of mercury-containing amalgam fillings (AMF) for MS remain to be elucidated. We conducted a case-control study to investigate the association between AMF and the risk of MS from the Taiwanese National Health Insurance Research Database (NHIRD). Case (<i>n</i> = 612) and control (<i>n</i> = 612) groups were matched by sex, age, urbanization level, monthly income, and Charlson comorbidity index by propensity score matched with a 1:1 ratio from 2000 to 2013. Differences between cases and controls was not statistically significant (OR: 0.82, 95% CI = 0.65-1.05). In subjects stratified by gender, MS was also not associated with AMF for women (OR: 0.743, 95% CI = 0.552-1.000) and men (OR: 1.006, 95% CI = 0.670-1.509), respectively. In summary, this Taiwanese nationwide population-based case-control study did not find an association between MS and AMF. 
  |  http://www.mdpi.com/resolver?pii=ijerph17082637  |  
------------------------------------------- 
10.1111/his.14120  |   Molecular biomarkers have become one of the cornerstones of oncological pathology. The method of classification not only directly affects the manner in which patients are diagnosed and treated, but also guide the development of drugs and of artificial intelligence tools. This work aims to organize and update gastrointestinal molecular biomarkers in order to produce an easy-to-use guide for routine diagnostics. For this purpose, we have extracted and re-organized the molecular information of epithelial neoplasms included in the new "WHO Classification of Tumours of the Digestive System" book (5<sup>th</sup> Edition 2019). 
  |  https://doi.org/10.1111/his.14120  |  
------------------------------------------- 
10.1007/s00261-019-02191-0  |   MRI and MRCP play an important role in the diagnosis of chronic pancreatitis (CP) by imaging pancreatic parenchyma and ducts. MRI/MRCP is more widely used than computed tomography (CT) for mild to moderate CP due to its increased sensitivity for pancreatic ductal and gland changes; however, it does not detect the calcifications seen in advanced CP. Quantitative MR imaging offers potential advantages over conventional qualitative imaging, including simplicity of analysis, quantitative and population-based comparisons, and more direct interpretation of detected changes. These techniques may provide quantitative metrics for determining the presence and severity of acinar cell loss and aid in the diagnosis of chronic pancreatitis. Given the fact that the parenchymal changes of CP precede the ductal involvement, there would be a significant benefit from developing MRI/MRCP-based, more robust diagnostic criteria combining ductal and parenchymal findings. Among cross-sectional imaging modalities, multi-detector CT (MDCT) has been a cornerstone for evaluating chronic pancreatitis (CP) since it is ubiquitous, assesses primary disease process, identifies complications like pseudocyst or vascular thrombosis with high sensitivity and specificity, guides therapeutic management decisions, and provides images with isotropic resolution within seconds. Conventional MDCT has certain limitations and is reserved to provide predominantly morphological (e.g., calcifications, organ size) rather than functional information. The emerging applications of radiomics and artificial intelligence are poised to extend the current capabilities of MDCT. In this review article, we will review advanced imaging techniques by MRI, MRCP, CT, and ultrasound. 
  |  https://dx.doi.org/10.1007/s00261-019-02191-0  |  
------------------------------------------- 
10.1177/1120672120902952  |    Purpose:  The aim was to assess the postoperative results of a biometric method using artificial intelligence (Hill-radial basis function 2.0), and data from a modern formula (Barrett Universal II) and the Sanders-Retzlaff-Kraft/Theoretical formula. 
  Methods:  Phacoemulsification and biconvex intraocular lens implantation were performed in 186 cataractous eyes. The diopters of intraocular lens were established with the Hill-radial basis function method, based on biometric data obtained using the Aladdin device. The required diopters of the intraocular lens were also calculated by the Barrett Universal II formula and with the Sanders-Retzlaff-Kraft/Theoretical formula. The differences between the manifest postoperative refractive errors and the planned refractive errors were calculated, as well as the percentage of eyes within ±0.5 D of the prediction error. The mean- and the median absolute refractive errors were also determined. 
  Results:  The mean age of the patients was 70.13 years (SD = 10.67 years), and the mean axial length was 23.47 mm (range = 20.72-28.78 mm). The percentage of eyes within a prediction error of ±0.5 D was 83.62% using the Hill-radial basis function method, 79.66% with the Barrett Universal II formula, and 74.01% in the case of the Sanders-Retzlaff-Kraft/Theoretical formula. The mean- and the median absolute refractive errors were not statistically different. 
  Conclusion:  Clinical success was the highest when using the biometric method, based on pattern recognition. The results obtained using Barrett Universal II came a close second. Both methods performed better compared to a traditionally used formula. 
  |  http://journals.sagepub.com/doi/full/10.1177/1120672120902952?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/s20061560  |   We designed an end-to-end dual-branch residual network architecture that inputs a low-resolution (LR) depth map and a corresponding high-resolution (HR) color image separately into the two branches, and outputs an HR depth map through a multi-scale, channel-wise feature extraction, interaction, and upsampling. Each branch of this network contains several residual levels at different scales, and each level comprises multiple residual groups composed of several residual blocks. A short-skip connection in every residual block and a long-skip connection in each residual group or level allow for low-frequency information to be bypassed while the main network focuses on learning high-frequency information. High-frequency information learned by each residual block in the color image branch is input into the corresponding residual block in the depth map branch, and this kind of channel-wise feature supplement and fusion can not only help the depth map branch to alleviate blur in details like edges, but also introduce some depth artifacts to feature maps. To avoid the above introduced artifacts, the channel interaction fuses the feature maps using weights referring to the channel attention mechanism. The parallel multi-scale network architecture with channel interaction for feature guidance is the main contribution of our work and experiments show that our proposed method had a better performance in terms of accuracy compared with other methods. 
  |  http://www.mdpi.com/resolver?pii=s20061560  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32168872/  |  
------------------------------------------- 
10.3389/fgene.2020.00090  |   Noncoding RNA (ncRNA) is a kind of RNA that plays an important role in many biological processes, diseases, and cancers, while cannot translate into proteins. With the development of next-generation sequence technology, thousands of novel RNAs with long open reading frames (ORFs, longest ORF length &gt; 303 nt) and short ORFs (longest ORF length ≤ 303 nt) have been discovered in a short time. How to identify ncRNAs more precisely from novel unannotated RNAs is an important step for RNA functional analysis, RNA regulation, <i>etc</i>. However, most previous methods only utilize the information of sequence features. Meanwhile, most of them have focused on long-ORF RNA sequences, but not adapted to short-ORF RNA sequences. In this paper, we propose a new reliable method called NCResNet. NCResNet employs 57 hybrid features of four categories as inputs, including sequence, protein, RNA structure, and RNA physicochemical properties, and introduces feature enhancement and deep feature learning policies in a neural net model to adapt to this problem. The experiments on benchmark datasets of 8 species shows NCResNet has higher accuracy and higher Matthews correlation coefficient (MCC) compared with other state-of-the-art methods. Particularly, on four short-ORF RNA sequence datasets, specifically mouse, <i>Saccharomyces cerevisiae</i>, zebrafish, and cow, NCResNet achieves greater than 10 and 15% improvements over other state-of-the-art methods in terms of accuracy and MCC. Meanwhile, for long-ORF RNA sequence datasets, NCResNet also has better accuracy and MCC than other state-of-the-art methods on most test datasets. Codes and data are available at https://github.com/abcair/NCResNet. 
  |  https://doi.org/10.3389/fgene.2020.00090  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32180792/  |  
------------------------------------------- 
10.1007/s10278-020-00339-9  |   Since morphology of retinal blood vessels plays a key role in ophthalmological disease diagnosis, retinal vessel segmentation is an indispensable step for the screening and diagnosis of retinal diseases with fundus images. In this paper, deep convolution adversarial network combined with short connection and dense block is proposed to separate blood vessels from fundus image, named SUD-GAN. The generator adopts U-shape encode-decode structure and adds short connection block between convolution layers to prevent gradient dispersion caused by deep convolution network. The discriminator is all composed of convolution block, and dense connection structure is added to the middle part of the convolution network to strengthen the spread of features and enhance the network discrimination ability. The proposed method is evaluated on two publicly available databases, the DRIVE and STARE. The results show that the proposed method outperforms the state-of-the-art performance in sensitivity and specificity, which were 0.8340 and 0.9820, and 0.8334 and 0.9897 respectively on DRIVE and STARE, and can detect more tiny vessels and locate the edge of blood vessels more accurately. 
  |  https://doi.org/10.1007/s10278-020-00339-9  |  
------------------------------------------- 
10.1186/s13148-020-00842-4  |    Background:  Machine learning is a sub-field of artificial intelligence, which utilises large data sets to make predictions for future events. Although most algorithms used in machine learning were developed as far back as the 1950s, the advent of big data in combination with dramatically increased computing power has spurred renewed interest in this technology over the last two decades. 
  Main body:  Within the medical field, machine learning is promising in the development of assistive clinical tools for detection of e.g. cancers and prediction of disease. Recent advances in deep learning technologies, a sub-discipline of machine learning that requires less user input but more data and processing power, has provided even greater promise in assisting physicians to achieve accurate diagnoses. Within the fields of genetics and its sub-field epigenetics, both prime examples of complex data, machine learning methods are on the rise, as the field of personalised medicine is aiming for treatment of the individual based on their genetic and epigenetic profiles. 
  Conclusion:  We now have an ever-growing number of reported epigenetic alterations in disease, and this offers a chance to increase sensitivity and specificity of future diagnostics and therapies. Currently, there are limited studies using machine learning applied to epigenetics. They pertain to a wide variety of disease states and have used mostly supervised machine learning methods. 
  |  https://clinicalepigeneticsjournal.biomedcentral.com/articles/10.1186/s13148-020-00842-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245523/  |  
------------------------------------------- 
10.3390/s20082436  |   Action recognition in robotics is a research field that has gained momentum in recent years. In this work, a video activity recognition method is presented, which has the ultimate goal of endowing a robot with action recognition capabilities for a more natural social interaction. The application of Common Spatial Patterns (CSP), a signal processing approach widely used in electroencephalography (EEG), is presented in a novel manner to be used in activity recognition in videos taken by a humanoid robot. A sequence of skeleton data is considered as a multidimensional signal and filtered according to the CSP algorithm. Then, characteristics extracted from these filtered data are used as features for a classifier. A database with 46 individuals performing six different actions has been created to test the proposed method. The CSP-based method along with a Linear Discriminant Analysis (LDA) classifier has been compared to a Long Short-Term Memory (LSTM) neural network, showing that the former obtains similar or better results than the latter, while being simpler. 
  |  http://www.mdpi.com/resolver?pii=s20082436  |  
------------------------------------------- 
10.3389/fonc.2020.00418  |   For stage-I lung adenocarcinoma, the 5-years disease-free survival (DFS) rates of non-invasive adenocarcinoma (non-IA) is different with invasive adenocarcinoma (IA). This study aims to develop CT image based artificial intelligence (AI) schemes to classify between non-IA and IA nodules, and incorporate deep learning (DL) and radiomics features to improve the classification performance. We collect 373 surgical pathological confirmed ground-glass nodules (GGNs) from 323 patients in two centers. It involves 205 non-IA (including 107 adenocarcinoma <i>in situ</i> and 98 minimally invasive adenocarcinoma), and 168 IA. We first propose a recurrent residual convolutional neural network based on U-Net to segment the GGNs. Then, we build two schemes to classify between non-IA and IA namely, DL scheme and radiomics scheme, respectively. Third, to improve the classification performance, we fuse the prediction scores of two schemes by applying an information fusion method. Finally, we conduct an observer study to compare our scheme performance with two radiologists by testing on an independent dataset. Comparing with DL scheme and radiomics scheme (the area under a receiver operating characteristic curve (AUC): 0.83 ± 0.05, 0.87 ± 0.04), our new fusion scheme (AUC: 0.90 ± 0.03) significant improves the risk classification performance (<i>p</i> &lt; 0.05). In a comparison with two radiologists, our new model yields higher accuracy of 80.3%. The kappa value for inter-radiologist agreement is 0.6. It demonstrates that applying AI method is an effective way to improve the invasiveness risk prediction performance of GGNs. In future, fusion of DL and radiomics features may have a potential to handle the classification task with limited dataset in medical imaging. 
  |  https://doi.org/10.3389/fonc.2020.00418  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296645/  |  
------------------------------------------- 
10.1371/journal.pcbi.1007783  |   Multi-species microbial communities are widespread in natural ecosystems. When employed for biomanufacturing, engineered synthetic communities have shown increased productivity in comparison with monocultures and allow for the reduction of metabolic load by compartmentalising bioprocesses between multiple sub-populations. Despite these benefits, co-cultures are rarely used in practice because control over the constituent species of an assembled community has proven challenging. Here we demonstrate, in silico, the efficacy of an approach from artificial intelligence-reinforcement learning-for the control of co-cultures within continuous bioreactors. We confirm that feedback via a trained reinforcement learning agent can be used to maintain populations at target levels, and that model-free performance with bang-bang control can outperform a traditional proportional integral controller with continuous control, when faced with infrequent sampling. Further, we demonstrate that a satisfactory control policy can be learned in one twenty-four hour experiment by running five bioreactors in parallel. Finally, we show that reinforcement learning can directly optimise the output of a co-culture bioprocess. Overall, reinforcement learning is a promising technique for the control of microbial communities. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007783  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32275710/  |  
------------------------------------------- 
10.1016/j.neunet.2020.03.005  |   For an animal to learn about its environment with limited motor and cognitive resources, it should focus its resources on potentially important stimuli. However, too narrow focus is disadvantageous for adaptation to environmental changes. Midbrain dopamine neurons are excited by potentially important stimuli, such as reward-predicting or novel stimuli, and allocate resources to these stimuli by modulating how an animal approaches, exploits, explores, and attends. The current study examined the theoretical possibility that dopamine activity reflects the dynamic allocation of resources for learning. Dopamine activity may transition between two patterns: (1) phasic responses to cues and rewards, and (2) ramping activity arising as the agent approaches the reward. Phasic excitation has been explained by prediction errors generated by experimentally inserted cues. However, when and why dopamine activity transitions between the two patterns remain unknown. By parsimoniously modifying a standard temporal difference (TD) learning model to accommodate a mixed presentation of both experimental and environmental stimuli, we simulated dopamine transitions and compared them with experimental data from four different studies. The results suggested that dopamine transitions from ramping to phasic patterns as the agent focuses its resources on a small number of reward-predicting stimuli, thus leading to task dimensionality reduction. The opposite occurs when the agent re-distributes its resources to adapt to environmental changes, resulting in task dimensionality expansion. This research elucidates the role of dopamine in a broader context, providing a potential explanation for the diverse repertoire of dopamine activity that cannot be explained solely by prediction error. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30082-4  |  
------------------------------------------- 
10.1039/c9nh00805e  |   Mixed-dimensional binary heterostructures, especially 0D/2D heterostructures, have attracted significant attention due to their unique physical properties. In this contribution, 0D bismuth quantum dots (Bi QDs, VA) are immobilized onto 2D Te nanosheets (Te NSs, VIA) to prepare Bi QDs/Te NSs binary heterostructures (Bi/Te) through a facile and cost-effective hydrothermal method. The results from both experiments and density functional theory (DFT) calculations demonstrate the enhanced photo-response behaviors of Bi/Te-based photoelectrochemical (PEC)-type photodetectors (PDs). The as-prepared PDs exhibit a high photocurrent of 18.21 μA cm<sup>-2</sup>, significantly higher than those of previously reported pristine Bi QD and Te NS-based PDs. The PDs are further demonstrated to have excellent self-power capability and long-term stability over 30 days. Additionally, the obtained 786 fs pulse output signal in the telecommunications band reveals the great potential of Bi/Te for ultrafast photonic devices. It is believed that such VA/VIA binary heterostructures provide opportunities for developing multifunctional optoelectronic devices for nano-science applications. 
  |  https://doi.org/10.1039/c9nh00805e  |  
------------------------------------------- 
10.3892/ijmm.2020.4526  |   Lung adenocarcinoma (LUAD) is one of the most common types of lung cancer and its poor prognosis largely depends on the tumor pathological stage. Critical roles of microRNAs (miRNAs) have been reported in the tumorigenesis and progression of lung cancer. However, whether the differential expression pattern of miRNAs could be used to distinguish early‑stage (stage I) from mid‑late‑stage (stages II‑IV) LUAD tumors is still unclear. In this study, clinical information and miRNA expression profiles of patients with LUAD were downloaded from The Cancer Genome Atlas (TCGA) and Gene Expression Omnibus databases. TCGA‑LUAD (n=470) dataset was used for model training and validation, and the GSE62182 (n=94) and GSE83527 (n=36) datasets were used as external independent test datasets. The diagnostic model was created through miRNA feature selection followed by SVM classifier and was confirmed by 5‑fold cross‑validation. A receiver operating characteristic curve was calculated to evaluate the accuracy and robustness of the model. Using the DX score and LIBSVM tool, a 16‑miRNA signature that could distinguish LUAD pathological stages was identified. The area under the curve rates were 0.62 [95% confidence interval (CI): 0.56‑0.67], 0.66 (95% CI: 0.54‑0.76) and 0.63 (95% CI: 0.43‑0.82) in TCGA‑LUAD internal validation dataset, the GSE62182 external validation dataset, and the GSE83527 external validation dataset, respectively. Kyoto Encyclopedia of Genes and Genomes and Gene Ontology enrichment analyses suggested that the target genes of the 16‑miRNA signature were mainly involved in metabolic pathways. The present findings demonstrate that a 16‑miRNA signature could serve as a promising diagnostic biomarker for pathological staging in LUAD. 
  |  http://www.spandidos-publications.com/10.3892/ijmm.2020.4526  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32323746/  |  
------------------------------------------- 
10.1016/j.neunet.2019.10.010  |   Multiview clustering has gained increasing attention recently due to its ability to deal with multiple sources (views) data and explore complementary information between different views. Among various methods, multiview subspace clustering methods provide encouraging performance. They mainly integrate the multiview information in the space where the data points lie. Hence, their performance may be deteriorated because of noises existing in each individual view or inconsistent between heterogeneous features. For multiview clustering, the basic premise is that there exists a shared partition among all views. Therefore, the natural space for multiview clustering should be all partitions. Orthogonal to existing methods, we propose to fuse multiview information in partition level following two intuitive assumptions: (i) each partition is a perturbation of the consensus clustering; (ii) the partition that is close to the consensus clustering should be assigned a large weight. Finally, we propose a unified multiview subspace clustering model which incorporates the graph learning from each view, the generation of basic partitions, and the fusion of consensus partition. These three components are seamlessly integrated and can be iteratively boosted by each other towards an overall optimal solution. Experiments on four benchmark datasets demonstrate the efficacy of our approach against the state-of-the-art techniques. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30332-6  |  
------------------------------------------- 
10.3390/cancers12041032  |    Background:  To deal with complexity in cancer care, computerized clinical decision support systems (CDSSs) are developed to support quality of care and improve decision-making. We performed a systematic review to explore the value of CDSSs using automated clinical guidelines, Artificial Intelligence, datamining or statistical methods (higher level CDSSs) on the quality of care in oncology. 
  Materials and methods:  The search strategy combined synonyms for 'CDSS' and 'cancer.' Pubmed, Embase, The Cochrane Library, Institute of Electrical and Electronics Engineers, Association of Computing Machinery digital library and Web of Science were systematically searched from January 2000 to December 2019. Included studies evaluated the impact of higher level CDSSs on process outcomes, guideline adherence and clinical outcomes. 
  Results:  11,397 studies were selected for screening, after which 61 full-text articles were assessed for eligibility. Finally, nine studies were included in the final analysis with a total population size of 7985 patients. Types of cancer included breast cancer (63.1%), lung cancer (27.8%), prostate cancer (4.1%), colorectal cancer (3.1%) and other cancer types (1.9%). The included studies demonstrated significant improvements of higher level CDSSs on process outcomes and guideline adherence across diverse settings in oncology. No significant differences were reported for clinical outcomes. 
  Conclusion:  Higher level CDSSs seem to improve process outcomes and guidelines adherence but not clinical outcomes. It should be noticed that the included studies primarily focused on breast and lung cancer. To further explore the impact of higher level CDSSs on quality of care, high-quality research is required. 
  |  http://www.mdpi.com/resolver?pii=cancers12041032  |  
------------------------------------------- 
10.2196/15645  |    Background:  Timely, precise, and localized surveillance of nonfatal events is needed to improve response and prevention of opioid-related problems in an evolving opioid crisis in the United States. Records of naloxone administration found in prehospital emergency medical services (EMS) data have helped estimate opioid overdose incidence, including nonhospital, field-treated cases. However, as naloxone is often used by EMS personnel in unconsciousness of unknown cause, attributing naloxone administration to opioid misuse and heroin use (OM) may misclassify events. Better methods are needed to identify OM. 
  Objective:  This study aimed to develop and test a natural language processing method that would improve identification of potential OM from paramedic documentation. 
  Methods:  First, we searched Denver Health paramedic trip reports from August 2017 to April 2018 for keywords naloxone, heroin, and both combined, and we reviewed narratives of identified reports to determine whether they constituted true cases of OM. Then, we used this human classification as reference standard and trained 4 machine learning models (random forest, k-nearest neighbors, support vector machines, and L1-regularized logistic regression). We selected the algorithm that produced the highest area under the receiver operating curve (AUC) for model assessment. Finally, we compared positive predictive value (PPV) of the highest performing machine learning algorithm with PPV of searches of keywords naloxone, heroin, and combination of both in the binary classification of OM in unseen September 2018 data. 
  Results:  In total, 54,359 trip reports were filed from August 2017 to April 2018. Approximately 1.09% (594/54,359) indicated naloxone administration. Among trip reports with reviewer agreement regarding OM in the narrative, 57.6% (292/516) were considered to include information revealing OM. Approximately 1.63% (884/54,359) of all trip reports mentioned heroin in the narrative. Among trip reports with reviewer agreement, 95.5% (784/821) were considered to include information revealing OM. Combined results accounted for 2.39% (1298/54,359) of trip reports. Among trip reports with reviewer agreement, 77.79% (907/1166) were considered to include information consistent with OM. The reference standard used to train and test machine learning models included details of 1166 trip reports. L1-regularized logistic regression was the highest performing algorithm (AUC=0.94; 95% CI 0.91-0.97) in identifying OM. Tested on 5983 unseen reports from September 2018, the keyword naloxone inaccurately identified and underestimated probable OM trip report cases (63 cases; PPV=0.68). The keyword heroin yielded more cases with improved performance (129 cases; PPV=0.99). Combined keyword and L1-regularized logistic regression classifier further improved performance (146 cases; PPV=0.99). 
  Conclusions:  A machine learning application enhanced the effectiveness of finding OM among documented paramedic field responses. This approach to refining OM surveillance may lead to improved first-responder and public health responses toward prevention of overdoses and other opioid-related problems in US communities. 
  |  https://www.jmir.org/2020/1/e15645/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31899451/  |  
------------------------------------------- 
10.3892/ijmm.2019.4418  |   NOTCH1, NOTCH2, NOTCH3 and NOTCH4 are transmembrane receptors that transduce juxtacrine signals of the delta‑like canonical Notch ligand (DLL)1, DLL3, DLL4, jagged canonical Notch ligand (JAG)1 and JAG2. Canonical Notch signaling activates the transcription of BMI1 proto‑oncogene polycomb ring finger, cyclin D1, CD44, cyclin dependent kinase inhibitor 1A, hes family bHLH transcription factor 1, hes related family bHLH transcription factor with YRPW motif 1, MYC, NOTCH3, RE1 silencing transcription factor and transcription factor 7 in a cellular context‑dependent manner, while non‑canonical Notch signaling activates NF‑κB and Rac family small GTPase 1. Notch signaling is aberrantly activated in breast cancer, non‑small‑cell lung cancer and hematological malignancies, such as T‑cell acute lymphoblastic leukemia and diffuse large B‑cell lymphoma. However, Notch signaling is inactivated in small‑cell lung cancer and squamous cell carcinomas. Loss‑of‑function NOTCH1 mutations are early events during esophageal tumorigenesis, whereas gain‑of‑function NOTCH1 mutations are late events during T‑cell leukemogenesis and B‑cell lymphomagenesis. Notch signaling cascades crosstalk with fibroblast growth factor and WNT signaling cascades in the tumor microenvironment to maintain cancer stem cells and remodel the tumor microenvironment. The Notch signaling network exerts oncogenic and tumor‑suppressive effects in a cancer stage‑ or (sub)type‑dependent manner. Small‑molecule γ‑secretase inhibitors (AL101, MRK‑560, nirogacestat and others) and antibody‑based biologics targeting Notch ligands or receptors [ABT‑165, AMG 119, rovalpituzumab tesirine (Rova‑T) and others] have been developed as investigational drugs. The DLL3‑targeting antibody‑drug conjugate (ADC) Rova‑T, and DLL3‑targeting chimeric antigen receptor‑modified T cells (CAR‑Ts), AMG 119, are promising anti‑cancer therapeutics, as are other ADCs or CAR‑Ts targeting tumor necrosis factor receptor superfamily member 17, CD19, CD22, CD30, CD79B, CD205, Claudin 18.2, fibroblast growth factor receptor (FGFR)2, FGFR3, receptor‑type tyrosine‑protein kinase FLT3, HER2, hepatocyte growth factor receptor, NECTIN4, inactive tyrosine‑protein kinase 7, inactive tyrosine‑protein kinase transmembrane receptor ROR1 and tumor‑associated calcium signal transducer 2. ADCs and CAR‑Ts could alter the therapeutic framework for refractory cancers, especially diffuse‑type gastric cancer, ovarian cancer and pancreatic cancer with peritoneal dissemination. Phase III clinical trials of Rova‑T for patients with small‑cell lung cancer and a phase III clinical trial of nirogacestat for patients with desmoid tumors are ongoing. Integration of human intelligence, cognitive computing and explainable artificial intelligence is necessary to construct a Notch‑related knowledge‑base and optimize Notch‑targeted therapy for patients with cancer. 
  |  http://www.spandidos-publications.com/ijmm/45/2/279  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31894255/  |  
------------------------------------------- 
10.1016/j.cell.2020.01.021  |   Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub-halicin-that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from &gt;107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0092-8674(20)30102-1  |  
------------------------------------------- 
10.1038/s41598-020-59603-1  |   Generation of irregular time series based on physical processes is indispensable in computing and artificial intelligence. In this report, we propose and demonstrate the generation of Schubert polynomials, which are the foundation of versatile permutations in mathematics, via optical near-field processes introduced in a photochromic crystal of diarylethene combined with a simple photon detection protocol. Optical near-field excitation on the surface of a photochromic single crystal yields a chain of local photoisomerization, forming a complex pattern on the opposite side of the crystal. The incoming photon travels through the nanostructured photochromic crystal, and the exit position of the photon exhibits a versatile pattern. We emulated trains of photons based on the optical pattern experimentally observed through double-probe optical near-field microscopy, where the detection position was determined based on a simple protocol, leading to Schubert matrices corresponding to Schubert polynomials. The versatility and correlations of the generated Schubert matrices could be reconfigured in either a soft or hard manner by adjusting the photon detection sensitivity. This is the first study of Schubert polynomial generation via physical processes or nanophotonics, paving the way for future nano-scale intelligence devices and systems. 
  |  http://dx.doi.org/10.1038/s41598-020-59603-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32066821/  |  
------------------------------------------- 
10.1002/cpt.1796  |   In the last few years, machine learning (ML) and artificial intelligence have seen a new wave of publicity fueled by the huge and ever-increasing amount of data and computational power as well as the discovery of improved learning algorithms. However, the idea of a computer learning some abstract concept from data and applying them to yet unseen situations is not new and has been around at least since the 1950s. Many of these basic principles are very familiar to the pharmacometrics and clinical pharmacology community. In this paper, we want to introduce the foundational ideas of ML to this community such that readers obtain the essential tools they need to understand publications on the topic. Although we will not go into the very details and theoretical background, we aim to point readers to relevant literature and put applications of ML in molecular biology as well as the fields of pharmacometrics and clinical pharmacology into perspective. 
  |  https://doi.org/10.1002/cpt.1796  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32128792/  |  
------------------------------------------- 
10.1016/j.mehy.2019.109507  |   Automatic decision support systems have gained importance in health sector in recent years. In parallel with recent developments in the fields of artificial intelligence and image processing, embedded systems are also used in decision support systems for tumor diagnosis. Extreme learning machine (ELM), is a recently developed, quick and efficient algorithm which can quickly and flawlessly diagnose tumors using machine learning techniques. Similarly, significantly fast and robust fuzzy C-means clustering algorithm (FRFCM) is a novel and fast algorithm which can display a high performance. In the present study, a brain tumor segmentation approach is proposed based on extreme learning machine and significantly fast and robust fuzzy C-means clustering algorithms (BTS-ELM-FRFCM) running on Raspberry Pi (PRI) hardware. The present study mainly aims to introduce a new segmentation system hardware containing new algorithms and offering a high level of accuracy the health sector. PRI's are useful mobile devices due to their cost-effectiveness and satisfying hardware. 3200 training images were used to train ELM in the present study. 20 pieces of MRI images were used for testing process. Figure of merid (FOM), Jaccard similarity coefficient (JSC) and Dice indexes were used in order to evaluate the performance of the proposed approach. In addition, the proposed method was compared with brain tumor segmentation based on support vector machine (BTS-SVM), brain tumor segmentation based on fuzzy C-means (BTS-FCM) and brain tumor segmentation based on self-organizing maps and k-means (BTS-SOM). The statistical analysis on FOM, JSC and Dice results obtained using four different approaches indicated that BTS-ELM-FRFCM displayed the highest performance. Thus, it can be concluded that the embedded system designed in the present study can perform brain tumor segmentation with a high accuracy rate. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0306-9877(19)31095-3  |  
------------------------------------------- 
10.1089/dna.2019.4859  |   Septic cardiomyopathy (SC) is a rare and harmful cardiovascular disease with decreased left ventricular (LV) output and multiple organ failure, which poses a serious threat to human life. Despite the advances in SC, its diagnostic basis and treatment methods are limited, and the specific diagnostic biomarkers and its candidate regulatory targets have not yet been fully established. In this study, the GSE79962 gene expression profile was retrieved, with 20 patients with SC and 11 healthy donors as control. Weighted gene coexpression network analysis (WGCNA) was employed to investigate gene modules that were strongly correlated with clinical phenotypes. Blue module was found to be most significantly related to SC. Moreover, Gene Ontology (GO) functional enrichment analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis were performed on the coexpression genes in blue module and showed that it was associated with metabolic pathways, oxidative phosphorylation, and cardiac muscle contraction. Furthermore, a total of 10 hub genes <i>NDUFB5</i>, <i>TIMMDC1</i>, <i>VDAC3</i>, <i>COQ10A</i>, <i>MRPL16</i> (mitochondrial ribosomal protein L16), <i>C3orf43</i>, <i>TMEM182</i>, <i>DLAT</i>, <i>NDUFA8</i>, and <i>PDHB</i> (pyruvate dehydrogenase E1 beta subunit) in the blue module were identified at transcriptional level and further validated at translational level in myocardium of an lipopolysaccharide-induced septic cardiac dysfunction mouse model. Overall, the results of quantitative real-time polymerase chain reaction were consistent with most of the microarray analysis results. Intriguingly, we observed that the highest change was <i>NDUFB5</i>, <i>TIMMDC1</i>, and <i>VDAC3</i>. These identified and validated genes provided references that would advance the understanding of molecular mechanisms of SC. Taken together, using WGCNA, the hub genes <i>NDUFB5</i>, <i>TIMMDC1</i>, and <i>VDAC3</i> might serve as potential biomarkers for diagnosis and/or therapeutic targets for precise treatment of SC in the future. 
  |  https://www.liebertpub.com/doi/full/10.1089/dna.2019.4859?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1039/c9nr09122j  |   Since the year 2014, when scientists first obtained black phosphorus using a sticky tape to peel the layers off, it has attracted tremendous interest as a novel two-dimensional material. After it was successfully produced, its outstanding optical properties have been unveiled. Various applications based on these properties have been reported. This study mainly reviews the unique optical properties and potential applications of black phosphorus. The optical performances of black phosphorus mainly include linear optical properties and nonlinear optical properties. Some examples include the anisotropic optical response, saturable absorption effect and Kerr effect. The researchers found that the nonlinear saturable absorption coefficients of black phosphorus are better than that of MoS<sub>2</sub> and WS<sub>2</sub> from the visible region to the near-infrared region. Compared with graphene, black phosphorus has a better nonlinear saturable absorption performance. After passivation or surface modification, black phosphorus is stable when exposed to oxygen and water. Herein, black phosphorus has the potential to be used in detector/sensors, solar energy harvesting, photocatalysts, optical saturable absorbers in ultrafast lasers, all optical switches, optical modulation, nanomedicine and some others in the near future. 
  |  https://doi.org/10.1039/c9nr09122j  |  
------------------------------------------- 
10.1093/jamia/ocaa023  |    Objective:  Predicting patient outcomes using healthcare/genomics data is an increasingly popular/important area. However, some diseases are rare and require data from multiple institutions to construct generalizable models. To address institutional data protection policies, many distributed methods keep the data locally but rely on a central server for coordination, which introduces risks such as a single point of failure. We focus on providing an alternative based on a decentralized approach. We introduce the idea using blockchain technology for this purpose, with a brief description of its own potential advantages/disadvantages. 
  Materials and methods:  We explain how our proposed EXpectation Propagation LOgistic REgRession on Permissioned blockCHAIN (ExplorerChain) can achieve the same results when compared to a distributed model that uses a central server on 3 healthcare/genomic datasets, and what trade-offs need to be considered when using centralized/decentralized methods. We explain how the use of blockchain technology can help decrease some of the problems encountered in decentralized methods. 
  Results:  We showed that the discrimination power of ExplorerChain can be statistically similar to its counterpart central server-based algorithm. While ExplorerChain inherited some benefits of blockchain, it had a small increased running time. 
  Discussion:  ExplorerChain has the same prerequisites as a distributed model with a centralized server for coordination. In a manner similar to secure multi-party computation strategies, it assumes that participating institutions are honest, but "curious." 
  Conclusion:  When evaluated on relatively small datasets, results suggest that ExplorerChain, which combines artificial intelligence and blockchain technologies, performs as well as a central server-based method, and may avoid some risks at the cost of efficiency. 
  |  None  |  
------------------------------------------- 
10.1158/0008-5472.CAN-19-3392  |   Considerable metabolic reprogramming has been observed in a conserved manner across multiple cancer types, but their true causes remain elusive. We present an analysis of around 50 such reprogrammed metabolisms (RM) including the Warburg effect, nucleotide <i>de novo</i> synthesis, and sialic acid biosynthesis in cancer. Analyses of the biochemical reactions conducted by these RMs, coupled with gene expression data of their catalyzing enzymes, in 7,011 tissues of 14 cancer types, revealed that all RMs produce more H<sup>+</sup> than their original metabolisms. These data strongly support a model that these RMs are induced or selected to neutralize a persistent intracellular alkaline stress due to chronic inflammation and local iron overload. To sustain these RMs for survival, cells must find metabolic exits for the nonproton products of these RMs in a continuous manner, some of which pose major challenges, such as nucleotides and sialic acids, because they are electrically charged. This analysis strongly suggests that continuous cell division and other cancerous behaviors are ways for the affected cells to remove such products in a timely and sustained manner. As supporting evidence, this model can offer simple and natural explanations to a range of long-standing open questions in cancer research including the cause of the Warburg effect. SIGNIFICANCE: Inhibiting acidifying metabolic reprogramming could be a novel strategy for treating cancer. 
  |  http://cancerres.aacrjournals.org/cgi/pmidlookup?view=long&pmid=31932456  |  
------------------------------------------- 
10.1093/cid/ciaa055  |    Background:  Reports of serious neuropsychiatric events (NPEs), specifically suicide/suicide attempts following the use of oseltamivir, have led to public concerns. This study aimed to determine whether an association exists between oseltamivir use and neuropsychiatric events. 
  Method:  This study was a population-based retrospective cohort study on a random sample of 50% of individuals in the Korean National Health Insurance Service (KNIS) database aged ≥8 years who were diagnosed with influenza from 2009 to 2017. The primary exposure was oseltamivir prescription at the time of influenza diagnosis, whereas the primary outcome was a diagnosis of an NPE within 30 days after the influenza diagnosis. Information on oseltamivir prescription, diagnoses of NPEs, demographic characteristics, comorbidities, drugs prescribed within the year before influenza diagnosis, and healthcare utilization were extracted from the KNIS database. 
  Results:  Of 3,352,015 individuals included in the analysis, 1,266,780 (37.8%) were prescribed oseltamivir. The incidence of NPEs was 0.86% and 1.16% in patients who were and were not prescribed oseltamivir, respectively (hazard ratio [HR]: 0.74, 95% confidence interval [CI]: 0.73-0.75, P&lt;0.001). Oseltamivir use was not associated with a difference in the overall risk of NPEs in the adjusted model (HR: 0.98, 95% CI: 0.96-1.01, P=0.16), but the incidence of moderate-to-severe NPEs was significantly lower in those prescribed oseltamivir (HR: 0.92, 95% CI: 0.88-0.96, P&lt;0.001). 
  Conclusion:  Treating influenza with oseltamivir does not increase the risk of NPEs; thus, public concern regarding its use is unwarranted. 
  |  https://academic.oup.com/cid/article-lookup/doi/10.1093/cid/ciaa055  |  
------------------------------------------- 
10.1016/j.tibtech.2020.01.001  |   Based on the development of automatic devices and rapid assay methods, various high-throughput screening (HTS) strategies have been established for improving the performance of industrial microorganisms. We discuss the most significant factors that can improve HTS efficiency, including the construction of screening libraries with high diversity and the use of new detection methods to expand the search range and highlight target compounds. We also summarize applications of HTS for enhancing the performance of industrial microorganisms. Current challenges and potential improvements to HTS in industrial biotechnology are discussed in the context of rapid developments in synthetic biology, nanotechnology, and artificial intelligence. Rational integration will be an important driving force for constructing more efficient industrial microorganisms with wider applications in biotechnology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0167-7799(20)30001-9  |  
------------------------------------------- 
10.1016/j.ijmedinf.2020.104089  |    Background:  Ophthalmology is one of the most requested medical speciality services in the elderly population. Although numerous studies have shown the potentials of telemedicine for the provision of ophthalmology services, the extent of its usability in older adults and the aged population is not clear. The aim of this study was to investigate the characteristics and usability features of teleophthalmology for the elderly population. 
  Method:  We searched PubMed, Embase, Scopus and CINAHL for relevant studies since 2008. Forty-five papers met the eligibility criteria and included in this review. We used a multifaceted model to extract the data and analyze findings by cross-tabulation. 
  Results:  The majority of the reviewed papers included participants of 65 years of age or older. Most of the studies were conducted in the USA (38 %). Diabetic retinopathy, glaucoma, age-related macular degeneration and cataract were the most researched eye diseases, and among the imaging technologies, retinal photography had been used the most (72 %). The studies showed teleophthalmology can improve access to specialty care, reduce the number of unnecessary visits, alleviate overloads on treatment centers, and provide more comprehensive exams. It also made services cost-saving for stakeholders and cost-effective in rural areas. However, teleophthalmology was not cost-effective for patients above 80 and low-density population areas. 
  Conclusion:  Evidence is lacking for the usability and effectiveness of teleophthalmology for the elderly population. The findings suggest that primary care providers in collaboration with ophthalmologists could provide more effective eye care to elderly population. Appropriate training is also necessary for primary care doctors to manage and refer older patients in a timely manner. Diagnostic value and cost-effective imaging modalities which are the core of the teleophthalmology, can be enhanced by image processing techniques and artificial intelligence. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1386-5056(19)30519-2  |  
------------------------------------------- 
10.3389/fchem.2020.00108  |   The cardinal role of microtubules in cell mitosis makes them interesting drug targets for many pharmacological treatments, including those against cancer. Moreover, different expression patterns between cell types for several tubulin isotypes represent a great opportunity to improve the selectivity and specificity of the employed drugs and to design novel compounds with higher activity only on cells of interest. In this context, tubulin isotype βIII represents an excellent target for anti-tumoral therapies since it is overexpressed in most cancer cells and correlated with drug resistance. Colchicine is a well-known antimitotic agent, which is able to bind the tubulin dimer and to halt the mitotic process. However, it shows high toxicity also on normal cells and it is not specific for isotype βIII. In this context, the search for colchicine derivatives is a matter of great importance in cancer research. In this study, homology modeling techniques, molecular docking, and molecular dynamics simulations have been employed to characterize the interaction between 55 new promising colchicine derivatives and tubulin isotype βIII. These compounds were screened and ranked based on their binding affinity and conformational stability in the colchicine binding site of tubulin βIII. Results from this study point the attention on an amide of 4-chlorine thiocolchicine. This colchicine-derivative is characterized by a unique mode of interaction with tubulin, compared to all other compounds considered, which is primarily characterized by the involvement of the α-T5 loop, a key player in the colchicine binding site. Information provided by the present study may be particularly important in the rational design of colchicine-derivatives targeting drug resistant cancer phenotypes. 
  |  https://doi.org/10.3389/fchem.2020.00108  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32154219/  |  
------------------------------------------- 
10.30773/pi.2019.0038  |    Objective:  Emotional processing dysfunction evident in eating disorders (ED) such as anorexia nervosa (AN) and bulimia nervosa (BN), is considered relevant to the development and maintenance of these disorders. The purpose of the current functional magnetic resonance imaging (fMRI) study was to pilot a comparison of the activity of the fronto-limbic and fronto-striatal brain areas during an emotion processing task in persons with ED. 
  Methods:  24 women patients with ED were scanned, while showing emotionally stimulating (pleasant, unpleasant) and neutral images from the International Affective Picture System (IAPS). 
  Results:  During the pleasant condition, significant differences in Dorsolateral Prefrontal Cortex (DLPFC) activations were found with AN participants presenting greater activation compared to BN and ED comorbid groups (EDc) and healthy controls also showing greater activation of this brain area compared to BN and EDc. Left putamen was less activated in EDc compared to both controls (C) and AN. During the unpleasant condition, AN participants showed hyperactivation of the Orbito-frontal Cortex (OFC) when compared to EDc. 
  Conclusion:  This study highlights the potential functional relevance of brain areas that have been associated with self-control. These findings should help advance understanding the neural substrate of ED, though they should be considered as preliminary and be cautiously interpreted. 
  |  http://psychiatryinvestigation.org/journal/view.php?doi=10.30773/pi.2019.0038  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32160692/  |  
------------------------------------------- 
10.3390/s20061748  |   Grip force control during robotic in-hand manipulation is usually modeled as a monolithic task, where complex controllers consider the placement of all fingers and the contact states between each finger and the gripped object in order to compute the necessary forces to be applied by each finger. Such approaches normally rely on object and contact models and do not generalize well to novel manipulation tasks. Here, we propose a modular grip stabilization method based on a proposition that explains how humans achieve grasp stability. In this biomimetic approach, independent tactile grip stabilization controllers ensure that slip does not occur locally at the engaged robot fingers. Local slip is predicted from the tactile signals of each fingertip sensor i.e., BioTac and BioTac SP by Syntouch. We show that stable grasps emerge without any form of central communication when such independent controllers are engaged in the control of multi-digit robotic hands. The resulting grasps are resistant to external perturbations while ensuring stable grips on a wide variety of objects. 
  |  http://www.mdpi.com/resolver?pii=s20061748  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245193/  |  
------------------------------------------- 
10.1002/prca.201900040  |   The increasing storage of information, data, and forms of knowledge has led to the development of new technologies that can help to accomplish complex tasks in different areas, such as in dentistry. In this context, the role of computational methods, such as radiomics and Artificial Intelligence (AI) applications, has been progressing remarkably for dentomaxillofacial radiology (DMFR). These tools bring new perspectives for diagnosis, classification, and prediction of oral diseases, treatment planning, and for the evaluation and prediction of outcomes, minimizing the possibilities of human errors. A comprehensive review of the state-of-the-art of using radiomics and machine learning (ML) for imaging in oral healthcare is presented in this paper. Although the number of published studies is still relatively low, the preliminary results are very promising and in a near future, an augmented dentomaxillofacial radiology (ADMFR) will combine the use of radiomics-based and AI-based analyses with the radiologist's evaluation. In addition to the opportunities and possibilities, some challenges and limitations have also been discussed for further investigations. 
  |  https://doi.org/10.1002/prca.201900040  |  
------------------------------------------- 
10.3233/JAD-200183  |    Background:  The Salzburg Dementia Test Prediction (SDTP), developed using artificial intelligence and based on the Mini-Mental State Examination (MMSE), was recently introduced as a brief cognitive screening tool for cognitive impairment. 
  Objective:  In the current study, we investigated whether the STDP can be used as a valid bed-side cognitive screening tool for dementia patients, in an English-speaking, medical inpatient setting. 
  Methods:  216 medically ill older patients who had completed the MMSE (from which the SDTP scores can be calculated), with a subsample 58 patients who had also completed the ACE-R/ACE-III scales. Diagnosis of one of four dementia types (n = 127) and socio-demographic information were also collected. MMSE, SDTP, ACE-R/ACE-III, and dementia diagnosis were used to examine the construct validity of the SDTP through assessments of the structural, concurrent, and convergent validity. 
  Results:  The SDTP shows structural validity through demonstrating uni-dimensionality. Construct validity was demonstrated by sufficient correlation sizes with MMSE scores against a benchmark correlation size for most of the subsample, except vascular dementia. Convergent validity was demonstrated for the STDP with equivalent correlations sizes with ACE-R/ACE-III as the MMSE across all samples, though for vascular dementia the magnitude of this correlation was not as strong. 
  Conclusions:  Our findings support using STDP as a brief assessment tool among patients who have been diagnosed with Alzheimer's disease, Lewy body disease, and mixed dementia; however, there is some statistical variability to overall MMSE scores and correlations with the ACE-R/ACE-III among patients diagnosed with vascular dementia. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/JAD-200183  |  
------------------------------------------- 
10.21037/jtd.2019.12.119  |    Background:  The main purpose of the study was to develop an early screening method for aortic dissection (AD) based on machine learning. Due to the rarity of AD and the complexity of symptoms, many doctors have no clinical experience with it. Many patients are not suspected of having AD, which lead to a high rate of misdiagnosis. Here, we report the preliminary study and feasibility of rapid and accurate screening method of AD with machine learning methods. 
  Methods:  The dataset analyzed was composed by examination data provided by the Xiangya Hospital Central South University of China which include a total of 60,000 samples, including aortic patients and non-aortic ones. Each sample has 76 features which is consist of routine examinations and other easily accessible information. Since the proportion of people who are affected is usually imbalanced compared to non-diseased people, multiple machine learning models were used, include AdaBoost, SmoteBagging, EasyEnsemble and CalibratedAdaMEC. They used different methods such as ensemble learning, undersampling, oversampling, and cost-sensitivity to solve data imbalance problems. 
  Results:  AdaBoost performed poorly with an average recall of 16.1% and a specificity of 99.8%. SmoteBagging achieved a statistically significant better performance for this problem with an average recall of 78.1% and a specificity of 79.2%. EasyEnsemble reached the values of 77.8% and 79.3% for recall and specificity respectively. CalibratedAdaMEC's recall and specificity are 75.8% and 76%. 
  Conclusions:  It was found that the screening performance of the models evaluated in this paper had a misdiagnosis rate lower than 25% except AdaBoost. The data used in these methods are only routine inspection data. This means that machine learning methods can help us build a fast, cheap, worthwhile and effective early screening approach for AD. 
  |  https://doi.org/10.21037/jtd.2019.12.119  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274126/  |  
------------------------------------------- 
10.1142/S0129065719500187  |   Fatigue is one problem with driving as it can lead to difficulties with sustaining attention, behavioral lapses, and a tendency to ignore vital information or operations. In this research, we explore multimodal physiological phenomena in response to driving fatigue through simultaneous functional near-infrared spectroscopy (fNIRS) and electroencephalography (EEG) recordings with the aim of investigating the relationships between hemodynamic and electrical features and driving performance. Sixteen subjects participated in an event-related lane-deviation driving task while measuring their brain dynamics through fNIRS and EEGs. Three performance groups, classified as Optimal, Suboptimal, and Poor, were defined for comparison. From our analysis, we find that tonic variations occur before a deviation, and phasic variations occur afterward. The tonic results show an increased concentration of oxygenated hemoglobin (HbO<sub>2</sub>) and power changes in the EEG theta, alpha, and beta bands. Both dynamics are significantly correlated with deteriorated driving performance. The phasic EEG results demonstrate event-related desynchronization associated with the onset of steering vehicle in all power bands. The concentration of phasic HbO<sub>2</sub> decreased as performance worsened. Further, the negative correlations between tonic EEG delta and alpha power and HbO<sub>2</sub> oscillations suggest that activations in HbO<sub>2</sub> are related to mental fatigue. In summary, combined hemodynamic and electrodynamic activities can provide complete knowledge of the brain's responses as evidence of state changes during fatigue driving. 
  |  https://www.worldscientific.com/doi/full/10.1142/S0129065719500187?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.ijid.2020.03.021  |    Objective:  An emerging outbreak of COVID-19 has been detected in at least 26 countries worldwide. Given this pandemic situation, robust risk communication is urgently needed particularly in affected countries. Therefore, this study explored the potential use of Google Trends (GT) to monitor public restlessness toward COVID-19 epidemic infection in Taiwan. 
  Methods:  We retrieved GT data for the specific locations of Taiwan nationwide and subregions using defined search terms related to coronavirus, handwashing, and face masks. 
  Results:  Searches related to COVID-19 and face masks in Taiwan increased rapidly, following the announcements of Taiwan' first imported case and reached its peak as local cases were reported. However, searches for handwashing were gradually increased in period of face masks shortage. Moreover, high to moderate correlations between Google relative search volume (RSV) and COVID-19 cases were found in Taipei (lag-3), New Taipei (lag-2), Taoyuan (lag-2), Tainan (lag-1), Taichung (lag0), and Kaohsiung (lag0). 
  Conclusion:  In response to the ongoing outbreak, our results demonstrated that GT could potentially define the proper timing and location for practicing appropriate risk communication strategies to the affected population. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1201-9712(20)30140-5  |  
------------------------------------------- 
10.1111/pedi.13029  |   Hypoglycaemia in children is a major risk factor for adverse neurodevelopment with rates as high as 50% in hyperinsulinaemic hypoglycaemia (HH). A key part of management relies upon timely identification and treatment of hypoglycaemia. The current standard of care for glucose monitoring is by infrequent fingerprick plasma glucose testing but this carries a high risk of missed hypoglycaemia identification. High-frequency Continuous Glucose Monitoring (CGM) offers an attractive alternative for glucose trend monitoring and glycaemic phenotyping but its utility remains largely unestablished in disorders of hypoglycaemia. Attempts to determine accuracy through correlation with plasma glucose measurements using conventional methods such as Mean Absolute Relative Difference (MARD) overestimate accuracy at hypoglycaemia. The inaccuracy of CGM in true hypoglycaemia is amplified by calibration algorithms that prioritize hyperglycaemia over hypoglycaemia with minimal objective evidence of efficacy in HH. Conversely, alternative algorithm design has significant potential for predicting hypoglycaemia to prevent neuroglycopaenia and consequent brain dysfunction in childhood disorders. Delays in the detection of hypoglycaemia, alarm fatigue, device calibration and current high cost are all barriers to the wider adoption of CGM in disorders of hypoglycaemia. However, machine learning, artificial intelligence and other computer-generated algorithms now offer significant potential for further improvement in CGM device technology and widespread application in childhood hypoglycaemia. 
  |  https://doi.org/10.1111/pedi.13029  |  
------------------------------------------- 
10.1007/s11224-020-01536-6  |   At the end of December 2019, a novel strain of coronavirus, given the name of 2019-nCoV, emerged for exhibiting symptoms of severe acute respiratory syndrome. The virus is spreading rapidly in China and around the globe, affecting thousands of people leading to a pandemic. To control the mortality rate associated with the 2019-nCoV, prompt steps are needed. Until now there is no effective treatment or drug present to control its life-threatening effects in the humans. The scientist is struggling to find new inhibitors of this deadly virus. In this study, to identify the effective inhibitor candidates against the main protease (Mpro) of 2019-nCoV, computational approaches were adopted. Phytochemicals having immense medicinal properties as ligands were docked against the Mpro of 2019-nCoV to study their binding properties. ADMET and DFT analyses were also further carried out to analyze the potential of these phytochemicals as an effective inhibitor against Mpro of 2019-nCoV. 
  |  None  |  
------------------------------------------- 
10.3390/s20061762  |   According to the World Health Organization (WHO), Diabetes Mellitus (DM) is one of the most prevalent diseases in the world. It is also associated with a high mortality index. Diabetic foot is one of its main complications, and it comprises the development of plantar ulcers that could result in an amputation. Several works report that thermography is useful to detect changes in the plantar temperature, which could give rise to a higher risk of ulceration. However, the plantar temperature distribution does not follow a particular pattern in diabetic patients, thereby making it difficult to measure the changes. Thus, there is an interest in improving the success of the analysis and classification methods that help to detect abnormal changes in the plantar temperature. All this leads to the use of computer-aided systems, such as those involved in artificial intelligence (AI), which operate with highly complex data structures. This paper compares machine learning-based techniques with Deep Learning (DL) structures. We tested common structures in the mode of transfer learning, including AlexNet and GoogleNet. Moreover, we designed a new DL-structure, which is trained from scratch and is able to reach higher values in terms of accuracy and other quality measures. The main goal of this work is to analyze the use of AI and DL for the classification of diabetic foot thermograms, highlighting their advantages and limitations. To the best of our knowledge, this is the first proposal of DL networks applied to the classification of diabetic foot thermograms. The experiments are conducted over thermograms of DM and control groups. After that, a multi-level classification is performed based on a previously reported thermal change index. The high accuracy obtained shows the usefulness of AI and DL as auxiliary tools to aid during the medical diagnosis. 
  |  http://www.mdpi.com/resolver?pii=s20061762  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32235780/  |  
------------------------------------------- 
10.1016/j.ijpharm.2020.119364  |   Cancer is a malignancy engendering enormous global mortality, steering extensive research for early diagnosis and efficacious prognosis leading to emergence of cancer sensing technologies for multitudinous biomarkers. In this context, nanofibers, imparting high surface area, facile production, morphology control, and synergistic properties attainable, are poised to be inevitable in futuristic sensing devices for predictive diagnostics when integrated with artificial intelligence and machine learning. To this end, fundamentals governing the sensor response and their analytical performance have been discussed. The headways in organic and inorganic nanofibers for biomarker gas sensing, fluid sample sensing and imaging have been supplemented with discussions on materials for nanofiber formation, along with sensitizing materials, and formation of sensing elements by processes like surface deposition on nanofibers, immobilising, calcination, etc. and their effect on final sensing device properties. The review culminates by summarising the conceptual understanding of the hitherto progress leading to achievement of excellent analytical performance giving detection limits to the order of 1.6 pM concentration and response time of as low as 0.5 s. Current bottlenecks in this state of the art have been delineated and pathways for future research are discussed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-5173(20)30348-3  |  
------------------------------------------- 
10.1016/j.envpol.2019.113411  |   Cadmium (Cd), which is a toxic non-essential heavy metal capable of entering plants and thus the food chain, constitutes a major environmental and health concern worldwide. An understanding of the tools used by plants to overcome Cd stress could lead to the production of food crops with lower Cd uptake capacity and of plants with greater Cd uptake potential for phytoremediation purposes in order to restore soil efficiency in self-sustaining ecosystems. The signalling molecule nitric oxide (NO), whose function remains unclear, has recently been involved in responses to Cd stress. Using different mutants, such as nia1nia2, nox1, argh1-1 and Atnoa1, which were altered in NO metabolism, we analysed various parameters related to reactive oxygen and nitrogen species (ROS/RNS) metabolism and seedling fitness following germination and growth under Cd treatment conditions for seven days. Seedling roots were the most affected, with an increase in ROS and RNS observed in wild type (WT) seedling roots, leading to increased oxidative damage and fitness loss. Mutants that showed lower NO levels in seedling roots under Cd stress were more resistant than WT seedlings due to the maintenance of antioxidant systems which protect against oxidative damage. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0269-7491(19)33570-5  |  
------------------------------------------- 
10.3390/s20071880  |   Planning an optimal path for a mobile robot is a complicated problem as it allows the mobile robots to navigate autonomously by following the safest and shortest path between starting and goal points. The present work deals with the design of intelligent path planning algorithms for a mobile robot in static and dynamic environments based on swarm intelligence optimization. A modification based on the age of the ant is introduced to standard ant colony optimization, called modified aging ant colony optimization (AACO). The AACO was implemented in association with grid-based modeling for the static and dynamic environments to solve the path planning problem. The simulations are run in the MATLAB environment to test the validity of the proposed algorithms. Simulations showed that the proposed path planning algorithms result in superior performance by finding the shortest and the most free-collision path under various static and dynamic scenarios. Furthermore, the superiority of the proposed algorithms was proved through comparisons with other traditional path planning algorithms with different static environments. 
  |  http://www.mdpi.com/resolver?pii=s20071880  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231091/  |  
------------------------------------------- 
10.3928/1081597X-20200103-01  |    Purpose:  To evaluate epithelial Zernike indices as a differentiator of epithelial remodeling after different refractive procedures. 
  Methods:  Optical coherence tomography (OCT) images of 22 laser in situ keratomileusis, 22 small incision lenticule extraction, 15 photorefractive keratectomy (PRK), and 17 transepithelial PRK eyes were evaluated retrospectively before and after surgery. A custom algorithm was used to calculate the epithelial Zernike indices from the three-dimensional distribution of epithelial thickness distribution. The epithelial Zernike indices were also compared with the local measurements of epithelial thickness, used conventionally from the current clinical OCT. A decision tree classifier was built, one in which flap/cap and surface procedures were classified (2G) and another in which all surgical groups were classified separately (4G). 
  Results:  Local measurements of thicknesses changed significantly after all surgeries (P &lt; .05), but these changes were similar in magnitude between the surgical platforms (P &gt; .05). The surgeries not only changed the epithelial Zernike indices (P &lt; .05), but also resulted in differential changes in epithelial thickness distribution based on the type of surgery (P &lt; .05). In the 2G analyses with local measurements of epithelial thickness, the area under the curve, sensitivity, and specificity were 0.57 ± 0.07, 42.11%, and 57.89%, respectively. Further, the accuracy was limited to less than 60%. In the 2G analyses with epithelial Zernike indices, the area under the curve, sensitivity, and specificity were 0.79 ± 0.05, 86.4%, and 71.9%, respectively. Here, the accuracy was limited between 70% and 80%. Similar trends were observed with 4G analyses. 
  Conclusions:  The epithelial Zernike indices were significantly better in identifying surgery-specific three-dimensional remodeling of the thickness compared to local measurements of epithelial thickness. Further, the changes in Zernike indices were independent of the magnitude of refractive error but not the type of surgery. [J Refract Surg. 2020;36(2):97-103.]. 
  |  https://www.healio.com/doiresolver?doi=10.3928/1081597X-20200103-01  |  
------------------------------------------- 
10.1016/j.mehy.2019.109503  |   Invasive ductal carcinoma cancer, which invades the breast tissues by destroying the milk channels, is the most common type of breast cancer in women. Approximately, 80% of breast cancer patients have invasive ductal carcinoma and roughly 66.6% of these patients are older than 55 years. This situation points out a powerful relationship between the type of breast cancer and progressed woman age. In this study, the classification of invasive ductal carcinoma breast cancer is performed by using deep learning models, which is the sub-branch of artificial intelligence. In this scope, convolutional neural network models and the autoencoder network model are combined. In the experiment, the dataset was reconstructed by processing with the autoencoder model. The discriminative features obtained from convolutional neural network models were utilized. As a result, the most efficient features were determined by using the ridge regression method, and classification was performed using linear discriminant analysis. The best success rate of classification was achieved as 98.59%. Consequently, the proposed approach can be admitted as a successful model in the classification. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0306-9877(19)31186-7  |  
------------------------------------------- 
10.1016/j.neuroimage.2020.116870  |   The ability to detect visual targets in complex background varies across individuals and are affected by factors such as stimulus saliency and top-down attention. Here, we investigated how the saliency of visual background (naturalistic cartoon video vs. blank screen) and top-down attention (single vs. dual tasks) separately affect individual ability to detect visual targets. Behaviorally, we found that target detection accuracy decreased and reaction time elongated when the background was salient or during dual tasking. The EEG response to visual background was recorded using a novel stimulus tagging technique. This response was strongest in occipital electrodes and was sensitive to background saliency but not dual tasking. In contrast, the event-related potential (ERP) evoked by the visual target was strongest in central electrodes, and was affected by both background saliency and dual tasking. With a cartoon background, the EEG responses to visual targets, presented in the central visual field, and the EEG responses to peripheral visual background could both predict individual target detection performance. When these two responses were combined, better prediction was achieved. These results suggest that neural processing of visual targets and background jointly contribute to individual visual target detection performance. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(20)30356-6  |  
------------------------------------------- 
10.1016/j.forsciint.2020.110142  |   Forensic science is one of the most modern and applied fields of science, today and comprises of various domains. These include Fingerprints analysis, Questioned document analysis, Forensic DNA and serology, Anthropometry, Cyber and Digital forensics, and many other fields. All these fields aid the process of decision making in the courts of law and legal settings; however, DNA profiling and its analyses are one of the most important aspects of forensic science today. In Forensic DNA analysis, the statistical calculations are very important to estimate the conclusiveness of DNA evidence in forensic cases; and to establish paternity and relatedness in civil and criminal matters. These statistics, when performed manually, leave a chance of error or ambiguity in the calculation, and are hectic and time-taking. Therefore, the computer-aided approaches are opted in forensics to perform DNA statistics calculations. Keeping its importance in mind, a highly accurate windows-based software program namely ForeStatistics is proposed in this study. ForeStatistics is rich in features such as DNA statistical calculations, DNA profile management and its matching. The software can estimate random match probabilities for single-source profiles, combined probability of inclusion for mixed profiles, paternity index of a disputed child in duo and trio cases, paternity of the disputed child when the alleged father is related to mother or biological father and relatedness in cases of grandparents/grandchild, avuncular relation and cousin. It is validated through different protocols and the validation of ForeStatistics depicts that it is highly accurate in terms of performing DNA statistics or DNA profile matching. Thus, it is concluded, that ForeStatistics has a great utility in the field of Forensic DNA analysis and can help DNA scientists, in performing various DNA related statistics, accurately and very efficiently. ForeStatistics can be downloaded freely from (http://zeetu.org/forestatistics.html). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0379-0738(20)30004-9  |  
------------------------------------------- 
10.1200/CCI.19.00079  |    Purpose:  Less than 5% of patients with cancer enroll in clinical trials, and 1 in 5 trials are stopped for poor accrual. We evaluated an automated clinical trial matching system that uses natural language processing to extract patient and trial characteristics from unstructured sources and machine learning to match patients to clinical trials. 
  Patients and methods:  Medical records from 997 patients with breast cancer were assessed for trial eligibility at Highlands Oncology Group between May and August 2016. System and manual attribute extraction and eligibility determinations were compared using the percentage of agreement for 239 patients and 4 trials. Sensitivity and specificity of system-generated eligibility determinations were measured, and the time required for manual review and system-assisted eligibility determinations were compared. 
  Results:  Agreement between system and manual attribute extraction ranged from 64.3% to 94.0%. Agreement between system and manual eligibility determinations was 81%-96%. System eligibility determinations demonstrated specificities between 76% and 99%, with sensitivities between 91% and 95% for 3 trials and 46.7% for the 4th. Manual eligibility screening of 90 patients for 3 trials took 110 minutes; system-assisted eligibility determinations of the same patients for the same trials required 24 minutes. 
  Conclusion:  In this study, the clinical trial matching system displayed a promising performance in screening patients with breast cancer for trial eligibility. System-assisted trial eligibility determinations were substantially faster than manual review, and the system reliably excluded ineligible patients for all trials and identified eligible patients for most trials. 
  |  http://ascopubs.org/doi/full/10.1200/CCI.19.00079?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.5935/1518-0557.20190081  |   Spermatozoa wage battle to conquer fertilization but the traits needed to succeed remain elusive. The natural advantageous qualities that enable only a few selected sperm cells to reach the site of fertilization remain unknown. Although in vitro fertilization (IVF) facilitates the job of spermatozoa, a universally acceptable means of sperm selection is yet to be developed. No objective or reliable sperm quality indicators have been established and sperm selection is, to a great extent, based on subjective qualitative evaluation. The best method for sperm selection in IVF presents several challenges: intrinsic sperm qualities cannot be evaluated and the ideal endpoint for these studies is debatable. An ideal method for sperm selection in ART should be noninvasive and cost-effective, and allow the identification of high-quality spermatozoa and yield better outcomes in terms of pregnancy and live birth rates. This narrative review included 85 papers and focused on the new available methods and technologies that might shed some light on sperm selection in IVF. It discusses the available data on microfluidic devices, omics profiling, micronuclei studies, sperm plasma membrane markers, and other techniques, such as Magnetic Activated Cell Sorting (MACS), Raman micro-spectroscopy, and artificial intelligence systems. The new techniques herein reviewed offer fresh approaches to an old problem, for which a definite solution has yet to cross the bridge from bench to IVF clinics around the world, since clinical usefulness and application remain unproven. 
  |  https://doi.org/10.5935/1518-0557.20190081  |  
------------------------------------------- 
10.1097/WCO.0000000000000795  |    Purpose of review:  With the advancement of computational approaches and abundance of biomedical data, a broad range of neurodegenerative disease models have been developed. In this review, we argue that computational models can be both relevant and useful in neurodegenerative disease research and although the current established models have limitations in clinical practice, artificial intelligence has the potential to overcome deficiencies encountered by these models, which in turn can improve our understanding of disease. 
  Recent findings:  In recent years, diverse computational approaches have been used to shed light on different aspects of neurodegenerative disease models. For example, linear and nonlinear mixed models, self-modeling regression, differential equation models, and event-based models have been applied to provide a better understanding of disease progression patterns and biomarker trajectories. Additionally, the Cox-regression technique, Bayesian network models, and deep-learning-based approaches have been used to predict the probability of future incidence of disease, whereas nonnegative matrix factorization, nonhierarchical cluster analysis, hierarchical agglomerative clustering, and deep-learning-based approaches have been employed to stratify patients based on their disease subtypes. Furthermore, the interpretation of neurodegenerative disease data is possible through knowledge-based models which use prior knowledge to complement data-driven analyses. These knowledge-based models can include pathway-centric approaches to establish pathways perturbed in a given condition, as well as disease-specific knowledge maps, which elucidate the mechanisms involved in a given disease. Collectively, these established models have revealed high granular details and insights into neurodegenerative disease models. 
  Summary:  In conjunction with increasingly advanced computational approaches, a wide spectrum of neurodegenerative disease models, which can be broadly categorized into data-driven and knowledge-driven, have been developed. We review the state of the art data and knowledge-driven models and discuss the necessary steps which are vital to bring them into clinical application. 
  |  http://dx.doi.org/10.1097/WCO.0000000000000795  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32073441/  |  
------------------------------------------- 
10.1186/s12967-020-02281-4  |   Prostate cancer (PCa) is a common malignant tumor with increasing incidence and high heterogeneity among males worldwide. In the era of big data and artificial intelligence, the paradigm of biomarker discovery is shifting from traditional experimental and small data-based identification toward big data-driven and systems-level screening. Complex interactions between genetic factors and environmental effects provide opportunities for systems modeling of PCa genesis and evolution. We hereby review the current research frontiers in informatics for PCa clinical translation. First, the heterogeneity and complexity in PCa development and clinical theranostics are introduced to raise the concern for PCa systems biology studies. Then biomarkers and risk factors ranging from molecular alternations to clinical phenotype and lifestyle changes are explicated for PCa personalized management. Methodologies and applications for multi-dimensional data integration and computational modeling are discussed. The future perspectives and challenges for PCa systems medicine and holistic healthcare are finally provided. 
  |  https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02281-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32143723/  |  
------------------------------------------- 
10.21037/cco.2020.01.06  |   The improvement of tumor biomarkers prepared for clinical use is a long process. A good biomarker should predict not only prognosis but also the response to therapies. In this review, we describe the biomarkers of neoadjuvant/adjuvant chemotherapy for breast cancer, considering different breast cancer subtypes. In hormone receptor (HR)-positive/human epidermal growth factor 2 (HER2)-negative breast cancers, various genomic markers highly associated with proliferation have been tested. Among them, only two genomic signatures, the 21-gene recurrence score and 70-gene signature, have been reported in prospective randomized clinical trials and met the primary endpoint. However, these genomic markers did not suffice in HER2-positive and triple-negative (TN) breast cancers, which present only classical clinical and pathological information (tumor size, nodal or distant metastatic status) for decision making in the adjuvant setting in daily clinic. Recently, patients with residual invasive cancer after neoadjuvant chemotherapy are at a high-risk of recurrence for metastasis, which, in turn, make these patients best applicants for clinical trials. Two clinical trials have shown improved outcomes with post-operative capecitabine and ado-trastuzumab emtansine treatment in patients with either TN or HER2-positive breast cancer, respectively, who had residual disease after neoadjuvant chemotherapy. Furthermore, tumor-infiltrating lymphocytes (TILs) have been reported to have a predictive value for prognosis and response to chemotherapy from the retrospective analyses. So far, TILs have to not be used to either withhold or prescribe chemotherapy based on the absence of standardized evaluation guidelines and confirmed information. To overcome the low reproducibility of evaluations of TILs, gene signatures or digital image analysis and machine learning algorithms with artificial intelligence may be useful for standardization of assessment for TILs in the future. 
  |  https://doi.org/10.21037/cco.2020.01.06  |  
------------------------------------------- 
10.1245/s10434-020-08307-x  |    Background:  Despite high success rates, flap failure remains an inherent risk in microvascular breast reconstruction. Identifying patients who are at high risk for flap failure would enable us to recommend alternative reconstructive techniques. However, as flap failure is a rare event, identification of risk factors is statistically challenging. Machine learning is a form of artificial intelligence that automates analytical model building. It has been proposed that machine learning can build superior prediction models when the outcome of interest is rare. 
  Methods:  In this study we evaluate machine learning resampling and decision-tree classification models for the prediction of flap failure in a large retrospective cohort of microvascular breast reconstructions. 
  Results:  A total of 1012 patients were included in the study. Twelve patients (1.1%) experienced flap failure. The ROSE informed oversampling technique and decision-tree classification resulted in a strong prediction model (AUC 0.95) with high sensitivity and specificity. In the testing cohort, the model maintained acceptable specificity and predictive power (AUC 0.67), but sensitivity was reduced. The model identified four high-risk patient groups. Obesity, comorbidities and smoking were found to contribute to flap loss. The flap failure rate in high-risk patients was 7.8% compared with 0.44% in the low-risk cohort (p = 0.001). 
  Conclusions:  This machine-learning risk prediction model suggests that flap failure may not be a random event. The algorithm indicates that flap failure is multifactorial and identifies a number of potential contributing factors that warrant further investigation. 
  |  https://dx.doi.org/10.1245/s10434-020-08307-x  |  
------------------------------------------- 
10.1063/1.5133026  |   We report the use of a deep learning model to design <i>de novo</i> proteins, based on the interplay of elementary building blocks via hierarchical patterns. The deep neural network model is based on translating protein sequences and structural information into a musical score that features different pitches for each of the amino acids, and variations in note length and note volume reflecting secondary structure information and information about the chain length and distinct protein molecules. We train a deep learning model whose architecture is composed of several long short-term memory units from data consisting of musical representations of proteins classified by certain features, focused here on alpha-helix rich proteins. Using the deep learning model, we then generate <i>de novo</i> musical scores and translate the pitch information and chain lengths into sequences of amino acids. We use a Basic Local Alignment Search Tool to compare the predicted amino acid sequences against known proteins, and estimate folded protein structures using the Optimized protein fold RecognitION method (ORION) and MODELLER. We find that the method proposed here can be used to design <i>de novo</i> proteins that do not exist yet, and that the designed proteins fold into specified secondary structures. We validate the newly predicted protein by molecular dynamics equilibration in explicit water and subsequent characterization using a normal mode analysis. The method provides a tool to design novel protein materials that could find useful applications as materials in biology, medicine, and engineering. 
  |  https://doi.org/10.1063/1.5133026  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32206742/  |  
------------------------------------------- 
10.1021/acsami.0c04482  |   Flexible electromagnetic interference (EMI) shielding materials with excellent thermal conductivities and Joule heating performances are of urgent demand in the communication industry, artificial intelligence, and wearable electronics. In this work, highly conductive silver nanowires (AgNWs) were prepared using the polyol method. Cellulose sheets were then prepared by dissolving natural cotton in a green and efficient NaOH/urea aqueous solution. Finally, multifunctional flexible EMI shielding AgNWs/cellulose films were fabricated based on vacuum-assisted filtration and hot-pressing. AgNWs are evenly embedded in the inner cellulose matrix and overlap with each other to form a 3D network. AgNWs/cellulose films, with a thickness of 44.5 μm, obtain the superior EMI shielding effectiveness of 101 dB, which is the highest value ever reported for shielding materials with the same thickness. In addition, AgNWs/cellulose films present excellent tensile strength (60.7 MPa) and tensile modulus (3.35 GPa), ultrahigh electrical conductivity (σ, 5571 S/cm), and excellent in-plane thermal conductivity coefficient (λ<sub>∥</sub>, 10.55 W/mK), which can effectively dissipate the heat accumulation. Interestingly, AgNWs/cellulose films also show outstanding Joule heating performances, good stability, and sensitive temperature response at driving voltages, absolutely safe for the human body. Therefore, our fabricated multifunctional flexible AgNWs/cellulose films have broad prospects in the fields of EMI shielding and protection of outdoor large-scale power transformers and wearable electronics. 
  |  https://dx.doi.org/10.1021/acsami.0c04482  |  
------------------------------------------- 
10.3389/fnhum.2020.00094  |   Previous literature on shooting performance neurofeedback training (SP-NFT) to enhance performance usually focused on changes in behavioral indicators, but research on the physiological features of SP-NFT is lacking. To explore the effects of SP-NFT on trainability and neuroplasticity, we conducted a study in which 45 healthy participants were randomly divided into three groups: based on sensory-motor rhythm of C3, Cz and C4 (SMR group), based on alpha rhythm of T3 and T4 (Alpha group), and no NFT (control group). The training was performed for six sessions for 3 weeks. Before and after the SP-NFT, we evaluated changes in shooting performance and resting electroencephalography (EEG) frequency power, participant's subjective task appraisal, neurofeedback trainability score, and EEG feature. Statistical analysis showed that the shooting performance of the participants in the SMR group improved significantly, the participants in the Alpha group decreased, and that of participants in the control group have no change. Meanwhile, the resting EEG power features of the two NFT groups changed specifically after training. The training process data showed that the training difficulty was significantly lower in the SMR group than in the Alpha group. Both NFT groups could improve the neurofeedback trainability scores and change the feedback features by means of their mind strategy. These results may provide evidence of trainability and neuroplasticity for SP-NFT, suggesting that the SP-NFT is effective in brain regulation and thus provide a potential method to improve shooting performance. 
  |  https://doi.org/10.3389/fnhum.2020.00094  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32265676/  |  
------------------------------------------- 
10.3389/fonc.2020.00364  |   Quality assured pathology services are integral to provision of optimal management for patients with head and neck cancer. Pathology services vary globally and are dependent on resources in terms of both laboratory provision and availability of a highly trained and accredited workforce. Ensuring a high-quality pathology service depends largely on close working and effective communication between the clinical team providing treatment and the pathologists providing laboratory input. Laboratory services should be quality assured by achieving external accreditation, most often by conforming to International Organization for Standardization (ISO) standards such as ISO15189 sometimes with ISO17025 or alternatively ISO17020. Quality of diagnostic reporting can be assured by the ISO but clinical teams should endeavor to work with pathologists who engage in continuing professional development, external quality assurance and audit. Research also contributes to diagnostic reporting quality. A number of initiatives in the UK such as the EPSRC/MRC funded Molecular Pathology Nodes and the National Cancer Research Institute Cellular-Molecular Pathology initiative (C-M Path), for example, have linked pathologists, industry and researchers. This has resulted in centers leading in digital innovation, artificial intelligence, translational research and clinical trials supported by pathologists. For rare tumors and contemporary molecular diagnostics, biopsy material can increasingly be shared with expert specialist pathologists working in specialist centers, particularly by using digital pathology platforms with potentially global reach. High quality services for the majority of diagnostic processes required for head and neck cancer management is best provided by local pathologists where communication with the treating team is more effective than with pathologists working in remote centers. Quality assurance is an increasingly important aspect of pathology, assuring not only effective turnaround times and accuracy for the diagnostic service but also high quality consistent reporting for clinical trials where even small pathology errors can potentially produce a significant bias and in the worst case negate the value of a completed trial. Better outcomes have been associated with centers engaged in clinical trials than in non-participating centers. Provision of a quality assured pathology service should extend to both the research and diagnostic services. 
  |  https://doi.org/10.3389/fonc.2020.00364  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32266144/  |  
------------------------------------------- 
10.1117/1.JMI.7.1.016502  |   We present a roadmap for integrating artificial intelligence (AI)-based image analysis algorithms into existing radiology workflows such that (1) radiologists can significantly benefit from enhanced automation in various imaging tasks due to AI, and (2) radiologists' feedback is utilized to further improve the AI application. This is achieved by establishing three maturity levels where (1) research enables the visualization of AI-based results/annotations by radiologists without generating new patient records; (2) production allows the AI-based system to generate results stored in an institution's picture-archiving and communication system; and (3) feedback equips radiologists with tools for editing the AI inference results for periodic retraining of the deployed AI systems, thereby allowing continuous organic improvement of AI-based radiology-workflow solutions. A case study (i.e., detection of brain metastases with T1-weighted contrast-enhanced three-dimensional MRI) illustrates the deployment details of a particular AI-based application according to the aforementioned maturity levels. It is shown that the given AI application significantly improves with feedback coming from radiologists; the number of incorrectly detected brain metastases (false positives) decreases from 14.2 to 9.12 per patient with the number of subsequently annotated datasets increasing from 93 to 217 as a result of radiologist adjudication. 
  |  https://doi.org/10.1117/1.JMI.7.1.016502  |  
------------------------------------------- 
10.7150/thno.43765  |   <b>Rationale</b>: Protective mechanisms allow healthy neurons to cope with diverse stresses. Excessive damage as well as aging can lead to defective functioning of these mechanisms. We recently designed NeuroHeal using artificial intelligence with the goal of bolstering endogenous neuroprotective mechanisms. Understanding the key nodes involved in neuroprotection will allow us to identify even more effective strategies for treatment of neurodegenerative diseases. <b>Methods</b>: We used a model of peripheral nerve axotomy in rat pups, that induces retrograde apoptotic death of motoneurons. Nourishing mothers received treatment with vehicle, NeuroHeal or NeuroHeal plus nicotinamide, an inhibitor of sirtuins, and analysis of the pups were performed by immunohistochemistry, electron microscopy, and immunoblotting. <i>In vitro</i>, the post-translational status of proteins of interest was detailed using organotypic spinal cord cultures and genetic modifications in cell lines to unravel the neuroprotective mechanisms involved. <b>Results</b>: We found that the concomitant activation of the NAD<sup>+</sup>-dependent deacetylase SIRT1 and the PI3K/AKT signaling pathway converge to increase the presence of deacetylated and phosphorylated FOXO3a, a transcription factor, in the nucleus. This favors the activation of autophagy, a pro-survival process, and prevents pro-apoptotic PARP1/2 cleavage. <b>Major conclusion</b>: NeuroHeal is a neuroprotective agent for neonatal motoneurons that fine-tunes autophagy on by converging SIRT1/AKT/FOXO3a axis. NeuroHeal is a combo of repurposed drugs that allow its readiness for prospective pediatric use. 
  |  http://www.thno.org/v10p5154.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32308774/  |  
------------------------------------------- 
10.3390/s20082320  |   Counter-drone technology by using artificial intelligence (AI) is an emerging technology and it is rapidly developing. Considering the recent advances in AI, counter-drone systems with AI can be very accurate and efficient to fight against drones. The time required to engage with the target can be less than other methods based on human intervention, such as bringing down a malicious drone by a machine-gun. Also, AI can identify and classify the target with a high precision in order to prevent a false interdiction with the targeted object. We believe that counter-drone technology with AI will bring important advantages to the threats coming from some drones and will help the skies to become safer and more secure. In this study, a deep reinforcement learning (DRL) architecture is proposed to counter a drone with another drone, the learning drone, which will autonomously avoid all kind of obstacles inside a suburban neighborhood environment. The environment in a simulator that has stationary obstacles such as trees, cables, parked cars, and houses. In addition, another non-malicious third drone, acting as moving obstacle inside the environment was also included. In this way, the learning drone is trained to detect stationary and moving obstacles, and to counter and catch the target drone without crashing with any other obstacle inside the neighborhood. The learning drone has a front camera and it can capture continuously depth images. Every depth image is part of the state used in DRL architecture. There are also scalar state parameters such as velocities, distances to the target, distances to some defined geofences and track, and elevation angles. The state image and scalars are processed by a neural network that joints the two state parts into a unique flow. Moreover, transfer learning is tested by using the weights of the first full-trained model. With transfer learning, one of the best jump-starts achieved higher mean rewards (close to 35 more) at the beginning of training. Transfer learning also shows that the number of crashes during training can be reduced, with a total number of crashed episodes reduced by 65%, when all ground obstacles are included. 
  |  http://www.mdpi.com/resolver?pii=s20082320  |  
------------------------------------------- 
10.1007/s10916-019-1481-4  |    Background:  The use of artificial intelligence, including machine learning, is increasing in medicine. Use of machine learning is rising in the prediction of patient outcomes. Machine learning may also be able to enhance and augment anesthesia clinical procedures such as airway management. In this study, we sought to develop a machine learning algorithm that could classify vocal cords and tracheal airway anatomy real-time during video laryngoscopy or bronchoscopy as well as compare the performance of three novel convolutional networks for detecting vocal cords and tracheal rings. 
  Methods:  Following institutional approval, a clinical dataset of 775 video laryngoscopy and bronchoscopy videos was used. The dataset was divided into two categories for use for training and testing. We used three convolutional neural networks (CNNs): ResNet, Inception and MobileNet. Backpropagation and a mean squared error loss function were used to assess accuracy as well as minimize bias and variance. Following training, we assessed transferability using the generalization error of the CNN, sensitivity and specificity, average confidence error, outliers, overall confidence percentage, and frames per second for live video feeds. After the training was complete, 22 models using 0 to 25,000 steps were generated and compared. 
  Results:  The overall confidence of classification for the vocal cords and tracheal rings for ResNet, Inception and MobileNet CNNs were as follows: 0.84, 0.78, and 0.64 for vocal cords, respectively, and 0.69, 0.72, 0.54 for tracheal rings, respectively. Transfer learning following additional training resulted in improved accuracy of ResNet and Inception for identifying the vocal cords (with a confidence of 0.96 and 0.93 respectively). The two best performing CNNs, ResNet and Inception, achieved a specificity of 0.985 and 0.971, respectively, and a sensitivity of 0.865 and 0.892, respectively. Inception was able to process the live video feeds at 10 FPS while ResNet processed at 5 FPS. Both were able to pass a feasibility test of identifying vocal cords and tracheal rings in a video feed. 
  Conclusions:  We report the development and evaluation of a CNN that can identify and classify airway anatomy in real time. This neural network demonstrates high performance. The availability of artificial intelligence may improve airway management and bronchoscopy by helping to identify key anatomy real time. Thus, potentially improving performance and outcomes during these procedures. Further, this technology may theoretically be extended to the settings of airway pathology or airway management in the hands of experienced providers. The researchers in this study are exploring the performance of this neural network in clinical trials. 
  |  https://dx.doi.org/10.1007/s10916-019-1481-4  |  
------------------------------------------- 
10.1089/sysm.2020.0001  |   The <i>First International Conference in Systems and Network Medicine</i> gathered together 200 global thought leaders, scientists, clinicians, academicians, industry and government experts, medical and graduate students, postdoctoral scholars and policymakers. Held at Georgetown University Conference Center in Washington D.C. on September 11-13, 2019, the event featured a day of pre-conference lectures and hands-on bioinformatic computational workshops followed by two days of deep and diverse scientific talks, panel discussions with eminent thought leaders, and scientific poster presentations. Topics ranged from: Systems and Network Medicine in Clinical Practice; the role of -omics technologies in Health Care; the role of Education and Ethics in Clinical Practice, Systems Thinking, and Rare Diseases; and the role of Artificial Intelligence in Medicine. The conference served as a unique nexus for interdisciplinary discovery and dialogue and fostered formation of new insights and possibilities for health care systems advances. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32226924/  |  
------------------------------------------- 
10.1021/acs.nanolett.0c00733  |   Tactile information is efficiently captured and processed through a complex sensory system combined with mechanoreceptors, neurons, and synapses in human skin. Synapses are essential for tactile signal transmission between pre/post-neurons. However, developing an electronic device that integrates the functions of tactile information sensation and transmission remains a challenge. Here, we present a piezotronic synapse based on a single GaN microwire that can simultaneously achieve the capabilities of strain sensing and synaptic functions. The piezotronic effect in the wurtzite GaN is introduced to strengthen synaptic weight updates (e.g., 330% enhancement at a compressive stress of -0.36%) with pulse trains. A high gauge factor for strain sensing (ranging from 0 to -0.81%) of about 736 is also obtained. Remarkably, the piezotronic synapse enables the neuromorphic hardware achievement of the perception and processing of tactile information in a single micro/nanowire system, demonstrating an advance in biorealistic artificial intelligence systems. 
  |  https://dx.doi.org/10.1021/acs.nanolett.0c00733  |  
------------------------------------------- 
10.1007/s00330-019-06533-w  |    Objective:  To develop a deep learning-based artificial intelligence (AI) scheme for predicting the likelihood of the ground-glass nodule (GGN) detected on CT images being invasive adenocarcinoma (IA) and also compare the accuracy of this AI scheme with that of two radiologists. 
  Methods:  First, we retrospectively collected 828 histopathologically confirmed GGNs of 644 patients from two centers. Among them, 209 GGNs are confirmed IA and 619 are non-IA, including 409 adenocarcinomas in situ and 210 minimally invasive adenocarcinomas. Second, we applied a series of pre-preprocessing techniques, such as image resampling, rescaling and cropping, and data augmentation, to process original CT images and generate new training and testing images. Third, we built an AI scheme based on a deep convolutional neural network by using a residual learning architecture and batch normalization technique. Finally, we conducted an observer study and compared the prediction performance of the AI scheme with that of two radiologists using an independent dataset with 102 GGNs. 
  Results:  The new AI scheme yielded an area under the receiver operating characteristic curve (AUC) of 0.92 ± 0.03 in classifying between IA and non-IA GGNs, which is equivalent to the senior radiologist's performance (AUC 0.92 ± 0.03) and higher than the score of the junior radiologist (AUC 0.90 ± 0.03). The Kappa value of two sets of subjective prediction scores generated by two radiologists is 0.6. 
  Conclusions:  The study result demonstrates using an AI scheme to improve the performance in predicting IA, which can help improve the development of a more effective personalized cancer treatment paradigm. 
  Key points:  • The feasibility of using a deep learning method to predict the likelihood of the ground-glass nodule being invasive adenocarcinoma. • Residual learning-based CNN model improves the performance in classifying between IA and non-IA nodules. • Artificial intelligence (AI) scheme yields higher performance than radiologists in predicting invasive adenocarcinoma. 
  |  https://dx.doi.org/10.1007/s00330-019-06533-w  |  
------------------------------------------- 
10.1021/acs.jmedchem.9b02120  |   Artificial intelligence and machine learning have demonstrated their potential role in predictive chemistry and synthetic planning of small molecules; there are at least a few reports of companies employing <i>in silico</i> synthetic planning into their overall approach to accessing target molecules. A data-driven synthesis planning program is one component being developed and evaluated by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, comprising MIT and 13 chemical and pharmaceutical company members. Together, we wrote this perspective to share how we think predictive models can be integrated into medicinal chemistry synthesis workflows, how they are currently used within MLPDS member companies, and the outlook for this field. 
  |  https://dx.doi.org/10.1021/acs.jmedchem.9b02120  |  
------------------------------------------- 
10.12927/hcq.2020.26143  |   Artificial intelligence offers the promise to revolutionize the way healthcare is delivered in the future. To capitalize on the value of advanced analytics and artificial intelligence, organizations must focus on building organizational capabilities. This report shares an operating model for insight and change in healthcare comprising six key components: analytics technology and operations, data governance, change and automation, advanced analytics and insights, analytics literacy and strategy and relationship management. The adoption of the proposed model will build core capabilities that will enable organizations to connect data to decision making and realize value from its investment in advanced analytics. 
  |  https://www.longwoods.com/content/26143  |  
------------------------------------------- 
10.2196/14679  |    Background:  Patients are increasingly seeking Web-based symptom checkers to obtain diagnoses. However, little is known about the characteristics of the patients who use these resources, their rationale for use, and whether they find them accurate and useful. 
  Objective:  The study aimed to examine patients' experiences using an artificial intelligence (AI)-assisted online symptom checker. 
  Methods:  An online survey was administered between March 2, 2018, through March 15, 2018, to US users of the Isabel Symptom Checker within 6 months of their use. User characteristics, experiences of symptom checker use, experiences discussing results with physicians, and prior personal history of experiencing a diagnostic error were collected. 
  Results:  A total of 329 usable responses was obtained. The mean respondent age was 48.0 (SD 16.7) years; most were women (230/304, 75.7%) and white (271/304, 89.1%). Patients most commonly used the symptom checker to better understand the causes of their symptoms (232/304, 76.3%), followed by for deciding whether to seek care (101/304, 33.2%) or where (eg, primary or urgent care: 63/304, 20.7%), obtaining medical advice without going to a doctor (48/304, 15.8%), and understanding their diagnoses better (39/304, 12.8%). Most patients reported receiving useful information for their health problems (274/304, 90.1%), with half reporting positive health effects (154/302, 51.0%). Most patients perceived it to be useful as a diagnostic tool (253/301, 84.1%), as a tool providing insights leading them closer to correct diagnoses (231/303, 76.2%), and reported they would use it again (278/304, 91.4%). Patients who discussed findings with their physicians (103/213, 48.4%) more often felt physicians were interested (42/103, 40.8%) than not interested in learning about the tool's results (24/103, 23.3%) and more often felt physicians were open (62/103, 60.2%) than not open (21/103, 20.4%) to discussing the results. Compared with patients who had not previously experienced diagnostic errors (missed or delayed diagnoses: 123/304, 40.5%), patients who had previously experienced diagnostic errors (181/304, 59.5%) were more likely to use the symptom checker to determine where they should seek care (15/123, 12.2% vs 48/181, 26.5%; P=.002), but they less often felt that physicians were interested in discussing the tool's results (20/34, 59% vs 22/69, 32%; P=.04). 
  Conclusions:  Despite ongoing concerns about symptom checker accuracy, a large patient-user group perceived an AI-assisted symptom checker as useful for diagnosis. Formal validation studies evaluating symptom checker accuracy and effectiveness in real-world practice could provide additional useful information about their benefit. 
  |  https://www.jmir.org/2020/1/e14679/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012052/  |  
------------------------------------------- 
10.1148/radiol.2020192224  |   Artificial intelligence (AI) continues to garner substantial interest in medical imaging. The potential applications are vast and include the entirety of the medical imaging life cycle from image creation to diagnosis to outcome prediction. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations). Current supervised AI methods require a curation process for data to optimally train, validate, and test algorithms. Currently, most research groups and industry have limited data access based on small sample sizes from small geographic areas. In addition, the preparation of data is a costly and time-intensive process, the results of which are algorithms with limited utility and poor generalization. In this article, the authors describe fundamental steps for preparing medical imaging data in AI algorithm development, explain current limitations to data curation, and explore new approaches to address the problem of data availability. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020192224?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s00330-019-06628-4  |    Objectives:  To evaluate the deep learning models for differentiating invasive pulmonary adenocarcinomas (IACs) among subsolid nodules (SSNs) considered for resection in a retrospective diagnostic cohort in comparison with a size-based logistic model and expert radiologists. 
  Methods:  This study included 525 patients (309 women; median, 62 years) to develop models, and an independent cohort of 101 patients (57 women; median, 66 years) was used for validation. A size-based logistic model and deep learning models using 2.5-dimension (2.5D) and three-dimension (3D) CT images were developed to discriminate IAC from less invasive pathologies. Overall performance, discrimination, and calibration were assessed. Diagnostic performances of the three thoracic radiologists were compared with those of the deep learning model. 
  Results:  The overall performances of the deep learning models (Brier score, 0.122 for the 2.5D DenseNet and 0.121 for the 3D DenseNet) were superior to those of the size-based logistic model (Brier score, 0.198). The area under the receiver operating characteristic curve (AUC) of the 2.5D DenseNet (0.921) was significantly higher than that of the 3D DenseNet (0.835; p = 0.037) and the size-based logistic model (0.836; p = 0.009). At equally high sensitivities of 90%, the 2.5D DenseNet showed significantly higher specificity (88.2%; all p &lt; 0.05) and positive predictive value (97.4%; all p &lt; 0.05) than other models. Model calibration was poor for all models (all p &lt; 0.05). The 2.5D DenseNet had a comparable performance with the radiologists (AUC, 0.848-0.910). 
  Conclusion:  The 2.5D DenseNet model could be used as a highly sensitive and specific diagnostic tool to differentiate IACs among SSNs for surgical candidates. 
  Key points:  • The deep learning model developed using 2.5D DenseNet showed higher overall performance and discrimination than the size-based logistic model for the differentiation of invasive adenocarcinomas among subsolid nodules for surgical candidates. • The 2.5D DenseNet demonstrated a thoracic radiologist-level diagnostic performance and had higher specificity (88.2%) at equal sensitivities (90%) than the size-based logistic model (specificity, 52.9%). • The 2.5D DenseNet could be used to reduce potential overtreatment for the indolent subsolid nodules or to select candidates for sublobar resection instead of the standard lobectomy. 
  |  https://dx.doi.org/10.1007/s00330-019-06628-4  |  
------------------------------------------- 
10.1016/j.neuron.2020.01.004  |   Unbiased in vivo genome-wide genetic screening is a powerful approach to elucidate new molecular mechanisms, but such screening has not been possible to perform in the mammalian central nervous system (CNS). Here, we report the results of the first genome-wide genetic screens in the CNS using both short hairpin RNA (shRNA) and CRISPR libraries. Our screens identify many classes of CNS neuronal essential genes and demonstrate that CNS neurons are particularly sensitive not only to perturbations to synaptic processes but also autophagy, proteostasis, mRNA processing, and mitochondrial function. These results reveal a molecular logic for the common implication of these pathways across multiple neurodegenerative diseases. To further identify disease-relevant genetic modifiers, we applied our screening approach to two mouse models of Huntington's disease (HD). Top mutant huntingtin toxicity modifier genes included several Nme genes and several genes involved in methylation-dependent chromatin silencing and dopamine signaling, results that reveal new HD therapeutic target pathways. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0896-6273(20)30004-0  |  
------------------------------------------- 
10.1089/tmj.2020.0006  |   Contributors The following document and appendices represent the third edition of the <i>Practice Guidelines for Ocular Telehealth-Diabetic Retinopathy</i>. These guidelines were developed by the Diabetic Retinopathy Telehealth Practice Guidelines Working Group. This working group consisted of a large number of subject matter experts in clinical applications for telehealth in ophthalmology. The editorial committee consisted of Mark B. Horton, OD, MD, who served as working group chair and Christopher J. Brady, MD, MHS, and Jerry Cavallerano, OD, PhD, who served as cochairs. The writing committees were separated into seven different categories. They are as follows: 1.Clinical/operational: Jerry Cavallerano, OD, PhD (Chair), Gail Barker, PhD, MBA, Christopher J. Brady, MD, MHS, Yao Liu, MD, MS, Siddarth Rathi, MD, MBA, Veeral Sheth, MD, MBA, Paolo Silva, MD, and Ingrid Zimmer-Galler, MD. 2.Equipment: Veeral Sheth, MD (Chair), Mark B. Horton, OD, MD, Siddarth Rathi, MD, MBA, Paolo Silva, MD, and Kristen Stebbins, MSPH. 3.Quality assurance: Mark B. Horton, OD, MD (Chair), Seema Garg, MD, PhD, Yao Liu, MD, MS, and Ingrid Zimmer-Galler, MD. 4.Glaucoma: Yao Liu, MD, MS (Chair) and Siddarth Rathi, MD, MBA. 5.Retinopathy of prematurity: Christopher J. Brady, MD, MHS (Chair) and Ingrid Zimmer-Galler, MD. 6.Age-related macular degeneration: Christopher J. Brady, MD, MHS (Chair) and Ingrid Zimmer-Galler, MD. 7.Autonomous and computer assisted detection, classification and diagnosis of diabetic retinopathy: Michael Abramoff, MD, PhD (Chair), Michael F. Chiang, MD, and Paolo Silva, MD. 
  |  https://www.liebertpub.com/doi/full/10.1089/tmj.2020.0006?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32209018/  |  
------------------------------------------- 
10.1016/j.media.2019.101570  |   Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(19)30110-0  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.136896  |   Few studies have investigated the acute effects of fine particulate matter (PM<sub>2.5</sub>) on the risk of stroke subtypes and transient ischemic attack (TIA) in low- and middle-income countries. The primary aim of this study was to assess the associations between short-term exposure to PM<sub>2.5</sub> and daily hospital admissions for total cerebrovascular disease, ischemic and hemorrhagic strokes, and TIA in China. A total of 8,359,162 hospital admissions in 248 Chinese cities from 2013 to 2017 were identified from the Hospital Quality Monitoring System of China. Generalized additive models with quasi-Poisson regression were used to estimate the associations in each city, and random-effect meta-analyses were conducted to combine the city-specific estimates. We found that a 10 μg/m<sup>3</sup> increase in PM<sub>2.5</sub> concentration was significantly associated with a 0.19% (95% CI, 0.13% to 0.25%), 0.26% (95% CI, 0.17% to 0.35%), and 0.26% (95% CI, 0.13% to 0.38%) increase in same-day hospital admissions for total cerebrovascular disease, ischemic stroke, and TIA, respectively. In contrast, a non-significant negative association with PM<sub>2.5</sub> was observed for hemorrhagic stroke in the main analyses (lag 0 day), which became statistically significant when using other single-day exposures (lag 1 or 2 days) or moving average exposures (lag 0-1, 0-2, or 0-3 days) as exposure metric. These associations were robust to adjustment for other criteria air pollutants in two-pollutant models. For ischemic stroke, the effect estimates were significantly larger in people aged 65-74 years, in cool season, and in cities with lower annual average PM<sub>2.5</sub> concentrations. The exposure-response curves were nonlinear with a leveling off at high concentrations. These results contribute to the relatively limited literature on the PM<sub>2.5</sub>-related risks of cerebrovascular events in low- and middle-income countries. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)30406-X  |  
------------------------------------------- 
10.1038/s41746-020-0238-2  |   The emergence of digital pathology has opened new horizons for histopathology. Artificial intelligence (AI) algorithms are able to operate on digitized slides to assist pathologists with different tasks. Whereas AI-involving classification and segmentation methods have obvious benefits for image analysis, image search represents a fundamental shift in computational pathology. Matching the pathology of new patients with already diagnosed and curated cases offers pathologists a new approach to improve diagnostic accuracy through visual inspection of similar cases and computational majority vote for consensus building. In this study, we report the results from searching the largest public repository (The Cancer Genome Atlas, TCGA) of whole-slide images from almost 11,000 patients. We successfully indexed and searched almost 30,000 high-resolution digitized slides constituting 16 terabytes of data comprised of 20 million 1000 × 1000 pixels image patches. The TCGA image database covers 25 anatomic sites and contains 32 cancer subtypes. High-performance storage and GPU power were employed for experimentation. The results were assessed with conservative "majority voting" to build consensus for subtype diagnosis through vertical search and demonstrated high accuracy values for both frozen section slides (e.g., bladder urothelial carcinoma 93%, kidney renal clear cell carcinoma 97%, and ovarian serous cystadenocarcinoma 99%) and permanent histopathology slides (e.g., prostate adenocarcinoma 98%, skin cutaneous melanoma 99%, and thymoma 100%). The key finding of this validation study was that computational consensus appears to be possible for rendering diagnoses if a sufficiently large number of searchable cases are available for each cancer subtype. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32195366/  |  
------------------------------------------- 
10.3390/s20082169  |   Medical image fusion techniques can fuse medical images from different morphologies to make the medical diagnosis more reliable and accurate, which play an increasingly important role in many clinical applications. To obtain a fused image with high visual quality and clear structure details, this paper proposes a convolutional neural network (CNN) based medical image fusion algorithm. The proposed algorithm uses the trained Siamese convolutional network to fuse the pixel activity information of source images to realize the generation of weight map. Meanwhile, a contrast pyramid is implemented to decompose the source image. According to different spatial frequency bands and a weighted fusion operator, source images are integrated. The results of comparative experiments show that the proposed fusion algorithm can effectively preserve the detailed structure information of source images and achieve good human visual effects. 
  |  http://www.mdpi.com/resolver?pii=s20082169  |  
------------------------------------------- 
10.1371/journal.pone.0228134  |   Chronic lameness affects bovine welfare and has a negative economic impact in dairy industry. Moreover, due to the translational gap between traditional pain models and new drugs development for treating chronic pain states, naturally occurring painful diseases could be a potential translational tool for chronic pain research. We therefore employed liquid chromatography tandem mass spectrometry (LC-MS/MS) to stablish the proteomic profile of the spinal cord samples from lumbar segments (L2-L4) of chronic lame dairy cows. Data were validated and quantified through software tool (Scaffold® v 4.0) using output data from two search engines (SEQUEST® and X-Tandem®). Search Tool for the Retrieval of Interacting Genes/Proteins (STRING) analysis was performed to detect proteins interactions. LC-MS/MS identified a total amount of 177 proteins; of which 129 proteins were able to be quantified. Lame cows showed a strong upregulation of interacting proteins with chaperone and stress functions such as Hsp70 (p &lt; 0.006), Hsc70 (p &lt; 0.0079), Hsp90 (p &lt; 0.015), STIP (p &gt; 0.0018) and Grp78 (p &lt;0.0068), and interacting proteins associated to glycolytic pathway such as; γ-enolase (p &lt; 0.0095), α-enolase (p &lt; 0.013) and hexokinase-1 (p &lt; 0.028). It was not possible to establish a clear network of interaction in several upregulated proteins in lame cows. Non-interacting proteins were mainly associated to redox process and cytoskeletal organization. The most relevant down regulated protein in lame cows was myelin basic protein (MBP) (p &lt; 0.02). Chronic inflammatory lameness in cows is associated to increased expression of stress proteins with chaperone, metabolism, redox and structural functions. A state of endoplasmic reticulum stress and unfolded protein response (UPR) might explain the changes in protein expression in lame cows; however, further studies need to be performed in order to confirm these findings. 
  |  http://dx.plos.org/10.1371/journal.pone.0228134  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31990932/  |  
------------------------------------------- 
10.1093/nar/gkz1112  |   IRF3, IRF5 and IRF9 are transcription factors, which play distinct roles in the regulation of antiviral and inflammatory responses. The determinants that mediate IRF-specific enhancer selection are not fully understood. To uncover regions occupied predominantly by IRF3, IRF5 or IRF9, we performed ChIP-seq experiments in activated murine dendritic cells. The identified regions were analysed with respect to the enrichment of DNA motifs, the interferon-stimulated response element (ISRE) and ISRE half-site variants, and chromatin accessibility. Using a machine learning method, we investigated the predictability of IRF-dominance. We found that IRF5-dominant regions differed fundamentally from the IRF3- and IRF9-dominant regions: ISREs were rare, while the NFKB motif and special ISRE half-sites, such as 5'-GAGA-3' and 5'-GACA-3', were enriched. IRF3- and IRF9-dominant regions were characterized by the enriched ISRE motif and lower frequency of accessible chromatin. Enrichment analysis and the machine learning method uncovered the features that favour IRF3 or IRF9 dominancy (e.g. a tripartite form of ISRE and motifs for NF-κB for IRF3, and the GAS motif and certain ISRE variants for IRF9). This study contributes to our understanding of how IRF members, which bind overlapping sets of DNA sequences, can initiate signal-dependent responses without activating superfluous or harmful programmes. 
  |  https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkz1112  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31799619/  |  
------------------------------------------- 
10.2196/13810  |    Background:  Several studies have shown that facial attention differs in children with autism. Measuring eye gaze and emotion recognition in children with autism is challenging, as standard clinical assessments must be delivered in clinical settings by a trained clinician. Wearable technologies may be able to bring eye gaze and emotion recognition into natural social interactions and settings. 
  Objective:  This study aimed to test: (1) the feasibility of tracking gaze using wearable smart glasses during a facial expression recognition task and (2) the ability of these gaze-tracking data, together with facial expression recognition responses, to distinguish children with autism from neurotypical controls (NCs). 
  Methods:  We compared the eye gaze and emotion recognition patterns of 16 children with autism spectrum disorder (ASD) and 17 children without ASD via wearable smart glasses fitted with a custom eye tracker. Children identified static facial expressions of images presented on a computer screen along with nonsocial distractors while wearing Google Glass and the eye tracker. Faces were presented in three trials, during one of which children received feedback in the form of the correct classification. We employed hybrid human-labeling and computer vision-enabled methods for pupil tracking and world-gaze translation calibration. We analyzed the impact of gaze and emotion recognition features in a prediction task aiming to distinguish children with ASD from NC participants. 
  Results:  Gaze and emotion recognition patterns enabled the training of a classifier that distinguished ASD and NC groups. However, it was unable to significantly outperform other classifiers that used only age and gender features, suggesting that further work is necessary to disentangle these effects. 
  Conclusions:  Although wearable smart glasses show promise in identifying subtle differences in gaze tracking and emotion recognition patterns in children with and without ASD, the present form factor and data do not allow for these differences to be reliably exploited by machine learning systems. Resolving these challenges will be an important step toward continuous tracking of the ASD phenotype. 
  |  https://www.jmir.org/2020/4/e13810/  |  
------------------------------------------- 
10.1016/j.gene.2020.144482  |   Arteriovenous malformations (AVMs) are congenital vascular lesions with a high tendency for aggravation and recurrence after treatment, and their genesis remains enigmatic. In this study, we investigated exosomal long non-coding RNA (lncRNA) and mRNA expression and constructed a competitive endogenous RNA regulatory network in AVMs. Ethics approval was provided, and informed written consent was given prior to the inclusion of all participants. Blood samples were obtained from patients with AVMs and healthy controls at Shanghai Ninth People's Hospital, China, from May to November 2018, and total exosomes were isolated and validated. Differentially expressed exosomal lncRNAs and mRNAs were detected by RNA-seq, analysed by bioinformatic methods and validated by qRT-PCR. A competitive endogenous RNA regulatory network was constructed. The characteristics of the captured extracellular vesicles conformed to the features of exosomes. A total of 117 dysregulated exosomal lncRNAs and 1159 dysregulated exosomal mRNAs were identified in AVMs. qRT-PCR demonstrated that the exosomal lncRNAs MIR4435-1HG, LINC00657, LOC101927854 and SEPT5-GP1BB were upregulated in AVM exosomes. The Gene Ontology (GO) terms haemopoiesis and negative regulation of neuron projection development were significantly enriched in relation to dysregulated exosomal cis lncRNAs. A total of 199 GO terms and 80 Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways were enriched for the dysregulated exosomal mRNAs. In the exosomal lncRNA-miRNA-mRNA-related ceRNA regulatory network, the top 3 significant modules involved 31 dysregulated exosomal lncRNAs and 114 dysregulated exosomal mRNAs, which were enriched in the Rap 1, Ras, MAPK signalling pathways and platelet activation KEGG pathway. This study comprehensively identified dysregulated exosomal lncRNAs and mRNAs in AVMs, demonstrated the involvement of dysregulated lncRNA and mRNA patterns in AVMs and constructed an exosomal competitive endogenous RNA regulatory network. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30151-7  |  
------------------------------------------- 
10.1177/0022034520901715  |   Lateral cephalometry has been widely used for skeletal classification in orthodontic diagnosis and treatment planning. However, this conventional system, requiring manual tracing of individual landmarks, contains possible errors of inter- and intravariability and is highly time-consuming. This study aims to provide an accurate and robust skeletal diagnostic system by incorporating a convolutional neural network (CNN) into a 1-step, end-to-end diagnostic system with lateral cephalograms. A multimodal CNN model was constructed on the basis of 5,890 lateral cephalograms and demographic data as an input. The model was optimized with transfer learning and data augmentation techniques. Diagnostic performance was evaluated with statistical analysis. The proposed system exhibited &gt;90% sensitivity, specificity, and accuracy for vertical and sagittal skeletal diagnosis. Clinical performance of the vertical classification showed the highest accuracy at 96.40 (95% CI, 93.06 to 98.39; model III). The receiver operating characteristic curve and the area under the curve both demonstrated the excellent performance of the system, with a mean area under the curve &gt;95%. The heat maps of cephalograms were also provided for deeper understanding of the quality of the learned model by visually representing the region of the cephalogram that is most informative in distinguishing skeletal classes. In addition, we present broad applicability of this system through subtasks. The proposed CNN-incorporated system showed potential for skeletal orthodontic diagnosis without the need for intermediary steps requiring complicated diagnostic procedures. 
  |  http://journals.sagepub.com/doi/full/10.1177/0022034520901715?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s10827-020-00739-4  |   A major goal in neuroscience is to estimate neural connectivity from large scale extracellular recordings of neural activity in vivo. This is challenging in part because any such activity is modulated by the unmeasured external synaptic input to the network, known as the common input problem. Many different measures of functional connectivity have been proposed in the literature, but their direct relationship to synaptic connectivity is often assumed or ignored. For in vivo data, measurements of this relationship would require a knowledge of ground truth connectivity, which is nearly always unavailable. Instead, many studies use in silico simulations as benchmarks for investigation, but such approaches necessarily rely upon a variety of simplifying assumptions about the simulated network and can depend on numerous simulation parameters. We combine neuronal network simulations, mathematical analysis, and calcium imaging data to address the question of when and how functional connectivity, synaptic connectivity, and latent external input variability can be untangled. We show numerically and analytically that, even though the precision matrix of recorded spiking activity does not uniquely determine synaptic connectivity, it is in practice often closely related to synaptic connectivity. This relation becomes more pronounced when the spatial structure of neuronal variability is jointly considered. 
  |  https://doi.org/10.1007/s10827-020-00739-4  |  
------------------------------------------- 
10.1002/bjs5.50235  |    Background:  Increased uptake of robotic surgery has led to interest in learning curves for robot-assisted procedures. Learning curves, however, are often poorly defined. This systematic review was conducted to identify the available evidence investigating surgeon learning curves in robot-assisted surgery. 
  Methods:  MEDLINE, Embase and the Cochrane Library were searched in February 2018, in accordance with PRISMA guidelines, alongside hand searches of key congresses and existing reviews. Eligible articles were those assessing learning curves associated with robot-assisted surgery in patients. 
  Results:  Searches identified 2316 records, of which 68 met the eligibility criteria, reporting on 68 unique studies. Of these, 49 assessed learning curves based on patient data across ten surgical specialties. All 49 were observational, largely single-arm (35 of 49, 71 per cent) and included few surgeons. Learning curves exhibited substantial heterogeneity, varying between procedures, studies and metrics. Standards of reporting were generally poor, with only 17 of 49 (35 per cent) quantifying previous experience. Methods used to assess the learning curve were heterogeneous, often lacking statistical validation and using ambiguous terminology. 
  Conclusion:  Learning curve estimates were subject to considerable uncertainty. Robust evidence was lacking, owing to limitations in study design, frequent reporting gaps and substantial heterogeneity in the methods used to assess learning curves. The opportunity remains for the establishment of optimal quantitative methods for the assessment of learning curves, to inform surgical training programmes and improve patient outcomes. 
  Antecedentes:  La aceptación creciente de la cirugía robótica ha generado interés en las curvas de aprendizaje para los procedimientos asistidos por robot. Sin embargo, las curvas de aprendizaje a menudo están mal definidas. Esta revisión sistemática se realizó para identificar la evidencia disponible en relación a las curvas de aprendizaje del cirujano en la cirugía asistida por robot. MÉTODOS: En Febrero de 2018, se realizaron búsquedas en MEDLINE, Embase y Cochrane Library, de acuerdo con las recomendaciones PRISMA, junto con búsquedas manuales de congresos clave y de revisiones ya existentes. Los artículos elegibles fueron aquellos que evaluaron las curvas de aprendizaje asociadas con la cirugía asistida por robot efectuada en pacientes. 
  Resultados:  Las búsquedas bibliográficas identificaron 2.316 registros de los cuales 68 cumplían los criterios de elegibilidad y correspondían a 68 estudios primarios. De estos 68 estudios, 49 evaluaron las curvas de aprendizaje basadas en datos de pacientes de 10 especialidades quirúrgicas. Los 49 estudios eran todos estudios observacionales, en su mayoría de un solo brazo (35/49 (71%)) e incluían pocos cirujanos. Las curvas de aprendizaje mostraban una notable heterogeneidad, variando entre procedimientos, estudios y parámetros analizados. Los estándares de presentación de informes fueron generalmente deficientes, con solo 17/49 (35%) cuantificando la experiencia previa. Los métodos utilizados para evaluar la curva de aprendizaje fueron heterogéneos, a menudo carecían de validación estadística y usaban terminología ambigua. CONCLUSIÓN: Las estimaciones de la curva de aprendizaje estaban sujetas a una considerable incertidumbre, careciendo de evidencia robusta por las limitaciones en el diseño del estudio, lagunas de información en los artículos y heterogeneidad sustancial en los métodos utilizados para evaluar las curvas de aprendizaje. Queda pendiente establecer métodos cuantitativos óptimos para evaluar las curvas de aprendizaje, informar de los programas de formación quirúrgica y mejorar los resultados del paciente. 
  |  https://doi.org/10.1002/bjs5.50235  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32011823/  |  
------------------------------------------- 
10.3389/fnins.2019.01383  |   The brain performs intelligent tasks with extremely low energy consumption. This work takes its inspiration from two strategies used by the brain to achieve this energy efficiency: the absence of separation between computing and memory functions and reliance on low-precision computation. The emergence of resistive memory technologies indeed provides an opportunity to tightly co-integrate logic and memory in hardware. In parallel, the recently proposed concept of a Binarized Neural Network, where multiplications are replaced by exclusive NOR (XNOR) logic gates, offers a way to implement artificial intelligence using very low precision computation. In this work, we therefore propose a strategy for implementing low-energy Binarized Neural Networks that employs brain-inspired concepts while retaining the energy benefits of digital electronics. We design, fabricate, and test a memory array, including periphery and sensing circuits, that is optimized for this in-memory computing scheme. Our circuit employs hafnium oxide resistive memory integrated in the back end of line of a 130-nm CMOS process, in a two-transistor, two-resistor cell, which allows the exclusive NOR operations of the neural network to be performed directly within the sense amplifiers. We show, based on extensive electrical measurements, that our design allows a reduction in the number of bit errors on the synaptic weights without the use of formal error-correcting codes. We design a whole system using this memory array. We show on standard machine learning tasks (MNIST, CIFAR-10, ImageNet, and an ECG task) that the system has inherent resilience to bit errors. We evidence that its energy consumption is attractive compared to more standard approaches and that it can use memory devices in regimes where they exhibit particularly low programming energy and high endurance. We conclude the work by discussing how it associates biologically plausible ideas with more traditional digital electronics concepts. 
  |  https://doi.org/10.3389/fnins.2019.01383  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31998059/  |  
------------------------------------------- 
10.1007/978-1-0716-0282-9_16  |   The routine use of in silico tools is already established in drug lead design. Besides the use of molecular docking methods to screen large chemical libraries and thus prioritize compounds for purchase or synthesis, more accurate calculations of protein-ligand binding free energy has shown the potential to guide lead optimization, thus saving time and resources. Theoretical developments and advances in computing power have allowed quantum mechanical-based methods applied to calculations on biomacromolecules to be increasingly explored and used, with the purpose of providing a more accurate description of protein-ligand interactions and an enhanced level of accuracy in the calculation of binding affinities. It should be noted that the quantum mechanical formulation includes, in principle, all contributions to the energy, considering terms usually neglected in molecular mechanics force fields, such as electronic polarization, metal coordination, and covalent binding; moreover, quantum mechanical approaches are systematically improvable. By treating all elements and interactions on equal footing, and avoiding the need of system-dependent parameterizations, they provide a greater degree of transferability. In this review, we illustrate the increasing relevance of quantum mechanical methods for binding free energy calculation in the context of structure-based drug lead optimization, showing representative applications of the different approaches available. 
  |  https://dx.doi.org/10.1007/978-1-0716-0282-9_16  |  
------------------------------------------- 
10.1016/j.theriogenology.2020.01.061  |   A highly accurate 'non-invasive quantitative embryo assessment for pregnancy' (NQEAP) technique that determines embryo quality has been an elusive goal. If developed, NQEAP would transform the selection of embryos from both Multiple Ovulation and Embryo Transfer (MOET), and even more so, in vitro produced (IVP) embryos for livestock breeding. The area where this concept is already having impact is in the field of clinical embryology, where great strides have been taken in the application of morphokinetics and artificial intelligence (AI); while both are already in practice, rigorous and robust evidence of efficacy is still required. Even the translation of advances in the qualitative scoring of human IVF embryos have yet to be translated to the livestock IVP industry, which remains dependent on the MOET-standardised 3-point scoring system. Furthermore, there are new ways to interrogate the biochemistry of individual embryonic cells by using new, light-based methodologies, such as FLIM and hyperspectral microscopy. Combinations of these technologies, in particular combining new imaging systems with AI, will lead to very accurate NQEAP predictive tools, improving embryo selection and recipient pregnancy success. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0093-691X(20)30074-1  |  
------------------------------------------- 
10.1007/s12275-020-9516-6  |   Computational analysis of biological data is becoming increasingly important, especially in this era of big data. Computational analysis of biological data allows efficiently deriving biological insights for given data, and sometimes even counterintuitive ones that may challenge the existing knowledge. Among experimental researchers without any prior exposure to computer programming, computational analysis of biological data has often been considered to be a task reserved for computational biologists. However, thanks to the increasing availability of user-friendly computational resources, experimental researchers can now easily access computational resources, including a scientific computing environment and packages necessary for data analysis. In this regard, we here describe the process of accessing Jupyter Notebook, the most popular Python coding environment, to conduct computational biology. Python is currently a mainstream programming language for biology and biotechnology. In particular, Anaconda and Google Colaboratory are introduced as two representative options to easily launch Jupyter Notebook. Finally, a Python package COBRApy is demonstrated as an example to simulate 1) specific growth rate of Escherichia coli as well as compounds consumed or generated under a minimal medium with glucose as a sole carbon source, and 2) theoretical production yield of succinic acid, an industrially important chemical, using E. coli. This protocol should serve as a guide for further extended computational analyses of biological data for experimental researchers without computational background. 
  |  https://dx.doi.org/10.1007/s12275-020-9516-6  |  
------------------------------------------- 
10.3390/pharmaceutics12020177  |   Compression effects on alpha and beta relaxation process of amorphous drugs are theoretically investigated by developing the elastically collective nonlinear Langevin equation theory. We describe the structural relaxation as a coupling between local and nonlocal activated process. Meanwhile, the secondary beta process is mainly governed by the nearest-neighbor interactions of a molecule. This assumption implies the beta relaxation acts as a precursor of the alpha relaxation. When external pressure is applied, a small displacement of a molecule is additionally exerted by a pressure-induced mechanical work in the dynamic free energy, which quantifies interactions between a molecule with its nearest neighbors. The local dynamics has more restriction and it induces stronger effects of collective motions on single-molecule dynamics. Thus, the alpha and beta relaxation times are significantly slowed down with increasing compression. We apply this approach to determine the temperature and pressure dependence of the alpha and beta relaxation time for curcumin, glibenclamide, and indomethacin, and compare numerical results with prior experimental studies. Both qualitative and quantitative agreement between theoretical calculations and experiments validate our assumptions and reveal their limitations. Our approach would pave the way for the development of the drug formulation process. 
  |  http://www.mdpi.com/resolver?pii=pharmaceutics12020177  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093033/  |  
------------------------------------------- 
10.1016/j.brainres.2020.146744  |   Different inputs from a multisensory object or event are often integrated into a coherent and unitary percept, despite differences in sensory formats, neural pathways, and processing times of the involved modalities. Presumably, multisensory integration occurs if the cross-modal inputs are presented within a certain window of temporal integration where inputs are perceived as being simultaneous. Here, we examine the role of ongoing neuronal alpha (i.e. 10-Hz) oscillations in multimodal synchrony perception. While EEG was measured, participants performed a simultaneity judgement task with visual stimuli preceding auditory ones. At stimulus onset asynchronies (SOA's) of 160-200 ms, simultaneity judgements were around 50%. For trials with these SOA's, occipital alpha power was smaller preceding correct judgements, and the individual alpha frequency was correlated with the size of the temporal window of integration. In addition, simultaneity judgements were modulated as a function of oscillatory phase at 12.5 Hz, but the latter effect was only marginally significant. These results support the notion that oscillatory neuronal activity in the alpha frequency range, which has been taken to shape perceptual cycles, is instrumental in multisensory perception. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0006-8993(20)30100-1  |  
------------------------------------------- 
10.1037/xge0000729  |   What role does deliberation play in susceptibility to political misinformation and "fake news"? The Motivated System 2 Reasoning (MS2R) account posits that deliberation causes people to fall for fake news, because reasoning facilitates identity-protective cognition and is therefore used to rationalize content that is consistent with one's political ideology. The classical account of reasoning instead posits that people ineffectively discern between true and false news headlines when they <i>fail</i> to deliberate (and instead rely on intuition). To distinguish between these competing accounts, we investigated the causal effect of reasoning on media truth discernment using a 2-response paradigm. Participants (<i>N</i> = 1,635 Mechanical Turkers) were presented with a series of headlines. For each, they were first asked to give an initial, intuitive response under time pressure and concurrent working memory load. They were then given an opportunity to rethink their response with no constraints, thereby permitting more deliberation. We also compared these responses to a (deliberative) 1-response baseline condition where participants made a single choice with no constraints. Consistent with the classical account, we found that deliberation corrected intuitive mistakes: Participants believed false headlines (but not true headlines) more in initial responses than in either final responses or the unconstrained 1-response baseline. In contrast-and inconsistent with the Motivated System 2 Reasoning account-we found that political polarization was equivalent across responses. Our data suggest that, in the context of fake news, deliberation facilitates accurate belief formation and not partisan bias. (PsycINFO Database Record (c) 2020 APA, all rights reserved). 
  |  None  |  
------------------------------------------- 
10.1097/PRS.0000000000006342  |    Background:  Male-to-female transgender patients desire to be identified, and treated, as female, in public and social settings. Facial feminization surgery entails a combination of highly visible changes in facial features. To study the effectiveness of facial feminization surgery, we investigated preoperative/postoperative gender-typing using facial recognition neural networks. 
  Methods:  In this study, standardized frontal and lateral view preoperative and postoperative images of 20 male-to-female patients who completed hard- and soft-tissue facial feminization surgery procedures were used, along with control images of unoperated cisgender men and women (n = 120 images). Four public neural networks trained to identify gender based on facial features analyzed the images. Correct gender-typing, improvement in gender-typing (preoperatively to postoperatively), and confidence in femininity were analyzed. 
  Results:  Cisgender male and female control frontal images were correctly identified 100 percent and 98 percent of the time, respectively. Preoperative facial feminization surgery images were misgendered 47 percent of the time (recognized as male) and only correctly identified as female 53 percent of the time. Postoperative facial feminization surgery images were gendered correctly 98 percent of the time; this was an improvement of 45 percent. Confidence in femininity also improved from a mean score of 0.27 before facial feminization surgery to 0.87 after facial feminization surgery. 
  Conclusions:  In the first study of its kind, facial recognition neural networks showed improved gender-typing of transgender women from preoperative facial feminization surgery to postoperative facial feminization surgery. This demonstrated the effectiveness of facial feminization surgery by artificial intelligence methods. 
  Clinical question/level of evidence:  Therapeutic, IV. 
  |  http://Insights.ovid.com/pubmed?pmid=31592946  |  
------------------------------------------- 
10.1111/tbj.13718  |   The medical literature has been growing exponentially, and its size has become a barrier for physicians to locate and extract clinically useful information. As a promising solution, natural language processing (NLP), especially machine learning (ML)-based NLP is a technology that potentially provides a promising solution. ML-based NLP is based on training a computational algorithm with a large number of annotated examples to allow the computer to "learn" and "predict" the meaning of human language. Although NLP has been widely applied in industry and business, most physicians still are not aware of the huge potential of this technology in medicine, and the implementation of NLP in breast cancer research and management is fairly limited. With a real-world successful project of identifying penetrance papers for breast and other cancer susceptibility genes, this review illustrates how to train and evaluate an NLP-based medical abstract classifier, incorporate it into a semiautomatic meta-analysis procedure, and validate the effectiveness of this procedure. Other implementations of NLP technology in breast cancer research, such as parsing pathology reports and mining electronic healthcare records, are also discussed. We hope this review will help breast cancer physicians and researchers to recognize, understand, and apply this technology to meet their own clinical or research needs. 
  |  https://doi.org/10.1111/tbj.13718  |  
------------------------------------------- 
10.1007/s11517-019-02074-y  |   Joined fragment segmentation for fractured bones segmented from CT (computed tomography) images is a time-consuming task and calls for lots of interactions. To alleviate segmentation burdens of radiologists, we propose a graphics processing unit (GPU)-accelerated 3D segmentation framework requiring less interactions and lower time cost compared with existing methods. We first leverage the normal-based erosion method to separate joined bone fragments. After labeling the separated fragments via CCL (connected component labeling) algorithm, the record-based dilation method is eventually employed to restore bone's original shape. Besides, we introduce an additional random walk algorithm to tackle the special case where fragments are strongly joined. For efficient fragment segmentation, the framework is carried out in parallel with GPU-acceleration technology. Experiments on realistic CT volumes demonstrate that our framework can attain accurate fragment segmentations with dice scores over 99% and averagely takes 3.47 s to complete the segmentation task for a fractured bone volume of 512 × 512 × 425 voxels. We propose a GPU accelerated segmentation framework, which mainly consists of normal-based erosion and record-based dilation, to automatically segment joined fragments for most cases. For the remaining cases, we introduce a random walk algorithm for segmentation with a few interactions. 
  |  https://dx.doi.org/10.1007/s11517-019-02074-y  |  
------------------------------------------- 
10.1093/nargab/lqz023  |   Transposable elements colonize genomes and with time may end up being incorporated into functional regions. SINE Alu elements, which appeared in the primate lineage, are ubiquitous in the human genome and more than a thousand overlap annotated coding exons. Although almost all Alu-derived coding exons appear to be in alternative transcripts, they have been incorporated into the main coding transcript in at least 11 genes. The extent to which Alu regions are incorporated into functional proteins is unclear, but we detected reliable peptide evidence to support the translation to protein of 33 Alu-derived exons. All but one of the Alu elements for which we detected peptides were frame-preserving and there was proportionally seven times more peptide evidence for Alu elements as for other primate exons. Despite this strong evidence for translation to protein we found no evidence of selection, either from cross species alignments or human population variation data, among these Alu-derived exons. Overall, our results confirm that SINE Alu elements have contributed to the expansion of the human proteome, and this contribution appears to be stronger than might be expected over such a relatively short evolutionary timeframe. Despite this, the biological relevance of these modifications remains open to question. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31886458/  |  
------------------------------------------- 
10.1088/1361-6560/ab6f99  |   The increasing incidence of pancreatic cancer will make it the second deadliest cancer in 2030. Imaging based early diagnosis and image guided treatment are emerging potential solutions. Artificial intelligence (AI) can help provide and improve widespread diagnostic expertise and accurate interventional image interpretation. Accurate segmentation of the pancreas is essential to create annotated data sets to train AI, and for computer assisted interventional guidance. Automated deep learning segmentation performance in pancreas computed tomography (CT) imaging is low due to poor grey value contrast and complex anatomy. A good solution seemed a recent interactive deep learning segmentation framework for brain CT that helped strongly improve initial automated segmentation with minimal user input. This method yielded no satisfactory results for pancreas CT, possibly due to a sub-optimal neural network architecture. We hypothesize that a state-of-the-art U-net neural network architecture is better because it can produce a better initial segmentation and is likely to be extended to work in a similar interactive approach. We implemented the existing interactive method, iFCN, and developed an interactive version of U-net method we call iUnet. The iUnet is fully trained to produce the best possible initial segmentation. In interactive mode it is additionally trained on a partial set of layers on user generated scribbles. We compare initial segmentation performance of iFCN and iUnet on a 100CT dataset using dice similarity coefficient analysis. Secondly, we assessed the performance gain in interactive use with three observers on segmentation quality and time. Average automated baseline performance was 78% (iUnet) versus 72% (FCN). Manual and semi-automatic segmentation performance was: 87% in 15 min. for manual, and 86% in 8 min. for iUNet. We conclude that iUnet provides a better baseline than iFCN and can reach expert manual performance significantly faster than manual segmentation in case of pancreas CT. Our novel iUnet architecture is modality and organ agnostic and can be a potential novel solution for semi-automatic medical imaging segmentation in general. 
  |  https://doi.org/10.1088/1361-6560/ab6f99  |  
------------------------------------------- 
10.1186/s41747-020-0145-y  |   Here, we summarise the unresolved debate about p value and its dichotomisation. We present the statement of the American Statistical Association against the misuse of statistical significance as well as the proposals to abandon the use of p value and to reduce the significance threshold from 0.05 to 0.005. We highlight reasons for a conservative approach, as clinical research needs dichotomic answers to guide decision-making, in particular in the case of diagnostic imaging and interventional radiology. With a reduced p value threshold, the cost of research could increase while spontaneous research could be reduced. Secondary evidence from systematic reviews/meta-analyses, data sharing, and cost-effective analyses are better ways to mitigate the false discovery rate and lack of reproducibility associated with the use of the 0.05 threshold. Importantly, when reporting p values, authors should always provide the actual value, not only statements of "p &lt; 0.05" or "p ≥ 0.05", because p values give a measure of the degree of data compatibility with the null hypothesis. Notably, radiomics and big data, fuelled by the application of artificial intelligence, involve hundreds/thousands of tested features similarly to other "omics" such as genomics, where a reduction in the significance threshold, based on well-known corrections for multiple testing, has been already adopted. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32157489/  |  
------------------------------------------- 
10.1098/rstb.2019.0661  |   Network connectivity fingerprints are among today's best choices to obtain a faithful sampling of an individual's brain and cognition. Widely available MRI scanners can provide rich information tapping into network recruitment and reconfiguration that now scales to hundreds and thousands of humans. Here, we contemplate the advantages of analysing such connectome profiles using Bayesian strategies. These analysis techniques afford full probability estimates of the studied network coupling phenomena, provide analytical machinery to separate epistemological uncertainty and biological variability in a coherent manner, usher us towards avenues to go beyond binary statements on existence versus non-existence of an effect, and afford credibility estimates around all model parameters at play which thus enable single-subject predictions with rigorous uncertainty intervals. We illustrate the brittle boundary between healthy and diseased brain circuits by autism spectrum disorder as a recurring theme where, we argue, network-based approaches in neuroscience will require careful probabilistic answers. This article is part of the theme issue 'Unifying the essential concepts of biological networks: biological insights and philosophical foundations'. 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0661?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.pbiomolbio.2019.07.001  |    Background:  The genetic control strategies of vector borne diseases includes the replacement of a vector population by "disease-refractory" mosquitoes and the release of mosquitoes with a gene to control the vector's reproduction rates. Wolbachia are common intracellular bacteria that are found in arthropods and nematodes. Wolbachia infected male mosquitos have been used in different experimental trials around the world to suppress the target population of Aedes aegypti and this genetic control strategy has proved to be a promising alternative to other treatment strategies. Due to certain limitations, the successful application of this strategy is still awaited. 
  Methods:  Mathematical frame work for Wolbachia induced genetic control strategy has been developed in this article. With the aid of Artificial Intelligence (AI) tools, accurate parametric values are depicted. For the first time, the model is well synchronized with the experimental findings. The model is comprised of the generalized varying coefficient and multiple mating rates between infected and uninfected compartments of Aedes aegypti dengue to forecast the disease control. 
  Results:  Two mathematical models are developed in this article to demonstrate different mating rates of the genetic control strategy. The important parameters and time varying coefficients are well demonstrated with the aid of numerical computations. The resulting thresholds and forecasting may prove to be a useful tool for future experimental studies. 
  Conclusions:  From our analysis, we have concluded that the genetic control strategy is a promising technique and the role of Wolbachia infected male mosquitos, in genetic control strategies, can be better interpreted in an inexpensive manner with the aid of a theoretical model. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0079-6107(19)30120-8  |  
------------------------------------------- 
10.2174/1386207323666191213142223  |    Background:  Human Immunodeficiency Virus 1 (HIV-1) is a lentivirus, which causes various HIV-associated infections. The HIV-1 core dissociation is essential for viral cDNA synthesis and phosphorylation of HIV-1 capsid protein (HIV-1 CA) plays an important role in it. 
  Objective:  The aim of this study was to explicate the role of three phosphoserine sites i.e. Ser109, Ser149 and Ser178 in the structural stability of HIV-1 CA, and it's binding with GS-CA1, a novel potent inhibitor. 
  Methods:  Eight complexes were analyzed and Molecular Dynamics (MD) simulations were performed to observe the stability of HIV-1 CA in the presence and absence of phosphorylation of serine residues at four different temperatures i.e. 300K, 325K, 340K and 350K, along with molecular docking and DFT analysis. 
  Results:  The structures showed maximum stability in the presence of phosphorylated serine residue. However, GS-CA1 docked most strongly with the native structure of HIV-1 CA i.e. binding affinity was -8.5 kcal/mol (Ki = 0.579 µM). 
  Conclusion:  These results suggest that the phosphorylation of these three serine residues weakens the binding of GS-CA1 with CA and casts derogatory effect on inhibition potential of this inhibitor, but it supports the stability of HIV-1 CA structure that can enhance regulation and replication of HIV-1 in host cells. 
  |  http://www.eurekaselect.com/177484/article  |  
------------------------------------------- 
10.1073/pnas.1914370116  |   The vast and growing number of publications in all disciplines of science cannot be comprehended by a single human researcher. As a consequence, researchers have to specialize in narrow subdisciplines, which makes it challenging to uncover scientific connections beyond the own field of research. Thus, access to structured knowledge from a large corpus of publications could help push the frontiers of science. Here, we demonstrate a method to build a semantic network from published scientific literature, which we call SemNet We use SemNet to predict future trends in research and to inspire personalized and surprising seeds of ideas in science. We apply it in the discipline of quantum physics, which has seen an unprecedented growth of activity in recent years. In SemNet, scientific knowledge is represented as an evolving network using the content of 750,000 scientific papers published since 1919. The nodes of the network correspond to physical concepts, and links between two nodes are drawn when two concepts are concurrently studied in research articles. We identify influential and prize-winning research topics from the past inside SemNet, thus confirming that it stores useful semantic knowledge. We train a neural network using states of SemNet of the past to predict future developments in quantum physics and confirm high-quality predictions using historic data. Using network theoretical tools, we can suggest personalized, out-of-the-box ideas by identifying pairs of concepts, which have unique and extremal semantic network properties. Finally, we consider possible future developments and implications of our findings. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=31937664  |  
------------------------------------------- 
10.3390/s20030931  |   Internet of multimedia things (IoMT) driving innovative product development in health care applications. IoMT requires delay-sensitive and higher bandwidth devices. Ultra-wideband (UWB) technology is a promising solution to improve communication between devices, tracking and monitoring of patients. In the future, this technology has the capability to expand the IoMT world with new capabilities and more devices can be integrated. At the present time, some people face different types of physiological problems because of the damage in different areas of the central nervous system. Thus, they lose their balance coordination. One of these types of coordination problems is named Ataxia, in which patients are unable to control their body movements. This kind of coordination disorder needs a proper supervision system for the caretaker. Previous Ataxia assessment methods are cumbersome and cannot handle regular monitoring and tracking of patients. One of the most challenging tasks is to detect different walking abnormalities of Ataxia patients. In our paper, we present a technique for monitoring and tracking of a patient with the help of UWB technology. This method expands the real-time location systems (RTLS) in the indoor environment by placing wearable receiving tags on the body of Ataxia patients. The location and four different walking movement data are collected by UWB transceiver for the classification and prediction in the two-dimensional path. For accurate classification, we use a support vector machine (SVM) algorithm to clarify the movement variations. Our proposed examined result successfully achieved and the accuracy is above 95%. 
  |  http://www.mdpi.com/resolver?pii=s20030931  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32050576/  |  
------------------------------------------- 
10.3390/s20030685  |   Short-term traffic state prediction has become an integral component of an advanced traveler information system (ATIS) in intelligent transportation systems (ITS). Accurate modeling and short-term traffic prediction are quite challenging due to its intricate characteristics, stochastic, and dynamic traffic processes. Existing works in this area follow different modeling approaches that are focused to fit speed, density, or the volume data. However, the accuracy of such modeling approaches has been frequently questioned, thereby traffic state prediction over the short-term from such methods inflicts an overfitting issue. We address this issue to accurately model short-term future traffic state prediction using state-of-the-art models via hyperparameter optimization. To do so, we focused on different machine learning classifiers such as local deep support vector machine (LD-SVM), decision jungles, multi-layers perceptron (MLP), and CN2 rule induction. Moreover, traffic states are evaluated using traffic attributes such as level of service (LOS) horizons and simple if-then rules at different time intervals. Our findings show that hyperparameter optimization via random sweep yielded superior results. The overall prediction performances obtained an average improvement by over 95%, such that the decision jungle and LD-SVM achieved an accuracy of 0.982 and 0.975, respectively. The experimental results show the robustness and superior performances of decision jungles (DJ) over other methods. 
  |  http://www.mdpi.com/resolver?pii=s20030685  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012650/  |  
------------------------------------------- 
10.3390/antibiotics9020054  |   The dissemination of multidrug-resistant Gram-negative bacteria (MDR-GNB) is associated with increased morbidity and mortality in several countries. Machine learning (ML) is a branch of artificial intelligence that consists of conferring on computers the ability to learn from data. In this narrative review, we discuss three existing examples of the application of ML algorithms for assessing three different types of risk: (i) the risk of developing a MDR-GNB infection, (ii) the risk of MDR-GNB etiology in patients with an already clinically evident infection, and (iii) the risk of anticipating the emergence of MDR in GNB through the misuse of antibiotics. In the next few years, we expect to witness an increasingly large number of research studies perfecting the application of ML techniques in the field of MDR-GNB infections. Very importantly, this cannot be separated from the availability of a continuously refined and updated ethical framework allowing an appropriate use of the large datasets of medical data needed to build efficient ML-based support systems that could be shared through appropriate standard infrastructures. 
  |  http://www.mdpi.com/resolver?pii=antibiotics9020054  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023986/  |  
------------------------------------------- 
10.3390/electronics9010099  |   Construction of an ensemble model is a process of combining many diverse base predictive learners. It arises questions of how to weight each model and how to tune the parameters of the weighting process. The most straightforward approach is simply to average the base models. However, numerous studies have shown that a weighted ensemble can provide superior prediction results to a simple average of models. The main goals of this article are to propose a new weighting algorithm applicable for each tree in the Random Forest model and the comprehensive examination of the optimal parameter tuning. Importantly, the approach is motivated by its flexibility, good performance, stability, and resistance to overfitting. The proposed scheme is examined and evaluated on the Physionet/Computing in Cardiology Challenge 2015 data set. It consists of signals (electrocardiograms and pulsatory waveforms) from intensive care patients which triggered an alarm for five cardiac arrhythmia types (Asystole, Bradycardia, Tachycardia, Ventricular Tachycardia, and Ventricular Fultter/Fibrillation). The classification problem regards whether the alarm should or should not have been generated. It was proved that the proposed weighting approach improved classification accuracy for the three most challenging out of the five investigated arrhythmias comparing to the standard Random Forest model. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32051761/  |  
------------------------------------------- 
10.1016/j.endinu.2019.11.009  |    Introduction:  Disease-related malnutrition (DRM) is underdiagnosed and underreported despite its well-known association with a worse prognosis. The emergence of Big Data and the application of artificial intelligence in Medicine have revolutionized the way knowledge is generated. The aim of this study is to assess whether a Big Data tool could help us detect the amount of DRM in our hospital. 
  Methodology:  This was a descriptive, retrospective study using the Savana Manager® tool, which allows for automatically analyzing and extracting the relevant clinical information contained in the free text of the electronic medical record. A search was performed using the term "malnutrition", comparing the characteristics of patients with DRM to the population of hospitalized patients between January 2012 and December 2017. 
  Results:  Among the 180,279 hospitalization records with a discharge report in that period, only 4,446 episodes (2.47%) included the diagnosis of malnutrition. The mean age of patients with DRM was 75 years (SD 16), as compared to 59 years (SD 25) for the overall population. There were no sex differences (51% male). In-hospital death occurred in 7.08% of patients with DRM and 2.98% in the overall group. Mean stay was longer in patients with DRM (8 vs. 5 days, P&lt;.0001) and there were no significant differences in the 72-hour readmission rate. The most common diagnoses associated with DRM were heart failure (35%), respiratory infection (23%), urinary infection (20%), and chronic kidney disease (15%). 
  Conclusion:  Underdiagnosis of DRM remains a problem. Savana Manager® helps us to better understand the profile of these patients. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2530-0164(20)30023-9  |  
------------------------------------------- 
10.1002/mp.14198  |    Purpose:  An indoor, real-time location system (RTLS) can benefit both hospitals and patients by improving clinical efficiency through data-driven optimization of procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy because Bluetooth signal is subject to significant fluctuation. We aim to improve the accuracy of RTLS using the deep learning technique. 
  Methods:  We installed a Bluetooth sensor network in a 3-floor clinic building to track patients, staff, and devices. The Bluetooth sensors measured the strength of the signal broadcasted from Bluetooth tags, which was fed into a deep neural network to calculate the location of the tags. The proposed deep neural network consists of a Long Short-Term Memory (LSTM) network and a deep classifier for tracking moving objects. Additionally, a spatial-temporal constraint algorithm was implemented to further increase the accuracy and stability of the results. To train the neural network, we divided the building into 115 zones and collected training data in each zone. We further augmented the training data to generate cross-zone trajectories, mimicking the real-world scenarios. We tuned the parameters for the proposed neural network to achieve relatively good accuracy. 
  Results:  The proposed deep neural network achieved an overall accuracy of about 97% for tracking objects in each individual zone in the whole 3-floor building, 1.5% higher than the baseline neural network that was proposed in an earlier paper, when using 10 seconds of signals. The accuracy increased with the density of Bluetooth sensors. For tracking moving objects, the proposed neural network achieved stable and accurate results. When latency is less of a concern, we eliminated the effect of latency from the accuracy and gained an accuracy of 100% for our testing trajectories, significantly improved from the baseline method. 
  Conclusions:  The proposed deep neural network composed of a LSTM, a deep classifier and a posterior constraint algorithm significantly improved the accuracy and stability of RTLS for tracking moving objects. 
  |  https://doi.org/10.1002/mp.14198  |  
------------------------------------------- 
10.1016/j.neunet.2020.02.017  |   With the rapid development and wide application of computer, camera device, network and hardware technology, 3D object (or model) retrieval has attracted widespread attention and it has become a hot research topic in the computer vision domain. Deep learning features already available in 3D object retrieval have been proven to be better than the retrieval performance of hand-crafted features. However, most existing networks do not take into account the impact of multi-view image selection on network training, and the use of contrastive loss alone only forcing the same-class samples to be as close as possible. In this work, a novel solution named Multi-view Discrimination and Pairwise CNN (MDPCNN) for 3D object retrieval is proposed to tackle these issues. It can simultaneously input multiple batches and multiple views by adding the Slice layer and the Concat layer. Furthermore, a highly discriminative network is obtained by training samples that are not easy to be classified by clustering. Lastly, we deploy the contrastive-center loss and contrastive loss as the optimization objective that has better intra-class compactness and inter-class separability. Large-scale experiments show that the proposed MDPCNN can achieve a significant performance over the state-of-the-art algorithms in 3D object retrieval. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30071-X  |  
------------------------------------------- 
10.1021/acs.jproteome.9b00736  |   Peptide-spectrum-match (PSM) scores used in database searching are calibrated to spectrum- or spectrum-peptide-specific null distributions. Some calibration methods rely on specific assumptions and use analytical models (e.g., binomial distributions), whereas other methods utilize exact empirical null distributions. The former may be inaccurate because of unjustified assumptions, while the latter are accurate, albeit computationally exhaustive. Here, we introduce a novel, nonparametric, heuristic PSM score calibration method, called Tailor, which calibrates PSM scores by dividing them with the top 100-quantile of the empirical, spectrum-specific null distributions (i.e., the score with an associated <i>p</i>-value of 0.01 at the tail, hence the name) observed during database searching. Tailor does not require any optimization steps or long calculations; it does not rely on any assumptions on the form of the score distribution (i.e., if it is, e.g., binomial); however, it relies on our empirical observation that the mean and the variance of the null distributions are correlated. In our benchmark, we re-calibrated the match scores of XCorr from Crux, HyperScore scores from X!Tandem, and the <i>p</i>-values from OMSSA with the Tailor method and obtained more spectrum annotations than with raw scores at any false discovery rate level. Moreover, Tailor provided slightly more annotations than <i>E</i>-values of X!Tandem and OMSSA and approached the performance of the computationally exhaustive exact <i>p</i>-value method for XCorr on spectrum data sets containing low-resolution fragmentation information (MS2) around 20-150 times faster. On high-resolution MS2 data sets, the Tailor method with XCorr achieved state-of-the-art performance and produced more annotations than the well-calibrated residue-evidence (Res-ev) score around 50-80 times faster. 
  |  https://dx.doi.org/10.1021/acs.jproteome.9b00736  |  
------------------------------------------- 
10.1016/j.wasman.2020.02.016  |   Municipal solid waste management is a major challenge for nowadays urban societies, because it accounts for a large proportion of public budget and, when mishandled, it can lead to environmental and social problems. This work focuses on the problem of locating waste bins in an urban area, which is considered to have a strong influence in the overall efficiency of the reverse logistic chain. This article contributes with an exact multiobjective approach to solve the waste bin location in which the optimization criteria that are considered are: the accessibility to the system (as quality of service measure), the investment cost, and the required frequency of waste removal from the bins (as a proxy of the posterior routing costs). In this approach, different methods to obtain the objectives ideal and nadir values over the Pareto front are proposed and compared. Then, a family of heuristic methods based on the PageRank algorithm is proposed which aims to optimize the accessibility to the system, the amount of collected waste and the installation cost. The experimental evaluation was performed on real-world scenarios of the cities of Montevideo, Uruguay, and Bahía Blanca, Argentina. The obtained results show the competitiveness of the proposed approaches for constructing a set of candidate solutions that considers the different trade-offs between the optimization criteria. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0956-053X(20)30074-X  |  
------------------------------------------- 
10.1148/radiol.2020201178  |    |  http://pubs.rsna.org/doi/10.1148/radiol.2020201178?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1089/omi.2020.0001  |   Artificial intelligence, machine learning, health care robots, and algorithms for clinical decision-making are currently being sought after in diverse fields of clinical medicine and bioengineering. The field of personalized medicine stands to benefit from new technologies so as to harness the omics big data, for example, to individualize and accelerate cancer diagnostics and therapeutics in particular. In this overarching context, breast cancer is one of the most common malignancies worldwide with multiple underlying molecular etiologies and each subtype displaying diverse clinical outcomes. Disease stratification for breast cancer is, therefore, vital to its effective and individualized clinical care. The support vector machine (SVM) is a rising machine learning approach that offers robust classification of high-dimensional big data into small numbers of data points (support vectors), achieving differentiation of subgroups in a short amount of time. Considering the rapid timelines required for both diagnosis and treatment of most aggressive cancers, this new machine learning technique has important clinical and public applications and implications for high-throughput data analysis and contextualization. This expert review describes and examines, first, the SVM models employed to forecast breast cancer subtypes using diverse systems science data, including transcriptomics, epigenetics, proteomics, and radiomics, as well as biological pathway, clinical, pathological, and biochemical data. Then, we compare the performance of the present SVM and other diagnostic and therapeutic prediction models across the data types. We conclude by emphasizing that data integration is a critical bottleneck in systems science, cancer research and development, and health care innovation and that SVM and machine learning approaches offer new solutions and ways forward in biomedical, bioengineering, and clinical applications. 
  |  https://www.liebertpub.com/doi/full/10.1089/omi.2020.0001?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1371/journal.pone.0230671  |   Coral reefs are biologically diverse and structurally complex ecosystems, which have been severally affected by human actions. Consequently, there is a need for rapid ecological assessment of coral reefs, but current approaches require time consuming manual analysis, either during a dive survey or on images collected during a survey. Reef structural complexity is essential for ecological function but is challenging to measure and often relegated to simple metrics such as rugosity. Recent advances in computer vision and machine learning offer the potential to alleviate some of these limitations. We developed an approach to automatically classify 3D reconstructions of reef sections and assessed the accuracy of this approach. 3D reconstructions of reef sections were generated using commercial Structure-from-Motion software with images extracted from video surveys. To generate a 3D classified map, locations on the 3D reconstruction were mapped back into the original images to extract multiple views of the location. Several approaches were tested to merge information from multiple views of a point into a single classification, all of which used convolutional neural networks to classify or extract features from the images, but differ in the strategy employed for merging information. Approaches to merging information entailed voting, probability averaging, and a learned neural-network layer. All approaches performed similarly achieving overall classification accuracies of ~96% and &gt;90% accuracy on most classes. With this high classification accuracy, these approaches are suitable for many ecological applications. 
  |  http://dx.plos.org/10.1371/journal.pone.0230671  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32208447/  |  
------------------------------------------- 
10.3390/cancers12030731  |   Receptor tyrosine kinases (RTKs) are key regulatory signaling proteins governing cancer cell growth and metastasis. During the last two decades, several molecules targeting RTKs were used in oncology as a first or second line therapy in different types of cancer. However, their effectiveness is limited by the appearance of resistance or adverse effects. In this review, we summarize the main features of RTKs and their inhibitors (RTKIs), their current use in oncology, and mechanisms of resistance. We also describe the technological advances of artificial intelligence, chemoproteomics, and microfluidics in elaborating powerful strategies that could be used in providing more efficient and selective small molecules inhibitors of RTKs. Finally, we discuss the interest of therapeutic combination of different RTKIs or with other molecules for personalized treatments, and the challenge for effective combination with less toxic and off-target effects. 
  |  http://www.mdpi.com/resolver?pii=cancers12030731  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32244867/  |  
------------------------------------------- 
10.3390/s20071933  |   Daily activity forecasts play an important role in the daily lives of residents in smart homes. Category forecasts and occurrence time forecasts of daily activity are two key tasks. Category forecasts of daily activity are correlated with occurrence time forecasts, however, existing research has only focused on one of the two tasks. Moreover, the performance of daily activity forecasts is low when the two tasks are performed in series. In this paper, a forecast model based on multi-task learning is proposed to forecast category and occurrence time of daily activity mutually and iteratively. Firstly, raw sensor events are pre-processed to form a feature space of daily activity. Secondly, a parallel multi-task learning model which combines a convolutional neural network (CNN) with bidirectional long short-term memory (Bi-LSTM) units are developed as the forecast model. Finally, five distinct datasets are used to evaluate the proposed model. The experimental results show that compared with the state-of-the-art single-task learning models, this model improves accuracy by at least 2.22%, and the metrics of NMAE, NRMSE and R<sup>2</sup> are improved by at least 1.542%, 7.79% and 1.69%, respectively. 
  |  http://www.mdpi.com/resolver?pii=s20071933  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32235653/  |  
------------------------------------------- 
10.3390/s20061772  |   For efficient and effective energy management, accurate energy consumption forecasting is required in energy management systems (EMSs). Recently, several artificial intelligence-based techniques have been proposed for accurate electric load forecasting; moreover, perfect energy consumption data are critical for the prediction. However, owing to diverse reasons, such as device malfunctions and signal transmission errors, missing data are frequently observed in the actual data. Previously, many imputation methods have been proposed to compensate for missing values; however, these methods have achieved limited success in imputing electric energy consumption data because the period of data missing is long and the dependency on historical data is high. In this study, we propose a novel missing-value imputation scheme for electricity consumption data. The proposed scheme uses a bagging ensemble of multilayer perceptrons (MLPs), called softmax ensemble network, wherein the ensemble weight of each MLP is determined by a softmax function. This ensemble network learns electric energy consumption data with explanatory variables and imputes missing values in this data. To evaluate the performance of our scheme, we performed diverse experiments on real electric energy consumption data and confirmed that the proposed scheme can deliver superior performance compared to other imputation methods. 
  |  http://www.mdpi.com/resolver?pii=s20061772  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210112/  |  
------------------------------------------- 
10.3389/fonc.2020.00401  |   Sialic acids (SA), negatively charged nine-carbon sugars, have long been implicated in cancer metastasis since 1960's but its detailed functional roles remain elusive. We present a computational analysis of transcriptomic data of cancer vs. control tissues of eight types in TCGA, aiming to elucidate the possible reason for the increased production and utilization of SAs in cancer and their possible driving roles in cancer migration. Our analyses have revealed for all cancer types: (1) the synthesis and deployment enzymes of SAs are persistently up-regulated throughout the progression for all but one cancer type; and (2) gangliosides, of which SAs are part, tend to converge to specific types that allow SAs to pack at high densities on cancer cell surface as a cancer advances. Statistical and modeling analyses suggest that (i) a highly plausible reason for the increased syntheses of SAs is to produce net protons, used for neutralizing the OH<sup>-</sup> persistently generated by elevated intracellular iron metabolism coupled with chronic inflammation in cancer tissues; (ii) the level of SA accumulation on cancer cell surface strongly correlates with the stage of cancer migration, as well as multiple migration-related characteristics such as altered cell-cell adhesion, mechanical stress, cell protrusion, and contraction; and (iii) the pattern of SA deployment correlates with the 5-year survival rate of a cancer type. Overall, our study provides strong evidence for that the continuous accumulation of SAs on cancer cell surface gives rise to increasingly stronger cell-cell repulsion due to their negative charges, leading to cell deformation by electrostatic force-induced mechanical compression, which is known to be able to drive cancer cell migration established by recent studies. 
  |  https://doi.org/10.3389/fonc.2020.00401  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296639/  |  
------------------------------------------- 
10.1093/jamia/ocaa017  |    Objectives:  Sharing patient data across institutions to train generalizable deep learning models is challenging due to regulatory and technical hurdles. Distributed learning, where model weights are shared instead of patient data, presents an attractive alternative. Cyclical weight transfer (CWT) has recently been demonstrated as an effective distributed learning method for medical imaging with homogeneous data across institutions. In this study, we optimize CWT to overcome performance losses from variability in training sample sizes and label distributions across institutions. 
  Materials and methods:  Optimizations included proportional local training iterations, cyclical learning rate, locally weighted minibatch sampling, and cyclically weighted loss. We evaluated our optimizations on simulated distributed diabetic retinopathy detection and chest radiograph classification. 
  Results:  Proportional local training iteration mitigated performance losses from sample size variability, achieving 98.6% of the accuracy attained by centrally hosting in the diabetic retinopathy dataset split with highest sample size variance across institutions. Locally weighted minibatch sampling and cyclically weighted loss both mitigated performance losses from label distribution variability, achieving 98.6% and 99.1%, respectively, of the accuracy attained by centrally hosting in the diabetic retinopathy dataset split with highest label distribution variability across institutions. 
  Discussion:  Our optimizations to CWT improve its capability of handling data variability across institutions. Compared to CWT without optimizations, CWT with optimizations achieved performance significantly closer to performance from centrally hosting. 
  Conclusion:  Our work is the first to identify and address challenges of sample size and label distribution variability in simulated distributed deep learning for medical imaging. Future work is needed to address other sources of real-world data variability. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocaa017  |  
------------------------------------------- 
10.1038/s41598-020-57521-w  |   Early in vivo studies demonstrated the involvement of a tumor-suppressing transcription factor, p53, into cellular droplets such as Cajal and promyelocytic leukemia protein bodies, suggesting that the liquid-liquid phase separation (LLPS) might be involved in the cellular functions of p53. To examine this possibility, we conducted extensive investigations on the droplet formation of p53 in vitro. First, p53 itself was found to form liquid-like droplets at neutral and slightly acidic pH and at low salt concentrations. Truncated p53 mutants modulated droplet formation, suggesting the importance of multivalent electrostatic interactions among the N-terminal and C-terminal domains. Second, FRET efficiency measurements for the dimer mutants of p53 revealed that distances between the core domains and between the C-terminal domains were modulated in an opposite manner within the droplets. Third, the molecular crowding agents were found to promote droplet formation, whereas ssDNA, dsDNA, and ATP, to suppress it. Finally, the p53 mutant mimicking posttranslational phosphorylation did not form the droplets. We conclude that p53 itself has a potential to form droplets that can be controlled by cellular molecules and by posttranslational modifications, suggesting that LLPS might be involved in p53 function. 
  |  http://dx.doi.org/10.1038/s41598-020-57521-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31953488/  |  
------------------------------------------- 
10.1016/j.crad.2019.10.022  |    Aim:  To investigate the feasibility of applying a deep convolutional neural network (CNN) for detection/localisation of acute proximal femoral fractures (APFFs) on hip radiographs. 
  Materials and methods:  This study had institutional review board approval. Radiographs of 307 patients with APFFs and 310 normal patients were identified. A split ratio of 3/1/1 was used to create training, validation, and test datasets. To test the validity of the proposed model, a 20-fold cross-validation was performed. The anonymised images from the test cohort were shown to two groups of radiologists: musculoskeletal radiologists and diagnostic radiology residents. Each reader was asked to assess if there was a fracture and localise it if one was detected. The area under the receiver operator characteristics curve (AUC), sensitivity, and specificity were calculated for the CNN and readers. 
  Results:  The mean AUC was 0.9944 with a standard deviation of 0.0036. Mean sensitivity and specificity for fracture detection was 97.1% (81.5/84) and 96.7% (118/122), respectively. There was good concordance with saliency maps for lesion identification, but sensitivity was lower for characterising location (subcapital/transcervical, 84.1%; basicervical/intertrochanteric, 77%; subtrochanteric, 20%). Musculoskeletal radiologists showed a sensitivity and specificity for fracture detection of 100% and 100% respectively, while residents showed 100% and 96.8%, respectively. For fracture localisation, the performance decreased slightly for human readers. 
  Conclusion:  The proposed CNN algorithm showed high accuracy for detection of APFFs, but the performance was lower for fracture localisation. Overall performance of the CNN was lower than that of radiologists, especially in localizing fracture location. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0009-9260(19)30640-3  |  
------------------------------------------- 
10.1016/j.jaci.2019.12.002  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0091-6749(19)31631-8  |  
------------------------------------------- 
10.1007/s11936-020-0803-7  |    Purpose of review:  Physiological assessment of coronary artery disease (CAD) is an essential component of the interventional cardiology toolbox. However, despite long-term data demonstrating improved outcomes, physiology-guided percutaneous coronary intervention (PCI) remains underutilized in current practice. This review outlines the indications and technical aspects involved in evaluating coronary stenosis physiology, focusing on the latest developments in the field. 
  Recent findings:  Beyond fractional flow reserve (FFR), non-hyperemic pressure ratios (NHPR) that assess coronary physiology at rest without hyperemia now abound. Additional advances in other alternative FFR approaches, including non-invasive coronary CT (FFR<sub>CT</sub>), invasive angiography (FFR<sub>angio</sub>), and optical coherence tomography (FFR<sub>OCT</sub>), are being realized. Artificial intelligence algorithms and robust tools that enable detailed pre-procedure "virtual" intervention are also emerging. The benefits of coronary physiological assessment to determine lesion functional significance are well established. In addition to stable CAD, coronary physiology can be especially helpful in clinical scenarios such as left main and multivessel CAD, serial lesions, non-infarct-related arteries in acute coronary syndromes, and residual ischemia post-PCI. Today, coronary physiological assessment remains an indispensable tool in the catheterization laboratory, with an exciting technological future that will further refine clinical practice and improve patient care. 
  |  https://dx.doi.org/10.1007/s11936-020-0803-7  |  
------------------------------------------- 
10.3390/mi11010084  |   This paper presents an adaptive hysteresis compensation approach for a piezoelectric actuator (PEA) using single-neuron adaptive control. For a given desired trajectory, the control input to the PEA is dynamically adjusted by the error between the actual and desired trajectories using Hebb learning rules. A single neuron with self-learning and self-adaptive capabilities is a non-linear processing unit, which is ideal for time-variant systems. Based on the single-neuron control, the compensation of the PEA's hysteresis can be regarded as a process of transmitting biological neuron information. Through the error information between the actual and desired trajectories, the control input is adjusted via the weight adjustment method of neuron learning. In addition, this paper also integrates the combination of Hebb learning rules and supervised learning as teacher signals, which can quickly respond to control signals. The weights of the single-neuron controller can be constantly adjusted online to improve the control performance of the system. Experimental results show that the proposed single-neuron adaptive hysteresis compensation method can track continuous and discontinuous trajectories well. The single-neuron adaptive controller has better adaptive and self-learning performance against the rate-dependence of the PEA's hysteresis. 
  |  http://www.mdpi.com/resolver?pii=mi11010084  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31940914/  |  
------------------------------------------- 
10.1093/scan/nsaa017  |   Behavioral synchronization has been found to facilitate social bonding and prosociality but the neural mechanisms underlying such effects are not well understood. In the current study, 60 dyads were hyperscanned using functional near-infrared spectroscopy (fNIRS) while they performed either a synchronous key-pressing task or a control task. After the task, they were asked to perform a Dictator Game to assess their prosocial behavior. We also measured three potential mediating variables: self-other overlap, perceived similarity, and interpersonal neural synchronization. Results showed that dyads in the synchronization group were higher in behavioral synchronization, INS at the right dorsolateral prefrontal cortex (r-DLPFC), self-other overlap, perceived similarity, and prosociality than those in the control group. INS was significantly associated with prosocial behaviors and self-other overlap. After testing four meditation models, we found that self-other overlap and INS played a serial mediation role in the effect of behavioral synchronization on prosociality. These results contribute to our understanding of the neural and cognitive mechanisms underlying the effect of behavioral synchronization on prosocial behavior. 
  |  https://academic.oup.com/scan/article-lookup/doi/10.1093/scan/nsaa017  |  
------------------------------------------- 
10.3390/s20041100  |   Due to the flourishing development of vehicle-to-vehicle (V2V) communications and autonomous driving, interference between radar sensing and communication signals becomes a challenging issue. We propose a transmit beamforming based spectrum sharing scheme to achieve peaceful coexistence between automotive multiple-input multiple-out (MIMO) radar and communication systems. Our objective is to maximize the signal-to-interference-plus-noise ratio (SINR) of the automotive radar receiver subject to the communication capacity and the transmitted power budget constraints to optimize both the communication covariance matrix and the radar transmit precoder. The formulated optimization problem is non-convex, which is converted to convex by introducing a new slack variable and then solving it using the block coordinate descent, also called alternation optimization, approach. Additionally, the ellipsoid sub-gradient method is then employed to reduce the computational complexity. Simulation results demonstrate that our proposed scheme outperforms the conventional schemes. 
  |  http://www.mdpi.com/resolver?pii=s20041100  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32079364/  |  
------------------------------------------- 
10.3791/59942  |   Mass spectrometry (MS) is a powerful tool in analytical chemistry because it provides very accurate information about molecules, such as mass-to-charge ratios (m/z), which are useful to deduce molecular weights and structures. While it is essentially a destructive analytical method, recent advancements in the ambient ionization technique have enabled us to acquire data while leaving tissue in a relatively intact state in terms of integrity. Probe electrospray ionization (PESI) is a so-called direct method because it does not require complex and time-consuming pretreatment of samples. A fine needle serves as a sample picker, as well as an ionization emitter. Based on the very sharp and fine property of the probe tip, destruction of the samples is minimal, allowing us to acquire the real-time molecular information from living things in situ. Herein, we introduce three applications of PESI-MS technique that will be useful for biomedical research and development. One involves the application to solid tissue, which is the basic application of this technique for the medical diagnosis. As this technique requires only 10 mg of the sample, it may be very useful in the routine clinical settings. The second application is for in vitro medical diagnostics where human blood serum is measured. The ability to measure fluid samples is also valuable in various biological experiments where a sufficient volume of sample for conventional analytical techniques cannot be provided. The third application leans toward the direct application of probe needles in living animals, where we can obtain real-time dynamics of metabolites or drugs in specific organs. In each application, we can infer the molecules that have been detected by MS or use artificial intelligence to obtain a medical diagnosis. 
  |  https://doi.org/10.3791/59942  |  
------------------------------------------- 
10.1016/j.isatra.2020.02.023  |   Virtual reality is becoming more and more improved primarily due to numerous applications and the powers of mobile devices. Using various sensors, precise displays and high computing powers smartphone are becoming devices that make the boost in technology. Now it is necessary to efficiently use various sensors without affecting system operation and improve control abilities for various purposes. Especially in practical applications received by mass users such as games and any kind of experience. In this article, we propose a system that allows to extend the perception of the virtual world by conveying information about the user's movements in reality into the supervised model. The system retrieves data from several sources, quickly analyzes them using artificial intelligence techniques, and returns information to the mobile phone about the activity that is being processed. The concept extends the understanding of today's virtual reality by allowing the user to move and perform simple gestures in a specially designed room. Moreover, we propose multiplayer mode in virtual reality, where players are in different places. The proposed architecture of the system has been tested on simple applications, and the results show high potential for implementations in various apps by achieving almost 90% efficiency in changing player direction in real time and only 7.5% of collision cases. 
  |  None  |  
------------------------------------------- 
10.3389/fbioe.2020.00027  |   Although lncRNAs lack the potential to be translated into proteins directly, their complicated and diversiform functions make them as a window into decoding the mechanisms of human physiological activities. Accumulating experiment studies have identified associations between lncRNA dysfunction and many important complex diseases. However, known experimentally confirmed lncRNA functions are still very limited. It is urgent to build effective computational models for rapid predicting of unknown lncRNA functions on a large scale. To this end, valid similarity measure between known and unknown lncRNAs plays a vital role. In this paper, an original model was developed to calculate functional similarities between lncRNAs by integrating heterogeneous network data. In this model, a novel integrated network was constructed based on the data of four single lncRNA functional similarity networks (miRNA-based similarity network, disease-based similarity network, GTEx expression-based network and NONCODE expression-based network). Using the lncRNA pairs that share the target mRNAs as the benchmark, the results show that this integrated network is more effective than any single networks with an AUC of 0.736 in the cross validation, while the AUC of four single networks were 0.703, 0.733, 0.611, and 0.602. To implement our model, a web server named IHNLncSim was constructed for inferring lncRNA functional similarity based on integrating heterogeneous network data. Moreover, the modules of network visualization and disease-based lncRNA function enrichment analysis were added into IHNLncSim. It is anticipated that IHNLncSim could be an effective bioinformatics tool for the researches of lncRNA regulation function studies. IHNLncSim is freely available at http://www.lirmed.com/ihnlncsim. 
  |  https://doi.org/10.3389/fbioe.2020.00027  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32117916/  |  
------------------------------------------- 
10.3389/fncom.2020.00001  |   Modality-invariant categorical representations, i.e., shared representation, is thought to play a key role in learning to categorize multi-modal information. We have investigated how a bimodal autoencoder can form a shared representation in an unsupervised manner with multi-modal data. We explored whether altering the depth of the network and mixing the multi-modal inputs at the input layer affect the development of the shared representations. Based on the activation of units in the hidden layers, we classified them into four different types: visual cells, auditory cells, inconsistent visual and auditory cells, and consistent visual and auditory cells. Our results show that the number and quality of the last type (i.e., shared representation) significantly differ depending on the depth of the network and are enhanced when the network receives mixed inputs as opposed to separate inputs for each modality, as occurs in typical two-stage frameworks. In the present work, we present a way to utilize information theory to understand the abstract representations formed in the hidden layers of the network. We believe that such an information theoretic approach could potentially provide insights into the development of more efficient and cost-effective ways to train neural networks using qualitative measures of the representations that cannot be captured by analyzing only the final outputs of the networks. 
  |  https://doi.org/10.3389/fncom.2020.00001  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32082133/  |  
------------------------------------------- 
10.1093/bioinformatics/btaa062  |    Motivation:  One of the most important problems in drug discovery research is to precisely predict a new indication for an existing drug, i.e. drug repositioning. Recent recommendation system-based methods have tackled this problem using matrix completion models. The models identify latent factors contributing to known drug-disease associations, and then infer novel drug-disease associations by the correlations between latent factors. However, these models have not fully considered the various drug data sources and the sparsity of the drug-disease association matrix. In addition, using the global structure of the drug-disease association data may introduce noise, and consequently limit the prediction power. 
  Results:  In this work, we propose a novel drug repositioning approach by using Bayesian inductive matrix completion (DRIMC). Firstly, we embed four drug data sources into a drug similarity matrix and two disease data sources in a disease similarity matrix. Then, for each drug or disease, its feature is described by similarity values between it and its nearest neighbors, and these features for drugs and diseases are mapped onto a shared latent space. We model the association probability for each drug-disease pair by inductive matrix completion, where the properties of drugs and diseases are represented by projections of drugs and diseases, respectively. As the known drug-disease associations have been manually verified, they are more trustworthy and important than the unknown pairs. We assign higher confidence levels to known association pairs compared with unknown pairs. We perform comprehensive experiments on three benchmark datasets, and DRIMC improves prediction accuracy compared with six stat-of-the-art approaches. 
  Availability and implementation:  Source code and datasets are available at https://github.com/linwang1982/DRIMC. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btaa062  |  
------------------------------------------- 
10.1038/s41598-020-61705-9  |   Multishot Magnetic Resonance Imaging (MRI) is a promising data acquisition technique that can produce a high-resolution image with relatively less data acquisition time than the standard spin echo. The downside of multishot MRI is that it is very sensitive to subject motion and even small levels of motion during the scan can produce artifacts in the final magnetic resonance (MR) image, which may result in a misdiagnosis. Numerous efforts have focused on addressing this issue; however, all of these proposals are limited in terms of how much motion they can correct and require excessive computational time. In this paper, we propose a novel generative adversarial network (GAN)-based conjugate gradient SENSE (CG-SENSE) reconstruction framework for motion correction in multishot MRI. First CG-SENSE reconstruction is employed to reconstruct an image from the motion-corrupted k-space data and then the GAN-based proposed framework is applied to correct the motion artifacts. The proposed method has been rigorously evaluated on synthetically corrupted data on varying degrees of motion, numbers of shots, and encoding trajectories. Our analyses (both quantitative as well as qualitative/visual analysis) establish that the proposed method is robust and reduces several-fold the computational time reported by the current state-of-the-art technique. 
  |  http://dx.doi.org/10.1038/s41598-020-61705-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32179823/  |  
------------------------------------------- 
10.5455/aim.2020.28.29-36  |    Introduction:  Machine Learning (ML) is a rapidly growing subfield of Artificial Intelligence (AI). It is used for different purposes in our daily life such as face recognition, speech recognition, text translation in different languages, weather prediction, and business prediction. In parallel, ML also plays an important role in the medical domain such as in medical imaging. ML has various algorithms that need to be trained with large volumes of data to produce a well-trained model for prediction. 
  Aim:  The aim of this study is to highlight the most suitable Data Augmentation (DA) technique(s) for medical imaging based on their results. 
  Methods:  DA refers to different approaches that are used to increase the size of datasets. In this study, eight DA approaches were used on publicly available low-grade glioma tumor datasets obtained from the Tumor Cancer Imaging Archive (TCIA) repository. The dataset included 1961 MRI brain scan images of low-grade glioma patients. You Only Look Once (YOLO) version 3 model was trained on the original dataset and the augmented datasets separately. A neural network training/testing ecosystem named as supervisely with Tesla K80 GPU was used for YOLO v3 model training on all datasets. 
  Results:  The results showed that the DA techniques rotate at 180o and rotate at 90o performed the best as data enhancement techniques for medical imaging. 
  Conclusion:  Rotation techniques are found significant to enhance the low volume of medical imaging datasets. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210512/  |  
------------------------------------------- 
10.1007/s00246-020-02303-4  |   There is no better representation of the need for personalization of care than the breadth and complexity of congenital heart disease. Advanced imaging modalities are now standard of care in the field, and the advancements being made to three-dimensional visualization technologies are growing as a means of pre-procedural preparation. Incorporating emerging modeling approaches, such as computational fluid dynamics, will push the limits of our ability to predict outcomes, and this information may be both obtained and utilized during a single procedure in the future. Artificial intelligence and customized devices may soon surface as realistic tools for the care of patients with congenital heart disease, as they are showing growing evidence of feasibility within other fields. This review illustrates the great strides that have been made and the persistent challenges that exist within the field of congenital interventional cardiology, a field which must continue to innovate and push the limits to achieve personalization of the interventions it provides. 
  |  https://dx.doi.org/10.1007/s00246-020-02303-4  |  
------------------------------------------- 
10.1093/bioinformatics/btz741  |    Motivation:  Thermodynamic analysis of biological reaction networks requires the availability of accurate and consistent values of Gibbs free energies of reaction and formation. These Gibbs energies can be measured directly via the careful design of experiments or can be computed from the curated Gibbs free energy databases. However, the computed Gibbs free energies of reactions and formations do not satisfy the thermodynamic constraints due to the compounding effect of measurement errors in the experimental data. The propagation of these errors can lead to a false prediction of pathway feasibility and uncertainty in the estimation of thermodynamic parameters. 
  Results:  This work proposes a data reconciliation framework for thermodynamically consistent estimation of Gibbs free energies of reaction, formation and group contributions from experimental data. In this framework, we formulate constrained optimization problems that reduce measurement errors and their effects on the estimation of Gibbs energies such that the thermodynamic constraints are satisfied. When a subset of Gibbs free energies of formations is unavailable, it is shown that the accuracy of their resulting estimates is better than that of existing empirical prediction methods. Moreover, we also show that the estimation of group contributions can be improved using this approach. Further, we provide guidelines based on this approach for performing systematic experiments to estimate unknown Gibbs formation energies. 
  Availability and implementation:  The MATLAB code for the executing the proposed algorithm is available for free on the GitHub repository: https://github.com/samansalike/DR-thermo. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz741  |  
------------------------------------------- 
10.1259/bjr.20200117  |   Among lesions with uncertain malignant potential found at percutaneous breast biopsy, atypical ductal hyperplasia (ADH) carries both the highest risk of underestimation and the closest and most pathologist-dependent differential diagnosis with ductal carcinoma <i>in situ</i> (DCIS), matching the latter's features save for size only. ADH is therefore routinely surgically excised, but single-centre studies with limited sample size found low rates of upgrade to invasive cancer or DCIS. This suggests the possibility of surveillance over surgery in selected subgroups, considering the 2% threshold allowing for follow-up according to the Breast Imaging Reporting and Data System. A recent meta-analysis on 6458 lesions counters this approach, confirming that, surgically excised or managed with surveillance, ADH carries a 29% and 5% upgrade rate, respectively, invariably higher than 2% even in subgroups considering biopsy guidance and technique, needle calibre, apparent complete lesion removal. The high heterogeneity (<i>I</i><sup>2</sup> = 80%) found in this meta-analysis reaffirmed the need to synthesise evidence from systematic reviews to achieve generalisable results, fit for guidelines development. Limited tissue sampling at percutaneous biopsy intrinsically hampers the prediction of ADH-associated malignancy. This prediction could be improved by using contrast-enhanced breast imaging and applying artificial intelligence on both pathology and imaging results, allowing for overtreatment reduction. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20200117?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/s20082223  |   Oscillation monitoring commonly requires complex setups integrating various types of sensors associated with intensive computations to achieve an adequate rate of observations and accuracy. This research presents a simple, cost-effective approach that allows two-dimensional oscillation monitoring by terrestrial photogrammetry using non-metric cameras. Tedious camera calibration procedures are eliminated by using a grid target that allows geometric correction to be performed to the frame's region of interest at which oscillations are monitored. Region-based convolutional neural networks (Faster R-CNN) techniques are adopted to minimize the light exposure limitations, commonly constraining applications of terrestrial photogrammetry. The proposed monitoring procedure is tested at outdoor conditions to check its reliability and accuracy and examining the effect of using Faster R-CNN on monitoring results. The proposed artificial intelligence (AI) aided oscillation monitoring allowed sub-millimeter accuracy monitoring with observation rates up to 60 frames per second and gained the benefit of high optical zoom offered by market available bridge cameras to monitor oscillation of targets 100 m apart with high accuracy. 
  |  http://www.mdpi.com/resolver?pii=s20082223  |  
------------------------------------------- 
10.1038/s41575-020-0290-z  |   The global numbers of robotic gastrointestinal surgeries are increasing. However, the evidence base for robotic gastrointestinal surgery does not yet support its widespread adoption or justify its cost. The reasons for its continued popularity are complex, but a notable driver is the push for innovation - robotic surgery is seen as a compelling solution for delivering on the promise of minimally invasive precision surgery - and a changing commercial landscape delivers the promise of increased affordability. Novel systems will leverage the robot as a data-driven platform, integrating advances in imaging, artificial intelligence and machine learning for decision support. However, if this vision is to be realized, lessons must be heeded from current clinical trials and translational strategies, which have failed to demonstrate patient benefit. In this Perspective, we critically appraise current research to define the principles on which the next generation of gastrointestinal robotics trials should be based. We also discuss the emerging commercial landscape and define existing and new technologies. 
  |  http://dx.doi.org/10.1038/s41575-020-0290-z  |  
------------------------------------------- 
10.1200/EDBK_288613  |   Cancer is the second leading cause of death worldwide, with approximately 70% of the 9.6 million deaths per year occurring in low- and middle-income countries (LMICs), where there is critical shortage of human and material resources or infrastructure to deal with cancer. If the current trend continues, the burden of cancer is expected to increase to 22 million new cases annually by 2030, with 81% of new cases and almost 88% of mortality occurring in LMICs. Global health places a priority on improving health and reducing these disparities to achieve equity in health for all people worldwide. In today's hyper-connected world, information and communication technologies (ICTs) will increasingly play an integral role in global health. Here, we focus on how the use of health-related technology, specifically ICTs and artificial intelligence (AI), can help in closing the gap between high-income countries (HICs) and LMICs in cancer care, research, and education. Key examples are highlighted on the use of telemedicine and tumor boards, as well as other online resources that can be leveraged to advance global health. 
  |  http://ascopubs.org/doi/full/10.1200/EDBK_288613?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1021/acsami.9b22707  |   Development of intelligent adaptable materials with unprecedented sensitivity that can mimic the tactile sensing functions of natural skin is a major driving force in the realization of artificial intelligence. Herein, we judiciously designed and synthesized a series of lauryl acrylate-based polymeric organogels with high transparency, mechanical adaptability, self-healing properties, and adhesive capability. Moreover, a robust capacitive sensor with high sensitivity (0.293 kPa<sup>-1</sup>) was developed by sandwiching the prepared soft, adaptable organogels between two tough conductive hydrogels and then used to monitor various human motions such as finger stretching, wrist bending, and throat movement during chewing. Interestingly, the resulting capacitive sensor could also function as prosthetic skin on a pneumatic soft artificial hand, enabling intelligent haptic perception. The research disclosed herein is expected to provide insights into the rational design of artificial human-like skins with unprecedented functionalities. 
  |  https://dx.doi.org/10.1021/acsami.9b22707  |  
------------------------------------------- 
10.1186/s12863-020-0828-7  |    Background:  POLG, located on nuclear chromosome 15, encodes the DNA polymerase γ(Pol γ). Pol γ is responsible for the replication and repair of mitochondrial DNA (mtDNA). Pol γ is the only DNA polymerase found in mitochondria for most animal cells. Mutations in POLG are the most common single-gene cause of diseases of mitochondria and have been mapped over the coding region of the POLG ORF. 
  Results:  Using PhyloCSF to survey alternative reading frames, we found a conserved coding signature in an alternative frame in exons 2 and 3 of POLG, herein referred to as ORF-Y that arose de novo in placental mammals. Using the synplot2 program, synonymous site conservation was found among mammals in the region of the POLG ORF that is overlapped by ORF-Y. Ribosome profiling data revealed that ORF-Y is translated and that initiation likely occurs at a CUG codon. Inspection of an alignment of mammalian sequences containing ORF-Y revealed that the CUG codon has a strong initiation context and that a well-conserved predicted RNA stem-loop begins 14 nucleotides downstream. Such features are associated with enhanced initiation at near-cognate non-AUG codons. Reanalysis of the Kim et al. (2014) draft human proteome dataset yielded two unique peptides that map unambiguously to ORF-Y. An additional conserved uORF, herein referred to as ORF-Z, was also found in exon 2 of POLG. Lastly, we surveyed Clinvar variants that are synonymous with respect to the POLG ORF and found that most of these variants cause amino acid changes in ORF-Y or ORF-Z. 
  Conclusions:  We provide evidence for a novel coding sequence, ORF-Y, that overlaps the POLG ORF. Ribosome profiling and mass spectrometry data show that ORF-Y is expressed. PhyloCSF and synplot2 analysis show that ORF-Y is subject to strong purifying selection. An abundance of disease-correlated mutations that map to exons 2 and 3 of POLG but also affect ORF-Y provides potential clinical significance to this finding. 
  |  https://bmcgenet.biomedcentral.com/articles/10.1186/s12863-020-0828-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32138667/  |  
------------------------------------------- 
10.1007/s00330-020-06771-3  |    Objectives:  Pneumothorax is the most common and potentially life-threatening complication arising from percutaneous lung biopsy. We evaluated the performance of a deep learning algorithm for detection of post-biopsy pneumothorax in chest radiographs (CRs), in consecutive cohorts reflecting actual clinical situation. 
  Methods:  We retrospectively included post-biopsy CRs of 1757 consecutive patients (1055 men, 702 women; mean age of 65.1 years) undergoing percutaneous lung biopsies from three institutions. A commercially available deep learning algorithm analyzed each CR to identify pneumothorax. We compared the performance of the algorithm with that of radiology reports made in the actual clinical practice. We also conducted a reader study, in which the performance of the algorithm was compared with those of four radiologists. Performances of the algorithm and radiologists were evaluated by area under receiver operating characteristic curves (AUROCs), sensitivity, and specificity, with reference standards defined by thoracic radiologists. 
  Results:  Pneumothorax occurred in 17.5% (308/1757) of cases, out of which 16.6% (51/308) required catheter drainage. The AUROC, sensitivity, and specificity of the algorithm were 0.937, 70.5%, and 97.7%, respectively, for identification of pneumothorax. The algorithm exhibited higher sensitivity (70.2% vs. 55.5%, p &lt; 0.001) and lower specificity (97.7% vs. 99.8%, p &lt; 0.001), compared with those of radiology reports. In the reader study, the algorithm exhibited lower sensitivity (77.3% vs. 81.8-97.7%) and higher specificity (97.6% vs. 81.7-96.0%) than the radiologists. 
  Conclusion:  The deep learning algorithm appropriately identified pneumothorax in post-biopsy CRs in consecutive diagnostic cohorts. It may assist in accurate and timely diagnosis of post-biopsy pneumothorax in clinical practice. 
  Key points:  • A deep learning algorithm can identify chest radiographs with post-biopsy pneumothorax in multicenter consecutive cohorts reflecting actual clinical situation. • The deep learning algorithm has a potential role as a surveillance tool for accurate and timely diagnosis of post-biopsy pneumothorax. 
  |  https://dx.doi.org/10.1007/s00330-020-06771-3  |  
------------------------------------------- 
10.2196/16934  |    Background:  Conversational agents (also known as chatbots) have evolved in recent decades to become multimodal, multifunctional platforms with potential to automate a diverse range of health-related activities supporting the general public, patients, and physicians. Multiple studies have reported the development of these agents, and recent systematic reviews have described the scope of use of conversational agents in health care. However, there is scarce research on the effectiveness of these systems; thus, their viability and applicability are unclear. 
  Objective:  The objective of this systematic review is to assess the effectiveness of conversational agents in health care and to identify limitations, adverse events, and areas for future investigation of these agents. 
  Methods:  The Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols will be used to structure this protocol. The focus of the systematic review is guided by a population, intervention, comparator, and outcome framework. A systematic search of the PubMed (Medline), EMBASE, CINAHL, and Web of Science databases will be conducted. Two authors will independently screen the titles and abstracts of the identified references and select studies according to the eligibility criteria. Any discrepancies will then be discussed and resolved. Two reviewers will independently extract and validate data from the included studies into a standardized form and conduct quality appraisal. 
  Results:  As of January 2020, we have begun a preliminary literature search and piloting of the study selection process. 
  Conclusions:  This systematic review aims to clarify the effectiveness, limitations, and future applications of conversational agents in health care. Our findings may be useful to inform the future development of conversational agents and promote the personalization of patient care. 
  International registered report identifier (irrid):  PRR1-10.2196/16934. 
  |  https://www.researchprotocols.org/2020/3/e16934/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32149717/  |  
------------------------------------------- 
10.1093/eurheartj/ehaa159  |   Providing therapies tailored to each patient is the vision of precision medicine, enabled by the increasing ability to capture extensive data about individual patients. In this position paper, we argue that the second enabling pillar towards this vision is the increasing power of computers and algorithms to learn, reason, and build the 'digital twin' of a patient. Computational models are boosting the capacity to draw diagnosis and prognosis, and future treatments will be tailored not only to current health status and data, but also to an accurate projection of the pathways to restore health by model predictions. The early steps of the digital twin in the area of cardiovascular medicine are reviewed in this article, together with a discussion of the challenges and opportunities ahead. We emphasize the synergies between mechanistic and statistical models in accelerating cardiovascular research and enabling the vision of precision medicine. 
  |  https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehaa159  |  
------------------------------------------- 
10.1016/j.spinee.2020.04.001  |    Background:  Intraoperative vascular injury (VI) may be an unavoidable complication of anterior lumbar spine surgery; however, vascular injury has implications for quality and safety reporting as this intraoperative complication may result in serious bleeding, thrombosis, and postoperative stricture. 
  Purpose:  The purpose of this study was to (1) develop machine learning algorithms for preoperative prediction of VI and (2) develop natural language processing (NLP) algorithms for automated surveillance of intraoperative VI from free-text operative notes. 
  Patient sample:  Adult patients, 18 years or age or older, undergoing anterior lumbar spine surgery at two academic and three community medical centers were included in this analysis. 
  Outcome measures:  The primary outcome was unintended VI during anterior lumbar spine surgery. 
  Methods:  Manual review of free-text operative notes was used to identify patients who had unintended VI. The available population was split into training and testing cohorts. Five machine learning algorithms were developed for preoperative prediction of VI. An NLP algorithm was trained for automated detection of intraoperative VI from free-text operative notes. Performance of the NLP algorithm was compared to current procedural terminology and international classification of diseases codes. 
  Results:  In all, 1035 patients underwent anterior lumbar spine surgery and the rate of intraoperative VI was 7.2% (n=75). Variables used for preoperative prediction of VI were age, male sex, body mass index, diabetes, L4-L5 exposure, and surgery for infection (discitis, osteomyelitis). The best performing machine learning algorithm achieved c-statistic of 0.73 for preoperative prediction of VI (https://sorg-apps.shinyapps.io/lumbar_vascular_injury/). For automated detection of intraoperative VI from free-text notes, the NLP algorithm achieved c-statistic of 0.92. The NLP algorithm identified 18 of the 21 patients (sensitivity 0.86) who had a VI whereas current procedural terminologyand international classification of diseases codes identified 6 of the 21 (sensitivity 0.29) patients. At this threshold, the NLP algorithm had a specificity of 0.93, negative predictive value of 0.99, positive predictive value of 0.51, and F1-score of 0.64. 
  Conclusion:  Relying on administrative procedural and diagnosis codes may underestimate the rate of unintended intraoperative VI in anterior lumbar spine surgery. External and prospective validation of the algorithms presented here may improve quality and safety reporting. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1529-9430(20)30135-2  |  
------------------------------------------- 
10.2196/14660  |    Background:  On January 1, 2019, a new regulation on the control of smoking in public places was officially implemented in Hangzhou, China. On the day of the implementation, a large number of Chinese media reported the contents of the regulation on the microblog platform Weibo, causing a strong response from and heated discussion among netizens. 
  Objective:  This study aimed to conduct a content and network analysis to examine topics and patterns in the social media response to the new regulation. 
  Methods:  We analyzed all microblogs on Weibo that mentioned and explained the regulation in the first 8 days following the implementation. We conducted a content analysis on these microblogs and used social network visualization and descriptive statistics to identify key users and key microblogs. 
  Results:  Of 7924 microblogs, 12.85% (1018/7924) were in support of the smoking control regulation, 84.12% (6666/7924) were neutral, and 1.31% (104/7924) were opposed to the smoking regulation control. For the negative posts, the public had doubts about the intentions of the policy, its implementation, and the regulations on electronic cigarettes. In addition, 1.72% (136/7924) were irrelevant to the smoking regulation control. Among the 1043 users who explicitly expressed their positive or negative attitude toward the policy, a large proportion of users showed supportive attitudes (956/1043, 91.66%). A total of 5 topics and 11 subtopics were identified. 
  Conclusions:  This study used a content and network analysis to examine topics and patterns in the social media response to the new smoking regulation. We found that the number of posts with a positive attitude toward the regulation was considerably higher than that of the posts with a negative attitude toward the regulation. Our findings may assist public health policy makers to better understand the policy's intentions, scope, and potential effects on public interest and support evidence-based public health regulations in the future. 
  |  https://www.jmir.org/2020/4/e14660/  |  
------------------------------------------- 
10.3389/fgene.2020.00350  |   Genome-wide association studies (GWAS) have revealed thousands of genetic loci that underpin the complex biology of many human traits. However, the strength of GWAS - the ability to detect genetic association by linkage disequilibrium (LD) - is also its limitation. Whilst the ever-increasing study size and improved design have augmented the power of GWAS to detect effects, differentiation of causal variants or genes from other highly correlated genes associated by LD remains the real challenge. This has severely hindered the biological insights and clinical translation of GWAS findings. Although thousands of disease susceptibility loci have been reported, causal genes at these loci remain elusive. Machine learning (ML) techniques offer an opportunity to dissect the heterogeneity of variant and gene signals in the post-GWAS analysis phase. ML models for GWAS prioritization vary greatly in their complexity, ranging from relatively simple logistic regression approaches to more complex ensemble models such as random forests and gradient boosting, as well as deep learning models, i.e., neural networks. Paired with functional validation, these methods show important promise for clinical translation, providing a strong evidence-based approach to direct post-GWAS research. However, as ML approaches continue to evolve to meet the challenge of causal gene identification, a critical assessment of the underlying methodologies and their applicability to the GWAS prioritization problem is needed. This review investigates the landscape of ML applications in three parts: selected models, input features, and output model performance, with a focus on prioritizations of complex disease associated loci. Overall, we explore the contributions ML has made towards reaching the GWAS end-game with consequent wide-ranging translational impact. 
  |  https://doi.org/10.3389/fgene.2020.00350  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32351543/  |  
------------------------------------------- 
10.1007/s10278-019-00207-1  |   The volume of pelvic hematoma at CT has been shown to be the strongest independent predictor of major arterial injury requiring angioembolization in trauma victims with pelvic fractures, and also correlates with transfusion requirement and mortality. Measurement of pelvic hematomas (unopacified extraperitoneal blood accumulated from time of injury) using semi-automated seeded region growing is time-consuming and requires trained experts, precluding routine measurement at the point of care. Pelvic hematomas are markedly variable in shape and location, have irregular ill-defined margins, have low contrast with respect to viscera and muscle, and reside within anatomically distorted pelvises. Furthermore, pelvic hematomas occupy a small proportion of the entire volume of a chest, abdomen, and pelvis (C/A/P) trauma CT. The challenges are many, and no automated methods for segmentation and volumetric analysis have been described to date. Traditional approaches using fully convolutional networks result in coarse segmentations and class imbalance with suboptimal convergence. In this study, we implement a modified coarse-to-fine deep learning approach-the Recurrent Saliency Transformation Network (RSTN) for pelvic hematoma volume segmentation. RSTN previously yielded excellent results in pancreas segmentation, where low contrast with adjacent structures, small target volume, variable location, and fine contours are also problematic. We have curated a unique single-institution corpus of 253 C/A/P admission trauma CT studies in patients with bleeding pelvic fractures with manually labeled pelvic hematomas. We hypothesized that RSTN would result in sufficiently high Dice similarity coefficients to facilitate accurate and objective volumetric measurements for outcome prediction (arterial injury requiring angioembolization). Cases were separated into five combinations of training and test sets in an 80/20 split and fivefold cross-validation was performed. Dice scores in the test set were 0.71 (SD ± 0.10) using RSTN, compared to 0.49 (SD ± 0.16) using a baseline Deep Learning Tool Kit (DLTK) reference 3D U-Net architecture. Mean inference segmentation time for RSTN was 0.90 min (± 0.26). Pearson correlation between predicted and manual labels was 0.95 with p &lt; 0.0001. Measurement bias was within 10 mL. AUC of hematoma volumes for predicting need for angioembolization was 0.81 (predicted) versus 0.80 (manual). Qualitatively, predicted labels closely followed hematoma contours and avoided muscle and displaced viscera. Further work will involve validation using a federated dataset and incorporation into a predictive model using multiple segmented features. 
  |  https://doi.org/10.1007/s10278-019-00207-1  |  
------------------------------------------- 
10.1016/j.joim.2020.02.005  |    Objective:  In this study we execute a rational screen to identify Chinese medical herbs that are commonly used in treating viral respiratory infections and also contain compounds that might directly inhibit 2019 novel coronavirus (2019-nCoV), an ongoing novel coronavirus that causes pneumonia. 
  Methods:  There were two main steps in the screening process. In the first step we conducted a literature search for natural compounds that had been biologically confirmed as against sever acute respiratory syndrome coronavirus or Middle East respiratory syndrome coronavirus. Resulting compounds were cross-checked for listing in the Traditional Chinese Medicine Systems Pharmacology Database. Compounds meeting both requirements were subjected to absorption, distribution, metabolism and excretion (ADME) evaluation to verify that oral administration would be effective. Next, a docking analysis was used to test whether the compound had the potential for direct 2019-nCoV protein interaction. In the second step we searched Chinese herbal databases to identify plants containing the selected compounds. Plants containing 2 or more of the compounds identified in our screen were then checked against the catalogue for classic herbal usage. Finally, network pharmacology analysis was used to predict the general in vivo effects of each selected herb. 
  Results:  Of the natural compounds screened, 13 that exist in traditional Chinese medicines were also found to have potential anti-2019-nCoV activity. Further, 125 Chinese herbs were found to contain 2 or more of these 13 compounds. Of these 125 herbs, 26 are classically catalogued as treating viral respiratory infections. Network pharmacology analysis predicted that the general in vivo roles of these 26 herbal plants were related to regulating viral infection, immune/inflammation reactions and hypoxia response. 
  Conclusion:  Chinese herbal treatments classically used for treating viral respiratory infection might contain direct anti-2019-nCoV compounds. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2095-4964(20)30015-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32113846/  |  
------------------------------------------- 
10.1093/clinchem/hvaa102  |    |  https://academic.oup.com/clinchem/article-lookup/doi/10.1093/clinchem/hvaa102  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32315390/  |  
------------------------------------------- 
10.1109/RBME.2020.2990959  |   Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19. 
  |  https://dx.doi.org/10.1109/RBME.2020.2990959  |  
------------------------------------------- 
10.12809/hkmj198080  |   Echocardiography is a key evaluation tool for the diagnosis, prognosis, and guidance of interventional management of numerous cardiovascular conditions, including ischaemia, heart failure, and structural heart diseases. Recent technological advancements have also seen the exploration of artificial intelligence, intracardiac vortex imaging, and three-dimensional printing in echocardiography. With cardiovascular diseases increasing in prevalence worldwide, it is important for clinicians including general practitioners to have updated knowledge of appropriate use of echocardiography. As such, this article reviews the current literature and summarises the latest developments and the general clinical usage of echocardiography. 
  |  http://www.hkmj.org/abstracts/v26n1/44.htm  |  
------------------------------------------- 
10.1159/000505411  |    Background:  Endoscopic imaging is a rapidly evolving field with a constant influx of new concepts and technologies. Since the introduction of video endoscopy and subsequently high-definition imaging as the first revolutions in gastrointestinal endoscopy, several technologies of virtual chromoendoscopy have been developed and brought to the market in the past decade, which have shaped and revolutionized for a second time our approach to endoscopic imaging. In parallel to these developments, microscopic imaging technologies, such as endomicroscopy and endocytoscopy, allow us to examine single cells within the mucosa in real time, thereby enabling histological diagnoses during ongoing endoscopy. 
  Summary:  In this review, we provide an overview on the technical background of different technologies of advanced endoscopic imaging, and then review and discuss their role and applications for the diagnosis and management of colorectal neoplasms as well as limitations and challenges that exist despite all technological improvements. 
  Key messages:  Technologies of advanced endoscopic imaging have profound impact not only on our imaging capabilities, they are also about to fundamentally change our approach to managing lesions in the gastrointestinal tract: not every lesion found during colonoscopy has to be excised or sent for histopathologic evaluation. However, before this becomes widespread reality, major obstacles such as patient acceptance, adoption by less trained endoscopists, and also legal aspects need to carefully addressed. The development of computer-aided diagnosis and artificial intelligence algorithms hold the potential to overcome the obstacles associated with the concept of optical biopsy and will most likely fundamentally facilitate, shape, and change decision making in the management of colorectal lesions. 
  |  https://www.karger.com?DOI=10.1159/000505411  |  
------------------------------------------- 
10.3389/fnins.2019.01408  |   Different from conventional single-task optimization, the recently proposed multitasking optimization (MTO) simultaneously deals with multiple optimization tasks with different types of decision variables. MTO explores the underlying similarity and complementarity among the component tasks to improve the optimization process. The well-known multifactorial evolutionary algorithm (MFEA) has been successfully introduced to solve MTO problems based on transfer learning. However, it uses a simple and random inter-task transfer learning strategy, thereby resulting in slow convergence. To deal with this issue, this paper presents a two-level transfer learning (TLTL) algorithm, in which the upper-level implements inter-task transfer learning via chromosome crossover and elite individual learning, and the lower-level introduces intra-task transfer learning based on information transfer of decision variables for an across-dimension optimization. The proposed algorithm fully uses the correlation and similarity among the component tasks to improve the efficiency and effectiveness of MTO. Experimental studies demonstrate the proposed algorithm has outstanding ability of global search and fast convergence rate. 
  |  https://doi.org/10.3389/fnins.2019.01408  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31992969/  |  
------------------------------------------- 
10.1089/soro.2019.0036  |   Self-regulation (or so-called homeostasis) is a property of all living organisms to maintain an internal stable state through specialized biofeedback mechanisms under varying external and internal conditions. Although these feedback mechanisms in living organisms are complex networks and hard to implement one-to-one in artificial systems, the new approaches in soft robotics may benefit from the concept of self-regulation-especially in the new endeavors of making untethered, autonomous soft robots. In this study, we show a simple system, in which plant robots display heliotropism (sun tracking) and nyctinasty (leaf opening) through artificial self-regulation attained through a bioinspired transpiration mechanism. The feedback involves dehydration/hydration and transpiration events that keep the stem continuously in a metastable position, which maximizes light on plant leaves and the efficiency of light harvesting when solar panels are attached on leaves. We also demonstrate that this artificial feedback can be regulated by doping with light-absorbing chemicals or by changing the geometry of the system, and it can further be expanded to other lightweight systems. Implementing self-regulation into (soft) robots through bioinspired material feedback is beneficial not only for energy efficiency and harvesting but also for achieving embodied intelligence in autonomous soft robots. 
  |  https://www.liebertpub.com/doi/full/10.1089/soro.2019.0036?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1097/AUD.0000000000000794  |    Objectives:  The purpose of this study was to develop a deep-learning framework for the diagnosis of chronic otitis media (COM) based on temporal bone computed tomography (CT) scans. 
  Design:  A total of 562 COM patients with 672 temporal bone CT scans of both ears were included. The final dataset consisted of 1147 ears, and each of them was assigned with a ground truth label from one of the 3 conditions: normal, chronic suppurative otitis media, and cholesteatoma. A random selection of 85% dataset (n = 975) was used for training and validation. The framework contained two deep-learning networks with distinct functions: a region proposal network for extracting regions of interest from 2-dimensional CT slices; and a classification network for diagnosis of COM based on the extracted regions. The performance of this framework was evaluated on the remaining 15% dataset (n = 172) and compared with that of 6 clinical experts who read the same CT images only. The panel included 2 otologists, 3 otolaryngologists, and 1 radiologist. 
  Results:  The area under the receiver operating characteristic curve of the artificial intelligence model in classifying COM versus normal was 0.92, with sensitivity (83.3%) and specificity (91.4%) exceeding the averages of clinical experts (81.1% and 88.8%, respectively). In a 3-class classification task, this network had higher overall accuracy (76.7% versus 73.8%), higher recall rates in identifying chronic suppurative otitis media (75% versus 70%) and cholesteatoma (76% versus 53%) cases, and superior consistency in duplicated cases (100% versus 81%) compared with clinical experts. 
  Conclusions:  This article presented a deep-learning framework that automatically extracted the region of interest from two-dimensional temporal bone CT slices and made diagnosis of COM. The performance of this model was comparable and, in some cases, superior to that of clinical experts. These results implied a promising prospect for clinical application of artificial intelligence in the diagnosis of COM based on CT images. 
  |  http://dx.doi.org/10.1097/AUD.0000000000000794  |  
------------------------------------------- 
10.1161/CIRCULATIONAHA.119.044666  |    Background:  Myocardial perfusion reflects the macro- and microvascular coronary circulation. Recent quantitation developments using cardiovascular magnetic resonance perfusion permit automated measurement clinically. We explored the prognostic significance of stress myocardial blood flow (MBF) and myocardial perfusion reserve (MPR, the ratio of stress to rest MBF). 
  Methods:  A 2-center study of patients with both suspected and known coronary artery disease referred clinically for perfusion assessment. Image analysis was performed automatically using a novel artificial intelligence approach deriving global and regional stress and rest MBF and MPR. Cox proportional hazard models adjusting for comorbidities and cardiovascular magnetic resonance parameters sought associations of stress MBF and MPR with death and major adverse cardiovascular events (MACE), including myocardial infarction, stroke, heart failure hospitalization, late (&gt;90 day) revascularization, and death. 
  Results:  A total of 1049 patients were included with a median follow-up of 605 (interquartile range, 464-814) days. There were 42 (4.0%) deaths and 188 MACE in 174 (16.6%) patients. Stress MBF and MPR were independently associated with both death and MACE. For each 1 mL·g<sup>-1</sup>·min<sup>-1</sup> decrease in stress MBF, the adjusted hazard ratios for death and MACE were 1.93 (95% CI, 1.08-3.48, <i>P</i>=0.028) and 2.14 (95% CI, 1.58-2.90, <i>P</i>&lt;0.0001), respectively, even after adjusting for age and comorbidity. For each 1 U decrease in MPR, the adjusted hazard ratios for death and MACE were 2.45 (95% CI, 1.42-4.24, <i>P</i>=0.001) and 1.74 (95% CI, 1.36-2.22, <i>P</i>&lt;0.0001), respectively. In patients without regional perfusion defects on clinical read and no known macrovascular coronary artery disease (n=783), MPR remained independently associated with death and MACE, with stress MBF remaining associated with MACE only. 
  Conclusions:  In patients with known or suspected coronary artery disease, reduced MBF and MPR measured automatically inline using artificial intelligence quantification of cardiovascular magnetic resonance perfusion mapping provides a strong, independent predictor of adverse cardiovascular outcomes. 
  |  http://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.119.044666?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1148/radiol.2020190283  |   Background Although artificial intelligence (AI) shows promise across many aspects of radiology, the use of AI to create differential diagnoses for rare and common diseases at brain MRI has not been demonstrated. Purpose To evaluate an AI system for generation of differential diagnoses at brain MRI compared with radiologists. Materials and Methods This retrospective study tested performance of an AI system for probabilistic diagnosis in patients with 19 common and rare diagnoses at brain MRI acquired between January 2008 and January 2018. The AI system combines data-driven and domain-expertise methodologies, including deep learning and Bayesian networks. First, lesions were detected by using deep learning. Then, 18 quantitative imaging features were extracted by using atlas-based coregistration and segmentation. Third, these image features were combined with five clinical features by using Bayesian inference to develop probability-ranked differential diagnoses. Quantitative feature extraction algorithms and conditional probabilities were fine-tuned on a training set of 86 patients (mean age, 49 years ± 16 [standard deviation]; 53 women). Accuracy was compared with radiology residents, general radiologists, neuroradiology fellows, and academic neuroradiologists by using accuracy of top one, top two, and top three differential diagnoses in 92 independent test set patients (mean age, 47 years ± 18; 52 women). Results For accuracy of top three differential diagnoses, the AI system (91% correct) performed similarly to academic neuroradiologists (86% correct; <i>P</i> = .20), and better than radiology residents (56%; <i>P</i> &lt; .001), general radiologists (57%; <i>P</i> &lt; .001), and neuroradiology fellows (77%; <i>P</i> = .003). The performance of the AI system was not affected by disease prevalence (93% accuracy for common vs 85% for rare diseases; <i>P</i> = .26). Radiologists were more accurate at diagnosing common versus rare diagnoses (78% vs 47% across all radiologists; <i>P</i> &lt; .001). Conclusion An artificial intelligence system for brain MRI approached overall top one, top two, and top three differential diagnoses accuracy of neuroradiologists and exceeded that of less-specialized radiologists. © RSNA, 2020 <i>Online supplemental material is available for this article.</i> See also the editorial by Zaharchuk in this issue. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020190283?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.biomaterials.2020.119827  |   Photodynamic therapy (PDT), as a non-invasive therapeutic modality that is alternative to radiotherapy and chemotherapy, is extensively investigated for cancer treatments. Although conventional organic photosensitizers (PSs) are still widely used and have achieved great progresses in PDT, the disadvantages such as hydrophobicity, poor stability within PDT environment and low cell/tissue specificity largely limit their clinical applications. Consequently, nano-agents with promising physicochemical and optical properties have emerged as an attractive alternative to overcome these drawbacks of traditional PSs. Herein, the up-to-date advances in the fabrication and fascinating applications of various nanomaterials in PDT have been summarized, including various types of nanoparticles, carbon-based nanomaterials, and two-dimensional nanomaterials, etc. In addition, the current challenges for the clinical use of PDT, and the corresponding strategies to address these issues, as well as future perspectives on further improvement of PDT have also been discussed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0142-9612(20)30073-9  |  
------------------------------------------- 
10.1021/acsnano.9b07687  |   Neuromorphic visual sensory and memory systems, which can perceive, process, and memorize optical information, represent core technology for artificial intelligence and robotics with autonomous navigation. An optoelectronic synapse with an elegant integration of biometric optical sensing and synaptic learning functions can be a fundamental element for the hardware-implementation of such systems. Here, we report a class of ferroelectric field-effect memristive transistors made of a two-dimensional WS<sub>2</sub> semiconductor atop a ferroelectric PbZr<sub>0.2</sub>Ti<sub>0.8</sub>O<sub>3</sub> (PZT) thin film for optoelectronic synaptic devices. The WS<sub>2</sub> channel exhibits voltage- and light-controllable memristive switching, dependent on the optically and electrically tunable ferroelectric domain patterns in the underlying PZT layer. These devices consequently show the emulation of optically driven synaptic functionalities including both short- and long-term plasticity as well as the implementation of brainlike learning rules. Integration of these rich synaptic functionalities into one single artificial optoelectronic device could allow the development of future neuromorphic electronics capable of optical information sensing and learning. 
  |  https://dx.doi.org/10.1021/acsnano.9b07687  |  
------------------------------------------- 
10.1007/s00330-019-06589-8  |    Objectives:  To perform test-retest reproducibility analyses for deep learning-based automatic detection algorithm (DLAD) using two stationary chest radiographs (CRs) with short-term intervals, to analyze influential factors on test-retest variations, and to investigate the robustness of DLAD to simulated post-processing and positional changes. 
  Methods:  This retrospective study included patients with pulmonary nodules resected in 2017. Preoperative CRs without interval changes were used. Test-retest reproducibility was analyzed in terms of median differences of abnormality scores, intraclass correlation coefficients (ICC), and 95% limits of agreement (LoA). Factors associated with test-retest variation were investigated using univariable and multivariable analyses. Shifts in classification between the two CRs were analyzed using pre-determined cutoffs. Radiograph post-processing (blurring and sharpening) and positional changes (translations in x- and y-axes, rotation, and shearing) were simulated and agreement of abnormality scores between the original and simulated CRs was investigated. 
  Results:  Our study analyzed 169 patients (median age, 65 years; 91 men). The median difference of abnormality scores was 1-2% and ICC ranged from 0.83 to 0.90. The 95% LoA was approximately ± 30%. Test-retest variation was negatively associated with solid portion size (β, - 0.50; p = 0.008) and good nodule conspicuity (β, - 0.94; p &lt; 0.001). A small fraction (15/169) showed discordant classifications when the high-specificity cutoff (46%) was applied to the model outputs (p = 0.04). DLAD was robust to the simulated positional change (ICC, 0.984, 0.996), but relatively less robust to post-processing (ICC, 0.872, 0.968). 
  Conclusions:  DLAD was robust to the test-retest variation. However, inconspicuous nodules may cause fluctuations of the model output and subsequent misclassifications. 
  Key points:  • The deep learning-based automatic detection algorithm was robust to the test-retest variation of the chest radiographs in general. • The test-retest variation was negatively associated with solid portion size and good nodule conspicuity. • High-specificity cutoff (46%) resulted in discordant classifications of 8.9% (15/169; p = 0.04) between the test-retest radiographs. 
  |  https://dx.doi.org/10.1007/s00330-019-06589-8  |  
------------------------------------------- 
10.1002/path.5425  |   This year's Annual Review Issue of The Journal of Pathology contains 18 invited reviews on current research areas in pathology. The subject areas reflect the broad range of topics covered by the journal and this year encompass the development and application of software in digital histopathology, implementation of biomarkers in pathology practice; genetics and epigenetics, and stromal influences in disease. The reviews are authored by experts in their field and provide comprehensive updates in the chosen areas, in which there has been considerable recent progress in our understanding of disease. © 2020 Pathological Society of Great Britain and Ireland. Published by John Wiley &amp; Sons, Ltd. 
  |  https://doi.org/10.1002/path.5425  |  
------------------------------------------- 
10.1371/journal.pone.0230717  |   In order to remedy the current problem of having been buffeted by competing requirements for both protection sensitivity and quick reaction of High Voltage Direct Current (HVDC) transmission lines simultaneously, a new intelligent fault identification method based on Random Forests (RF) for HVDC transmission lines is proposed. S transform is implemented to extract fault current traveling wave of 8 frequencies and calculate the fluctuation index and energy sum ratio, in which the wave index is used to identify internal and external faults, and energy sum ratio is used to identify the positive and negative pole faults occurred on the transmission line. The intelligent fault identification model of RF is established, and the fault characteristic sample set of HVDC transmission lines is constructed by using multi-scale S transform fluctuation index and multi-scale S-transform energy sum ratio. Training and testing have been carried out to identify HVDC transmission line faults. According to theoretical researches and a large number of results of simulation experiments, the proposed intelligent fault identification method based on RF for HVDC transmission lines can effectively solve the problem of protection failure caused by inaccurate identification of traditional traveling wave wavefront or wavefront data loss. It can accurately and quickly realize the identification of internal and external faults and the selection of fault poles under different fault distances and transitional resistances, and has a strong ability to withstand transitional resistance and a strong ability to resist interference. 
  |  http://dx.plos.org/10.1371/journal.pone.0230717  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32214364/  |  
------------------------------------------- 
10.1248/cpb.c19-00900  |   Organocatalytic enantioselective domino reactions are an extremely attractive methodology, as their use enables the construction of complex chiral skeletons from readily available starting materials in two or more steps by a single operation under mild reaction conditions. Thus, these reactions can save both the quantity of chemicals and length of time typically required for the isolation and/or purification of synthetic intermediates. Additionally, no metal contamination of the products occurs, given that organocatalysts include no expensive or toxic metals. The aza-Morita-Baylis-Hillman (aza-MBH) reaction is an atom-economical carbon-carbon bond-forming reaction between α,β-unsaturated carbonyl compounds and imines mediated by Lewis base (LB) catalysts, such as nucleophilic phosphines and amines. aza-MBH products are functionalized chiral β-amino acid derivatives that are highly valuable as pharmaceutical raw materials. Although various enantioselective aza-MBH processes have been investigated, very few studies of aza-MBH-type domino reactions have been reported due to the complexity of the aza-MBH process, which involves a Michael/Mannich/H-transfer/β-elimination sequence. Accordingly, in this review article, our recent efforts in the development of enantioselective domino reactions initiated by MBH processes are described. In the domino reactions, chiral organocatalysts bearing Brønsted acid (BA) and/or LB units impart synergistic activation to substrates, leading to the easy synthesis of highly functionalized heterocycles (some of which have tetrasubstituted and/or quaternary carbon stereocenters) in high yield and enantioselectivity. 
  |  https://dx.doi.org/10.1248/cpb.c19-00900  |  
------------------------------------------- 
10.1021/acs.accounts.9b00470  |   The world needs new materials to stimulate the chemical industry in key sectors of our economy: environment and sustainability, information storage, optical telecommunications, and catalysis. Yet, nearly all functional materials are still discovered by "trial-and-error", of which the lack of predictability affords a major materials bottleneck to technological innovation. The average "molecule-to-market" lead time for materials discovery is currently 20 years. This is far too long for industrial needs, as highlighted by the Materials Genome Initiative, which has ambitious targets of up to 4-fold reductions in average molecule-to-market lead times. Such a large step change in progress can only be realistically achieved if one adopts an entirely new approach to materials discovery. Fortunately, a fundamentally new approach to materials discovery has been emerging, whereby data science with artificial intelligence offers a prospective solution to speed up these average molecule-to-market lead times.This approach is known as data-driven materials discovery. Its broad prospects have only recently become a reality, given the timely and major advances in "big data", artificial intelligence, and high-performance computing (HPC). Access to massive data sets has been stimulated by government-regulated open-access requirements for data and literature. Natural-language processing (NLP) and machine-learning (ML) tools that can mine data and find patterns therein are becoming mainstream. Exascale HPC capabilities that can aid data mining and pattern recognition and also generate their own data from calculations are now within our grasp. These timely advances present an ideal opportunity to develop data-driven materials-discovery strategies to systematically design and predict new chemicals for a given device application.This Account shows how data science can afford materials discovery via a four-step "design-to-device" pipeline that entails (1) data extraction, (2) data enrichment, (3) material prediction, and (4) experimental validation. Massive databases of cognate chemical and property information are first forged from "chemistry-aware" natural-language-processing tools, such as ChemDataExtractor, and enriched using machine-learning methods and high-throughput quantum-chemical calculations. New materials for a bespoke application can then be predicted by mining these databases with algorithmic encodings of relationships between chemical structures and physical properties that are known to deliver functional materials. These may take the form of classification, enumeration, or machine-learning algorithms. A data-mining workflow short-lists these predictions to a handful of lead candidate materials that go forward to experimental validation. This design-to-device approach is being developed to offer a roadmap for the accelerated discovery of new chemicals for functional applications. Case studies presented demonstrate its utility for photovoltaic, optical, and catalytic applications. While this Account is focused on applications in the physical sciences, the generic pipeline discussed is readily transferable to other scientific disciplines such as biology and medicine. 
  |  https://dx.doi.org/10.1021/acs.accounts.9b00470  |  
------------------------------------------- 
10.1021/acs.nanolett.9b05271  |   To construct an artificial intelligence system with high efficient information integration and computing capability like the human brain, it is necessary to realize the biological neurotransmission and information processing in artificial neural network (ANN), rather than a single electronic synapse as most reports. Because the power consumption of single synaptic event is ∼10 fJ in biology, designing an intelligent memristors-based 3D ANN with energy consumption lower than femtojoule-level (e.g., attojoule-level) and faster operating speed than millisecond-level makes it possible for constructing a higher energy efficient and higher speed computing system than the human brain. In this paper, a flexible 3D crossbar memristor array is presented, exhibiting the multilevel information transmission functionality with the power consumption of 4.28 aJ and the response speed of 50 ns per synaptic event. This work is a significant step toward the development of an ultrahigh efficient and ultrahigh-speed wearable 3D neuromorphic computing system. 
  |  https://dx.doi.org/10.1021/acs.nanolett.9b05271  |  
------------------------------------------- 
10.1111/cyt.12835  |   Cervical cancer prevention has undergone dramatic changes over the past decade. With the introduction of HPV vaccination, some countries have seen a dramatic decline in HPV-mediated cervical disease. However, widespread implementation has been limited by economic considerations and the varying healthcare priorities of different countries, as well asby vaccine availability, and in some instances, vaccine hesitancy amongst the population/ government. In this environment,it is clear that cervical screening will retain a critical role in the prevention of cervical cancer and will in due course need to adapt to the changing incidence of HPV-associated neoplasia. Cervical screening has for many years, been performed using Pap staining ofcytology samples. As our understanding of the role of HPV in cervical cancer progression has advanced, and with theavailability of sensitive detection systems, cervical screeningnowincorporates HPV testing. Although such tests improvedisease-detection, they are not specific,and cannot discriminate high-grade disease from low-grade disease. This has necessitated the development of effective triage approachesto stratify HPV-positive women according to their risk of cancer progression. Although cytology triage remains the mainstay of screening, novel strategies under evaluation include DNA methylation, biomarker detection, and the incorporation of 'artificial intelligence' systems to detect cervical abnormalities. These tests, which can be partially anchored in a molecular understanding of HPV pathogenesis, willenhance the sensitivity of disease detection and improve patient outcomes. This review will provide insight on these innovative methodologies while explaining their scientific basis drawing from our understanding of HPV tumour biology. 
  |  https://doi.org/10.1111/cyt.12835  |  
------------------------------------------- 
10.1093/bioinformatics/btz756  |    Motivation:  With the rapidly growing biomedical literature, automatically indexing biomedical articles by Medical Subject Heading (MeSH), namely MeSH indexing, has become increasingly important for facilitating hypothesis generation and knowledge discovery. Over the past years, many large-scale MeSH indexing approaches have been proposed, such as Medical Text Indexer, MeSHLabeler, DeepMeSH and MeSHProbeNet. However, the performance of these methods is hampered by using limited information, i.e. only the title and abstract of biomedical articles. 
  Results:  We propose FullMeSH, a large-scale MeSH indexing method taking advantage of the recent increase in the availability of full text articles. Compared to DeepMeSH and other state-of-the-art methods, FullMeSH has three novelties: (i) Instead of using a full text as a whole, FullMeSH segments it into several sections with their normalized titles in order to distinguish their contributions to the overall performance. (ii) FullMeSH integrates the evidence from different sections in a 'learning to rank' framework by combining the sparse and deep semantic representations. (iii) FullMeSH trains an Attention-based Convolutional Neural Network for each section, which achieves better performance on infrequent MeSH headings. FullMeSH has been developed and empirically trained on the entire set of 1.4 million full-text articles in the PubMed Central Open Access subset. It achieved a Micro F-measure of 66.76% on a test set of 10 000 articles, which was 3.3% and 6.4% higher than DeepMeSH and MeSHLabeler, respectively. Furthermore, FullMeSH demonstrated an average improvement of 4.7% over DeepMeSH for indexing Check Tags, a set of most frequently indexed MeSH headings. 
  Availability and implementation:  The software is available upon request. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz756  |  
------------------------------------------- 
10.3389/fbioe.2020.00227  |   RNA 5-hydroxymethylcytosine (5hmC) modification plays an important role in a series of biological processes. Characterization of its distributions in transcriptome is fundamentally important to reveal the biological functions of 5hmC. Sequencing-based technologies allow the high-throughput identification of 5hmC; however, they are labor-intensive, time-consuming, as well as expensive. Thus, there is an urgent need to develop more effective and efficient computational methods, at least complementary to the high-throughput technologies. In this study, we developed iRNA5hmC, a computational predictive protocol to identify RNA 5hmC sites using machine learning. In this predictor, we introduced a sequence-based feature algorithm consisting of two feature representations, (1) <i>k</i>-mer spectrum and (2) positional nucleotide binary vector, to capture the sequential characteristics of 5hmC sites. Afterward, we utilized a two-stage feature space optimization strategy to improve the feature representation ability, and trained a predictive model using support vector machine (SVM). Our feature analysis results showed that feature optimization can help to capture the most discriminative features. As compared to well-known existing feature descriptors, our proposed representations can more accurately separate true 5hmC from non-5hmC sites. To the best of our knowledge, iRNA5hmC is the first RNA 5hmC predictor that enables to make predictions based on RNA primary sequences only, without any need of prior experimental knowledge. Importantly, we have established an easy-to-use webserver which is currently available at http://server.malab.cn/iRNA5hmC. We expect it has potential to be a useful tool for the prediction of 5hmC sites. 
  |  https://doi.org/10.3389/fbioe.2020.00227  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296686/  |  
------------------------------------------- 
10.1016/j.knosys.2020.105812  |   Sequential pattern mining (SPM) has been applied in many fields. However, traditional SPM neglects the pattern repetition in sequence. To solve this problem, gap constraint SPM was proposed and can avoid finding too many useless patterns. Nonoverlapping SPM, as a branch of gap constraint SPM, means that any two occurrences cannot use the same sequence letter in the same position as the occurrences. Nonoverlapping SPM can make a balance between efficiency and completeness. The frequent patterns discovered by existing methods normally contain redundant patterns. To reduce redundant patterns and improve the mining performance, this paper adopts the closed pattern mining strategy and proposes a complete algorithm, named Nettree for Nonoverlapping Closed Sequential Pattern (NetNCSP) based on the Nettree structure. NetNCSP is equipped with two key steps, support calculation and closeness determination. A backtracking strategy is employed to calculate the nonoverlapping support of a pattern on the corresponding Nettree, which reduces the time complexity. This paper also proposes three kinds of pruning strategies, inheriting, predicting, and determining. These pruning strategies are able to find the redundant patterns effectively since the strategies can predict the frequency and closeness of the patterns before the generation of the candidate patterns. Experimental results show that NetNCSP is not only more efficient but can also discover more closed patterns with good compressibility. Furtherly, in biological experiments NetNCSP mines the closed patterns in SARS-CoV-2 and SARS viruses. The results show that the two viruses are of similar pattern composition with different combinations. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32292248/  |  
------------------------------------------- 
10.3390/jcm9020392  |   <b>:</b> Dental panoramic radiographs (DPRs) provide information required to potentially evaluate bone density changes through a textural and morphological feature analysis on a mandible. This study aims to evaluate the discriminating performance of deep convolutional neural networks (CNNs), employed with various transfer learning strategies, on the classification of specific features of osteoporosis in DPRs. For objective labeling, we collected a dataset containing 680 images from different patients who underwent both skeletal bone mineral density and digital panoramic radiographic examinations at the Korea University Ansan Hospital between 2009 and 2018. Four study groups were used to evaluate the impact of various transfer learning strategies on deep CNN models as follows: a basic CNN model with three convolutional layers (CNN3), visual geometry group deep CNN model (VGG-16), transfer learning model from VGG-16 (VGG-16_TF), and fine-tuning with the transfer learning model (VGG-16_TF_FT). The best performing model achieved an overall area under the receiver operating characteristic of 0.858. In this study, transfer learning and fine-tuning improved the performance of a deep CNN for screening osteoporosis in DPR images. In addition, using the gradient-weighted class activation mapping technique, a visual interpretation of the best performing deep CNN model indicated that the model relied on image features in the lower left and right border of the mandibular. This result suggests that deep learning-based assessment of DPR images could be useful and reliable in the automated screening of osteoporosis patients. 
  |  http://www.mdpi.com/resolver?pii=jcm9020392  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32024114/  |  
------------------------------------------- 
10.1016/j.transci.2020.102785  |   Hematopoietic stem cell (HSC) cryopreservation is a critical step in autologous and cord blood transplantation (CBT). In most circumstances, cryopreservation is performed in a mixture containing dimethyl sulfoxide (DMSO), since DMSO is necessary to secure cell viability. Most centers use a controlled rate (slow) freezing before the long-term storage at vapor phase liquid nitrogen (LN<sub>2</sub>) temperatures (≤ -160 °C). The primary objectives for laboratories supporting HSCT programs are to provide secure storage for leukapheresis and cord blood products, and to adequately characterize the functional properties of the grafts before their infusion. In the autologous setting, the large majority of the published results dealt with the assessment of the graft before cryopreservation. On the contrary, in CBT, before a CB unit is released, a sample obtained from a contiguous segment of that CB unit needs to be tested to verify HLA type and cell viability. The effects of graft handling, cryopreservation, storage and thawing on the recovery of CD34+ cells needs to be carefully analyzed and standardized on a global level. Some technical unresolved issues still limit the application of the ISHAGE derived single platform flow cytometry protocol for the assessment of the thawed material; based on these considerations, an adaptation of both the acquisition setting and the gating strategyis necessary for reliable measurement of CD34-expressing HSC in cryopreserved grafts. Artificial intelligence applied to "big data" may provide a new tool for improving advanced processing procedures and quality management guidelines in this area of investigation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1473-0502(20)30074-4  |  
------------------------------------------- 
10.3390/s20072136  |   The automatic detection of atrial fibrillation (AF) is crucial for its association with the risk of embolic stroke. Most of the existing AF detection methods usually convert 1D time-series electrocardiogram (ECG) signal into 2D spectrogram to train a complex AF detection system, which results in heavy training computation and high implementation cost. This paper proposes an AF detection method based on an end-to-end 1D convolutional neural network (CNN) architecture to raise the detection accuracy and reduce network complexity. By investigating the impact of major components of a convolutional block on detection accuracy and using grid search to obtain optimal hyperparameters of the CNN, we develop a simple, yet effective 1D CNN. Since the dataset provided by PhysioNet Challenge 2017 contains ECG recordings with different lengths, we also propose a length normalization algorithm to generate equal-length records to meet the requirement of CNN. Experimental results and analysis indicate that our method of 1D CNN achieves an average <i>F</i><sub>1</sub> score of 78.2%, which has better detection accuracy with lower network complexity, as compared with the existing deep learning-based methods. 
  |  http://www.mdpi.com/resolver?pii=s20072136  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32290113/  |  
------------------------------------------- 
PMID:31797592  |   Identification and subsequent intervention of patients at risk of becoming High Cost Users (HCUs) presents the opportunity to improve outcomes while also providing significant savings for the healthcare system. In this paper, the 2016 HCU status of patients was predicted using free-form text data from the 2015 cumulative patient profiles within the electronic medical records of family care practices in Ontario. These unstructured notes make substantial use of domain-specific spellings and abbreviations; we show that word embeddings derived from the same context provide more informative features than pre-trained ones based on Wikipedia, MIMIC, and Pubmed. We further demonstrate that a model using features derived from aggregated word embeddings (EmbEncode) provides a significant performance improvement over the bag-of-words representation (82.48±0.35% versus 81.85±0.36% held-out AUROC, p = 3.2 × 10-4), using far fewer input features (5,492 versus 214,750) and fewer non-zero coefficients (1,177 versus 4,284). The future HCUs of greatest interest are the transitional ones who are not already HCUs, because they provide the greatest scope for interventions. Predicting these new HCU is challenging because most HCUs recur. We show that removing recurrent HCUs from the training set improves the ability of EmbEncode to predict new HCUs, while only slightly decreasing its ability to predict recurrent ones. 
  |  https://doi.org/10.1142/9789811215636_0012  |  
------------------------------------------- 
10.1002/anie.202004333  |   Electronic textiles that are thin, lightweight, flexible and breathable have been widely explored with a variety of functionalities including power supplying, displaying and sensing, which may revolutionize many fields such as communication, health care and artificial intelligence. To date, unfortunately, computing is the missing puzzle to close their functional loop. Memristor is compatible with the interwoven structure and fabricating process in textile due to its two-terminal and crossbar configuration. However, it remains challenging to realize textile memristors due to the difficulties in designing advanced memristive materials and achieving high-quality active layers on fiber electrodes. Here we report a robust textile memristor based on an electrophoretic-deposited active layer of d eoxyribonucleic acid (DNA) on fiber electrodes. The unique architecture and orientation of DNA molecules with the incorporation of Ag nanoparticles offer the best-in-class performances, e.g., both ultra- low operation voltage of ~0.3 V and power consumption of ~100 pW and high switching speed of 20 ns. Fundamental logic calculations such as implication and NAND are demonstrated as functions of textile chips, and it has been thus integrated with power-supplying and displaying modules to make a proof-of-concept fabric computer . This work provides a promising route towards wearable computers and artificial neural networks for effective brain-machine interfaces. 
  |  https://doi.org/10.1002/anie.202004333  |  
------------------------------------------- 
10.1038/s41598-020-62642-3  |   The development of brain-inspired neuromorphic computing, including artificial intelligence (AI) and machine learning, is of considerable importance because of the rapid growth in hardware and software capacities, which allows for the efficient handling of big data. Devices for neuromorphic computing must satisfy basic requirements such as multilevel states, high operating speeds, low energy consumption, and sufficient endurance, retention and linearity. In this study, inorganic perovskite-type amorphous strontium vanadate (a-SrVO<sub>x</sub>: a-SVO) synthesized at room temperature is utilized to produce a high-performance memristor that demonstrates nonvolatile multilevel resistive switching and synaptic characteristics. Analysis of the electrical characteristics indicates that the a-SVO memristor illustrates typical bipolar resistive switching behavior. Multilevel resistance states are also observed in the off-to-on and on-to-off transition processes. The retention resistance of the a-SVO memristor is shown to not significantly change for a period of 2 × 10<sup>4</sup> s. The conduction mechanism operating within the Ag/a-SVO/Pt memristor is ascribed to the formation of Ag-based filaments. Nonlinear neural network simulations are also conducted to evaluate the synaptic behavior. These results demonstrate that a-SVO-based memristors hold great promise for use in high-performance neuromorphic computing devices. 
  |  http://dx.doi.org/10.1038/s41598-020-62642-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32238846/  |  
------------------------------------------- 
10.1093/bioinformatics/btz947  |    Motivation:  Much effort has been made toward understanding the genetic architecture of complex traits and diseases. In the past decade, fruitful GWAS findings have highlighted the important role of regulatory variants and pervasive pleiotropy. Because of the accumulation of GWAS data on a wide range of phenotypes and high-quality functional annotations in different cell types, it is timely to develop a statistical framework to explore the genetic architecture of human complex traits by integrating rich data resources. 
  Results:  In this study, we propose a unified statistical approach, aiming to characterize relationship among complex traits, and prioritize risk variants by leveraging regulatory information collected in functional annotations. Specifically, we consider a latent probit model (LPM) to integrate summary-level GWAS data and functional annotations. The developed computational framework not only makes LPM scalable to hundreds of annotations and phenotypes but also ensures its statistically guaranteed accuracy. Through comprehensive simulation studies, we evaluated LPM's performance and compared it with related methods. Then, we applied it to analyze 44 GWASs with 9 genic category annotations and 127 cell-type specific functional annotations. The results demonstrate the benefits of LPM and gain insights of genetic architecture of complex traits. 
  Availability and implementation:  The LPM package, all simulation codes and real datasets in this study are available at https://github.com/mingjingsi/LPM. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz947  |  
------------------------------------------- 
10.1016/j.compbiomed.2020.103697  |   In this work we examine some of the problems associated with the development of machine learning models with the objective to achieve robust generalization capabilities on common-task multiple-database scenarios. Referred to as the "database variability problem", we focus on a specific medical domain (sleep staging in sleep medicine) to show the non-triviality of translating the estimated model's local generalization capabilities into independent external databases. We analyze some of the scalability problems when multiple-database data are used as inputs to train a single learning model. Then, we introduce a novel approach based on an ensemble of local models, and we show its advantages in terms of inter-database generalization performance and data scalability. In addition, we analyze different model configurations and data pre-processing techniques to determine their effects on the overall generalization performance. For this purpose, we carry out experimentation that involves several sleep databases and evaluates different machine learning models based on convolutional neural networks. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(20)30085-8  |  
------------------------------------------- 
10.1103/PhysRevLett.124.114301  |   An elastic cloak is a coating material that can be applied to an arbitrary inclusion to make it indistinguishable from the background medium. Cloaking against elastic disturbances, in particular, has been demonstrated using several designs and gauges. None, however, tolerate the coexistence of normal and shear stresses due to a shortage of physical realization of transformation-invariant elastic materials. Here, we overcome this limitation to design and fabricate a new class of polar materials with a distribution of body torque that exhibits asymmetric stresses. A static cloak for full two-dimensional elasticity is thus constructed based on the transformation method. The proposed cloak is made of a functionally graded multilayered lattice embedded in an isotropic continuum background. While one layer is tailored to produce a target elastic behavior, the other layers impose a set of kinematic constraints equivalent to a distribution of body torque that breaks the stress symmetry. Experimental testing under static compressive and shear loads demonstrates encouraging cloaking performance in good agreement with our theoretical prediction. The work sets a precedent in the field of transformation elasticity and should find applications in mechanical stress shielding and stealth technologies. 
  |  http://link.aps.org/abstract/PRL/v124/p114301  |  
------------------------------------------- 
10.1016/j.acra.2019.06.025  |   Blockchain, the underlying technology for Bitcoin, is a distributed digital ledger technology that enables record verification by many independent parties rather than a centralized authority, therefore making it more difficult to tamper with the data. This emerging technology has the potential to enhance various authentication and verification processes in image sharing and data security. It has the potential to promote patient-centered healthcare by giving greater control to patients over their own data. Blockchain can also be utilized for administrative tasks, such as credentialing, claims adjudication, and billing management. It can also be utilized to enhance software supporting research and clinical trials. Blockchain complements artificial intelligence (AI) and these can work synergistically to create better solutions. Although many challenges exist for increased adoption of blockchain within radiology and healthcare in general, it can play a major role in our practice and consequently, it is important for medical imaging professionals to become familiar with the technology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30445-3  |  
------------------------------------------- 
10.1038/s41433-019-0728-0  |   An amendment to this paper has been published and can be accessed via a link at the top of the paper. 
  |  http://dx.doi.org/10.1038/s41433-019-0728-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31822855/  |  
------------------------------------------- 
10.1177/0031512520914680  |    |  http://journals.sagepub.com/doi/full/10.1177/0031512520914680?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1097/COH.0000000000000608  |    Purpose of review:  Update findings regarding polypharmacy among people with HIV (PWH) and consider what research is most needed. 
  Recent findings:  Among PWH, polypharmacy is common, occurs in middle age, and is predominantly driven by nonantiretroviral (ARV) medications. Many studies have demonstrated strong associations between polypharmacy and receipt of potentially inappropriate medications (PIMS), but few have considered actual adverse events. Falls, delirium, pneumonia, hospitalization, and mortality are associated with polypharmacy among PWH and risks remain after adjustment for severity of illness. 
  Summary:  Polypharmacy is a growing problem and mechanisms of injury likely include potentially inappropriate medications, total drug burden, known pairwise drug interactions, higher level drug interactions, drug--gene interactions, and drug--substance use interactions (alcohol, extra-medical prescription medication, and drug use). Before we can effectively design interventions, we need to use observational data to gain a better understanding of the modifiable mechanisms of injury. As sicker individuals take more medications, analyses must account for severity of illness. As self-report of substance use may be inaccurate, direct biomarkers, such as phosphatidylethanol (PEth) for alcohol are needed. Large samples including electronic health records, genetics, accurate measures of substance use, and state of the art statistical and artificial intelligence techniques are needed to advance our understanding and inform clinical management of polypharmacy in PWH. 
  |  http://dx.doi.org/10.1097/COH.0000000000000608  |  
------------------------------------------- 
10.1177/0306312720905116  |   In this study, we explore the constitution of user representations of robots in design practice. Using the results of ethnographic research in two robot laboratories, we show how user representations emerge in and are entangled with design activities. Our study speaks to the growing popularity of and investment in robotics, robots and other forms of artificial intelligence. Scholars in Science and Technology Studies (STS) have shown that it is often difficult for designers and engineers to develop accurate ideas about potential users of such technologies. However, the social context of robots and design settings themselves have received significantly less attention. Based on our laboratory ethnographies, we argue that the practices in which engineers are engaged are important as they can shape the kind of user images designers create. To capture these dynamics, we propose two new concepts: 'image-evoking activities' as well as 'user image landscape'. Our findings provide pertinent input for researchers, designers and policy-makers, as they raise questions with regards to contemporary fears of robots replacing humans, for the effectiveness of user involvement and participatory design, and for user studies in STS. If design activities co-constitute the user images that engineers develop, a greater awareness is needed specifically of the locales in which the design of robots and other types of technologies takes place. 
  |  https://journals.sagepub.com/doi/10.1177/0306312720905116?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  
------------------------------------------- 
10.1016/j.copbio.2020.02.013  |   Metabolomics is a rapidly expanding technology that finds increasing application in a variety of fields, form metabolic disorders to cancer, from nutrition and wellness to design and optimization of cell factories. The integration of metabolic snapshots with metabolic fluxes, physiological readouts, metabolic models, and knowledge-informed Artificial Intelligence tools, is required to obtain a system-level understanding of metabolism. The emerging power of multi-omic approaches and the development of integrated experimental and computational tools, able to dissect metabolic features at cellular and subcellular resolution, provide unprecedented opportunities for understanding design principles of metabolic (dis)regulation and for the development of precision therapies in multifactorial diseases, such as cancer and neurodegenerative diseases. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0958-1669(20)30029-X  |  
------------------------------------------- 
10.3390/s20030663  |   We have developed a device, the Rehapiano, for the fast and quantitative assessment of action tremor. It uses strain gauges to measure force exerted by individual fingers. This article verifies the device's capability to measure and monitor the development of upper limb tremor. The Rehapiano uses a precision, 24-bit, analog-to-digital converter and an Arduino microcomputer to transfer raw data via a USB interface to a computer for processing, database storage, and evaluation. First, our experiments validated the device by measuring simulated tremors with known frequencies. Second, we created a measurement protocol, which we used to measure and compare healthy patients and patients with Parkinson's disease. Finally, we evaluated the repeatability of a quantitative assessment. We verified our hypothesis that the Rehapiano is able to detect force changes, and our experimental results confirmed that our system is capable of measuring action tremor. The Rehapiano is also sensitive enough to enable the quantification of Parkinsonian tremors. 
  |  http://www.mdpi.com/resolver?pii=s20030663  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31991705/  |  
------------------------------------------- 
10.18553/jmcp.2020.26.2.90  |   Twenty-five years ago, the <i>Journal of Managed Care Pharmacy</i> introduced its readers to disease state management, which attempted to break the siloed culture of the U.S. health care system. Disease state management has been transformed, in part, to population health management. This shift was marked by 3 main inflection points: the rise of the web-enabled smartphone, the Patient Protection and Affordable Care Act (ACA), and the adoption of artificial intelligence (AI). The introduction of smartphones filled the communication gap through improved patient engagement and accessible mobile applications, giving patients access to their clinical data. In addition, through the ACA, bundled payment models moved away from a volume-based to a value-based payment approach and attempted to incorporate population health concerns, such as the social determinants of health. The advancement of AI will allow the health care system to collect comprehensive health data and to predict the population at higher risk. Despite these advancements, some challenges from 25 years ago remain, yet rapid technology advancements may expedite the next wave of change. DISCLOSURES: No funding contributed to the writing of this article. The authors have nothing to disclose with respect to research, authorship, and/or publication of this article. 
  |  https://dx.doi.org/10.18553/jmcp.2020.26.2.90  |  
------------------------------------------- 
10.1016/j.tibs.2019.12.005  |   Molecular processes of neuronal learning have been well described. However, learning mechanisms of non-neuronal cells are not yet fully understood at the molecular level. Here, we discuss molecular mechanisms of cellular learning, including conformational memory of intrinsically disordered proteins (IDPs) and prions, signaling cascades, protein translocation, RNAs [miRNA and long noncoding RNA (lncRNA)], and chromatin memory. We hypothesize that these processes constitute the learning of signaling networks and correspond to a generalized Hebbian learning process of single, non-neuronal cells, and we discuss how cellular learning may open novel directions in drug design and inspire new artificial intelligence methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0968-0004(20)30003-7  |  
------------------------------------------- 
10.3390/s20030730  |   We investigate how pressure-sensitive smart textiles, in the form of a headband, can detect changes in facial expressions that are indicative of emotions and cognitive activities. Specifically, we present the Expressure system that performs surface pressure mechanomyography on the forehead using an array of textile pressure sensors that is not dependent on specific placement or attachment to the skin. Our approach is evaluated in systematic psychological experiments. First, through a mimicking expression experiment with 20 participants, we demonstrate the system's ability to detect well-defined facial expressions. We achieved accuracies of 0.824 to classify among three eyebrow movements (0.333 chance-level) and 0.381 among seven full-face expressions (0.143 chance-level). A second experiment was conducted with 20 participants to induce cognitive loads with N-back tasks. Statistical analysis has shown significant correlations between the Expressure features on a fine time granularity and the cognitive activity. The results have also shown significant correlations between the Expressure features and the N-back score. From the 10 most facially expressive participants, our approach can predict whether the N-back score is above or below the average with 0.767 accuracy. 
  |  http://www.mdpi.com/resolver?pii=s20030730  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32013009/  |  
------------------------------------------- 
10.3389/fpsyg.2020.00341  |   Quarter-life crisis (QLC) is a popular term for developmental crisis episodes that occur during early adulthood (18-30). Our aim was to explore what linguistic themes are associated with this phenomenon as discussed on social media. We analyzed 1.5 million tweets written by over 1,400 users from the United Kingdom and United States that referred to QLC, comparing their posts to those used by a control set of users who were matched by age, gender and period of activity. Logistic regression was used to uncover significant associations between words, topics, and sentiments of users and QLC, controlling for demographics. Users who refer to a QLC were found to post more about feeling mixed emotions, feeling stuck, wanting change, career, illness, school, and family. Their language tended to be focused on the future. Of 20 terms selected according to early adult crisis theory, 16 were mentioned by the QLC group more than the control group. The insights from this study could be used by clinicians and coaches to better understand the developmental challenges faced by young adults and how these are portrayed naturalistically in the language of social media. 
  |  https://doi.org/10.3389/fpsyg.2020.00341  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210878/  |  
------------------------------------------- 
10.1093/bioinformatics/btz908  |    Motivation:  Carbohydrate-active enzymes (CAZymes) are extremely important to bioenergy, human gut microbiome, and plant pathogen researches and industries. Here we developed a new amino acid k-mer-based CAZyme classification, motif identification and genome annotation tool using a bipartite network algorithm. Using this tool, we classified 390 CAZyme families into thousands of subfamilies each with distinguishing k-mer peptides. These k-mers represented the characteristic motifs (in the form of a collection of conserved short peptides) of each subfamily, and thus were further used to annotate new genomes for CAZymes. This idea was also generalized to extract characteristic k-mer peptides for all the Swiss-Prot enzymes classified by the EC (enzyme commission) numbers and applied to enzyme EC prediction. 
  Results:  This new tool was implemented as a Python package named eCAMI. Benchmark analysis of eCAMI against the state-of-the-art tools on CAZyme and enzyme EC datasets found that: (i) eCAMI has the best performance in terms of accuracy and memory use for CAZyme and enzyme EC classification and annotation; (ii) the k-mer-based tools (including PPR-Hotpep, CUPP and eCAMI) perform better than homology-based tools and deep-learning tools in enzyme EC prediction. Lastly, we confirmed that the k-mer-based tools have the unique ability to identify the characteristic k-mer peptides in the predicted enzymes. 
  Availability and implementation:  https://github.com/yinlabniu/eCAMI and https://github.com/zhanglabNKU/eCAMI. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz908  |  
------------------------------------------- 
10.1088/1361-6579/ab69b9  |    Objective:  Non-invasive fetal electrocardiography has the potential to provide vital information for evaluating the health status of the fetus. However, the low signal-to-noise ratio of the fetal electrocardiogram (ECG) impedes the applicability of the method in clinical practice. Quality improvement of the fetal ECG is of great importance for providing accurate information to enable support in medical decision-making. In this paper we propose the use of artificial intelligence for the task of one-channel fetal ECG enhancement as a post-processing step after maternal ECG suppression. 
  Approach:  We propose a deep fully convolutional encoder-decoder framework, learning end-to-end mappings from noise-contaminated fetal ECGs to clean ones. Symmetric skip-layer connections are used between corresponding convolutional and transposed convolutional layers to help recover the signal details. 
  Main results:  Experiments on synthetic data show an average improvement of 7.5 dB in the signal-to-noise ratio (SNR) for input SNRs in the range of -15 to 15 dB. Application of the method with real signals and subsequent ECG interval analysis demonstrates a root mean square error of 9.9 and 14 ms for the PR and QT intervals, respectively, when compared with simultaneous scalp measurements. The proposed network can achieve substantial noise removal on both synthetic and real data. In cases of highly noise-contaminated signals some morphological features might be unreliably reconstructed. 
  Significance:  The presented method has the advantage of preserving individual variations in pulse shape and beat-to-beat intervals. Moreover, no prior knowledge on the power spectra of the noise or the pulse locations is required. 
  |  https://doi.org/10.1088/1361-6579/ab69b9  |  
------------------------------------------- 
10.3390/s20041176  |   The application of artificial intelligence enhances the ability of sensor and networking technologies to realize smart systems that sense, monitor and automatically control our everyday environments. Intelligent systems and applications often automate decisions based on the outcome of certain machine learning models. They collaborate at an ever increasing scale, ranging from smart homes and smart factories to smart cities. The best performing machine learning model, its architecture and parameters for a given task are ideally automatically determined through a hyperparameter tuning process. At the same time, edge computing is an emerging distributed computing paradigm that aims to bring computation and data storage closer to the location where they are needed to save network bandwidth or reduce the latency of requests. The challenge we address in this work is that hyperparameter tuning does not take into consideration resource trade-offs when selecting the best model for deployment in smart environments. The most accurate model might be prohibitively expensive to computationally evaluate on a resource constrained node at the edge of the network. We propose a multi-objective optimization solution to find acceptable trade-offs between model accuracy and resource consumption to enable the deployment of machine learning models in resource constrained smart environments. We demonstrate the feasibility of our approach by means of an anomaly detection use case. Additionally, we evaluate the extent that transfer learning techniques can be applied to reduce the amount of training required by reusing previous models, parameters and trade-off points from similar settings. 
  |  http://www.mdpi.com/resolver?pii=s20041176  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093354/  |  
------------------------------------------- 
10.1016/j.neunet.2020.01.004  |   In this paper, a novel value iteration adaptive dynamic programming (ADP) algorithm is presented, which is called an improved value iteration ADP algorithm, to obtain the optimal policy for discrete stochastic processes. In the improved value iteration ADP algorithm, for the first time we propose a new criteria to verify whether the obtained policy is stable or not for stochastic processes. By analyzing the convergence properties of the proposed algorithm, it is shown that the iterative value functions can converge to the optimum. In addition, our algorithm allows the initial value function to be an arbitrary positive semi-definite function. Finally, two simulation examples are presented to validate the effectiveness of the developed method. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30006-X  |  
------------------------------------------- 
10.1097/EDE.0000000000001169  |    Background:  Food-borne disease outbreaks constitute a large health burden on society. One of the challenges when investigating such outbreaks is to trace the origin of the outbreak. In this study, we consider a network model to determine the spatial origin of the contaminated food product that caused the outbreak. 
  Methods:  The network model we use replaces the classic geographic distance of a network by an effective distance so that two nodes connected by a long-range link may be more strongly connected than their geographic distance would suggest. Furthermore, the effective distance transforms complex spatial patterns into regular topological patterns, creating a means for easier identification of the origin of the spreading phenomenon. Because detailed information on food distribution is generally not available, the model uses the gravity model from economics: the flow of goods from one node to another increases with population size and decreases with the geographical distance between them. 
  Results:  This effective distance network approach has been shown to perform well in a large Escherichia coli O104:H4 outbreak in Germany in 2011. In this article, we apply the same method to various food-borne disease outbreaks in the Netherlands. We found the effective distance network approach to fail in certain scenarios. 
  Conclusions:  Great care should be taken as to whether the underlying network model correctly captures the spreading mechanism of the outbreak in terms of spatial scale and single or multiple source outbreak. 
  |  http://dx.doi.org/10.1097/EDE.0000000000001169  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32079833/  |  
------------------------------------------- 
10.3390/s20041121  |   Road boundary detection is an important part of the perception of the autonomous driving. It is difficult to detect road boundaries of unstructured roads because there are no curbs. There are no clear boundaries on mine roads to distinguish areas within the road boundary line and areas outside the road boundary line. This paper proposes a real-time road boundary detection and tracking method by a 3D-LIDAR sensor.The road boundary points are extracted from the detected elevated point clouds above the ground point cloud according to the spatial distance characteristics and the angular features. Road tracking is to predict and update the boundary point information in real-time, in order to prevent false and missed detection. The experimental verification of mine road data shows the accuracy and robustness of the proposed algorithm. 
  |  http://www.mdpi.com/resolver?pii=s20041121  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32085668/  |  
------------------------------------------- 
10.1093/bib/bbaa001  |   Circular RNAs (circRNAs) are a unique class of RNA molecule identified more than 40 years ago which are produced by a covalent linkage via back-splicing of linear RNA. Recent advances in sequencing technologies and bioinformatics tools have led directly to an ever-expanding field of types and biological functions of circRNAs. In parallel with technological developments, practical applications of circRNAs have arisen including their utilization as biomarkers of human disease. Currently, circRNA-associated bioinformatics tools can support projects including circRNA annotation, circRNA identification and network analysis of competing endogenous RNA (ceRNA). In this review, we collected about 100 circRNA-associated bioinformatics tools and summarized their current attributes and capabilities. We also performed network analysis and text mining on circRNA tool publications in order to reveal trends in their ongoing development. 
  |  https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbaa001  |  
------------------------------------------- 
10.1039/c9cp06392g  |   Resistive switching (RS) memory behaviors are observed in an Ag|α-Fe<sub>2</sub>O<sub>3</sub>|Ti device after operating under an ultralow bias voltage of ±0.1 V. An SET voltage of ∼20 mV is obtained under illumination. Multilevel RS memory is realized under photoelectric signal control. The separation and fast transfer of hole-electron pairs are responsible for the enhanced RS memory under illumination. 
  |  https://doi.org/10.1039/c9cp06392g  |  
------------------------------------------- 
10.1111/den.13593  |   Flexible endoscopes became generally available 50 years ago and created a revolution in the practice of gastroenterology. They improved diagnosis enormously, enabled quicker, less invasive, and more cost-effective surgical treatment, while endoscopic screening has prevented many cancer deaths. The new technology stimulated research leading to a better understanding of gastrointestinal pathology, identifying new diseases and clarifying the etiology of others. Better-controlled clinical trials accelerated the use of newer and more effective drugs. National and international endoscopy societies supported nursing input, encouraged research, stimulated specialist journals, and devised guidelines that encouraged audit and quality assurance. Advances in instrument design and the manufacture of new accessories enhanced endoscopic technique, diagnostic ability, patient comfort, and safety. The risk of cross-infection inherent in the use of complex labile equipment that cannot be autoclaved remains a challenge. Endoscopy societies working closely with industry have established rigid protocols for high-level disinfection that minimize the risks, but strict adherence to guidelines and continued vigilance is essential, especially with the increasing prevalence of antibiotic-resistant commensals that can give rise to opportunistic infection. Government health departments have a responsibility to encourage and support research in this area by endoscopists, instrument manufacturers, and the pharmaceutical industry. Current trends suggest that in the future, artificial intelligence will greatly improve endoscopic diagnosis, and that therapeutic endoscopy will expand, encouraging endoscopists to subspecialize. 
  |  https://doi.org/10.1111/den.13593  |  
------------------------------------------- 
10.1089/cyber.2019.0409  |   Virtual reality (VR) is demonstrating increasing potential for therapeutic benefit in elderly care, but it is still generally considered to be the domain of the visually unimpaired. Even where VR and augmented reality (AR) are being explored for use with low vision, it is generally with a focus on creating bespoke software and hardware. However, the properties of commercial off-the-shelf (COTS) headsets, such as high luminance, may render them accessible even to very low vision users. Using a case-study approach, we explored the differences in visual perception from baseline to pass-through AR and commercial VR applications for an elderly female (Mrs. M) with advanced age-related macular degeneration. We found notable improvements in object, face, and color recognition, particularly with higher display brightness. Furthermore, Mrs. M was able to engage fully and enthusiastically with a number of (unmodified) VR applications, providing detailed descriptions of both static and moving elements. We suggest that the high luminance available in COTS VR may support more stable fixation closer to the fovea, improving visual resolution. Furthermore, the improvements we noted in color perception support previous suggestions that increasing luminance may improve photosensitivity by reducing the uptake of limited oxygen by the rod cells. We conclude that low vision should not automatically preclude users from engaging in VR research or entertainment, and that they may be able to use well-illuminated VR applications without any special modifications. 
  |  https://www.liebertpub.com/doi/full/10.1089/cyber.2019.0409?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.ajo.2020.02.022  |   Artificial intelligence (AI) describes systems capable of making decisions of high cognitive complexity; autonomous AI systems in healthcare are AI systems that make clinical decisions without human oversight. Such rigorously validated medical diagnostic AI systems hold great promise for improving access to care, increasing accuracy, and lowering cost, while enabling specialist physicians to provide the greatest value by managing and treating patients whose outcomes can be improved. Ensuring that autonomous AI provides these benefits requires evaluation of the autonomous AI's effect on patient outcome, design, validation, data usage, and accountability, from a bioethics and accountability perspective. We performed a literature review of bioethical principles for AI, and derived evaluation rules for autonomous AI, grounded in bioethical principles. The rules include patient outcome, validation, reference standard, design, data usage, and accountability for medical liability. Application of the rules explains successful US Food and Drug Administration (FDA) de novo authorization of an example, the first autonomous point-of-care diabetic retinopathy examination de novo authorized by the FDA, after a preregistered clinical trial. Physicians need to become competent in understanding the potential risks and benefits of autonomous AI, and understand its design, safety, efficacy and equity, validation, and liability, as well as how its data were obtained. The autonomous AI evaluation rules introduced here can help physicians understand limitations and risks as well as the potential benefits of autonomous AI for their patients. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9394(20)30093-3  |  
------------------------------------------- 
10.1021/acsnano.0c01304  |   Condensation freezing inhibition is of great practical importance for anti-icing applications; however, no coatings with this performance have been reported. Here, we report the inhibition of condensation freezing on patterned polyelectrolyte coatings, including polyelectrolyte brush (PB), polyelectrolyte multilayer (PEM), and polyelectrolyte hydrogel (PH) surfaces, benefiting from their feature in regulating ice nucleation and propagation <i>via</i> changing counterions. On the reported surfaces, ice nucleation can be initiated exclusively at the domains with the polyelectrolytes; moreover, spontaneous ice propagation can be achieved atop the patterned polyelectrolyte surface. Consequently, condensed water surrounding the frozen drops on the patterned polyelectrolyte surface evaporates due to the instantaneously released latent heat in the course of ice propagation. Afterward, ice grows specifically on polyelectrolyte surfaces <i>via</i> desublimation as the saturated vapor pressure of ice is smaller than that of condensed water drops. As such, an ice-free region up to 96% of the entire surface area can be accomplished. We demonstrate that various polyelectrolyte coatings can be easily introduced on almost all surfaces, revealing great promise for anti-icing applications. 
  |  https://dx.doi.org/10.1021/acsnano.0c01304  |  
------------------------------------------- 
10.3389/fneur.2020.00191  |   Motor Unit Number Index (MUNIX) is a technique that provides a susceptive biomarker for monitoring innervation conditions in patients with neurodegenerative diseases. A satisfactory repeatability is essential for the interpretation of MUNIX results. This study aims to examine the effect of channel number and location on the repeatability of MUNIX. In this study, 128 channels of high-density surface electromyography (EMG) signals were recorded from the biceps brachii muscles of eight healthy participants, at 10, 20, 30, 40, 50, 60, 70, 80, and 100% of maximal voluntary contraction. The repeatability was defined by the coefficient of variation (CV) of MUNIX estimated from three experiment trials. Single-channel MUNIX (sMUNIX) was calculated on a channel-specific basis and a multi-channel MUNIX (mMUNIX) approach as the weighted average of multiple sMUNIX results. Results have shown (1) significantly improved repeatability with the proposed mMUNIX approach; (2) a higher variability of sMUNIX when the recording channel is positioned away from the innervation zone. Our results have demonstrated that (1) increasing the number of EMG channels and (2) placing recording channels close to the innervation zone (IZ) are effective methods to improve the repeatability of MUNIX. This study investigated two potential causes of MUNIX variations and provided novel perspectives to improve the repeatability, using high-density surface EMG. The mMUNIX technique proposed can serve as a promising tool for reliable neurodegeneration evaluation. 
  |  https://doi.org/10.3389/fneur.2020.00191  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32256444/  |  
------------------------------------------- 
10.1093/bioinformatics/btaa256  |    Motivation:  In evidence-based medicine (EBM), defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short term memory (biLSTM) plus conditional random field (CRF) architecture, we add another layer of biLSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. 
  Results:  We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 3.9%, 15.6%, and 1.3% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection. 
  Availability and implementation:  Code is available at https://github.com/jind11/Deep-PICO-Detection. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btaa256  |  
------------------------------------------- 
10.1016/j.jbi.2020.103435  |   The task of electronic medical record named entity recognition (NER) refers to automatically identify all kinds of named entities in the medical record text. Chinese clinical NER remains a major challenge. One of the main reasons is that Chinese word segmentation will lead to the wrong downstream works. Besides, existing methods only use the information of the general field, not consider the knowledge from field of medicine. To address these issues, we propose a dynamic embedding method based on dynamic attention which combines features of both character and word in embedding layer. Domain knowledge is provided by word vector trained by domain dataset. In addition, spatial attention is added to enable the model to obtain more and more effective context encoding information. Finally, we conduct extensive experiments to demonstrate the effectiveness of our proposed algorithm. Experiments on CCKS2017 and Common dataset shows that the proposed method outperforms the baseline. 
  |  None  |  
------------------------------------------- 
10.1089/cyber.2019.0472  |   This study examined whether parents are less responsive to their young children (0-5) when they use a phone. We systematically observed 53 parent-child dyads in consultation bureau waiting rooms and playgrounds. Twenty-three parents used their phone at least once during the observation. Across the dyads, we observed parent and child behavior during a total of 1,038 ten-second intervals. Of these intervals, 641 contained a bid for attention from the child. Accounting for the nested nature of the data, we found that the odds of parents responding to their child's bid for attention were five times lower when using a phone than when not using one. Moreover, parents' responses were less timely, weaker, showed less affect, and were less likely to prioritize the child over other activities. While being fully absorbed in one's phone significantly decreased the odds of responding compared to when not using a phone, occasionally glancing at the phone did not, suggesting that parents may have developed a "mode" of phone use for managing dual attention over the phone and the child. In addition, while a higher intensity of phone use does seem to matter, it did not differ from intense engagement in other nonchild directed activities. The incidence of fully absorbed phone use, however, is greater. Finally, the results show that asking for consent for the observation beforehand leads to a decrease in the odds of phone use, suggesting a social desirability bias. Overall, the findings support concerns over the impact of parental phone use on child development. 
  |  https://www.liebertpub.com/doi/full/10.1089/cyber.2019.0472?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1186/s13073-020-00737-2  |   Clinical exome sequencing is frequently used to identify gene-disrupting variants in individuals with neurodevelopmental disorders. While splice-disrupting variants are known to contribute to these disorders, clinical interpretation of cryptic splice variants outside of the canonical splice site has been challenging. Here, we discuss papers that improve such detection. 
  |  https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-020-00737-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32331533/  |  
------------------------------------------- 
10.3390/brainsci10010041  |   We performed a bibliometric analysis of the peer-reviewed literature on vividness between 1900 and 2019 indexed by the Web of Science and compared it with the same analysis of publications on consciousness and mental imagery. While we observed a similarity between the citation growth rates for publications about each of these three subjects, our analysis shows that these concepts rarely overlap (co-occur) in the literature, revealing a surprising paucity of research about these concepts taken together. A disciplinary analysis shows that the field of Psychology dominates the topic of vividness, even though the total number of publications containing that term is small and the concept occurs in several other disciplines such as Computer Science and Artificial Intelligence. The present findings suggest that without a coherent unitary framework for the use of vividness in research, important opportunities for advancing the field might be missed. In contrast, we suggest that an evidence-based framework (such as the bibliometric analytic methods as exemplified here) will help to guide research from all disciplines that are concerned with vividness and help to resolve the challenge of epistemic incommensurability amongst published research in multidisciplinary fields. 
  |  http://www.mdpi.com/resolver?pii=brainsci10010041  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31936760/  |  
------------------------------------------- 
10.3748/wjg.v26.i4.433  |    Background:  Esophageo-gastro-duodenoscopy (EGD) is an important procedure used for detection and diagnosis of esophago-gastric lesions. There exists no consensus on the technique of examination. 
  Aim:  To identify recent advances in diagnostic EGDs to improve diagnostic yield. 
  Methods:  We queried the PubMed database for relevant articles published between January 2001 and August 2019 as well as hand searched references from recently published endoscopy guidelines. Keywords used included free text and MeSH terms addressing quality indicators and technological innovations in EGDs. Factors affecting diagnostic yield and EGD quality were identified and divided into the follow segments: Pre endoscopy preparation, sedation, examination schema, examination time, routine biopsy, image enhanced endoscopy and future developments. 
  Results:  We identified 120 relevant abstracts of which we utilized 67 of these studies in our review. Adequate pre-endoscopy preparation with simethicone and pronase increases gastric visibility. Proper sedation, especially with propofol, increases patient satisfaction after procedure and may improve detection of superficial gastrointestinal lesions. There is a movement towards mandatory picture documentation during EGD as well as dedicating sufficient time for examination improves diagnostic yield. The use of image enhanced endoscopy and magnifying endoscopy improves detection of squamous cell carcinoma and gastric neoplasm. The magnifying endoscopy simple diagnostic algorithm is useful for diagnosis of early gastric cancer. 
  Conclusion:  There is a steady momentum in the past decade towards improving diagnostic yield, quality and reporting in EGDs. Other interesting innovations, such as Raman spectroscopy, endocytoscopy and artificial intelligence may have widespread endoscopic applications in the near future. 
  |  http://www.wjgnet.com/1007-9327/full/v26/i4/433.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32063692/  |  
------------------------------------------- 
10.2215/CJN.15611219  |   The American Society of Nephrology has established a new initiative, <i>AKI!Now</i>, with the goal of promoting excellence in the prevention and treatment of AKI by building a foundational program that transforms education and delivery of AKI care, aiming to reduce morbidity and associated mortality, and to improve long-term outcomes. In this article, we describe our current efforts to improve early recognition and management involving inclusive interdisciplinary collaboration between providers, patients, and their families; discuss the ongoing need to change some of our current AKI paradigms and diagnostic methods; and provide specific recommendations to improve AKI recognition and care. In the hospital and the community, AKI is a common and increasingly frequent condition that generates risks of adverse events and high costs. Unfortunately, patients with AKI may frequently have received less than optimal quality of care. New classifications have facilitated understanding of AKI incidence and its impact on outcomes, but they are not always well aligned with AKI pathophysiology. Despite ongoing research efforts, treatments to promote or to hasten kidney recovery remain ineffective. To avoid progression, the current approach to AKI emphasizes the promotion of early recognition and timely response. However, a lack of awareness of the importance of early recognition and treatment among healthcare team members and heterogeneity of approaches within the healthcare teams assessing the patient remains a major challenge. Early identification is further complicated by differences in settings where AKI occurs (the community or the hospital), and by differences in patient populations and cultures between the intensive care unit and ward environments. To address these obstacles, we discuss the need to improve education at all levels of care and to generate specific guidance on AKI evaluation and management, including the development of a widely applicable education and an AKI management toolkit, engaging hospital administrators to incorporate AKI as a quality initiative, and raising awareness of AKI as a complication of other disease processes. 
  |  http://cjasn.asnjournals.org/cgi/pmidlookup?view=long&pmid=32317329  |  
------------------------------------------- 
10.2174/1570163817666200312102659  |    Background:  Chikungunya fever is a challenging threat to human health in various parts of the world nowadays. Many attempts have been made for developing an effective drug against this viral disease and no effective antiviral treatment has been developed to control the spread of the Chikungunya virus (CHIKV) in humans. 
  Objective:  This research is aimed at the discovery of potential inhibitors against this virus by employing computational techniques to study the interactions between non-structural proteins of Chikungunya virus and phytochemicals from plants. 
  Method:  Four non-structural proteins were docked with 2035 phytochemicals from various plants. The ligands having binding energies ≥ -8.0 kcal/mol were considered as potential inhibitors for these proteins. ADMET studies were also performed to analyze different pharmacological properties of these docked compounds and to further analyze the reactivity of these phytochemicals against CHIKV, DFT analysis was carried out based on HOMO and LUMO energies. 
  Results:  By analyzing the binding energies, Ki, ADMET properties and band energy gaps, it was observed that 13 phytochemicals passed all the criteria to be a potent inhibitor against CHIKV in humans. 
  Conclusion:  A total of 13 phytochemicals were identified as potent inhibiting candidates which can be used against Chikungunya virus. 
  |  http://www.eurekaselect.com/180128/article  |  
------------------------------------------- 
10.1016/j.tibtech.2019.12.021  |   Individualizing patient treatment is a core objective of the medical field. Reaching this objective has been elusive owing to the complex set of factors contributing to both disease and health; many factors, from genes to proteins, remain unknown in their role in human physiology. Accurately diagnosing, monitoring, and treating disorders requires advances in biomarker discovery, the subsequent development of accurate signatures that correspond with dynamic disease states, as well as therapeutic interventions that can be continuously optimized and modulated for dose and drug selection. This work highlights key breakthroughs in the development of enabling technologies that further the goal of personalized and precision medicine, and remaining challenges that, when addressed, may forge unprecedented capabilities in realizing truly individualized patient care. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0167-7799(19)30316-6  |  
------------------------------------------- 
10.1097/JCMA.0000000000000299  |    Background:  Prostate cancer (PCa) is the most common malignancy seen in men and the second leading cause of cancer-related death in males. The incidence and mortality associated with PCa has been rapidly increasing in China recently. 
  Methods:  Multiple diagnostic models of human PCa were developed based on Taylor database by combining the artificial neural networks (ANNs) to enhance the ability of PCa diagnosis. Genetic algorithm (GA) is used to select feature genes as numerical encoded parameters that reflect cancer, metastatic, or normal samples. Back propagation (BP) neural network and learning vector quantization (LVQ) neural network were used to build different Cancer/Normal, Primary/Metastatic, and Gleason Grade diagnostic models. 
  Results:  The performance of these modeling approaches was evaluated by predictive accuracy (ACC) and area under the receiver operating characteristic curve (AUC). By observing the statistically significant parameters of the three training sets, our Cancer/Normal, Primary/Metastatic, and Gleason Grade models' with ACC and AUC can be drawn (97.33%, 0.9832), (99.17%, 0.9952), and (90.48%, 0.8742), respectively. 
  Conclusion:  These results indicated that our diagnostic models of human PCa based on Taylor database combining the feature gene expression profiling data and artificial intelligence algorithms might act as a powerful tool for diagnosing PCa. Gleason Grade diagnostic models were used as novel prognostic diagnosis models for biochemical recurrence-free survival and overall survival, which might be helpful in the prognostic diagnosis of PCa in patients. 
  |  http://Insights.ovid.com/pubmed?pmid=32217993  |  
------------------------------------------- 
10.2174/1573405614666180319160045  |    Background:  In this paper, a method for adaptive Pure Interpolation (PI) in the frequency domain, with gradient auto-regularization, is proposed. 
  Methods:  The input image is transformed into the frequency domain and convolved with the Fourier Transform (FT) of a 2D sampling array (interpolation kernel) of initial size L × M. The Inverse Fourier Transform (IFT) is applied to the output coefficients and the edges are detected and counted. To get a denser kernel, the sampling array is interpolated in the frequency domain and convolved again with the transform coefficients of the original image of low resolution and transformed back into the spatial domain. The process is repeated until a maximum number of edges is reached in the output image, indicating that a locally optimal magnification factor has been attained. Finally, a maximum ascend-descend gradient auto-regularization method is designed and the edges are sharpened. 
  Results:  For the gradient management, a new strategy is proposed, referred to as the Natural bi- Directional Gradient Field (NBGF). It uses a natural following of a pair of directional and orthogonal gradient fields. 
  Conclusion:  The proposed procedure is comparable to novel algorithms reported in the state of the art with good results for high scales of amplification. 
  |  None  |  
------------------------------------------- 
10.1016/j.media.2020.101698  |   Registration is a core component of many imaging pipelines. In case of clinical scans, with lower resolution and sometimes substantial motion artifacts, registration can produce poor results. Visual assessment of registration quality in large clinical datasets is inefficient. In this work, we propose to automatically assess the quality of registration to an atlas in clinical FLAIR MRI scans of the brain. The method consists of automatically segmenting the ventricles of a given scan using a neural network, and comparing the segmentation to the atlas ventricles propagated to image space. We used the proposed method to improve clinical image registration to a general atlas by computing multiple registrations - one directly to the general atlas and others via different age-specific atlases - and then selecting the registration that yielded the highest ventricle overlap. Finally, as an example application of the complete pipeline, a voxelwise map of white matter hyperintensity burden was computed using only the scans with registration quality above a predefined threshold. Methods were evaluated in a single-site dataset of more than 1000 scans, as well as a multi-center dataset comprising 142 clinical scans from 12 sites. The automated ventricle segmentation reached a Dice coefficient with manual annotations of 0.89 in the single-site dataset, and 0.83 in the multi-center dataset. Registration via age-specific atlases could improve ventricle overlap compared to a direct registration to the general atlas (Dice similarity coefficient increase up to 0.15). Experiments also showed that selecting scans with the registration quality assessment method could improve the quality of average maps of white matter hyperintensity burden, instead of using all scans for the computation of the white matter hyperintensity map. In this work, we demonstrated the utility of an automated tool for assessing image registration quality in clinical scans. This image quality assessment step could ultimately assist in the translation of automated neuroimaging pipelines to the clinic. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1361-8415(20)30063-3  |  
------------------------------------------- 
10.1016/j.aap.2020.105468  |   Run-off-road (ROR) crashes have always been a major concern as this type of crash is usually associated with a considerable number of serious injury and fatal crashes. A substantial portion of ROR fatalities occur in collisions with fixed objects at the roadside. Thus, this study seeks to investigate the severity of ROR crashes where elderly drivers, aged 65 years or more, hit a fixed object. The reason why the present study investigates this issue among older drivers is that, comparing to younger drivers, this age group of drivers have different psychological and physical features. Because of these differences, they are more likely to get injured in ROR types of crashes. This paper applies two types of Artificial Intelligence (AI) techniques, including hybrid Intelligent Genetic Algorithm and Artificial Neural Network (ANN) using the crashe information of California in 2012 obtained from Highway Safety Information System (HSIS) database. Although the results showed that the developed ANN outperformed the hybrid Intelligent Genetic Algorithm, the hybrid approach was more capable of predicting high-severity crashes. This is rooted in the way the hybrid model was trained by taking advantage of the Genetic Algorithm (GA). The results also indicated that the light condition has been the most significant parameter in evaluating the level of severity associated with fixed object crashes among elderly drivers, which is followed by the existence of the right and left shoulders. Following these three contributing factors, cause of collision, Average Annual Daily Traffic (AADT), number of involved vehicles, age, road surface condition, and gender have been identified as the most important variables in the developed ANN, respectively. This helps to identify gaps and improve public safety towards improving the overall highway safety situation of older drivers. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0001-4575(19)31408-3  |  
------------------------------------------- 
10.1007/s00109-020-01874-2  |   In this review, we highlight the role of intratumoral heterogeneity, focusing on the clinical and biological ramifications this phenomenon poses. Intratumoral heterogeneity arises through complex genetic, epigenetic, and protein modifications that drive phenotypic selection in response to environmental pressures. Functionally, heterogeneity provides tumors with significant adaptability. This ranges from mutual beneficial cooperation between cells, which nurture features such as growth and metastasis, to the narrow escape and survival of clonal cell populations that have adapted to thrive under specific conditions such as hypoxia or chemotherapy. These dynamic intercellular interplays are guided by a Darwinian selection landscape between clonal tumor cell populations and the tumor microenvironment. Understanding the involved drivers and functional consequences of such tumor heterogeneity is challenging but also promises to provide novel insight needed to confront the problem of therapeutic resistance in tumors. 
  |  https://dx.doi.org/10.1007/s00109-020-01874-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31970428/  |  
------------------------------------------- 
10.3390/s20030763  |   In the near future, value streams associated with Industry 4.0 will be formed by interconnected cyber-physical elements forming complex networks that generate huge amounts of data in real time. The success or failure of industry leaders interested in the continuous improvement of lean management systems in this context is determined by their ability to recognize behavioral patterns in these big data structured within non-Euclidean domains, such as these dynamic sociotechnical complex networks. We assume that artificial intelligence in general and deep learning in particular may be able to help find useful patterns of behavior in 4.0 industrial environments in the lean management of cyber-physical systems. However, although these technologies have meant a paradigm shift in the resolution of complex problems in the past, the traditional methods of deep learning, focused on image or video analysis, both with regular structures, are not able to help in this specific field. This is why this work focuses on proposing geometric deep lean learning, a mathematical methodology that describes deep-lean-learning operations such as convolution and pooling on cyber-physical Industry 4.0 graphs. Geometric deep lean learning is expected to positively support sustainable organizational growth because customers and suppliers ought to be able to reach new levels of transparency and traceability on the quality and efficiency of processes that generate new business for both, hence generating new products, services, and cooperation opportunities in a cyber-physical environment. 
  |  http://www.mdpi.com/resolver?pii=s20030763  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32019148/  |  
------------------------------------------- 
10.3390/ijerph17062113  |    Purpose:  To review the role of corneal biomechanics for the clinical evaluation of patients with ectatic corneal diseases. 
  Methods:  A total of 1295 eyes were included for analysis in this study. The normal healthy group (group N) included one eye randomly selected from 736 patients with healthy corneas, the keratoconus group (group KC) included one eye randomly selected from 321 patients with keratoconus. The 113 nonoperated ectatic eyes from 125 patients with very asymmetric ectasia (group VAE-E), whose fellow eyes presented relatively normal topography (group VAE-NT), were also included. The parameters from corneal tomography and biomechanics were obtained using the Pentacam HR and Corvis ST (Oculus Optikgeräte GmbH, Wetzlar, Germany). The accuracies of the tested variables for distinguishing all cases (KC, VAE-E, and VAE-NT), for detecting clinical ectasia (KC + VAE-E) and for identifying abnormalities among the VAE-NT, were investigated. A comparison was performed considering the areas under the receiver operating characteristic curve (AUC; DeLong's method). 
  Results:  Considering all cases (KC, VAE-E, and VAE-NT), the AUC of the tomographic-biomechanical parameter (TBI) was 0.992, which was statistically higher than all individual parameters (DeLong's; <i>p</i> &lt; 0.05): PRFI- Pentacam Random Forest Index (0.982), BAD-D- Belin -Ambrosio D value (0.959), CBI -corneal biomechanical index (0.91), and IS Abs- Inferior-superior value (0.91). The AUC of the TBI for detecting clinical ectasia (KC + VAE-E) was 0.999, and this was again statistically higher than all parameters (DeLong's; <i>p</i> &lt; 0.05): PRFI (0.996), BAD-D (0.995), CBI (0.949), and IS Abs (0.977). Considering the VAE-NT group, the AUC of the TBI was 0.966, which was also statistically higher than all parameters (DeLong's; <i>p</i> &lt; 0.05): PRFI (0.934), BAD- D (0.834), CBI (0.774), and IS Abs (0.677). 
  Conclusions:  Corneal biomechanical data enhances the evaluation of patients with corneal ectasia and meaningfully adds to the multimodal diagnostic armamentarium. The integration of biomechanical data and corneal tomography with artificial intelligence data augments the sensitivity and specificity for screening and enhancing early diagnosis. Besides, corneal biomechanics may be relevant for determining the prognosis and staging the disease. 
  |  http://www.mdpi.com/resolver?pii=ijerph17062113  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32209975/  |  
------------------------------------------- 
10.2174/1573405616666200423085826  |    Background:  Breast cancer is considered as the most perilous sickness among females worldwide and the ratio of new cases is expanding yearly. Many researchers have proposed efficient algorithms to diagnose breast cancer at early stages, which have increased the efficiency and performance by utilizing the learned features of gold standard histopathological images. 
  Objective:  Most of these systems have either used traditional handcrafted features or deep features which had a lot of noise and redundancy, which ultimately decrease the performance of the system. 
  Methods:  A hybrid approach is proposed by fusing and optimizing the properties of handcrafted and deep features to classify the breast cancer images. HOG and LBP features are serially fused with pretrained models VGG19 and InceptionV3. PCR and ICR are used to evaluate the classification performance of proposed method. 
  Results:  The method concentrates on histopathological images to classify the breast cancer. The performance is compared with state-of-the-art techniques, where an overall patient-level accuracy of 97.2% and image-level accuracy of 96.7% is recorded. 
  Conclusion:  The proposed hybrid method achieves the best performance as compared to previous methods and it can be used for the intelligent healthcare systems and early breast cancer detection. 
  |  None  |  
------------------------------------------- 
10.21037/atm.2020.02.67  |    Background:  This study aims to quantitatively and qualitatively investigate the trends in scoliosis research and evaluate research hotspots using bibliometric analysis. 
  Methods:  All relevant publications on scoliosis from the period from 2009 to 2018 were extracted from the Web of Science and PubMed databases. Publication trends were analyzed using an Online analysis platform of literature metrology, Bibliographic Item Co-occurrence Matrix Builder (BICOMB), and CiteSpace software. Hotspots were analyzed and visualized using the gCLUTO software package. 
  Results:  A total of 7,445 scoliosis research publications dated between 2009 and 2018 were found. The spine was the most popular journal in this field during this period. The United States maintained a top position in global scoliosis research throughout the 10 years and has had a pivotal influence, followed by China and Canada. Among all institutions, the University of California, San Francisco, was a leader in research collaboration. At the same time, Professors Yong Qiu and Lawrence G. Lenke made great achievements in scoliosis research. We analyzed the major Medical Subject Headings (MeSH) terms/MeSH subheadings and identified eight hotspots in scoliosis research. 
  Conclusions:  We summarized the publication information of scoliosis-related literature in the 10 years from 2009 to 2018, including country and institution of origin, authors, and publication journal. We analyzed former research hotspots in the field of scoliosis and predicted future areas of interest. The development of various new orthopedic plants, artificial intelligence diagnosis, and genetic research will be future hotspots in scoliosis research. 
  |  https://doi.org/10.21037/atm.2020.02.67  |  
------------------------------------------- 
10.1098/rsos.191941  |   Leaf nitrogen concentration (LNC) is a major indicator in the estimation of the crop growth status which has been diffusely applied in remote sensing. Thus, it is important to accurately obtain LNC by using passive or active technology. Laser-induced fluorescence can be applied to monitor LNC in crops through analysing the changing of fluorescence spectral information. Thus, the performance of fluorescence spectrum (FS) and first-derivative fluorescence spectrum (FDFS) for paddy rice (Yangliangyou 6 and Manly Indica) LNC estimation was discussed, and then the proposed FS + FDFS was used to monitor LNC by multivariate analysis. The results showed that the difference between FS (<i>R</i> <sup>2</sup> = 0.781, s.d. = 0.078) and FDFS (<i>R</i> <sup>2</sup> = 0.779, s.d. = 0.097) for LNC estimation by using the artificial neural network is not obvious. The proposed FS + FDFS can improved the accuracy of LNC estimation to some extent (<i>R</i> <sup>2</sup> = 0.813, s.d. = 0.051). Then, principal component analysis was used in FS and FDFS, and extracted the main fluorescence characteristics. The results indicated that the proposed FS + FDFS exhibited higher robustness and stability for LNC estimation (<i>R</i> <sup>2</sup> = 0.851, s.d. = 0.032) than that only using FS (<i>R</i> <sup>2</sup> = 0.815, s.d. = 0.059) or FDFS (<i>R</i> <sup>2</sup> = 0.801, s.d. = 0.065). 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rsos.191941?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32257346/  |  
------------------------------------------- 
10.1016/j.ecoenv.2020.110410  |   Environment pollutants, especially those from total petroleum hydrocarbons (TPH), have a highly complex chemical, biological and physical impact on soils. Here we study this influence via modelling the TPH acute phytotoxicity effects on eleven samples of soils from Sakhalin island in greenhouse conditions. The soils were contaminated with crude oil in different doses ranging from the 3.0-100.0 g kg<sup>-1</sup>. Measuring the Hordeum vulgare root elongation, the crucial ecotoxicity parameter, we have estimated. We have also investigated the contrast effect in different soils. To predict TPH phytotoxicity different machine learning models were used, namely artificial neural network (ANN) and support vector machine (SVM). The models under discussion were proved to be valid using the mean absolute error method (MAE), the root mean square error method (RMSE), and the coefficient of determination (R<sup>2</sup>). We have shown that ANN and SVR can successfully predict barley response based on soil chemical properties (pH, LOI, N, P, K, clay, TPH). The best achieved accuracy was as following: MAE - 8.44, RMSE -11.05, and R<sup>2</sup> -0.80. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0147-6513(20)30249-9  |  
------------------------------------------- 
10.1155/2020/3684963  |    Purpose:  Our study aimed to evaluate the efficiency of intense pulsed light (IPL) combined with meibomian gland expression (MGX) in treating meibomian gland dysfunction (MGD). 
  Methods:  This study was a prospective interventional study. A total of 53 patients were included in the study and received a series of three treatments at an interval of 3-4 weeks. Follow-up examinations were completed 4 weeks after the last treatment. The Ocular Surface Disease Index (OSDI) questionnaire, tear meniscus height (TMH), tear break-up time (TBUT), slit-lamp examinations, and in vivo confocal microscopy (IVCM) were recorded before and after treatment. Additionally, an artificial intelligence automated software program was applied in our study for corneal nerve analysis. 
  Results:  The OSDI score was significantly reduced after the IPL treatment compared with baseline (<i>P</i> &lt; 0.001). Meibomian gland assessment scores, including meibum quality and expressibility, eyelid margin abnormalities, and corneal staining, significantly decreased after treatment (<i>P</i> &lt; 0.05). Moreover, the corneal nerve fiber length (CNFL) significantly increased after the treatment (<i>P</i> &lt; 0.001). 
  Conclusion:  Intense pulsed light (IPL) combined with MGX is an effective treatment for MGD, and neurotrophism could be one of the mechanisms of IPL. 
  |  https://doi.org/10.1155/2020/3684963  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32351719/  |  
------------------------------------------- 
10.1093/neuros/nyz286  |   Brain-computer interface (BCI) technology is rapidly developing and changing the paradigm of neurorestoration by linking cortical activity with control of an external effector to provide patients with tangible improvements in their ability to interact with the environment. The sensor component of a BCI circuit dictates the resolution of brain pattern recognition and therefore plays an integral role in the technology. Several sensor modalities are currently in use for BCI applications and are broadly either electrode-based or functional neuroimaging-based. Sensors vary in their inherent spatial and temporal resolutions, as well as in practical aspects such as invasiveness, portability, and maintenance. Hybrid BCI systems with multimodal sensory inputs represent a promising development in the field allowing for complimentary function. Artificial intelligence and deep learning algorithms have been applied to BCI systems to achieve faster and more accurate classifications of sensory input and improve user performance in various tasks. Neurofeedback is an important advancement in the field that has been implemented in several types of BCI systems by showing users a real-time display of their recorded brain activity during a task to facilitate their control over their own cortical activity. In this way, neurofeedback has improved BCI classification and enhanced user control over BCI output. Taken together, BCI systems have progressed significantly in recent years in terms of accuracy, speed, and communication. Understanding the sensory components of a BCI is essential for neurosurgeons and clinicians as they help advance this technology in the clinical setting. 
  |  https://academic.oup.com/neurosurgery/article-lookup/doi/10.1093/neuros/nyz286  |  
------------------------------------------- 
PMID:32308925  |   Patient recruitment for clinical trials is known to be a challenging aspect of clinical research. There are multiple competing concerns from the sponsor, patient and principal investigator's perspectives resulting in most clinical trials not meeting recruitment requirements on time. Conducting under-enrolled clinical trials affects the power of conclusive results or causes premature trial termination. The Blockchain is a distributed ledger technology originally applied in the financial sector. Its features as a peer-to-peer system with publicly audited transactions, data security, and patient privacy are a good fit for the needs of clinical trials recruitment. The "Smart Contract" is a programmable self-executing protocol that regulates the blockchain transactions. Given current recruitment challenges, we have proposed a blockchain model containing multiple trial-based contracts for trial management and patient engagement and a master smart contract for automated subject matching, patient recruitment, and trial-based contracts management. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32308925/  |  
------------------------------------------- 
10.23922/jarc.2019-045  |   Integrating artificial intelligence (AI) applications into colonoscopy practice is being accelerated as deep learning technologies emerge. In this field, most of the preceding research has focused on polyp detection and characterization, which can mitigate inherent human errors accompanying colonoscopy procedures. On the other hand, more challenging research areas are currently capturing attention: the automated prediction of invasive cancers. Colorectal cancers (CRCs) harbor potential lymph node metastasis when they invade deeply into submucosal layers, which should be resected surgically rather than endoscopically. However, pretreatment discrimination of deeply invasive submucosal CRCs is considered difficult, according to previous prospective studies (e.g., &lt;70% sensitivity), leading to an increased number of unnecessary surgeries for large adenomas or slightly invasive submucosal CRCs. AI is now expected to overcome this challenging hurdle because it is considered to provide better performance in predicting invasive cancer than non-expert endoscopists. In this review, we introduce five relevant publications in this area. Unfortunately, progress in this research area is in a very preliminary phase, compared to that of automated polyp detection and characterization, because of the lack of number of invasive CRCs used for machine learning. However, this issue will be overcome with more target images and cases. The research field of AI for invasive CRCs is just starting but could be a game changer of patient care in the near future, given rapidly growing technologies, and research will gradually increase. 
  |  https://doi.org/10.23922/jarc.2019-045  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32346642/  |  
------------------------------------------- 
PMID:31797614  |   The diagnosis of disease often requires analysis of a biopsy. Many diagnoses depend not only on the presence of certain features but on their location within the tissue. Recently, a number of deep learning diagnostic aids have been developed to classify digitized biopsy slides. Clinical workflows often involve processing of more than 500 slides per day. But, clinical use of deep learning diagnostic aids would require a preprocessing workflow that is cost-effective, flexible, scalable, rapid, interpretable, and transparent. Here, we present such a workflow, optimized using Dask and mixed precision training via APEX, capable of handling any patch-level or slide level classification and prediction problem. The workflow uses a flexible and fast preprocessing and deep learning analytics pipeline, incorporates model interpretation and has a highly storage-efficient audit trail. We demonstrate the utility of this package on the analysis of a prototypical anatomic pathology specimen, liver biopsies for evaluation of hepatitis from a prospective cohort. The preliminary data indicate that PathFlowAI may become a cost-effective and time-efficient tool for clinical use of Artificial Intelligence (AI) algorithms. 
  |  https://doi.org/10.1142/9789811215636_0036  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31797614/  |  
------------------------------------------- 
10.1158/1940-6207.CAPR-19-0556  |   Although endometrial cancer (EC) is often diagnosed at an early curable stage, the incidence and mortality from EC is rising and minority women are particularly at risk. We hypothesize that delays in clinical presentation contribute to racial disparities in EC mortality and treatment related morbidity. Improved methods for EC risk assessment and distinguishing abnormal uterine bleeding and postmenopausal bleeding from physiological variation are needed. Accordingly, we propose a multi-pronged strategy that combines innovative patient education with novel early detection strategies to reduce health impacts of EC and its precursors, especially among Black women. Futuristic approaches using gamification, smartphone apps, artificial intelligence, and health promotion outside of the physical clinic hold promise in preventing EC and reducing morbidity and mortality related to the disease, but they also raise a number of questions that will need to be addressed by future research. 
  |  http://cancerpreventionresearch.aacrjournals.org/cgi/pmidlookup?view=long&pmid=32047026  |  
------------------------------------------- 
10.3390/mi11020154  |   The oxygen vacancies in the TiO<sub>x</sub> active layer play the key role in determining the electrical characteristics of TiO<sub>x</sub>-based memristors such as resistive-switching behaviour. In this paper, we investigated the effect of a multi-layer stacking sequence of TiO<sub>x</sub> active layers on the resistive-switching characteristics of memristor devices. In particular, the stacking sequence of the multi-layer TiO<sub>x</sub> sub-layers, which have different oxygen contents, was varied. The optimal stacking sequence condition was confirmed by measuring the current-voltage characteristics, and also the retention test confirmed that the characteristics were maintained for more than 10,000 s. Finally, the simulation using the Modified National Institute of Standards and Technology handwriting recognition data set revealed that the multi-layer TiO<sub>x</sub> memristors showed a learning accuracy of 89.18%, demonstrating the practical utilization of the multi-layer TiO<sub>x</sub> memristors in artificial intelligence systems. 
  |  http://www.mdpi.com/resolver?pii=mi11020154  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32019257/  |  
------------------------------------------- 
10.1142/S0129065720500082  |   This paper proposes a new variant of spiking neural P systems (in short, SNP systems), nonlinear spiking neural P systems (in short, NSNP systems). In NSNP systems, the state of each neuron is denoted by a real number, and a real configuration vector is used to characterize the state of the whole system. A new type of spiking rules, nonlinear spiking rules, is introduced to handle the neuron's firing, where the consumed and generated amounts of spikes are often expressed by the nonlinear functions of the state of the neuron. NSNP systems are a class of distributed parallel and nondeterministic computing systems. The computational power of NSNP systems is discussed. Specifically, it is proved that NSNP systems as number-generating/accepting devices are Turing-universal. Moreover, we establish two small universal NSNP systems for function computing and number generator, containing 117 neurons and 164 neurons, respectively. 
  |  https://www.worldscientific.com/doi/full/10.1142/S0129065720500082?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/ndt/gfaa015  |   Digitization of healthcare will be a major innovation driver in the coming decade. Also, enabled by technological advancements and electronics miniaturization, wearable health device (WHD) applications are expected to grow exponentially. This, in turn, may make 4P medicine (predictive, precise, preventive and personalized) a more attainable goal within dialysis patient care. This article discusses different use cases where WHD could be of relevance for dialysis patient care, i.e. measurement of heart rate, arrhythmia detection, blood pressure, hyperkalaemia, fluid overload and physical activity. After adequate validation of the different WHD in this specific population, data obtained from WHD could form part of a body area network (BAN), which could serve different purposes such as feedback on actionable parameters like physical inactivity, fluid overload, danger signalling or event prediction. For a BAN to become clinical reality, not only must technical issues, cybersecurity and data privacy be addressed, but also adequate models based on artificial intelligence and mathematical analysis need to be developed for signal optimization, data representation, data reliability labelling and interpretation. Moreover, the potential of WHD and BAN can only be fulfilled if they are part of a transformative healthcare system with a shared responsibility between patients, healthcare providers and the payors, using a step-up approach that may include digital assistants and dedicated 'digital clinics'. The coming decade will be critical in observing how these developments will impact and transform dialysis patient care and will undoubtedly ask for an increased 'digital literacy' for all those implicated in their care. 
  |  https://academic.oup.com/ndt/article-lookup/doi/10.1093/ndt/gfaa015  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32162666/  |  
------------------------------------------- 
10.1097/SCS.0000000000005587  |   With the development of computer-assisted surgery, preoperational design is detailed in software. However, it is still a challenge for surgeons to realize the surgical plan in the craniofacial surgery. Robot-assisted surgery has advantages of high accuracy and stability. It is suitable for the high-stress procedures like drilling, milling, and cutting. This study aims to verify the feasibility for automatic drilling without soft tissues in model test based on an industrial robot platform.This study chose the data from digital laboratory in Shanghai 9th People's Hospital. The mandibular was reconstructed in software and surgical plan was also designed. Then, the coordinate data was input to the robot's software and matrix conversion was calculated by 4 marked points. The trajectory generation was calculated by inverse kinematics for target coordinates and robot coordinates. The model was fixed and calibrated for automatic drilling. At last, the accuracy was calculated by optic scanning instrument.The installment and preparation cost 10 minutes, the drilling procedure cost 12 minutes. The outside position error was (1.71 ± 0.16) mm, the inside position error was (1.37 ± 0.28) mm, the orientation error was (3.04 ± 1.02)°. Additionally, a total of 5 beagles were tested, with an accuracy error of (2.78 ± 1.52) mm. No postoperative complications occurred.This is the first study reported for robot-assisted automatic surgery in craniofacial surgery. The result shows it is possible to realize the automatic drilling procedure under the condition of no interference like soft tissues. With the development of artificial intelligence and machine vision, robot-assisted surgery may help surgeons to fulfill more automatic procedures for craniofacial surgery. 
  |  http://dx.doi.org/10.1097/SCS.0000000000005587  |  
------------------------------------------- 
10.1097/RTI.0000000000000490  |   During the latest years, artificial intelligence, and especially machine learning (ML), have experienced a growth in popularity due to their versatility and potential in solving complex problems. In fact, ML allows the efficient handling of big volumes of data, allowing to tackle issues that were unfeasible before, especially with deep learning, which utilizes multilayered neural networks. Cardiac computed tomography (CT) is also experiencing a rise in examination numbers, and ML might help handle the increasing derived information. Moreover, cardiac CT presents some fields wherein ML may be pivotal, such as coronary calcium scoring, CT angiography, and perfusion. In particular, the main applications of ML involve image preprocessing and postprocessing, and the development of risk assessment models based on imaging findings. Concerning image preprocessing, ML can help improve image quality by optimizing acquisition protocols or removing artifacts that may hinder image analysis and interpretation. ML in image postprocessing might help perform automatic segmentations and shorten examination processing times, also providing tools for tissue characterization, especially concerning plaques. The development of risk assessment models from ML using data from cardiac CT could aid in the stratification of patients who undergo cardiac CT in different risk classes and better tailor their treatment to individual conditions. While ML is a powerful tool with great potential, applications in the field of cardiac CT are still expanding, and not yet routinely available in clinical practice due to the need for extensive validation. Nevertheless, ML is expected to have a big impact on cardiac CT in the near future. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000490  |  
------------------------------------------- 
10.1007/s00146-020-00956-6  |   Sustainability is typically viewed as consisting of three forces, economic, social, and ecological, in tension with one another. In this paper, we address the dangers posed to societal sustainability. The concern being addressed is the very survival of societies where the rights of individuals, personal and collective freedoms, an independent judiciary and media, and democracy, despite its messiness, are highly valued. We argue that, as a result of various technological innovations, a range of dysfunctional impacts are threatening social and political stability. For instance, robotics and automation are replacing human labor and decision-making in a range of industries; search engines, monetized through advertising, have access to, and track, our interests and preferences; social media, in connecting us to one another often know more about us than we ourselves do, enabling them to profit in ways which may not coincide with our well-being; online retailers have not only acquired the ability to track and predict our buying choices, but also they can squeeze vendors based on their outsize bargaining power; and, in general, virtual technologies have changed both the way we think and our sense of self. With the rising deployment of the Internet of Things, and developments in machine learning and artificial intelligence, the threats to individual freedoms and rights, societal cohesion and harmony, employment and economic well-being, and trust in democracy are being ratcheted up. This paper lauds the benefits and addresses the harm wrought by the high tech giants in Information and Communication Technologies (ICTs). The search for rapidly growing revenues (and shareholder returns and stock prices) drives firms to accelerate product innovation without fully investigating the entire gamut of their impacts. As greater wealth accrues to the leaders of tech firms, inequalities within firms and societies are widening, creating social tensions and political ferment. We explore the ethical nature of the challenge employing a simple utilitarian calculus, complemented by approaches rooted in rights, justice, and the common good. Various options to address the challenges posed by ICTs are considered and evaluated. We argue that regulation may do little more than slow down the damage to society, particularly since societal values and political preferences vary internationally. Firms need to establish ethical standards, imbuing the upholders of these standards with sufficient authority, while creating a culture of morality. User involvement and activism, and shareholders' concerns for the sustainability of societies on whose continued prosperity they depend, are imperative to humanity's ability to decide the future direction of technology. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32218647/  |  
------------------------------------------- 
10.1016/j.tice.2019.101322  |   Despite profound knowledge of the incidence of oral cancers and a large body of research beyond it, it continues to beat diagnosis and treatment management. Post physical observation by clinicians, a biopsy is a gold standard for accurate detection of any abnormalities. Towards the application of artificial intelligence as an aid to diagnosis, automated cell nuclei segmentation is the most essential step for the recognition of the cancer cells. In this study, we have extracted the shape, texture and color features from the histopathological images collected indigenously from regional hospitals. A dataset of 42 whole slide slices was used to automatically segment and generate a cell level dataset of 720 nuclei. Next, different classifiers were applied for classification purposes. 99.4 % accuracy using Decision Tree Classifier, 100 % accuracy using both SVM and Logistic regression and 100 % accuracy using SVM, Logistic regression and Linear Discriminant were acquired for shape, textural and color features respectively. The in-depth analysis showed SVM and Linear Discriminant classifier gave the best result for texture and color features respectively. The achieved result can be effectively converted to software as an assistant diagnostic tool. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0040-8166(19)30386-6  |  
------------------------------------------- 
10.1136/medethics-2019-105860  |   Making good decisions in extremely complex and difficult processes and situations has always been both a key task as well as a challenge in the clinic and has led to a large amount of clinical, legal and ethical routines, protocols and reflections in order to guarantee fair, participatory and up-to-date pathways for clinical decision-making. Nevertheless, the complexity of processes and physical phenomena, time as well as economic constraints and not least further endeavours as well as achievements in medicine and healthcare continuously raise the need to evaluate and to improve clinical decision-making. This article scrutinises if and how clinical decision-making processes are challenged by the rise of so-called artificial intelligence-driven decision support systems (AI-DSS). In a first step, this article analyses how the rise of AI-DSS will affect and transform the modes of interaction between different agents in the clinic. In a second step, we point out how these changing modes of interaction also imply shifts in the conditions of trustworthiness, epistemic challenges regarding transparency, the underlying normative concepts of agency and its embedding into concrete contexts of deployment and, finally, the consequences for (possible) ascriptions of responsibility. Third, we draw first conclusions for further steps regarding a 'meaningful human control' of clinical AI-DSS. 
  |  http://jme.bmj.com/cgi/pmidlookup?view=long&pmid=32245804  |  
------------------------------------------- 
10.1371/journal.pone.0230904  |   With the widespread of multicore systems, automatic parallelization becomes more pronounced, particularly for legacy programs, where the source code is not generally available. An essential operation in any parallelization system is detecting data dependence among parallelization candidate instructions. Conducting dependence analysis at the binary-level is more challenging than that at the source-level due to the much lower semantics of the binary code. In this paper, we consider using the elaborate 'static' analysis of abstract interpretation, for the first time, at runtime for data dependence detection. Specifically, our system interprets instructions at a hot region, while at the same time, collect programs semantics for seen program points, thereby conducting abstract interpretation analysis dynamically. The analysis is guaranteed to be correct as long as execution does not exit the region prematurely. Moreover, successive hot region re-entries will resume previous analysis, albeit much faster in case no major change in the program semantics. Such approach provides for more rigorous analysis than other simple dynamic analysis which would typically miss parallelization opportunities. The proposed approach also does not require any hardware support, availability of the source code, as well as any code re-compilation. To study the performance and accuracy of our approach, we have extended the Padrone dynamic code modification framework, and conduct an initial study on a set of PolyBench kernels and selected programs from SPEC CPU. Experimental results show accurate dependence detection with low overhead. 
  |  http://dx.plos.org/10.1371/journal.pone.0230904  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32271797/  |  
------------------------------------------- 
10.1093/arclin/acz047  |    Objective:  The present study investigated the effect of the apolipoprotein E (ApoE) ε4 allele on the four memory components (i.e., who, when, where, and what) among cognitively intact older adults. 
  Methods:  Participants comprised 47 cognitively intact older adults, who were classified into 2 groups based on the presence or absence of at least 1 ApoE ε4 allele. All participants completed standardized neuropsychological tests, including the Logical Memory subtest of the Wechsler Memory Scale-III with a revised scoring method. 
  Results:  The results revealed that recollection for each component followed a pattern of who &gt; what &gt; when = where. Furthermore, a significant group-by-component-by-condition interaction indicated that the presence of the ApoE ε4 allele resulted in a disproportionately detrimental effect on the where component retention in the verbal episodic memory task; this finding was significantly correlated with hippocampal volumes. 
  Conclusion:  These results highlighted the importance of evaluating the subcomponents of verbal episodic memory to detect subtle cognitive differences related to ApoE ε4 status, which could help elucidate the mechanism behind the cascades caused by ApoE ε4 in the trajectories of cognitive aging. 
  |  https://academic.oup.com/acn/article-lookup/doi/10.1093/arclin/acz047  |  
------------------------------------------- 
10.1089/lap.2019.0419  |   <b><i>Introduction:</i></b> Obesity rates continue to rise in America and around the World. Numerous studies show the benefit of bariatric surgery on all-cause mortality in obese patients. Given its substantial role in the future of patient care, we continue to search for the most beneficial ways to optimize patient outcomes and procedural costs in bariatric surgery. Much like laparoscopy was found to greatly improve the morbidity of weight loss surgery, we seek to evaluate the role of robotic surgery in bariatric procedures. <b><i>Methods:</i></b> We critically reviewed the available literature accessed through PubMed on the use of robotics in bariatric surgery. We aim to provide an overview of the conclusions from the most recent publications with commentary by the authors. <b><i>Results:</i></b> Although the outliers exist, it would appear that the majority of cases point to robotic surgery increasing operating room time and cost without providing significant generalizable improvements in patient outcomes. Promise exists in the use for special groups such as super obese patients or revisional bariatric procedures, however current studies in this subset are equally variable in their outcomes. <b><i>Conclusion:</i></b> Despite the current assumptions, we believe there is a future in bariatric surgery for robotics. This may inevitably be seen in the more demanding and difficult cases or in the advancement of the available technology. Likely, as robotics continues to mature, applied artificial intelligence will provide enhanced cues during surgery that augment the surgeon's judgment and skill and result in unanimously improved patient outcomes. 
  |  https://www.liebertpub.com/doi/full/10.1089/lap.2019.0419?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1371/journal.pone.0230706  |   Intensive care data are valuable for improvement of health care, policy making and many other purposes. Vast amount of such data are stored in different locations, on many different devices and in different data silos. Sharing data among different sources is a big challenge due to regulatory, operational and security reasons. One potential solution is federated machine learning, which is a method that sends machine learning algorithms simultaneously to all data sources, trains models in each source and aggregates the learned models. This strategy allows utilization of valuable data without moving them. One challenge in applying federated machine learning is the possibly different distributions of data from diverse sources. To tackle this problem, we proposed an adaptive boosting method named LoAdaBoost that increases the efficiency of federated machine learning. Using intensive care unit data from hospitals, we investigated the performance of learning in IID and non-IID data distribution scenarios, and showed that the proposed LoAdaBoost method achieved higher predictive accuracy with lower computational complexity than the baseline method. 
  |  http://dx.plos.org/10.1371/journal.pone.0230706  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32302316/  |  
------------------------------------------- 
10.1016/j.pt.2019.12.012  |   Spatial lifecourse epidemiology aims to utilize advanced spatial, location-aware, and artificial intelligence technologies to investigate long-term effects of measurable biological, environmental, behavioral, and psychosocial factors on individual risk for chronic diseases. It could also further the research on infectious disease dynamics, risks, and consequences across the life course. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1471-4922(20)30005-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32044243/  |  
------------------------------------------- 
10.13703/j.0255-2930.20190723-0001  |   Based on the analysis of the present situation of standardization of moxibustion, it is found that the published standards of acupuncture and moxibustion are predominated at acupoint standard and acupuncture manipulation standard. Moxibusiton standardization mainly focuses on the manipulation. It is relatively lack of the standards of moxibustion materials and device. Four suggestions are put forward on the development strategies of moxibustion standardization: 1. Rectify the current situation that more attention paid to acupuncture rather than moxibustion, strengthen the inheritance of traditional experiences and the excavation of ancient literature, expand the indications of moxibustion and confirm the clinical effect of it. 2. Promote the whole process of moxibustion standardization, starting from moxibustion technique to its material, device and manipulation. 3. Enhance the equipment construction of moxibustion, combine with other build engineering disciplines, e.g. artificial intelligence and communication technology, and construct a multi-disciplinary intersection system. 4. Improve the promotion and development mode of moxibustion, propel all-round development of moxibustion in the clinical application, promotion mode and standardization construction, etc. 
  |  None  |  
------------------------------------------- 
10.3389/fnins.2020.00087  |   Robust cross-subject emotion recognition based on multichannel EEG has always been hard work. In this work, we hypothesize that there exist default brain variables across subjects in emotional processes. Hence, the states of the latent variables that relate to emotional processing must contribute to building robust recognition models. Specifically, we propose to utilize an unsupervised deep generative model (e.g., variational autoencoder) to determine the latent factors from the multichannel EEG. Through a sequence modeling method, we examine the emotion recognition performance based on the learnt latent factors. The performance of the proposed methodology is verified on two public datasets (DEAP and SEED) and compared with traditional matrix factorization-based (ICA) and autoencoder-based approaches. Experimental results demonstrate that autoencoder-like neural networks are suitable for unsupervised EEG modeling, and our proposed emotion recognition framework achieves an inspiring performance. As far as we know, it is the first work that introduces variational autoencoder into multichannel EEG decoding for emotion recognition. We think the approach proposed in this work is not only feasible in emotion recognition but also promising in diagnosing depression, Alzheimer's disease, mild cognitive impairment, etc., whose specific latent processes may be altered or aberrant compared with the normal healthy control. 
  |  https://doi.org/10.3389/fnins.2020.00087  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32194367/  |  
------------------------------------------- 
10.1007/s00105-019-04531-z  |    Background:  Since the establishment of dermoscopy as a routine examination procedure in dermatology, the spectrum of noninvasive, optical devices has further expanded. In difficult-to-diagnose clinical cases, these systems may support dermatologists to arrive at a correct diagnosis without the need for a surgical biopsy. 
  Objective:  To give an overview about technical background, indications and diagnostic performance regarding four new optical procedures: reflectance confocal microscopy, in vivo multiphoton tomography, dermatofluoroscopy, and systems based on image analysis by artificial intelligence (AI). 
  Materials and methods:  This article is based on a selective review of the literature, as well as the authors' personal experience from clinical studies relevant for market approval of the devices. 
  Results:  In contrast to standard histopathological slides with vertical cross sections, reflectance confocal microscopy and in vivo multiphoton tomography allow for "optical biopsies" with horizontal cross sections. Dermatofluoroscopy and AI-based image analyzers provide a numerical score, which helps to correctly classify a skin lesion. The presented new optical procedures may be applied for the diagnosis of skin cancer as well as inflammatory skin diseases. 
  Conclusion:  The presented optical procedures provide valuable additional information that supports dermatologists in making the correct diagnosis. However, a surgical biopsy followed by dermatohistopathological examination remains the diagnostic gold standard in dermatology. 
  |  https://dx.doi.org/10.1007/s00105-019-04531-z  |  
------------------------------------------- 
10.1002/jcc.26128  |   Recent advances in artificial intelligence along with the development of large data sets of energies calculated using quantum mechanical (QM)/density functional theory (DFT) methods have enabled prediction of accurate molecular energies at reasonably low computational cost. However, machine learning models that have been reported so far require the atomic positions obtained from geometry optimizations using high-level QM/DFT methods as input in order to predict the energies and do not allow for geometry optimization. In this study, a transferable and molecule size-independent machine learning model bonds (B), angles (A), nonbonded (N) interactions, and dihedrals (D) neural network (BAND NN) based on a chemically intuitive representation inspired by molecular mechanics force fields is presented. The model predicts the atomization energies of equilibrium and nonequilibrium structures as sum of energy contributions from bonds (B), angles (A), nonbonds (N), and dihedrals (D) at remarkable accuracy. The robustness of the proposed model is further validated by calculations that span over the conformational, configurational, and reaction space. The transferability of this model on systems larger than the ones in the data set is demonstrated by performing calculations on selected large molecules. Importantly, employing the BAND NN model, it is possible to perform geometry optimizations starting from nonequilibrium structures along with predicting their energies. © 2019 Wiley Periodicals, Inc. 
  |  https://doi.org/10.1002/jcc.26128  |  
------------------------------------------- 
10.3988/jcn.2020.16.2.183  |   Stroke is a major health-care problem that represents a leading cause of death and also the top cause of disability in adulthood. In recent years there has been a significant paradigm shift in treatments for acute ischemic stroke to favor earlier reperfusion therapy, mainly using the systemic infusion of recombinant tissue plasminogen activator. Subsequent trials found that combining this treatment with endovascular therapy was effective in selected patients. The increased complexity of acute stroke treatments has resulted in a substantial reorganization of stroke care. This review reports on the evolution of acute ischemic stroke treatment and describes the main organizational models based on the hub-and-spoke system. The lack of evidence for comparisons of the effectiveness of different paradigms means that some decision-analysis models predicting the best organizational pathways are also reported, with a particular emphasis on the workflow timing in the prehospital and in-hospital settings. Major benchmarks and performance measures are also reported, focusing on the timing of interventions and rates of process indicators. Finally, future directions are illustrated, including using telemedicine for stroke, mobile stroke units, and artificial intelligence and automated machines to produce software for detecting large-vessel occlusion. 
  |  https://thejcn.com/DOIx.php?id=10.3988/jcn.2020.16.2.183  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32319234/  |  
------------------------------------------- 
10.1093/bioinformatics/btz813  |    Motivation:  The increasing availability of annotated genome sequences enables construction of genome-scale metabolic networks, which are useful tools for studying organisms of interest. However, due to incomplete genome annotations, draft metabolic models contain gaps that must be filled in a time-consuming process before they are usable. Optimization-based algorithms that fill these gaps have been developed, however, gap-filling algorithms show significant error rates and often introduce incorrect reactions. 
  Results:  Here, we present a new gap-filling method that computes the costs of candidate gap-filling reactions from a universal reaction database (MetaCyc) based on taxonomic information. When gap-filling a metabolic model for an organism M (such as Escherichia coli), the cost for reaction R is based on the frequency with which R occurs in other organisms within the phylum of M (in this case, Proteobacteria). The assumption behind this method is that different taxonomic groups are biased toward using different metabolic reactions. Evaluation of the new gap-filler on randomly degraded variants of the EcoCyc metabolic model for E.coli showed an increase in the average F1-score to 99.0 (when using the variable weights by frequency method at the phylum level), compared to 91.0 using the previous MetaFlux gap-filler and 80.3 using a basic gap-filler. Evaluation on two other microbial metabolic models showed similar improvements. 
  Availability and implementation:  The Pathway Tools software (including MetaFlux) is free for academic use and is available at http://pathwaytools.com. Additional code for reproducing the results presented here is available at www.ai.sri.com/pkarp/pubs/taxgap/supplementary.zip. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz813  |  
------------------------------------------- 
10.1021/acssynbio.9b00447  |   Metabolic engineering aims to produce chemicals of interest from living organisms, to advance toward greener chemistry. Despite efforts, the research and development process is still long and costly, and efficient computational design tools are required to explore the chemical biosynthetic space. Here, we propose to explore the bioretrosynthesis space using an artificial intelligence based approach relying on the Monte Carlo Tree Search reinforcement learning method, guided by chemical similarity. We implement this method in RetroPath RL, an open-source and modular command line tool. We validate it on a golden data set of 20 manually curated experimental pathways as well as on a larger data set of 152 successful metabolic engineering projects. Moreover, we provide a novel feature that suggests potential media supplements to complement the enzymatic synthesis plan. 
  |  https://dx.doi.org/10.1021/acssynbio.9b00447  |  
------------------------------------------- 
10.1016/j.neunet.2019.11.018  |   Recurrent neural networks (RNNs) have recently achieved remarkable successes in a number of applications. However, the huge sizes and computational burden of these models make it difficult for their deployment on edge devices. A practically effective approach is to reduce the overall storage and computation costs of RNNs by network pruning techniques. Despite their successful applications, those pruning methods based on Lasso either produce irregular sparse patterns in weight matrices, which is not helpful in practical speedup. To address these issues, we propose a structured pruning method through neuron selection which can remove the independent neuron of RNNs. More specifically, we introduce two sets of binary random variables, which can be interpreted as gates or switches to the input neurons and the hidden neurons, respectively. We demonstrate that the corresponding optimization problem can be addressed by minimizing the L<sub>0</sub> norm of the weight matrix. Finally, experimental results on language modeling and machine reading comprehension tasks have indicated the advantages of the proposed method in comparison with state-of-the-art pruning competitors. In particular, nearly 20× practical speedup during inference was achieved without losing performance for the language model on the Penn TreeBank dataset, indicating the promising performance of the proposed method. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30377-6  |  
------------------------------------------- 
10.3389/fgene.2019.01316  |   miRNA plays an important role in many biological processes, and increasing evidence shows that miRNAs are closely related to human diseases. Most existing miRNA-disease association prediction methods were only based on data related to miRNAs and diseases and failed to effectively use other existing biological data. However, experimentally verified miRNA-disease associations are limited, there are complex correlations between biological data. Therefore, we propose a novel Three-layer heterogeneous network Combined with unbalanced Random Walk for MiRNA-Disease Association prediction algorithm (TCRWMDA), which can effectively integrate multi-source association data. TCRWMDA based not only on the known miRNA-disease associations, also add the new priori information (lncRNA-miRNA and lncRNA-disease associations) to build a three-layer heterogeneous network, lncRNA was added as the transition path of the intermediate point to mine more effective information between networks. The AUC value obtained by the TCRWMDA algorithm on 5-fold cross validation is 0.9209, compared with other models based on the same similarity calculation method, TCRWMDA obtained better results. TCRWMDA was applied to the analysis of four types of cancer, the results proved that TCRWMDA is an effective tool to predict the potential miRNA-disease association. The source code and dataset of TCRWMDA are available at: https://github.com/ylm0505/TCRWMDA. 
  |  https://doi.org/10.3389/fgene.2019.01316  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31998371/  |  
------------------------------------------- 
10.1002/cpt.1744  |   Despite the application of advanced statistical and pharmacometric approaches to pediatric trial data, a large pediatric evidence gap still remains. Here, we discuss how to collect more data from children by using real-world data from electronic health records, mobile applications, wearables, and social media. The large datasets collected with these approaches enable and may demand the use of artificial intelligence and machine learning to allow the data to be analyzed for decision making. Applications of this approach are presented, which include the prediction of future clinical complications, medical image analysis, identification of new pediatric end points and biomarkers, the prediction of treatment nonresponders, and the prediction of placebo-responders for trial enrichment. Finally, we discuss how to bring machine learning from science to pediatric clinical practice. We conclude that advantage should be taken of the current opportunities offered by innovations in data science and machine learning to close the pediatric evidence gap. 
  |  https://doi.org/10.1002/cpt.1744  |  
------------------------------------------- 
10.1016/j.dss.2019.113158  |   Content sharing platforms such as product review websites largely depend on reviewers' voluntary contributions. In order to motivate reviewers to contribute more, many platforms established incentive mechanisms, either reputation-based or financial. Yet most of the existing research has focused on reputations that are everlasting, such as badges and virtual points, or financial rewards where no evaluation exists about the users' contributed content, such as rebates. There is still a significant gap in our understanding of how incentives with reevaluation mechanism actually influence reviewers' behaviors such as their contribution levels, the opinion they express, and how they express. In this paper, we fill this gap using data collected from Yelp Elite Squad where reviewers with good reviewing history are awarded into the elite group and most importantly reevaluated each year. We draw from the accountability theory and conduct a difference-in-differences analysis to empirically study the effect of incentives with reevaluation mechanism on reviewers' behaviors in both short term and long term. The results show that in short term, reviewers significantly increase their contribution levels, become more conservative with lower percentage of extreme ratings, and also increase the readability of their reviews. In long term, they continue improving the quality of reviews though their numerical rating behaviors stabilize. Our research has significant implications for business models that rely on user contributions. 
  |  None  |  
------------------------------------------- 
10.1016/j.dib.2020.105114  |   The repository is composed of 1224 images divided into two sets of images with two different resolutions. First set consists of 89 histopathological images with the normal epithelium of the oral cavity and 439 images of Oral Squamous Cell Carcinoma (OSCC) in 100x magnification. The second set consists of 201 images with the normal epithelium of the oral cavity and 495 histopathological images of OSCC in 400x magnification. The images were captured using a Leica ICC50 HD microscope from Hematoxyline and Eosin (H&amp;E) stained tissue slides collected, prepared and catalogued by medical experts from 230 patients. A subset of 269 images from the second data set was used to detect OSCC based on textural features [1]. Histopathology plays a very important role in diagnosing a disease. It is the investigation of biological tissues to detect the presence of diseased cells in microscopic detail. It usually involves a biopsy. Till date biopsy is the gold-standard test to diagnose cancer. The biopsy slides are examined based on various cytological criteria under a microscope. Therefore, there is a high possibility of not retaining uniformity and ensuring reproducibility in outcomes [2, 3]. Computational diagnostic tools, on the other hand, facilitate objective judgments by making the use of the quantitative measure. This dataset can be utilized in establishing automated diagnostic tool using Artificial Intelligence approaches. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3409(20)30008-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32021884/  |  
------------------------------------------- 
10.1021/acsami.9b22627  |   Two-dimensional (2D) perovskite solar cell (PSC) can achieve high stability by alternating interface cations. However, its main transmissive charge is limited owing to the 2D structure. Therefore, compared with a 3D device, the 2D PSC has poor power conversion efficiency (PCE). Further enhanced performance will require an increase in the transmission dimension of 2D PSC. Here, a novel tetraethylenepent (TEPA)-MAPbI<sub>3-<i>x</i></sub>Cl<sub><i>x</i></sub> analogous 2D unsymmetrical perovskite film was developed to improve the stability and PCE of the corresponding device. Based on the interaction of the active amino linear short chain of TEPA and the halogen ion, the symmetry of the mechanical structure of ions is disrupted, and the TEPA ion is embedded in the perovskite structure to form a perovskite structure with a dimension between 3D and 2D. Noticeably, the TEPA-MAPbI<sub>3-<i>x</i></sub>Cl<sub><i>x</i></sub> devices deliver high PCEs up to 19.73% which stands as the highest for MAPbI<sub>3-<i>x</i></sub>Cl<sub><i>x</i></sub>-based PSC. The environmental, thermal, and illumination stability also showed improvements ranging between 10%-30%. The enhanced PSCs are due to the higher quality of perovskite films, stronger charge transmission, and less trap density. This approach provides a new method to improve and modify 2D PSCs. 
  |  https://dx.doi.org/10.1021/acsami.9b22627  |  
------------------------------------------- 
10.1021/acschemneuro.9b00671  |   A kinetic model describing the pulse of increased oxygen concentrations and the subsequent changes in the concentration of <i>N</i>-acetylaspartate in the excited nervous tissue of the human brain in response to an external signal is presented. The model is based on biochemical data, a multistage and nonlinear dynamic process the BOLD signal and <i>N</i>-acetylaspartate. The existence of multiple steady states explains the triggering effect of the system. The inhibitory effect of the substrate is a necessary factor for the autostabilization of <i>N</i>-acetylaspartate. The kinetic model allows the dynamic behavior of previously unmeasurable metabolites, namely, products of the hydrolysis of <i>N</i>-acetylaspartate, such as acetic and aspartic acid, and glutamic acid to be predicted. Kinetic modeling of the BOLD signal and the subsequent hydrolysis of <i>N</i>-acetylaspartate provides information about the biochemical and dynamic characteristics of some pathological conditions (schizophrenia, Canavan disease, and the superexcitation of the neural network). 
  |  https://dx.doi.org/10.1021/acschemneuro.9b00671  |  
------------------------------------------- 
10.1016/j.jagp.2020.02.012  |    Objectives:  Alzheimer's Disease (AD)-related behavioral symptoms (i.e. agitation and/or pacing) develop in nearly 90% of AD patients. In this N = 1 study, we provide proof-of-concept of detecting changes in movement patterns that may reflect underlying behavioral symptoms using a highly novel radio sensor and identifying environmental triggers. 
  Methods:  The Emerald device is a Wi-Fi-like box without on-body sensors, which emits and processes radio-waves to infer patient movement, spatial location and activity. It was installed for 70 days in the room of patient 'E', exhibiting agitated behaviors. 
  Results:  Daily motion episode aggregation revealed motor activity fluctuation throughout the data collection period which was associated with potential socio-environmental triggers. We did not detect any adverse events attributable to the use of the device. 
  Conclusion:  This N-of-1 study suggests the Emerald device is feasible to use and can potentially yield actionable data regarding behavioral symptom management. No active or potential device risks were encountered. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1064-7481(20)30243-8  |  
------------------------------------------- 
10.1016/j.neuropsychologia.2020.107434  |   Recent electrophysiological research highlights the significance of global scene properties (GSPs) for scene perception. However, since real-world scenes span a range of low-level stimulus properties and high-level contextual semantics, GSP effects may also reflect additional processing of such non-global factors. We examined this question by asking whether Event-Related Potentials (ERPs) to GSPs will still be observed when specific low- and high-level scene properties are absent from the scene. We presented participants with computer-based artificially-manipulated scenes varying in two GSPs (spatial expanse and naturalness) which minimized other sources of scene information (color and semantic object detail). We found that the peak amplitude of the P2 component was sensitive to the spatial expanse and naturalness of the artificially-generated scenes: P2 amplitude was higher to closed than open scenes, and in response to manmade than natural scenes. A control experiment showed that the effect of Naturalness on the P2 is not driven by local texture information, while earlier effects of naturalness, expressed as a modulation of the P1 and N1 amplitudes, are sensitive to texture information. Our results demonstrate that GSPs are processed robustly around 220 ms and that P2 can be used as an index of global scene perception. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0028-3932(20)30105-6  |  
------------------------------------------- 
PMID:32105592  |   Rheumatologists use classification criteria to separate patients with inflammatory rheumatic diseases (IRD). They change over time, and the concepts of the diseases also change. The paradigm is currently moving as the goal of classification in the future will be more to select which patients may be relevant for a specific treatment rather than to describe their characteristics. Therefore, the challenge will be to reclassify multifactorial diseases on the basis of their biological mechanisms rather than their clinical phenotype. Currently, various projects are trying to reclassify diseases using bioinformatics approaches and in the near future the use of advanced machine learning algorithms with large omics datasets could lead to new classification models not only based on a clinical phenotype but also on complex biological profile and common sensitivity to targeted treatment. These models would highlight common biological pathways between patients classified in the same cluster and provide a deep understanding of the mechanisms involved in the patient's clinical phenotype. Such approaches would ultimately lead to classification models that rely more on biological causes than on symptoms. This overview on current classification of subgroups of IRD summarises the classification criteria that we use routinely, and how we will classify IRD in the future using bioinformatics and artificial intelligence techniques. 
  |  None  |  
------------------------------------------- 
10.1016/j.canrad.2020.02.009  |   Radiation therapy has undergone significant advances these last decades, particularly thanks to technical improvements, computer science and a better ability to define the target volumes via morphological and functional imaging breakthroughs. Imaging contributes to all three stages of patient care in radiation oncology: before, during and after treatment. Before the treatment, the choice of optimal imaging type and, if necessary, the adequate functional tracer will allow a better definition of the volume target. During radiation therapy, image-guidance aims at locating the tumour target and tailoring the volume target to anatomical and tumoral variations. Imaging systems are now integrated with conventional accelerators, and newer accelerators have techniques allowing tumour tracking during the irradiation. More recently, MRI-guided systems have been developed, and are already active in a few French centres. Finally, after radiotherapy, imaging plays a major role in most patients' monitoring, and must take into account post-radiation tissue modification specificities. In this review, we will focus on the ongoing projects of nuclear imaging in oncology, and how they can help the radiation oncologist to better treat patients. To this end, a literature review including the terms "Radiotherapy", "Radiation Oncology" and "PET-CT" was performed in August 2019 on Medline and ClinicalTrials.gov. We chose to review successively these novelties organ-by-organ, focusing on the most promising advances. As a conclusion, the help of modern functional imaging thanks to a better definition and new specific radiopharmaceuticals tracers could allow even more precise treatments and enhanced surveillance. Finally, it could provide determinant information to artificial intelligence algorithms in "-omics" models. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1278-3218(20)30073-1  |  
------------------------------------------- 
10.1016/j.cognition.2020.104263  |   Objects and their parts can be visually recognized from purely spatial or purely temporal information but the mechanisms integrating space and time are poorly understood. Here we show that visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. This analysis is obtained by identifying minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. Human recognition in minimal videos is invariably accompanied by full interpretation of the internal components of the video. State-of-the-art deep convolutional networks for dynamic recognition cannot replicate human behavior in these configurations. The gap between human and machine vision demonstrated here is due to critical mechanisms for full spatiotemporal interpretation that are lacking in current computational models. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-0277(20)30082-2  |  
------------------------------------------- 
10.1097/WCO.0000000000000823  |    Purpose of review:  To outline recent applications of e-health data and digital tools for improving the care and management of healthcare for people with multiple sclerosis. 
  Recent findings:  The digitization of most clinical data, along with developments in communication technologies, miniaturization of sensors and computational advances are enabling aggregation and clinically meaningful analyses of real-world data from patient registries, digital patient-reported outcomes and electronic health records (EHR). These data are allowing more confident descriptions of prognoses for multiple sclerosis patients and the long-term relative benefits and safety of disease-modifying treatments (DMT). Registries allow detailed, multiple sclerosis-specific data to be shared between clinicians more easily, provide data needed to improve the impact of DMT and, with EHR, characterize clinically relevant interactions between multiple sclerosis and other diseases. Wearable sensors provide continuous, long-term measures of performance dynamics in relevant ecological settings. In conjunction with telemedicine and online apps, they promise a major expansion of the scope for patients to manage aspects of their own care. Advances in disease understanding, decision support and self-management using these Big Data are being accelerated by machine learning and artificial intelligence. 
  Summary:  Both health professionals and patients can employ e-health approaches and tools for development of a more patient-centred learning health system. 
  |  http://dx.doi.org/10.1097/WCO.0000000000000823  |  
------------------------------------------- 
10.2174/1386207323666200428115449  |    Background:  ZIKV has been a well-known global threat, which hit almost all of the American countries and posed a serious threat to the entire globe in 2016. The first outbreak of ZIKV was reported in 2007 in the Pacific area followed by another severe outbreak, which occurred in 2013/2014 and subsequently, ZIKV spread to all other Pacific islands. A broad spectrum of ZIKV associated neurological malformations in neonates and adults have driven this deadly virus into the limelight. Though tremendous efforts have been focused on understanding the molecular basis of ZIKV, the viral proteins of ZIKV have still not been studied, extensively. 
  Objectives:  Herein, we report the first and the novel predictor for identification of ZIKV proteins. 
  Method:  We have employed Chou's pseudo amino acid composition (PseAAC), statistical moments and various position-based features. 
  Results:  The predictor is validated through 10-fold cross-validation and Jackknife testing. In 10-fold cross-validation, 94.09% accuracy, 93.48% specificity, 94.20% sensitivity and 0.80 MCC was achieved, while in Jackknife testing, 96.62% accuracy, 94.57% specificity, 97.00% sensitivity and 0.88 MCC was achieved. 
  Conclusion:  Thus, ZIKVPred-PseAAC can help in predicting the ZIKV proteins in an efficient and accurate way and can provide baseline data for the discovery of new drugs and biomarkers against ZIKV. 
  |  None  |  
------------------------------------------- 
10.1007/s11948-019-00121-3  |   Since its first publication in 1818, Mary Shelley's Frankenstein; or, The Modern Prometheus has transcended genres and cultures to become a foundational myth about science and technology across a multitude of media forms and adaptations. Following in the footsteps of the brilliant yet troubled Victor Frankenstein, professionals and practitioners have been debating the scientific ethics of creating life for decades, never before have powerful tools for doing so been so widely available. This paper investigates how engaging with the Frankenstein myth may help scientists gain a more accurate understanding of their own beliefs and opinions about the social and ethical aspects of their profession and their work. The paper presents findings from phenomenological interviews with twelve scientists working on biotechnology, robotics, or artificial intelligence projects. The results suggest that the Frankenstein myth, and the figure of Victor Frankenstein in particular, establishes norms for scientists about what is considered unethical and dangerous in scientific work. The Frankenstein myth both serves as a social and ethical reference for scientists and a mediator between scientists and the society. Grappling with the cultural ubiquity of the Frankenstein myth prepares scientists to face their ethical dilemmas and create a more transparent research agenda. Meanwhile, by focusing on the differences between real scientists and the imaginary figure of Victor Frankenstein, scientists may avoid being labeled as dangerous individuals, and could better conceptualize the potential societal and ethical perceptions and implications of their research. 
  |  https://dx.doi.org/10.1007/s11948-019-00121-3  |  
------------------------------------------- 
10.2174/1568026620666200502005853  |   Kinases remain one of the major attractive therapeutic targets for a large number of indications such as cancer, rheumatoid arthritis, cardiac failure and many others. Design and development of kinase inhibitors (ATP-competitive, allosteric or covalent) is a clinically validated and successful strategy in the pharmaceutical industry. . The perks come with limitations, particularly development of resistance to highly potent and selective inhibitors. That's when the cycle needs to be repeated, i.e., design and development of kinase inhibitors active against the mutated forms. The complexity of tumor milieu makes it awfully difficult for these molecularly-targeted therapies to work. Every year newer and better versions of these agents are introduced in the clinic. Several computational approaches such as structure-, ligand-based or hybrid ones continue to live up to their potential in discovering novel kinase inhibitors. Newer throught processes in this area continue to emerge, e.g., development of dual-target kinase inhibitors. But there are fundamental issues with this approach. It is indeed difficult to selectively optimize binding at two entirely different or related kinases. In addition to the conventional strategies, the modern technologies (machine learning, deep learning, artificial intelligence, etc.) started yielding the results and building success stories. Computational tools invariably played a critical role in catalysing the phenomenal progress in kinase drug discovery field. The present review summarized the progress in utilizing computational methods and tools for discovering (mutant-)selective tyrosine kinase inhibitor drugs in the last three years (2017-2019). Representative investigations are discussed briefly while others are merely listed.. The authors believe that the enthusiastic reader will be inspired to dig out the cited literature extensively to appreciate the progress made so far and the future prospects of the field. 
  |  None  |  
------------------------------------------- 
10.3390/ijerph17082948  |   In view of the urgent need for intelligent rehabilitation equipment for some disabled people, an intelligent, upper limb rehabilitation training robot is designed by applying the theories of artificial intelligence, information, control, human-machine engineering, and more. A new robot structure is proposed that combines the use of a flexible rope with an exoskeleton. By introducing environmentally intelligent ergonomics, combined with virtual reality, multi-channel information fusion interaction technology and big-data analysis, a collaborative, efficient, and intelligent remote rehabilitation system based on a human's natural response and other related big-data information is constructed. For the multi-degree of the freedom robot system, optimal adaptive robust control design is introduced based on Udwdia-Kalaba theory and fuzzy set theory. The new equipment will help doctors and medical institutions to optimize both rehabilitation programs and their management, so that patients are more comfortable, safer, and more active in their rehabilitation training in order to obtain better rehabilitation results. 
  |  http://www.mdpi.com/resolver?pii=ijerph17082948  |  
------------------------------------------- 
10.3390/ijerph17030780  |   Essential tremor (ET) is a common neurological disorder and the most common movement disorder. Low-level occupational exposure to mercury vapor is known to be a crucial factor that increases the risk of tremor. Dental amalgam is one of the main sources of mercury in those who possess amalgam restorations. However, the relationship between ET and amalgam filling (AMF) is not quite clear. The purpose of this study was to investigate the association between AMF and the risk of ET using a population-based administrative databank. The data for this study were sourced from the Taiwanese National Health Insurance Research Database (NHIRD). A retrospective case-control study was conducted using this databank from 2000 to 2013. Case and control groups were matched by sex, age, urbanization level, monthly income, and Charlson comorbidity index using the propensity score method with a 1:1 ratio. In this study, 3008 cases and 3008 controls were included. The results from this nationwide population-based case-control study did not indicate any association between ET and AMF in Taiwan. Although the results were not significantly statistical, the findings may be worthy to be valued. 
  |  http://www.mdpi.com/resolver?pii=ijerph17030780  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012693/  |  
------------------------------------------- 
10.1021/acs.inorgchem.0c00268  |   The narrow band gap of silicene severely hinders its application in nanoelectronic devices. Therefore, it is significant to open the band gap of silicene and maintain its high carrier mobility. And for that, the adsorption of different coverage superhalogens BO<sub>2</sub> on the silicene surface have been investigated based on density functional theory and the CALYPSO method. The results show that BO<sub>2</sub> unit prefers to adsorb on silicene with adjacent mode irrespective of the size of substrate. The electronic structure analysis indicates that the density of states near the Fermi level are mainly contributed by Si-<i>p</i> and BO<sub>2</sub>-<i>p</i> orbitals. (BO<sub>2</sub>)<sub><i>n</i></sub>-silicene exhibits metallic character with the exception of (BO<sub>2</sub>)<sub>2</sub> adsorbed on 4 × 4 supercell. As for (BO<sub>2</sub>)<sub>2</sub>-silicene, silicene transforms from a gapless direct semiconductor to an indirect semiconductor. Furthermore, the effective electron mass of two BO<sub>2</sub> superhalogens on 4 × 4 silicene is estimated and found to be smaller than that of graphene. It is expected to result in higher electron mobility. 
  |  https://dx.doi.org/10.1021/acs.inorgchem.0c00268  |  
------------------------------------------- 
10.1128/JCM.02053-19  |   Intestinal protozoa are responsible for relatively few infections in the developed world but the testing volume is disproportionately high. Manual light microscopy of stool remains the gold standard but can be insensitive, time consuming, and difficult to maintain competency. Artificial intelligence and digital slide scanning show promise for revolutionizing the clinical parasitology laboratory by augmenting detection of parasites and slide interpretation using a convolutional neural network (CNN) model. The goal of this study was to develop a sensitive model that could screen out negative trichrome slides, while flagging potential parasites for manual confirmation. Conventional protozoa were trained as "classes" in a deep CNN. Between 1394 and 23566 exemplars per class were used for training, based on specimen availability, from a minimum of 10 unique slides per class.. Scanning was performed using a 40X dry objective automated slide scanner. Data labeling was performed using a proprietary web interface. Clinical validation of the model was performed using 10 unique positive slides per class and 125 negative slides. Accuracy was calculated as slide-level agreement (e.g. parasite present or absent) with microscopy. Positive agreement was 98.88% [95% CI 93.76% to 99.98%] and negative agreement was 98.11% [95% CI 93.35% to 99.77%]. The model showed excellent reproducibility using slides containing multiple classes, a single class, or no parasites. The limit of detection of the model and scanner using serially diluted stool was 5-fold more sensitive than manual examinations by multiple parasitologists using 4 unique slide sets. Digital slide scanning and a CNN model are robust tools for augmenting the conventional detection of intestinal protozoa. 
  |  http://jcm.asm.org/cgi/pmidlookup?view=long&pmid=32295888  |  
------------------------------------------- 
10.1148/rg.2020190112  |   Long acquisition times can limit the use of MRI in pediatric patients, and the use of sedation or general anesthesia is frequently necessary to facilitate diagnostic examinations. The use of sedation or anesthesia has disadvantages including increased cost and imaging time and potential risks to the patient. Reductions in imaging time may decrease or eliminate the need for sedation or general anesthesia. Over the past decade, a number of imaging techniques that can decrease imaging time have become commercially available. These products have been used increasingly in clinical practice and include parallel imaging, simultaneous multisection imaging, radial k-space acquisition, compressed sensing MRI reconstruction, and automated protocol selection software. The underlying concepts, supporting data, current clinical applications, and available products for each of these strategies are reviewed in this article. In addition, emerging techniques that are still under investigation may provide further reductions in imaging time, including artificial intelligence-based reconstruction, gradient-controlled aliasing sampling and reconstruction, three-dimensional MR spectroscopy, and prospective motion correction. The preliminary results for these techniques are also discussed. <sup>©</sup>RSNA, 2020 See discussion on this article by Greer and Vasanawala. 
  |  http://pubs.rsna.org/doi/10.1148/rg.2020190112?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1155/2020/5294840  |   As population aging is becoming more common worldwide, applying artificial intelligence into the diagnosis of Alzheimer's disease (AD) is critical to improve the diagnostic level in recent years. In early diagnosis of AD, the fusion of complementary information contained in multimodality data (e.g., magnetic resonance imaging (MRI), positron emission tomography (PET), and cerebrospinal fluid (CSF)) has obtained enormous achievement. Detecting Alzheimer's disease using multimodality data has two difficulties: (1) there exists noise information in multimodal data; (2) how to establish an effective mathematical model of the relationship between multimodal data? To this end, we proposed a method named LDF which is based on the combination of low-rank representation and discriminant correlation analysis (DCA) to fuse multimodal datasets. Specifically, the low-rank representation method is used to extract the latent features of the submodal data, so the noise information in the submodal data is removed. Then, discriminant correlation analysis is used to fuse the submodal data, so the complementary information can be fully utilized. The experimental results indicate the effectiveness of this method. 
  |  https://doi.org/10.1155/2020/5294840  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32256681/  |  
------------------------------------------- 
10.1016/j.micpath.2020.104231  |   Mycoplasma genitalium is one of the sexually transmitted pathogens that cause significant morbidity in the host. The development of effective therapeutic procedures is urgently needed to counter the multi-drug resistant events imposed by this pathogen. In the current version of M. genitalium G37 genome, 512 open reading frames have been identified. The function of 91 proteins encoded by M. genitalium genes was found to be hypothetical and these proteins were termed hypothetical proteins (HPs). This study aims to carry out functional characterization of HPs by a systems biology approach. Functional assignments of 61 HPs were made with high confidence. They belong to different functional groups, such as DNA-binding proteins, helicases and transporters. Approximately 26% of HPs were identified as transporters, suggesting that M. genitalium is likely to rely on the exogenous nutrient supply for survival. A group of 20 proteins was predicted to be virulence factors, indicating the pathogenic characteristics of M. genitalium. Of the coding proteins, six proteins were pathogen-specific and could serve as potential drug targets by subtractive proteomics analysis. Network analysis of the HPs suggested that several critical proteins were involved in SOS response and stringent response in M. genitalium. These findings provided a better picture for M. genitalium genome and novel clues for studying the potential infection mechanism in this bacterium. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0882-4010(20)30518-0  |  
------------------------------------------- 
10.1016/j.mehy.2020.109722  |   Parkinson's disease (PD) is a neurodegenerative disorder that has important economic and social effects influencing the quality of patient life. Diagnosis of PD is performed in terms of certain criteria depending on the clinical symptom evaluation. However, this method may be inadequate, especially during the onset of the disease. Acoustic analysis of PD is a cost-effective, easy, and non-invasive method for early diagnosis. The mining of association rules is one of the problems in data mining that aims to find valuable and interesting associations in huge data sets. Although association analysis is very popular and useful, to the best of our knowledge, there is not any study on association analysis of PD using vocal change characteristics. Automatic mining of comprehensible, interesting, and accurate association rules in PD data sets containing huge numerical processed voice data is aimed in this study. Due to the numerical characteristics of the vocal attributes in pre-processed PD data, classical association rules mining methods cannot be efficiently applied to this problem. For this reason; MOPNAR, NICGAR, and QAR_CIP_NSGAII that are artificial intelligence-based algorithms were modeled for mining of numerical association rules in order to obtain better performances without using any pre-process for numerical data for the first time. Furthermore, the problem of association analysis of PD with vocal change characteristics was modeled as a multi-objective optimization problem considering many different complementary/contradictory metrics such as support, confidence, comprehensibility, interestingness, etc. in this study. According to the obtained multi-objective rule sets, the NICGAR outperformed in terms of average confidence, average CF, average netconf, average yulesQ, and average number of attributes. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0306-9877(20)30514-4  |  
------------------------------------------- 
10.1016/j.tibtech.2019.07.004  |   Viral proteins evade host immune function by molecular mimicry, often achieved by short linear motifs (SLiMs) of three to ten consecutive amino acids (AAs). Motif mimicry tolerates mutations, evolves quickly to modify interactions with the host, and enables modular interactions with protein complexes. Host cells cannot easily coordinate changes to conserved motif recognition and binding interfaces under selective pressure to maintain critical signaling pathways. SLiMs offer potential for use in synthetic biology, such as better immunogens and therapies, but may also present biosecurity challenges. We survey viral uses of SLiMs to mimic host proteins, and information resources available for motif discovery. As the number of examples continues to grow, knowledge management tools are essential to help organize and compare new findings. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0167-7799(19)30173-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31427097/  |  
------------------------------------------- 
10.1016/j.ygeno.2019.07.017  |   DNase I hypersensitive site (DHS) is related to DNA regulatory elements, so the understanding of DHS sites is of great significance for biomedical research. However, traditional experiments are not very good at identifying recombinant sites of a large number of emerging DNA sequences by sequencing. Some machine learning methods have been proposed to identify DHS, but most methods ignore spatial autocorrelation of the DNA sequence. In this paper, we proposed a predictor called iDHS-DSAMS to identify DHS based on the benchmark datasets. We develop a feature extraction method called dinucleotide-based spatial autocorrelation (DSA). Then we use Min-Redundancy-Max-Relevance (mRMR) to remove irrelevant and redundant features and a 100-dimensional feature vector is selected. Finally, we utilize ensemble bagged tree as classifier, which is based on the oversampled datasets using SMOTE. Five-fold cross validation tests on two benchmark datasets indicate that the proposed method outperforms its existing counterparts on the individual accuracy (Acc), Matthews correlation coefficient (MCC), sensitivity (Sn) and specificity (Sp). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0888-7543(19)30356-8  |  
------------------------------------------- 
10.1109/RBME.2019.2934500  |   This article presents a systematic review of the current computational technologies applied to medical images for the detection, segmentation, and classification of strokes. Besides, analyzing and evaluating the technological advances, the challenges to be overcome and the future trends are discussed. The principal approaches make use of artificial intelligence, digital image processing and analysis, and various other technologies to develop computer-aided diagnosis (CAD) systems to improve the accuracy in the diagnostic process, as well as the interpretation consistency of medical images. However, there are some points that require greater attention such as low sensitivity, optimization of the algorithm, a reduction of false positives, and improvement in the identification and segmentation processes of different sizes and shapes. Also, there is a need to improve the classification steps of different stroke types and subtypes. Furthermore, there is an additional need for further research to improve the current techniques and develop new algorithms to overcome disadvantages identified here. The main focus of this research is to analyze the applied technologies for the development of CAD systems and verify how effective they are for stroke detection, segmentation, and classification. The main contributions of this review are that it analyzes only up-to-date studies, mainly from 2015 to 2018, as well as organizing the various studies in the area according to the research proposal, i.e., detection, segmentation, and classification of the types of stroke and the respective techniques used. Thus, the review has great relevance for future research, since it presents an ample comparison of the most recent works in the area, clearly showing the existing difficulties and the models that have been proposed to overcome such difficulties. 
  |  https://dx.doi.org/10.1109/RBME.2019.2934500  |  
------------------------------------------- 
10.1016/j.sxmr.2019.09.005  |    Introduction:  Cervical cancer is the leading cause of cancer deaths in women in the developing world. New technologies have been developed to allow for more rapid, cost-effective, and sensitive cervical cancer screening and treatment. 
  Aim:  The aim of this study was to describe methods for detection and treatment of human papillomavirus (HPV), cervical dysplasia (CD), and cervical cancer. New technologies and updated screening strategies will be emphasized. 
  Methods:  A literature search was conducted using PubMed to identify publications relevant to the subject. 
  Main outcome measure:  Sensitivity and cost-effectiveness of new cervical cancer screening methods were the main outcome measures. 
  Results:  HPV and cervical cancer have a significant global impact. Research and innovations related to detection and treatment are key in reducing their burden worldwide. 
  Conclusion:  Screening a woman for HPV and CD can dramatically decrease her risk of dying from cervical cancer. New, rapid, low-cost, HPV testing can allow for high-volume screening for the approximately 1.5 billion women who have never been screened. HPV screening can then be combined with high resolution digital colposcopy to detect CD. In the near future, these colposcopic images will be interpreted by artificial intelligence software. Detected lesions can then be treated easily and effectively with thermocoagulation. This see-and-treat model is a sensitive, efficient, and low-cost vision for the future. Bedell SL, Goldstein LS, Goldstein AR, et al. Cervical Cancer Screening: Past, Present, and Future. Sex Med Rev 2020;8:28-37. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2050-0521(19)30101-5  |  
------------------------------------------- 
10.1093/nar/gkz1122  |   Stereochemical restraints are commonly used to aid the refinement of macromolecular structures obtained by experimental methods at lower resolution. The standard restraint library for nucleic acids has not been updated for over two decades and needs revision. In this paper, geometrical restraints for nucleic acids sugars are derived using information from high-resolution crystal structures in the Cambridge Structural Database. In contrast to the existing restraints, this work shows that different parts of the sugar moiety form groups of covalent geometry dependent on various chemical and conformational factors, such as the type of ribose or the attached nucleobase, and ring puckering or rotamers of the glycosidic (χ) or side-chain (γ) torsion angles. Moreover, the geometry of the glycosidic link and the endocyclic ribose bond angles are functionally dependent on χ and sugar pucker amplitude (τm), respectively. The proposed restraints have been positively validated against data from the Nucleic Acid Database, compared with an ultrahigh-resolution Z-DNA structure in the Protein Data Bank, and tested by re-refining hundreds of crystal structures in the Protein Data Bank. The conformation-dependent sugar restraints presented in this work are publicly available in REFMAC, PHENIX and SHELXL format through a dedicated RestraintLib web server with an API function. 
  |  https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkz1122  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31799624/  |  
------------------------------------------- 
10.1016/j.neunet.2019.12.009  |   Face alignment is a typical facial behavior analysis task in computer vision. However, the performance of face alignment is degraded greatly when the face image is partially occluded. In order to achieve better mapping between facial appearance features and shape increments, we propose a robust and occlusion-free face alignment algorithm in which a face de-occlusion module and a deep regression module are integrated into a cascaded deep generative regression model. The face de-occlusion module is a disentangled representation learning Generative Adversarial Networks (GANs) which aims to locate occlusions and recover the genuine appearance from partially occluded face image. The deep regression module can enhance facial appearance representation by utilizing the recovered faces to obtain more accurate regressors. Then, by the cascaded deep generative regression model, we recover the partially occluded face image and achieve accurate locating of landmarks gradually. It is interesting to show that the cascaded deep generative regression model can effectively locate occlusions and recover more genuine faces, which can be further used to improve the performance of face alignment. Experimental results conducted on four challenging occluded face datasets demonstrate that our method outperforms state-of-the-art methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30401-0  |  
------------------------------------------- 
10.1039/c9tb02531f  |   This article reviews several categories of electronic skins (e-skins) for monitoring signals involved in human health. It covers advanced candidate materials, compositions, structures, and integrate strategies of e-skin, focusing on stretchable and wearable electronics. In addition, this article further discusses the potential applications and expected development of e-skins. It is possible to provide a new generation of sensors which are able to introduce artificial intelligence to the clinic and daily healthcare. 
  |  https://doi.org/10.1039/c9tb02531f  |  
------------------------------------------- 
10.1016/j.jenvman.2020.110142  |   As one of the largest emitters of sulfur dioxide (SO<sub>2</sub>), China has faced increasing pressure to achieve sustainable development. This study investigates the decoupling relationship between industrial SO<sub>2</sub> emissions and the industrial economy in China during 1996-2015. According to the decoupling results, the study period is divided into four stages: 1996-2001, 2001-2006, 2006-2010, and 2010-2015. These four stages are closely aligned with the major adjustments of the national socio-economic policies. Then, the logarithmic mean Divisia index (LMDI) decomposition method is used to analyze the driving factors of industrial SO<sub>2</sub> emissions. The results demonstrate that the SO<sub>2</sub> generation intensity and SO<sub>2</sub> abatement are the major contributors to reducing industrial SO<sub>2</sub> emissions, while the economic activity effect is the primary inhibitory factor. Moreover, the provincial results show that most provinces with weak decoupling state since 2006 are located in less developed provinces with energy-intensive industries. Besides, the economic structure and SO<sub>2</sub> generation intensity show negative contributions to reducing industrial SO<sub>2</sub> emissions in some of these regions. Based on the results, the attention should be focused on cleaner production to reduce industrial SO<sub>2</sub> emissions further, and environmental policies should be tailored to local conditions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0301-4797(20)30079-7  |  
------------------------------------------- 
10.1186/s12859-019-3312-5  |    Background:  Non-negative matrix factorization (NMF) is a technique widely used in various fields, including artificial intelligence (AI), signal processing and bioinformatics. However existing algorithms and R packages cannot be applied to large matrices due to their slow convergence or to matrices with missing entries. Besides, most NMF research focuses only on blind decompositions: decomposition without utilizing prior knowledge. Finally, the lack of well-validated methodology for choosing the rank hyperparameters also raises concern on derived results. 
  Results:  We adopt the idea of sequential coordinate-wise descent to NMF to increase the convergence rate. We demonstrate that NMF can handle missing values naturally and this property leads to a novel method to determine the rank hyperparameter. Further, we demonstrate some novel applications of NMF and show how to use masking to inject prior knowledge and desirable properties to achieve a more meaningful decomposition. 
  Conclusions:  We show through complexity analysis and experiments that our implementation converges faster than well-known methods. We also show that using NMF for tumour content deconvolution can achieve results similar to existing methods like ISOpure. Our proposed missing value imputation is more accurate than conventional methods like multiple imputation and comparable to missForest while achieving significantly better computational efficiency. Finally, we argue that the suggested rank tuning method based on missing value imputation is theoretically superior to existing methods. All algorithms are implemented in the R package NNLM, which is freely available on CRAN and Github. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3312-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31906867/  |  
------------------------------------------- 
10.3389/fmolb.2019.00158  |   Dementia-related diseases like Alzheimer's Disease (AD) have a tremendous social and economic cost. A deeper understanding of its underlying pathophysiologies may provide an opportunity for earlier detection and therapeutic intervention. Previous approaches for characterizing AD were targeted at single aspects of the disease. Yet, due to the complex nature of AD, the success of these approaches was limited. However, in recent years, advancements in integrative disease modeling, built on a wide range of AD biomarkers, have taken a global view on the disease, facilitating more comprehensive analysis and interpretation. Integrative AD models can be sorted in two primary types, namely hypothetical models and data-driven models. The latter group split into two subgroups: (i) Models that use traditional statistical methods such as linear models, (ii) Models that take advantage of more advanced artificial intelligence approaches such as machine learning. While many integrative AD models have been published over the last decade, their impact on clinical practice is limited. There exist major challenges in the course of integrative AD modeling, namely data missingness and censoring, imprecise human-involved priori knowledge, model reproducibility, dataset interoperability, dataset integration, and model interpretability. In this review, we highlight recent advancements and future possibilities of integrative modeling in the field of AD research, showcase and discuss the limitations and challenges involved, and finally, propose avenues to address several of these challenges. 
  |  https://doi.org/10.3389/fmolb.2019.00158  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31993440/  |  
------------------------------------------- 
10.1002/adma.201907156  |   Humans are undergoing a fateful transformation focusing on artificial intelligence, quantum information technology, virtual reality, etc., which is inseparable from intelligent nano-micro devices. However, the booming of "Big Data" brings about an even greater challenge by growing electromagnetic radiation. Herein, an innovative flexible multifunctional microsensor is proposed, opening up a new horizon for intelligent devices. It integrates "non-crosstalk" multiple perception and green electromagnetic interference shielding only in one pixel, with satisfactory sensitivity and fast information feedback. Importantly, beneficial by deep insight into the variable-temperature electromagnetic response, the microsensor tactfully transforms the urgent threat of electromagnetic radiation into "wealth," further integrating self-power. This result will refresh researchers' realization of next-generation devices, ushering in a new direction for aerospace engineering, remote sensing, communications, medical treatment, biomimetic robot, prosthetics, etc. 
  |  https://doi.org/10.1002/adma.201907156  |  
------------------------------------------- 
10.3390/polym12010122  |   The fire behavior of materials is usually modeled on the basis of fire physics and material composition. However, significant strides have been made recently in applying soft computing methods such as artificial intelligence in flammability studies. In this paper, multiple linear regression (MLR) was employed to test the degree of non-linearities in flammability parameter modeling by assessing the linear relationship between sample mass, heating rate, heat release capacity (HRC) and total heat release (THR). Adaptive neuro-fuzzy inference system (ANFIS) was then adopted to predict the HRC and THR of the extruded polystyrene measured from microscale combustion calorimetry experiments. The ANFIS models presented excellent predictions, showing very low mean training and testing errors as well as reasonable agreements between experimental and predicted datasets. Hence, it can be inferred that ANFIS can handle the non-linearities in flammability modeling, making it apt as a modeling technique for accurate and effective flammability assessments. 
  |  http://www.mdpi.com/resolver?pii=polym12010122  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31948059/  |  
------------------------------------------- 
10.2174/1389203721666200117171403  |   Peptides act as promising anticancer agents due to their ease of synthesis and modifications, enhanced tumor penetration, and less systemic toxicity. However, only limited success has been achieved so far, as experimental design and synthesis of anticancer peptides (ACPs) are prohibitively costly and time-consuming. Furthermore, sequential increase in the protein sequence data via high-throughput sequencing makes it difficult to identify ACPs only through experimentation, that often involves months or years of speculation and failure. All these limitations could be conquered by applying machine learning (ML) approaches, which is a field of artificial intelligence that automates analytical model building for rapid and accurate outcome predictions. Recently, ML approaches hold great promise in the rapid discovery of ACPs, which could be witnessed by the growing number of ML-based anticancer prediction tools. In this review, we aim to provide a comprehensive view on the existing ML approaches for ACP predictions. Initially, we will briefly discuss the currently available ACP databases. This is followed by the main text, where state-of-the-art ML approaches working principles and their performances based on the ML algorithms are reviewed. Lastly, we discuss on the limitations and future directions of the ML methods in the prediction of ACPs. 
  |  http://www.eurekaselect.com/178416/article  |  
------------------------------------------- 
10.1136/heartjnl-2019-316033  |    |  http://heart.bmj.com/cgi/pmidlookup?view=long&pmid=31974212  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31974212/  |  
------------------------------------------- 
10.1039/c9nr10687a  |   As we seek to discover new functional materials, we need ways to explore the vast chemical space of precursor building blocks, not only generating large numbers of possible building blocks to investigate, but trying to find non-obvious options, that we might not suggest by chemical experience alone. Artificial intelligence techniques provide a possible avenue to generate large numbers of organic building blocks for functional materials, and can even do so from very small initial libraries of known building blocks. Specifically, we demonstrate the application of deep recurrent neural networks for the exploration of the chemical space of building blocks for a test case of donor-acceptor oligomers with specific electronic properties. The recurrent neural network learned how to produce novel donor-acceptor oligomers by trading off between selected atomic substitutions, such as halogenation or methylation, and molecular features such as the oligomer's size. The electronic and structural properties of the generated oligomers can be tuned by sampling from different subsets of the training database, which enabled us to enrich the library of donor-acceptors towards desired properties. We generated approximately 1700 new donor-acceptor oligomers with a recurrent neural network tuned to target oligomers with a HOMO-LUMO gap &lt;2 eV and a dipole moment &lt;2 Debye, which could have potential application in organic photovoltaics. 
  |  https://doi.org/10.1039/c9nr10687a  |  
------------------------------------------- 
10.1109/TCYB.2020.2981733  |   Humans have the ability to identify recurring patterns in diverse situations encountered over a lifetime, constantly understanding relationships between tasks and efficiently solving them through knowledge reuse. The capacity of artificial intelligence systems to mimic such cognitive behaviors for effective problem solving is deemed invaluable, particularly when tackling real-world problems where speed and accuracy are critical. Recently, the notion of evolutionary multitasking has been explored as a means of solving multiple optimization tasks simultaneously using a single population of evolving individuals. In the presence of similarities (or even partial overlaps) between high-quality solutions of related optimization problems, the resulting scope for intertask genetic transfer often leads to significant performance speedup--as the cost of re-exploring overlapping regions of the search space is reduced. While multitasking solvers have led to recent success stories, a known shortcoming of existing methods is their inability to adapt the extent of transfer in a principled manner. Thus, in the absence of any prior knowledge about the relationships between optimization functions, a threat of predominantly negative (harmful) transfer prevails. With this in mind, this article presents a realization of a cognizant evolutionary multitasking engine within the domain of multiobjective optimization. Our proposed algorithm learns intertask relationships based on overlaps in the probabilistic search distributions derived from data generated during the course of multitasking--and accordingly adapts the extent of genetic transfers online. The efficacy of the method is substantiated on multiobjective benchmark problems as well as a practical case study of knowledge transfers from low-fidelity optimization tasks to substantially reduce the cost of high-fidelity optimization. 
  |  https://dx.doi.org/10.1109/TCYB.2020.2981733  |  
------------------------------------------- 
10.1007/s00246-020-02295-1  |   Increasingly the importance of how and why we make decisions in the medical arena has been questioned. Traditionally the aeronautical and business worlds have shed a light on this complex area of human decision-making. In this review we reflect on what we already know about the complexity of decision-making in addition to directing particular focus on the challenges to decision-making in the high-intensity environment of the pediatric cardiac catheterization laboratory. We propose that the most critical factor in outcomes for children in the catheterization lab may not be technical failures but rather human factors and the lack of preparation and robust shared decision-making process between the catheterization team. Key technical factors involved in the decision-making process include understanding the anatomy, the indications and objective to be achieved, equipment availability, procedural flow, having a back-up plan and post-procedural care plan. Increased awareness, pre-catheterization planning, use of standardized clinical assessment and management plans and artificial intelligence may provide solutions to pitfalls in decision-making. Further research and efforts should be directed towards studying the impact of human factors in the cardiac catheterization laboratory as well as the broader medical environment. 
  |  https://dx.doi.org/10.1007/s00246-020-02295-1  |  
------------------------------------------- 
10.3390/biom10020306  |   Gene network estimation is a method key to understanding a fundamental cellular system from high throughput omics data. However, the existing gene network analysis relies on having a sufficient number of samples and is required to handle a huge number of nodes and estimated edges, which remain difficult to interpret, especially in discovering the clinically relevant portions of the network. Here, we propose a novel method to extract a biomedically significant subnetwork using a Bayesian network, a type of unsupervised machine learning method that can be used as an explainable and interpretable artificial intelligence algorithm. Our method quantifies sample specific networks using our proposed <i>Edge Contribution value</i> (ECv) based on the estimated system, which realizes condition-specific subnetwork extraction using a limited number of samples. We applied this method to the Epithelial-Mesenchymal Transition (EMT) data set that is related to the process of metastasis and thus prognosis in cancer biology. We established our method-driven EMT network representing putative gene interactions. Furthermore, we found that the sample-specific ECv patterns of this EMT network can characterize the survival of lung cancer patients. These results show that our method unveils the explainable network differences in biological and clinical features through artificial intelligence technology. 
  |  http://www.mdpi.com/resolver?pii=biom10020306  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32075209/  |  
------------------------------------------- 
10.3389/fphar.2019.01675  |   Ultrafast Shape Recognition (USR), along with its derivatives, are Ligand-Based Virtual Screening (LBVS) methods that condense 3-dimensional information about molecular shape, as well as other properties, into a small set of numeric descriptors. These can be used to efficiently compute a measure of similarity between pairs of molecules using a simple inverse Manhattan Distance metric. In this study we explore the use of suitable Machine Learning techniques that can be trained using USR descriptors, so as to improve the similarity detection of potential new leads. We use molecules from the Directory for Useful Decoys-Enhanced to construct machine learning models based on three different algorithms: Gaussian Mixture Models (GMMs), Isolation Forests and Artificial Neural Networks (ANNs). We train models based on full molecule conformer models, as well as the Lowest Energy Conformations (LECs) only. We also investigate the performance of our models when trained on smaller datasets so as to model virtual screening scenarios when only a small number of actives are known <i>a priori</i>. Our results indicate significant performance gains over a state of the art USR-derived method, ElectroShape 5D, with GMMs obtaining a mean performance up to 430% better than that of ElectroShape 5D in terms of Enrichment Factor with a maximum improvement of up to 940%. Additionally, we demonstrate that our models are capable of maintaining their performance, in terms of enrichment factor, within 10% of the mean as the size of the training dataset is successively reduced. Furthermore, we also demonstrate that running times for retrospective screening using the machine learning models we selected are faster than standard USR, on average by a factor of 10, including the time required for training. Our results show that machine learning techniques can significantly improve the virtual screening performance and efficiency of the USR family of methods. 
  |  https://doi.org/10.3389/fphar.2019.01675  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32140104/  |  
------------------------------------------- 
10.1038/s41598-020-62117-5  |   Recent cancer studies have found that the netrin family of proteins plays vital roles in the development of some cancers. However, the functions of the many variants of these proteins in cancer remain incompletely understood. In this work, we used the most comprehensive database available, including more than 10000 samples across more than 30 tumor types, to analyze the six members of the netrin family. We performed comprehensive analysis of genetic change and expression of the netrin genes and analyzed epigenetic and pathway relationships, as well as the correlation of expression of these proteins with drug sensitivity. Although the mutation rate of the netrin family is low in pan-cancer, among the tumor patients with netrin mutations, the highest number are Uterine Corpus Endometrial Carcinoma patients, accounting for 13.6% of cases (54 of 397). Interestingly, the highest mutation rate of a netrin family member is 38% for NTNG1 (152 of 397). Netrin proteins may participate in the development of endocrine-related tumors and sex hormone-targeting organ tumors. Additionally, the participation of NTNG1 and NTNG2 in various cancers shows their potential for use as new tumor markers and therapeutic targets. This analysis provides a broad molecular perspective of this protein family and suggests some new directions for the treatment of cancer. 
  |  http://dx.doi.org/10.1038/s41598-020-62117-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32251318/  |  
------------------------------------------- 
10.1038/s41586-019-1924-6  |   Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain<sup>1-3</sup>. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning<sup>4-6</sup>. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning. 
  |  https://doi.org/10.1038/s41586-019-1924-6  |  
------------------------------------------- 
10.1007/s10278-019-00310-3  |   Blockchain is an immutable, encrypted, distributed ledger technology. While initially devised for and most commonly referenced with cryptocurrencies, there are an increasing number of applications outside finance, many of which are relevant to medical imaging. In this paper, the concepts and principles underlying the technology and applications relevant to medical imaging are discussed, in addition to potential challenges with implementations such as public versus private key access, distributed ledger size constraints, speed, complexity, and security pitfalls. Potential use cases for blockchain specifically relevant to medical imaging include image sharing including direct patient ownership of images, tracking of implanted medical devices, research, teleradiology, and artificial intelligence. While blockchain offers exciting ways to facilitate the storage and distribution of medical images, similar to the advent of picture archiving and communication systems decades ago, it does have several key limitations of which healthcare providers of medical imaging and imaging informatics professionals should be aware. 
  |  https://doi.org/10.1007/s10278-019-00310-3  |  
------------------------------------------- 
10.1371/journal.pone.0230377  |   We investigate a no-boarding policy in a system of N buses serving M bus stops in a loop, which is an entrainment mechanism to keep buses synchronised in a reasonably staggered configuration. Buses always allow alighting, but would disallow boarding if certain criteria are met. For an analytically tractable theory, buses move with the same natural speed (applicable to programmable self-driving buses), where the average waiting time experienced by passengers waiting at the bus stop for a bus to arrive can be calculated. The analytical results show that a no-boarding policy can dramatically reduce the average waiting time, as compared to the usual situation without the no-boarding policy. Subsequently, we carry out simulations to verify these theoretical analyses, also extending the simulations to typical human-driven buses with different natural speeds based on real data. Finally, a simple general adaptive algorithm is implemented to dynamically determine when to implement no-boarding in a simulation for a real university shuttle bus service. 
  |  http://dx.plos.org/10.1371/journal.pone.0230377  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32203548/  |  
------------------------------------------- 
10.3390/s20051457  |   With the development of population aging, the recognition of elderly activity in smart homes has received increasing attention. In recent years, single-resident activity recognition based on smart homes has made great progress. However, few researchers have focused on multi-resident activity recognition. In this paper, we propose a method to recognize two-resident activities based on time clustering. First, to use a de-noising method to extract the feature of the dataset. Second, to cluster the dataset based on the begin time and end time. Finally, to complete activity recognition using a similarity matching method. To test the performance of the method, we used two two-resident datasets provided by Center for Advanced Studies in Adaptive Systems (CASAS). We evaluated our method by comparing it with some common classifiers. The results show that our method has certain improvements in the accuracy, recall, precision, and F-Measure. At the end of the paper, we explain the parameter selection and summarize our method. 
  |  http://www.mdpi.com/resolver?pii=s20051457  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32155888/  |  
------------------------------------------- 
PMID:31797603  |   The activity of mutational processes differs across the genome, and is influenced by chromatin state and spatial genome organization. At the scale of one megabase-pair (Mb), regional mutation density correlate strongly with chromatin features and mutation density at this scale can be used to accurately identify cancer type. Here, we explore the relationship between genomic region and mutation rate by developing an information theory driven, dynamic programming algorithm for dividing the genome into regions with differing relative mutation rates between cancer types. Our algorithm improves mutual information when compared to the naive approach, effectively reducing the average number of mutations required to identify cancer type. Our approach provides an efficient method for associating regional mutation density with mutation labels, and has future applications in exploring the role of somatic mutations in a number of diseases. 
  |  https://doi.org/10.1142/9789811215636_0025  |  
------------------------------------------- 
10.3390/s20061779  |   Activity recognition plays a central role in many sensor-based applications, such as smart homes for instance. Given a stream of sensor data, the goal is to determine the activities that triggered the sensor data. This article shows how spatial information can be used to improve the process of recognizing activities in smart homes. The sensors that are used in smart homes are in most cases installed in fixed locations, which means that when a particular sensor is triggered, we know approximately where the activity takes place. However, since different sensors may be involved in different occurrences of the same type of activity, the set of sensors associated with a particular activity is not precisely defined. In this article, we use rough sets rather than standard sets to denote the sensors involved in an activity to model, which enables us to deal with this imprecision. Using publicly available data sets, we will demonstrate that rough sets can adequately capture useful information to assist with the activity recognition process. We will also show that rough sets lend themselves to creating Explainable Artificial Intelligence (XAI). activity recognition; context awareness; spatial reasoning; rough sets. 
  |  http://www.mdpi.com/resolver?pii=s20061779  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210199/  |  
------------------------------------------- 
10.1089/tmj.2020.0011  |   <b><i>Background:</i></b> <i>As the leading cause of vision loss in the United States, age-related macular degeneration (AMD) would seem to be amenable to interventions that increase access to screening and management services for patients. AMD poses several unique challenges for telemedicine, however. The disease lacks clinical consensus on the effectiveness and cost-effectiveness of screening the general population, and more complex imaging modalities may be required than for what has traditionally been used for diabetic retinopathy telehealth systems.</i> <b><i>Methods:</i></b> <i>The current literature was reviewed to find clinical trials and expert consensus documents on the state-of-the-art of telemedicine for AMD.</i> <b><i>Results:</i></b> <i>A range of feasibility studies have reported success with telemedicine strategies for AMD. Several investigators have reported experience with AMD screening and remote-monitoring systems as well as artificial intelligence applications.</i> <b><i>Conclusions:</i></b> <i>There are currently no large-scale telemedicine programs for either screening or managing AMD, but new approaches to screening and managing the condition may allow for expansion of high-quality convenient care for an increasing patient population.</i> 
  |  https://www.liebertpub.com/doi/full/10.1089/tmj.2020.0011?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1017/S1431927620001361  |   With the growing importance of three-dimensional and very large field of view imaging, acquisition time becomes a serious bottleneck. Additionally, dose reduction is of importance when imaging material like biological tissue that is sensitive to electron radiation. Random sparse scanning can be used in the combination with image reconstruction techniques to reduce the acquisition time or electron dose in scanning electron microscopy. In this study, we demonstrate a workflow that includes data acquisition on a scanning electron microscope, followed by a sparse image reconstruction based on compressive sensing or alternatively using neural networks. Neuron structures are automatically segmented from the reconstructed images using deep learning techniques. We show that the average dwell time per pixel can be reduced by a factor of 2-3, thereby providing a real-life confirmation of previous results on simulated data in one of the key segmentation applications in connectomics and thus demonstrating the feasibility and benefit of random sparse scanning techniques for a specific real-world scenario. 
  |  https://www.cambridge.org/core/product/identifier/S1431927620001361/type/journal_article  |  
------------------------------------------- 
10.1039/c9nr10701k  |   Nanodiamonds containing the nitrogen vacancy centre (NV) have a significant role in biosensing, bioimaging, drug delivery, and as biomarkers in fluorescence imaging, due to their photo-stability and biocompatibility. The optical read out of the NV unpaired electron spin has been used in diamond magnetometry to image living cells and magnetically labelled cells. Diamond magnetometry is mostly based on the use of bulk diamond with a large concentration of NV centres in a wide field fluorescence microscope equipped with microwave excitation. It is possible to correlate the fluorescence maps with the magnetic field maps of magnetically labelled cells with diffraction limit resolution. Nanodiamonds have not as yet been implemented to image magnetic fields within complex biological systems at the nanometre scale. Here we demonstrate the suitability of nanodiamonds to correlate the fluorescence map with the magnetic imaging map of magnetically labelled cells. Nanoscale optical images with 17 nm resolution of nanodiamonds labelling fixed cells bound to iron oxide magnetic nanoparticles are demonstrated by using a single molecule localisation microscope. Nanoscale magnetic field images of the magnetised magnetic nanoparticles spatially assigned to individual cells are superresolved by the NV centres within nanodiamonds conjugated with the magnetic nanoparticles with 20 nm resolutions. Our method offers a new platform for the super-resolution of optical magnetic imaging in biological samples conjugated with nanodiamonds and iron-oxide magnetic nanoparticles. 
  |  https://doi.org/10.1039/c9nr10701k  |  
------------------------------------------- 
10.1097/CM9.0000000000000563  |    |  http://dx.doi.org/10.1097/CM9.0000000000000563  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31929357/  |  
------------------------------------------- 
10.6224/JN.202004_67(2).05  |   In line with information and communication technology (ICT) trends, nursing practice and its preparations are expected to change fundamentally over the coming decade. The Sustainable Development Goals (SDGs), issued by the United Nations, identify nurses as playing a significant role in achieving these goals and as using new technologies that will likely change how they deliver care to patients. Particularly for nursing care in developing countries, it is essential to emphasize the increased use and interoperability of electronic health records (EHR), mobile devices, big data analytics, and artificial intelligence. Four case studies, including the Malawi HIV system, community-based mobile APP in South Africa, the cloud base hospital information system (HIS) project in Marshall island, and smart hemodialysis in Vietnam, are examined in this article. In the near future, both information technology skills and interaction will have a significant impact on the nursing profession. 
  Title:  智慧醫療對護理從業人員在開發中國家的探討. 
 未來十年護理工作將伴隨著資訊應用而改變各種護理工作，人員與技術的交流更加頻繁，疾病的多樣性與資源的不平均也讓許多護理工作存在更多不確定性，聯合國永續發展目標中，護理人員可扮演做出重大貢獻的角色，支持護理工作實現未來的技術進步和挑戰更加重要。面對開發中國家的護理實務，應重視包括擴大電子健康記錄的使用；行動裝置及可穿戴設備對臨床工作的影響；大數據分析後的人工智慧潛力；最終提升患者的參與度，如在馬拉威的愛滋病門診中，透過雲端串連起全國統計數據；在南非的社區計畫中利用行動裝置結合穿載設備，收集偏遠村莊患者生理數據；在馬紹爾的案例中導入全院性資訊系統，並與世界衛生組織合作在該國的全國性肺結核公共衛生檢疫工作；最後在越南智能血液透析照護利用設備物聯網的概念，減輕護理人員的工作與技能上的培訓，降低病患風險。在可預期的未來，護理從業人員在對資訊科技的態度對臨床工作有重大影響，同時和人性關懷間如何取得平衡也將考驗未來的護理實務。. 
  |  None  |  
------------------------------------------- 
10.1097/WCO.0000000000000779  |    Purpose of review:  The retina is growingly recognized as a window into cerebrovascular and systemic vascular conditions. The utility of noninvasive retinal vessel biomarkers in cerebrovascular risk assessment has expanded due to advances in retinal imaging techniques and machine learning-based digital analysis. The purpose of this review is to underscore the latest evidence linking retinal vascular abnormalities with stroke and vascular-related cognitive disorders; to highlight modern developments in retinal vascular imaging modalities and software-based vasculopathy quantification. 
  Recent findings:  Longitudinal studies undertaken for extended periods indicate that retinal vascular changes can predict cerebrovascular disorders (CVD). Cerebrovascular ties to dementia provoked recent explorations of retinal vessel imaging tools for conceivable early cognitive decline detection. Innovative biomedical engineering technologies and advanced dynamic and functional retinal vascular imaging methods have recently been added to the armamentarium, allowing an unbiased and comprehensive analysis of the retinal vasculature. Improved artificial intelligence-based deep learning algorithms have boosted the application of retinal imaging as a clinical and research tool to screen, risk stratify, and monitor with precision CVD and vascular cognitive impairment. 
  Summary:  Mounting evidence supports the use of quantitative retinal vessel analysis in predicting CVD, from clinical stroke to neuroimaging markers of stroke and neurodegeneration. 
  |  http://dx.doi.org/10.1097/WCO.0000000000000779  |  
------------------------------------------- 
10.1080/08820538.2020.1729818  |   The introduction of ultrawide field imaging has allowed the visualization of approximately 82% of the total retinal area compared to only 30% using 7-standard field Early Treatment Diabetic Retinopathy (ETDRS) photography. This substantially wider field of view, while useful in many retinal vascular diseases, is particularly important in diabetic retinopathy where eyes with predominantly peripheral lesions or PPL have been shown to have significantly greater progression rates compared to eyes without PPL. In telemedicine settings, ultrawide field imaging has substantially reduced image ungradable rates and increased rate of disease identification allowing care to be delivered more effectively. Furthermore, the use of ultrawide field fluorescein angiography allows the visualization of significantly more diabetic retinal lesions and allows more accurate quantification of total retinal nonperfusion, with potential implications in the management of diabetic retinopathy and diabetic macular edema. The focus of this paper is to review the current role of ultrawide field imaging in diabetic retinopathy and its possible future role in innovations for retinal image analysis such as artificial intelligence and vessel caliber measurements. 
  |  http://www.tandfonline.com/doi/full/10.1080/08820538.2020.1729818  |  
------------------------------------------- 
10.1049/iet-syb.2018.5019  |   In this study, a closed-loop control scheme is proposed for the glucose-insulin regulatory system in type-1 diabetic mellitus (T1DM) patients. Some innovative hybrid glucose-insulin regulators have combined artificial intelligence such as fuzzy logic and genetic algorithm with well known Palumbo model to regulate the blood glucose (BG) level in T1DM patients. However, most of these approaches have focused on the glucose reference tracking, and the qualitative of this tracking such as chattering reduction of insulin injection has not been well-studied. Higher-order sliding mode (HoSM) controllers have been employed to attenuate the effect of chattering. Owing to the delayed nature and non-linear property of glucose-insulin mechanism as well as various unmeasurable disturbances, even the HoSM methods are partly successful. In this study, data fusion of adaptive neuro-fuzzy inference systems optimised by particle swarm optimisation has been presented. The excellent performance of the proposed hybrid controller, i.e. desired BG-level tracking and chattering reduction in the presence of daily glucose-level disturbances is verified. 
  |  None  |  
------------------------------------------- 
10.1017/ice.2020.61  |   We are proposing to use machine learning algorithms to be able to improve possible case identifications of COVID-19 more quicker when we use a mobile phone-based web survey. This will also reduce the spread in the susceptible populations. 
  |  https://www.cambridge.org/core/product/identifier/S0899823X20000616/type/journal_article  |  
------------------------------------------- 
10.1016/j.jaapos.2020.01.014  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1091-8531(20)30067-7  |  
------------------------------------------- 
10.1136/jech-2018-211654  |   Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field. 
  |  http://jech.bmj.com/cgi/pmidlookup?view=long&pmid=32332114  |  
------------------------------------------- 
10.1088/1361-6560/ab8954  |   The primary cone-beam computed tomography (CBCT) imaging beam scatters inside the patient and produces a contaminating photon fluence that is registered by the detector. Scattered photons cause artifacts in the image reconstruction, and are partially responsible for the inferior image quality compared to diagnostic fan-beam CT. In this work, a deep convolutional autoencoder (DCAE) and projection-based scatter removal algorithm were constructed for the ImagingRingTM system on rails (IRr), which allows for non-isocentric acquisitions around virtual rotation centers with its independently rotatable source and detector arms. A Monte Carlo model was developed to simulate (i) a non-isocentric training dataset of 1200 projection pairs (primary + scatter) from 27 digital head-and-neck cancer patients around five different virtual rotation centers (DCAENONISO), and (ii) an isocentric dataset existing of 1200 projection pairs around the physical rotation center (DCAEISO). The scatter removal performance of both DCAE networks was investigated in two digital anthropomorphic phantom simulations and due to superior performance only the DCAENONISO was applied on eight real patient acquisitions. Measures for the quantitative error, the signal-to-noise ratio, and the similarity were evaluated for two simulated digital head-and-neck patients, and the contrast-to-noise ratio (CNR) was investigated between muscle and adipose tissue in the real patient image reconstructions. Image quality metrics were compared between the uncorrected data, the currently implemented heuristic scatter correction data, and the DCAE corrected image reconstruction. The DCAENONISO corrected image reconstructions of two digital patient simulations showed superior image quality metrics compared to the uncorrected and corrected image reconstructions using a scatter removal heuristic. The proposed DCAENONISO scatter correction in this study was successfully demonstrated in real non-isocentric patient CBCT acquisitions and achieved statistically significant higher CNRs compared to the uncorrected or the heuristic corrected image data. This paper presents a projection-based scatter removal algorithm for CBCT imaging using a deep convolutional autoencoder trained on Monte Carlo composed datasets. The algorithm was successfully applied to real patient data. 
  |  https://doi.org/10.1088/1361-6560/ab8954  |  
------------------------------------------- 
10.1186/s41747-019-0131-4  |    Background:  Differentiate malignant from benign enhancing foci on breast magnetic resonance imaging (MRI) through radiomic signature. 
  Methods:  Forty-five enhancing foci in 45 patients were included in this retrospective study, with needle biopsy or imaging follow-up serving as a reference standard. There were 12 malignant and 33 benign lesions. Eight benign lesions confirmed by over 5-year negative follow-up and 15 malignant histopathologically confirmed lesions were added to the dataset to provide reference cases to the machine learning analysis. All MRI examinations were performed with a 1.5-T scanner. One three-dimensional T1-weighted unenhanced sequence was acquired, followed by four dynamic sequences after intravenous injection of 0.1 mmol/kg of gadobenate dimeglumine. Enhancing foci were segmented by an expert breast radiologist, over 200 radiomic features were extracted, and an evolutionary machine learning method ("training with input selection and testing") was applied. For each classifier, sensitivity, specificity and accuracy were calculated as point estimates and 95% confidence intervals (CIs). 
  Results:  A k-nearest neighbour classifier based on 35 selected features was identified as the best performing machine learning approach. Considering both the 45 enhancing foci and the 23 additional cases, this classifier showed a sensitivity of 27/27 (100%, 95% CI 87-100%), a specificity of 37/41 (90%, 95% CI 77-97%), and an accuracy of 64/68 (94%, 95% CI 86-98%). 
  Conclusion:  This preliminary study showed the feasibility of a radiomic approach for the characterisation of enhancing foci on breast MRI. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31993839/  |  
------------------------------------------- 
10.1016/j.spinee.2020.02.021  |    Background:  Surgical site infections are a major driver of morbidity and increased costs in the postoperative period after spine surgery. Current tools for surveillance of these adverse events rely on prospective clinical tracking, manual retrospective chart review, or administrative procedural and diagnosis codes. 
  Purpose:  The purpose of this study was to develop natural language processing (NLP) algorithms for automated reporting of postoperative wound infection requiring reoperation after lumbar discectomy. 
  Patient sample:  Adult patients undergoing discectomy at two academic and three community medical centers between January 1, 2000 and July 31, 2019 for lumbar disc herniation. 
  Outcome measures:  Reoperation for wound infection within 90 days after surgery METHODS: Free-text notes of patients who underwent surgery from January 1, 2000 to December 31, 2015 were used for algorithm training. Free-text notes of patients who underwent surgery after January 1, 2016 were used for algorithm testing. Manual chart review was used to label which patients had reoperation for wound infection. An extreme gradient-boosting NLP algorithm was developed to detect reoperation for postoperative wound infection. 
  Results:  Overall, 5,860 patients were included in this study and 62 (1.1%) had a reoperation for wound infection. In patients who underwent surgery after January 1, 2016 (n=1,377), the NLP algorithm detected 15 of the 16 patients (sensitivity=0.94) who had reoperation for infection. In comparison, current procedural terminology and international classification of disease codes detected 12 of these 16 patients (sensitivity=0.75). At a threshold of 0.05, the NLP algorithm had positive predictive value of 0.83 and F1-score of 0.88. 
  Conclusion:  Temporal validation of the algorithm developed in this study demonstrates a proof-of-concept application of NLP for automated reporting of adverse events after spine surgery. Adapting this methodology for other procedures and outcomes in spine and orthopedics has the potential to dramatically improve and automatize quality and safety reporting. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1529-9430(20)30088-7  |  
------------------------------------------- 
10.1016/j.intimp.2020.106465  |   Follistatin-like protein 1 (FSTL1) showed overexpression in the inflammatory synovial pannus, serum, and synovial tissues of osteoarthritis (OA) patients. However, FSTL1 knock out (KO) embryos exhibited reduced vertebral cartilage cellularity, extensive skeleton defects and reduced MSCs proliferation. Thus, the role of FSTL1 in chondrocyte proliferation is not completely understood. In vitro studies revealed that Human recombinant FSTL1 (hFSTL1) promoted chondrogenic signals in the MSCs and cell viability only at low concentrations. Recent reports suggest that high levels of FSTL-1 are proposed to enhance inflammatory signals which suppress chondrogenesis leading to cartilage destruction. Altogether, FSTL1 has the potential to promote MSC proliferation and chondrogenic differentiation in a low concentration-dependent manner. However, the mechanism by which FSTL-1 affects MSCs chondrogenic differentiation and chondrogenesis remains unknown. Therefore, this review introduces a deep discussion of FSTL1's molecular functions in the OA pathophysiology, which will contribute to the deep understanding of FSTL1 molecular activity in the OA pathogenesis. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1567-5769(20)30059-X  |  
------------------------------------------- 
10.2196/13174  |    Background:  Autism spectrum disorder (ASD) is a developmental disorder characterized by deficits in social communication and interaction, and restricted and repetitive behaviors and interests. The incidence of ASD has increased in recent years; it is now estimated that approximately 1 in 40 children in the United States are affected. Due in part to increasing prevalence, access to treatment has become constrained. Hope lies in mobile solutions that provide therapy through artificial intelligence (AI) approaches, including facial and emotion detection AI models developed by mainstream cloud providers, available directly to consumers. However, these solutions may not be sufficiently trained for use in pediatric populations. 
  Objective:  Emotion classifiers available off-the-shelf to the general public through Microsoft, Amazon, Google, and Sighthound are well-suited to the pediatric population, and could be used for developing mobile therapies targeting aspects of social communication and interaction, perhaps accelerating innovation in this space. This study aimed to test these classifiers directly with image data from children with parent-reported ASD recruited through crowdsourcing. 
  Methods:  We used a mobile game called Guess What? that challenges a child to act out a series of prompts displayed on the screen of the smartphone held on the forehead of his or her care provider. The game is intended to be a fun and engaging way for the child and parent to interact socially, for example, the parent attempting to guess what emotion the child is acting out (eg, surprised, scared, or disgusted). During a 90-second game session, as many as 50 prompts are shown while the child acts, and the video records the actions and expressions of the child. Due in part to the fun nature of the game, it is a viable way to remotely engage pediatric populations, including the autism population through crowdsourcing. We recruited 21 children with ASD to play the game and gathered 2602 emotive frames following their game sessions. These data were used to evaluate the accuracy and performance of four state-of-the-art facial emotion classifiers to develop an understanding of the feasibility of these platforms for pediatric research. 
  Results:  All classifiers performed poorly for every evaluated emotion except happy. None of the classifiers correctly labeled over 60.18% (1566/2602) of the evaluated frames. Moreover, none of the classifiers correctly identified more than 11% (6/51) of the angry frames and 14% (10/69) of the disgust frames. 
  Conclusions:  The findings suggest that commercial emotion classifiers may be insufficiently trained for use in digital approaches to autism treatment and treatment tracking. Secure, privacy-preserving methods to increase labeled training data are needed to boost the models' performance before they can be used in AI-enabled approaches to social therapy of the kind that is common in autism treatments. 
  |  https://mental.jmir.org/2020/4/e13174/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32234701/  |  
------------------------------------------- 
10.1016/j.ebiom.2020.102724  |    Background:  Interstitial lung disease requires frequent re-examination, which directly causes excessive cumulative radiation exposure. To date, AI has not been applied to CT for enhancing clinical care; thus, we hypothesize AI may empower CT with intelligence to realize automatic and accurate pulmonary scanning, thus dramatically decrease medical radiation exposure without compromising patient care. 
  Methods:  Facial boundary detection was realized by recognizing adjacent jaw position through training and testing a region proposal network (RPN) on 76,882 human faces using a preinstalled 2-dimensional camera; the lung-fields was then segmented by V-Net on another training set with 314 subjects and calculated the moving distance of the scanning couch based on a pre-generated calibration table. A multi-cohort study, including 1,186 patients was used for validation and radiation dose quantification under three clinical scenarios. 
  Findings:  A U-HAPPY (United imaging Human Automatic Planbox for PulmonarY) scanning CT was designed. Error distance of RPN was 4·46±0·02 pixels with a success rate of 98·7% in training set and 2·23±0·10 pixels with 100% success rate in testing set. Average Dice's coefficient was 0·99 in training set and 0·96 in testing set. A calibration table with 1,344,000 matches was generated to support the linkage between camera and scanner. This real-time automation makes an accurate plan-box to cover exact location and area needed to scan, thus reducing amounts of radiation exposures significantly (all, P&lt;0·001). 
  Interpretation:  U-HAPPY CT designed for pulmonary imaging acquisition standardization is promising for reducing patient risk and optimizing public health expenditures. 
  Funding:  The National Natural Science Foundation of China. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3964(20)30099-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32251997/  |  
------------------------------------------- 
10.3390/s20061597  |   Multi exposure image fusion (MEF) provides a concise way to generate high-dynamic-range (HDR) images. Although the precise fusion can be achieved by existing MEF methods in different static scenes, the corresponding performance of ghost removal varies in different dynamic scenes. This paper proposes a precise MEF method based on feature patches (FPM) to improve the robustness of ghost removal in a dynamic scene. A reference image is selected by a priori exposure quality first and then used in the structure consistency test to solve the image ghosting issues existing in the dynamic scene MEF. Source images are decomposed into spatial-domain structures by a guided filter. Both the base and detail layer of the decomposed images are fused to achieve the MEF. The structure decomposition of the image patch and the appropriate exposure evaluation are integrated into the proposed solution. Both global and local exposures are optimized to improve the fusion performance. Compared with six existing MEF methods, the proposed FPM not only improves the robustness of ghost removal in a dynamic scene, but also performs well in color saturation, image sharpness, and local detail processing. 
  |  http://www.mdpi.com/resolver?pii=s20061597  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32182986/  |  
------------------------------------------- 
10.1136/gutjnl-2019-320056  |    Background:  The objective evaluation of endoscopic disease activity is key in ulcerative colitis (UC). A composite of endoscopic and histological factors is the goal in UC treatment. We aimed to develop an operator-independent computer-based tool to determine UC activity based on endoscopic images. 
  Methods:  First, we built a computer algorithm using data from 29 consecutive patients with UC and 6 healthy controls (construction cohort). The algorithm (red density: RD) was based on the red channel of the red-green-blue pixel values and pattern recognition from endoscopic images. The algorithm was refined in sequential steps to optimise correlation with endoscopic and histological disease activity. In a second phase, the operating properties were tested in patients with UC flares requiring treatment escalation. To validate the algorithm, we tested the correlation between RD score and clinical, endoscopic and histological features in a validation cohort. 
  Results:  We constructed the algorithm based on the integration of pixel colour data from the redness colour map along with vascular pattern detection. These data were linked with Robarts histological index (RHI) in a multiple regression analysis. In the construction cohort, RD correlated with RHI (r=0.74, p&lt;0.0001), Mayo endoscopic subscores (r=0.76, p&lt;0.0001) and UC Endoscopic Index of Severity scores (r=0.74, p&lt;0.0001). The RD sensitivity to change had a standardised effect size of 1.16. In the validation set, RD correlated with RHI (r=0.65, p=0.00002). 
  Conclusions:  RD provides an objective computer-based score that accurately assesses disease activity in UC. In a validation study, RD correlated with endoscopic and histological disease activity. 
  |  http://gut.bmj.com/cgi/pmidlookup?view=long&pmid=31915237  |  
------------------------------------------- 
10.1016/j.cmi.2020.02.006  |    Background:  Machine learning (ML) allows the analysis of complex and large data sets and has the potential to improve health care. The clinical microbiology laboratory, at the interface of clinical practice and diagnostics, is of special interest for the development of ML systems. 
  Aims:  This narrative review aims to explore the current use of ML In clinical microbiology. 
  Sources:  References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, arXiV, ACM Digital Library and IEEE Xplore Digital Library up to November 2019. 
  Content:  We found 97 ML systems aiming to assist clinical microbiologists. Overall, 82 ML systems (85%) targeted bacterial infections, 11 (11%) parasitic infections, nine (9%) viral infections and three (3%) fungal infections. Forty ML systems (41%) focused on microorganism detection, identification and quantification, 36 (37%) evaluated antimicrobial susceptibility, and 21 (22%) targeted the diagnosis, disease classification and prediction of clinical outcomes. The ML systems used very diverse data sources: 21 (22%) used genomic data of microorganisms, 19 (20%) microbiota data obtained by metagenomic sequencing, 19 (20%) analysed microscopic images, 17 (18%) spectroscopy data, eight (8%) targeted gene sequencing, six (6%) volatile organic compounds, four (4%) photographs of bacterial colonies, four (4%) transcriptome data, three (3%) protein structure, and three (3%) clinical data. Most systems used data from high-income countries (n = 71, 73%) but a significant number used data from low- and middle-income countries (n = 36, 37%). Performance measures were reported for the 97 ML systems, but no article described their use in clinical practice or reported impact on processes or clinical outcomes. 
  Implications:  In clinical microbiology, ML has been used with various data sources and diverse practical applications. The evaluation and implementation processes represent the main gap in existing ML systems, requiring a focus on their interpretability and potential integration into real-world settings. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1198-743X(20)30085-9  |  
------------------------------------------- 
10.1016/j.compbiomed.2020.103618  |   This paper presents a self-paced brain-computer interface (BCI) based on the incorporation of an intelligent environment-understanding approach into a motor imagery (MI) BCI system for rehabilitation hospital environmental control. The interface integrates four types of daily assistance tasks: medical calls, service calls, appliance control and catering services. The system introduces intelligent environment understanding technology to establish preliminary predictions concerning a user's control intention by extracting potential operational objects in the current environment through an object detection neural network. According to the characteristics of the four types of control and services, we establish different response mechanisms and use an intelligent decision-making method to design and dynamically optimize the relevant control instruction set. The control feedback is communicated to the user via voice prompts; it avoids the use of visual channels throughout the interaction. The asynchronous and synchronous modes of the MI-BCI are designed to launch the control process and to select specific operations, respectively. In particular, the reliability of the MI-BCI is enhanced by the optimized identification algorithm. An online experiment demonstrated that the system can respond quickly and it generates an activation command in an average of 3.38s while effectively preventing false activations; the average accuracy of the BCI synchronization commands was 89.2%, which represents sufficiently effective control. The proposed system is efficient, applicable and can be used to both improve system information throughput and to reduce mental loads. The proposed system can be used to assist with the daily lives of patients with severe motor impairments. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(20)30017-2  |  
------------------------------------------- 
10.3389/fnins.2020.00088  |   Development of spiking neural networks (SNNs) controlling mobile robots is one of the modern challenges in computational neuroscience and artificial intelligence. Such networks, being replicas of biological ones, are expected to have a higher computational potential than traditional artificial neural networks (ANNs). The critical problem is in the design of robust learning algorithms aimed at building a "living computer" based on SNNs. Here, we propose a simple SNN equipped with a Hebbian rule in the form of spike-timing-dependent plasticity (STDP). The SNN implements associative learning by exploiting the spatial properties of STDP. We show that a LEGO robot controlled by the SNN can exhibit classical and operant conditioning. Competition of spike-conducting pathways in the SNN plays a fundamental role in establishing associations of neural connections. It replaces the irrelevant associations by new ones in response to a change in stimuli. Thus, the robot gets the ability to relearn when the environment changes. The proposed SNN and the stimulation protocol can be further enhanced and tested in developing neuronal cultures, and also admit the use of memristive devices for hardware implementation. 
  |  https://doi.org/10.3389/fnins.2020.00088  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32174804/  |  
------------------------------------------- 
10.1007/s00330-020-06831-8  |    Objectives:  To compare the diagnostic accuracy of texture analysis (TA)-derived parameters combined with machine learning (ML) of non-contrast-enhanced T1w and T2w fat-saturated (fs) images with MR elastography (MRE) for liver fibrosis quantification. 
  Methods:  In this IRB-approved prospective study, liver MRIs of participants with suspected chronic liver disease who underwent liver biopsy between August 2015 and May 2018 were analyzed. Two readers blinded to clinical and histopathological findings performed TA. The participants were categorized into no or low-stage (0-2) and high-stage (3-4) fibrosis groups. Confusion matrices were calculated using a support vector machine combined with principal component analysis. The diagnostic accuracy of ML-based TA of liver fibrosis and MRE was assessed by area under the receiver operating characteristic curves (AUC). Histopathology served as reference standard. 
  Results:  A total of 62 consecutive participants (40 men; mean age ± standard deviation, 48 ± 13 years) were included. The accuracy of TA and ML on T1w was 85.7% (95% confidence interval [CI] 63.7-97.0) and 61.9% (95% CI 38.4-81.9) on T2w fs for classification of liver fibrosis into low-stage and high-stage fibrosis. The AUC for TA on T1w was similar to MRE (0.82 [95% CI 0.59-0.95] vs. 0.92 [95% CI 0.71-0.99], p = 0.41), while the AUC for T2w fs was significantly lower compared to MRE (0.57 [95% CI 0.34-0.78] vs. 0.92 [95% CI 0.71-0.99], p = 0.008). 
  Conclusion:  Our results suggest that liver fibrosis can be quantified with TA-derived parameters of T1w when combined with a ML algorithm with similar accuracy compared to MRE. 
  Key points:  • Liver fibrosis can be categorized into low-stage fibrosis (0-2) and high-stage fibrosis (3-4) using texture analysis-derived parameters of T1-weighted images with a machine learning approach. • For the differentiation of low-stage fibrosis and high-stage fibrosis, the diagnostic accuracy of texture analysis on T1-weighted images combined with a machine learning algorithm is similar compared to MR elastography. 
  |  https://dx.doi.org/10.1007/s00330-020-06831-8  |  
------------------------------------------- 
10.5664/jcsm.8288  |   Sleep medicine is well positioned to benefit from advances that use big data to create artificially intelligent computer programs. One obvious initial application in the sleep disorders center is the assisted (or enhanced) scoring of sleep and associated events during polysomnography (PSG). This position statement outlines the potential opportunities and limitations of integrating artificial intelligence (AI) into the practice of sleep medicine. Additionally, although the most apparent and immediate application of AI in our field is the assisted scoring of PSG, we propose potential clinical use cases that transcend the sleep laboratory and are expected to deepen our understanding of sleep disorders, improve patient-centered sleep care, augment day-to-day clinical operations, and increase our knowledge of the role of sleep in health at a population level. 
  |  https://doi.org/10.5664/jcsm.8288  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32022674/  |  
------------------------------------------- 
10.1038/s41433-020-0883-3  |    Purpose:  The purpose of this study is to develop and assess the accuracy of a new intraocular lens (IOL) power calculation method based on machine learning techniques. 
  Methods:  The following data were retrieved for 260 eyes of 260 patients undergoing cataract surgery: preoperative simulated keratometry, mean keratometry of posterior surface, axial length, anterior chamber depth, lens thickness, and white-to-white diameter; model and power of implanted IOL; and subjective refraction at 3 months post surgery. These data were used to train different machine learning models (k-Nearest Neighbor, Artificial Neural Networks, Support Vector Machine, Random Forest, etc). Implanted lens characteristics and biometric data were used as input to predict IOL power and refractive outcomes. For external validation, a dataset of 52 eyes was used. The accuracy of the trained models was compared with that of the power formulas Holladay 2, Haigis, Barrett Universal II, and Hill-RBF v2.0. 
  Results:  The SD of the prediction error in order of lowest to highest was the new method (designated Karmona) (0.30), Haigis (0.36), Holladay 2 (0.38), Barrett Universal II (0.38), and Hill-RBF v2.0 (0.40). Using the Karmona method, 90.38% and 100% of eyes were within ±0.50 and ±1.00 D respectively. 
  Conclusions:  The method proposed emerged as the most accurate to predict IOL power. 
  |  http://dx.doi.org/10.1038/s41433-020-0883-3  |  
------------------------------------------- 
10.1021/acsomega.0c00795  |   The Cu migration is controlled by using an optimized AlO <i><sub>x</sub></i> interfacial layer, and effects on resistive switching performance, artificial synapse, and human saliva detection in an amorphous-oxygenated-carbon (a-CO <i><sub>x</sub></i> )-based CBRAM platform have been investigated for the first time. The 4 nm-thick AlO <i><sub>x</sub></i> layer in the Cu/AlO <i><sub>x</sub></i> /a-CO <i><sub>x</sub></i> /TiN <i><sub>x</sub></i> O <i><sub>y</sub></i> /TiN structure shows consecutive &gt;2000 DC switching, tight distribution of SET/RESET voltages, a long program/erase (P/E) endurance of &gt;10<sup>9</sup> cycles at a low operation current of 300 μA, and artificial synaptic characteristics under a small pulse width of 100 ns. After a P/E endurance of &gt;10<sup>8</sup> cycles, the Cu migration is observed by both ex situ high-resolution transmission electron microscopy and energy-dispersive X-ray spectroscopy mapping images. Furthermore, the optimized Cu/AlO <i><sub>x</sub></i> /a-CO <i><sub>x</sub></i> /TiN <i><sub>x</sub></i> O <i><sub>y</sub></i> /TiN CBRAM detects glucose with a low concentration of 1 pM, and real-time measurement of human saliva with a small sample volume of 1 μL is also detected repeatedly in vitro. This is owing to oxidation-reduction of Cu electrode, and the switching mechanism is explored. Therefore, this CBRAM device is beneficial for future artificial intelligence application. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32258939/  |  
------------------------------------------- 
10.1007/s00330-019-06601-1  |    Objective:  To investigate externally validated magnetic resonance (MR)-based and computed tomography (CT)-based machine learning (ML) models for grading clear cell renal cell carcinoma (ccRCC). 
  Materials and methods:  Patients with pathologically proven ccRCC in 2009-2018 were retrospectively included for model development and internal validation; patients from another independent institution and The Cancer Imaging Archive dataset were included for external validation. Features were extracted from T1-weighted, T2-weighted, corticomedullary-phase (CMP), and nephrographic-phase (NP) MR as well as precontrast-phase (PCP), CMP, and NP CT. CatBoost was used for ML-model investigation. The reproducibility of texture features was assessed using intraclass correlation coefficient (ICC). Accuracy (ACC) was used for ML-model performance evaluation. 
  Results:  Twenty external and 440 internal cases were included. Among 368 and 276 texture features from MR and CT, 322 and 250 features with good to excellent reproducibility (ICC ≥ 0.75) were included for ML-model development. The best MR- and CT-based ML models satisfactorily distinguished high- from low-grade ccRCCs in internal (MR-ACC = 73% and CT-ACC = 79%) and external (MR-ACC = 74% and CT-ACC = 69%) validation. Compared to single-sequence or single-phase images, the classifiers based on all-sequence MR (71% to 73% in internal and 64% to 74% in external validation) and all-phase CT (77% to 79% in internal and 61% to 69% in external validation) images had significant increases in ACC. 
  Conclusions:  MR- and CT-based ML models are valuable noninvasive techniques for discriminating high- from low-grade ccRCCs, and multiparameter MR- and multiphase CT-based classifiers are potentially superior to those based on single-sequence or single-phase imaging. 
  Key points:  • Both the MR- and CT-based machine learning models are reliable predictors for differentiating high- from low-grade ccRCCs. • ML models based on multiparameter MR sequences and multiphase CT images potentially outperform those based on single-sequence or single-phase images in ccRCC grading. 
  |  https://dx.doi.org/10.1007/s00330-019-06601-1  |  
------------------------------------------- 
10.1016/j.neuroimage.2020.116682  |   Recently, several magnetic resonance imaging (MRI) studies have reported time-of-day effects on brain structure and function. Due to the possibility that time-of-day effects reflect mechanisms of circadian regulation, the aim of this prospective study was to assess these effects while under strict experimental control of variables that might influence biological clocks, such as caffeine intake and exposure to blue-emitting light. In addition, the current study assessed whether time-of-day effects were driven by changes to extracellular space, by including estimations of non-Gaussian diffusion metrics obtained from diffusion kurtosis imaging, white matter tract integrity and the spherical mean technique, in addition to conventional diffusion tensor imaging -derived parameters. Participants were 47 healthy adults who underwent diffusion-weighted imaging in the morning and evening of the same day. Morning and evening scans were compared using voxel-wise tract based spatial statistics and permutation testing. A day of wakefulness was associated with widespread increases in fractional anisotropy, indices of kurtosis and indices of the axonal water fraction. In addition, wakefulness was associated with widespread decreases in radial diffusivity, both in the single compartment and in extra-axonal space. These results suggest that an increase in the intra-axonal space relative to the extra-axonal volume underlies time-of-day effects in human white matter, which is in line with activity-induced reductions to the extracellular volume. These findings provide important insight into possible mechanisms driving time-of-day effects in MRI. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(20)30169-5  |  
------------------------------------------- 
10.1016/j.chroma.2020.460871  |   Affinity adsorbents have been the cornerstone in protein purification. The selective nature of the molecular recognition interactions established between an affinity ligands and its target provide the basis for efficient capture and isolation of proteins. The plethora of affinity adsorbents available in the market reflects the importance of affinity chromatography in the bioseparation industry. Ligand discovery relies on the implementation of rational design techniques, which provides the foundation for the engineering of novel affinity ligands. The main goal for the design of affinity ligands is to discover or improve functionality, such as increased stability or selectivity. However, the methodologies must adapt to the current needs, namely to the number and diversity of biologicals being developed, and the availability of new tools for big data analysis and artificial intelligence. In this review, we offer an overview on the development of affinity ligands for bioseparation, including the evolution of rational design techniques, dating back to the years of early discovery up to the current and future trends in the field. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0021-9673(20)30036-4  |  
------------------------------------------- 
10.1002/jmv.25914  |   COVID-19 has a significant impact on public health and poses a challenge to medical staffs, especially to front-line medical staffs who are exposed to direct contact with patients. To understand the psychological stress status of medical staffs during the outbreak of COVID-19. Random sample questionnaire survey was conducted among 2110 medical staffs and 2158 college students in all provinces of china through a questionnaire which was compiled and completed through the Questionnaire Star platform relying on Wechat, QQ and other social software. The differences in psychological stress status of different groups were compared through the analysis of the questionnaire. Results revealed that in all provinces of china, medical staffs scored significantly higher on all items of psychological stress than college students(P&lt;0.001). In Wuhan, medical staff scored significantly higher than college students in all items of psychological stress(P&lt;0.001). While for medical staff, the group in Wuhan area scored significantly higher than the group outside Wuhan on "Thought of being in danger", "The possibility of self-illness", "Worrying about family infection"(P&lt;0.05), "Poor sleep quality", "Needing psychological guidance" and "Worrying about being infected"(P&lt;0.01) items in the psychological stress questionnaire, and in the item of "Confidence in the victory of the epidemic", the group in Wuhan area scored significantly lower than the group in the area outside Wuhan(P&lt;0.05). The emotion, cognition, physical and mental response of front-line medical staff showed obvious "exposure effect", and psychological crisis intervention strategy can be helpful. This article is protected by copyright. All rights reserved. 
  |  https://doi.org/10.1002/jmv.25914  |  
------------------------------------------- 
10.1186/s12911-019-1014-6  |    Background:  In classification and diagnostic testing, the receiver-operator characteristic (ROC) plot and the area under the ROC curve (AUC) describe how an adjustable threshold causes changes in two types of error: false positives and false negatives. Only part of the ROC curve and AUC are informative however when they are used with imbalanced data. Hence, alternatives to the AUC have been proposed, such as the partial AUC and the area under the precision-recall curve. However, these alternatives cannot be as fully interpreted as the AUC, in part because they ignore some information about actual negatives. 
  Methods:  We derive and propose a new concordant partial AUC and a new partial c statistic for ROC data-as foundational measures and methods to help understand and explain parts of the ROC plot and AUC. Our partial measures are continuous and discrete versions of the same measure, are derived from the AUC and c statistic respectively, are validated as equal to each other, and validated as equal in summation to whole measures where expected. Our partial measures are tested for validity on a classic ROC example from Fawcett, a variation thereof, and two real-life benchmark data sets in breast cancer: the Wisconsin and Ljubljana data sets. Interpretation of an example is then provided. 
  Results:  Results show the expected equalities between our new partial measures and the existing whole measures. The example interpretation illustrates the need for our newly derived partial measures. 
  Conclusions:  The concordant partial area under the ROC curve was proposed and unlike previous partial measure alternatives, it maintains the characteristics of the AUC. The first partial c statistic for ROC plots was also proposed as an unbiased interpretation for part of an ROC curve. The expected equalities among and between our newly derived partial measures and their existing full measure counterparts are confirmed. These measures may be used with any data set but this paper focuses on imbalanced data with low prevalence. 
  Future work:  Future work with our proposed measures may: demonstrate their value for imbalanced data with high prevalence, compare them to other measures not based on areas; and combine them with other ROC measures and techniques. 
  |  https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-1014-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31906931/  |  
------------------------------------------- 
10.1371/journal.pone.0229361  |    Background:  Online Cancer Support Groups (OCSG) are becoming an increasingly vital source of information, experiences and empowerment for patients with cancer. Despite significant contributions to physical, psychological and emotional wellbeing of patients, OCSG are yet to be formally recognised and used in multidisciplinary cancer support programs. This study highlights the opportunity of using Artificial Intelligence (AI) in OCSG to address psychological morbidity, with supporting empirical evidence from prostate cancer (PCa) patients. 
  Methods:  A validated framework of AI techniques and Natural Language Processing (NLP) methods, was used to investigate PCa patient activities based on conversations in ten international OCSG (18,496 patients- 277,805 conversations). The specific focus was on activities that indicate psychological morbidity; the reasons for joining OCSG, deep emotions and the variation from joining through to milestones in the cancer trajectory. Comparative analyses were conducted using t-tests, One-way ANOVA and Tukey-Kramer post-hoc analysis. 
  Findings:  PCa patients joined OCSG at four key phases of psychological distress; diagnosis, treatment, side-effects, and recurrence, the majority group was 'treatment' (61.72%). The four groups varied in expression of the intense emotional burden of cancer. The 'side-effects' group expressed increased negative emotions during the first month compared to other groups (p&lt;0.01). A comparison of pre-treatment vs post-treatment emotions showed that joining pre-treatment had significantly lower negative emotions after 12-months compared to post-treatment (p&lt;0.05). Long-term deep emotion analysis reveals that all groups except 'recurrence' improved in emotional wellbeing. 
  Conclusion:  This is the first empirical study of psychological morbidity and deep emotions expressed by men with a new diagnosis of cancer, using AI. PCa patients joining pre-treatment had improved emotions, and long-term participation in OCSG led to an increase in emotional wellbeing, indicating a decrease in psychological distress. It is opportune to further investigate AI in OCSG for early psychological intervention as an adjunct to conventional intervention programs. 
  |  http://dx.plos.org/10.1371/journal.pone.0229361  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32130256/  |  
------------------------------------------- 
10.1007/s00330-019-06553-6  |    Objectives:  We aimed to establish and validate an artificial intelligence-based radiomics strategy for predicting personalized responses of hepatocellular carcinoma (HCC) to first transarterial chemoembolization (TACE) session by quantitatively analyzing contrast-enhanced ultrasound (CEUS) cines. 
  Methods:  One hundred and thirty HCC patients (89 for training, 41 for validation), who received ultrasound examination (CEUS and B-mode) within 1 week before the first TACE session, were retrospectively enrolled. Ultrasonographic data was used for building and validating deep learning radiomics-based CEUS model (R-DLCEUS), machine learning radiomics-based time-intensity curve of CEUS model (R-TIC), and machine learning radiomics-based B-Mode images model (R-BMode), respectively, to predict responses (objective-response and non-response) to TACE with reference to modified response evaluation criteria in solid tumor. The performance of models was compared by areas under the receiver operating characteristic curve (AUC) and the DeLong test was used to compare different AUCs. The prediction robustness was assessed for each model. 
  Results:  AUCs of R-DLCEUS, R-TIC, and R-BMode were 0.93 (95% CI, 0.80-0.98), 0.80 (95% CI, 0.64-0.90), and 0.81 (95% CI, 0.67-0.95) in the validation cohort, respectively. AUC of R-DLCEUS shows significant difference compared with that of R-TIC (p = 0.034) and R-BMode (p = 0.039), whereas R-TIC was not significantly different from R-BMode. The performance was highly reproducible with different training and validation cohorts. 
  Conclusions:  DL-based radiomics method can effectively utilize CEUS cines to achieve accurate and personalized prediction. It is easy to operate and holds good potential for benefiting TACE candidates in clinical practice. 
  Key points:  • Deep learning (DL) radiomics-based CEUS model can accurately predict responses of HCC patients to their first TACE session by quantitatively analyzing their pre-operative CEUS cines. • The visualization of the 3D CNN analysis adopted in CEUS model provided direct insight into what computers "see" on CEUS cines, which can help people understand the interpretation of CEUS data. • The proposed prediction method is easy to operate and labor-saving for clinical practice, facilitating the clinical treatment decision of HCCs with very few time costs. 
  |  https://dx.doi.org/10.1007/s00330-019-06553-6  |  
------------------------------------------- 
10.1097/ICU.0000000000000647  |    Purpose of review:  To describe the VISION 2020: The Right to Sight, global initiative for the elimination of avoidable blindness, the contribution of glaucoma to the magnitude of global blindness and priorities going forward. 
  Recent findings:  Although the target of the World Health Organization's Global Action Plan (2014-2019) has not been met in terms of 25% reduction in avoidable blindness, there is evidence that the contribution of glaucoma to blindness and vision impairment is reducing. Yet this focus on a threshold of visual acuity by which to measure prevalence underestimates the true burden of glaucoma. Recent population-based studies demonstrate the scale of the unmet need. 
  Summary:  Scaling up of integrated people-centered eye care, by embedding glaucoma detection and care pathways in health systems with a strong focus on primary healthcare, is necessary. Solutions include reinforcing existing pathways while emphasizing high-quality glaucoma care, in addition to novel solutions such as self-testing, digital portable technology, artificial intelligence, and multilevel care pathways that extend to the most underserved parts of the global community. 
  |  http://dx.doi.org/10.1097/ICU.0000000000000647  |  
------------------------------------------- 
10.1126/sciadv.aay2378  |   Machine learning has been getting attention in recent years as a tool to process big data generated by the ubiquitous sensors used in daily life. High-speed, low-energy computing machines are in demand to enable real-time artificial intelligence processing of such data. These requirements challenge the current metal-oxide-semiconductor technology, which is limited by Moore's law approaching its end and the communication bottleneck in conventional computing architecture. Novel computing concepts, architectures, and devices are thus strongly needed to accelerate data-intensive applications. Here, we show that a cross-point resistive memory circuit with feedback configuration can train traditional machine learning algorithms such as linear regression and logistic regression in just one step by computing the pseudoinverse matrix of the data within the memory. One-step learning is further supported by simulations of the prediction of housing price in Boston and the training of a two-layer neural network for MNIST digit recognition. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32064342/  |  
------------------------------------------- 
10.1038/s41598-020-58467-9  |   Histopathological classification of gastric and colonic epithelial tumours is one of the routine pathological diagnosis tasks for pathologists. Computational pathology techniques based on Artificial intelligence (AI) would be of high benefit in easing the ever increasing workloads on pathologists, especially in regions that have shortages in access to pathological diagnosis services. In this study, we trained convolutional neural networks (CNNs) and recurrent neural networks (RNNs) on biopsy histopathology whole-slide images (WSIs) of stomach and colon. The models were trained to classify WSI into adenocarcinoma, adenoma, and non-neoplastic. We evaluated our models on three independent test sets each, achieving area under the curves (AUCs) up to 0.97 and 0.99 for gastric adenocarcinoma and adenoma, respectively, and 0.96 and 0.99 for colonic adenocarcinoma and adenoma respectively. The results demonstrate the generalisation ability of our models and the high promising potential of deployment in a practical histopathological diagnostic workflow system. 
  |  http://dx.doi.org/10.1038/s41598-020-58467-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32001752/  |  
------------------------------------------- 
10.1038/s41598-019-57272-3  |   Spreading rumors on the Internet has become increasingly pervasive due to the proliferation of online social media. This paper investigates how rumors are amplified by a group of users who share similar interests or views, dubbed as an echo chamber. To this end, we identify and analyze 'rumor' echo chambers, each of which is a group of users who have participated in propagating common rumors. By collecting and analyzing 125 recent rumors from six popular fact-checking sites, and their associated 289,202 tweets/retweets generated by 176,362 users, we find that the rumors that are spread by rumor echo chamber members tend to be more viral and quickly propagated than those that are not spread by echo chamber members. We propose the notion of an echo chamber network that represents relations among rumor echo chambers. By identifying the hub rumor echo chambers (in terms of connectivity to other rumor echo chambers) in the echo chamber network, we show that the top 10% of hub rumor echo chambers contribute to propagation of 24% rumors by eliciting more than 36% of retweets, implying that core rumor echo chambers significantly contribute to rumor spreads. 
  |  http://dx.doi.org/10.1038/s41598-019-57272-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31941980/  |  
------------------------------------------- 
10.3390/ijerph17082738  |   Recently, healthcare has undergone a sector-wide transformation thanks to advances in computing, networking technologies, big data, and artificial intelligence. Healthcare is not only changing from being reactive and hospital-centered to preventive and personalized, but it is also changing from being disease focused to well-being centered. Healthcare systems, as well as fundamental medicine research, are becoming smarter and enabled in biomedical engineering. This special issue on "Selected Papers from 2019 IEEE Eurasia Conference on Biomedical Engineering, Healthcare, and Sustainability (IEEE ECBIOS 2019)" selected nine excellent papers from 160 papers presented at IEEE ECBIOS 2019 on the topics of environmental health sciences and public health. Our aim is to encourage scientists to publish their experimental and theoretical research to promote scientific predictions and impact assessments of global change and development. 
  |  http://www.mdpi.com/resolver?pii=ijerph17082738  |  
------------------------------------------- 
10.1038/s41746-020-0232-8  |   Artificial intelligence (AI) algorithms continue to rival human performance on a variety of clinical tasks, while their actual impact on human diagnosticians, when incorporated into clinical workflows, remains relatively unexplored. In this study, we developed a deep learning-based assistant to help pathologists differentiate between two subtypes of primary liver cancer, hepatocellular carcinoma and cholangiocarcinoma, on hematoxylin and eosin-stained whole-slide images (WSI), and evaluated its effect on the diagnostic performance of 11 pathologists with varying levels of expertise. Our model achieved accuracies of 0.885 on a validation set of 26 WSI, and 0.842 on an independent test set of 80 WSI. Although use of the assistant did not change the mean accuracy of the 11 pathologists (<i>p</i> = 0.184, OR = 1.281), it significantly improved the accuracy (<i>p</i> = 0.045, OR = 1.499) of a subset of nine pathologists who fell within well-defined experience levels (GI subspecialists, non-GI subspecialists, and trainees). In the assisted state, model accuracy significantly impacted the diagnostic decisions of all 11 pathologists. As expected, when the model's prediction was correct, assistance significantly improved accuracy (<i>p</i> = 0.000, OR = 4.289), whereas when the model's prediction was incorrect, assistance significantly decreased accuracy (<i>p</i> = 0.000, OR = 0.253), with both effects holding across all pathologist experience levels and case difficulty levels. Our results highlight the challenges of translating AI models into the clinical setting, and emphasize the importance of taking into account potential unintended negative consequences of model assistance when designing and testing medical AI-assistance tools. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32140566/  |  
------------------------------------------- 
10.1002/cssc.202000778  |   To aid safe, more efficient, sustainable, even fully automated mode of production involving continuous-flow hydrogenation reactions, one of the most often used reactions in chemical synthesis, new catalyst types, immobilization methods, flow reactors and technologies have been developed in the last years, along with the new and transformational technologies in other fields such as artificial intelligence. Thus, improved performance of hydrogenation in flow mode such as reduced reaction times, higher selectivities and safe operations have led to increased attention from academic and industry practitioners. This review aims to summarize the most recent (2015-2020) research results on this topic with focuses on the advantages, current limitations and future directions of flow chemistry. 
  |  https://doi.org/10.1002/cssc.202000778  |  
------------------------------------------- 
10.1080/14740338.2020.1745184  |   <b>Introduction</b>: Increased mortality has been observed in patients with mental health disorders. Specifically, exposure to antipsychotic medications conveys a greater than 2 fold risk of sudden death, thought to be mediated through effects on QT prolongation and risk of torsades de pointes.<b>Areas covered</b>: We review the association between antipsychotic drugs and sudden cardiac death, the physiologic basis for these associations, assessment of patients at risk, and strategies to minimize risk of sudden cardiac death.<b>Expert opinion</b>: Despite the prevalence of antipsychotic medication use for many decades, there remain considerable challenges in reducing the associated risk of sudden cardiac death. A structured algorithm that incorporates patient clinical factors and antipsychotic drug factors may improve risk assessment and reduce the risk of adverse cardiac events. Future advancements in genetics and artificial intelligence may allow for enhanced risk stratification and predicting response (efficacy and adverse effects) to therapy. 
  |  http://www.tandfonline.com/doi/full/10.1080/14740338.2020.1745184  |  
------------------------------------------- 
10.1111/ceo.13716  |    |  https://doi.org/10.1111/ceo.13716  |  
------------------------------------------- 
10.1103/PhysRevLett.124.140504  |   Active learning is a machine learning method aiming at optimal design for model training. At variance with supervised learning, which labels all samples, active learning provides an improved model by labeling samples with maximal uncertainty according to the estimation model. Here, we propose the use of active learning for efficient quantum information retrieval, which is a crucial task in the design of quantum experiments. Meanwhile, when dealing with large data output, we employ active learning for the sake of classification with minimal cost in fidelity loss. Indeed, labeling only 5% samples, we achieve almost 90% rate estimation. The introduction of active learning methods in the data analysis of quantum experiments will enhance applications of quantum technologies. 
  |  http://link.aps.org/abstract/PRL/v124/p140504  |  
------------------------------------------- 
10.3390/s20071909  |   Conservation and restoration of cultural heritage is something more than a simple process of maintaining the existing. It is an integral part of the improvement of the cultural asset. The social context around the restoration shapes the specific actions. Today, preservation, restoration, enhancement of cultural heritage are increasingly a multidisciplinary science, meeting point of researchers coming from heterogeneous study areas. Data scientists and Information technology (IT) specialists are increasingly important. In this context, networks of a new generation of smart sensors integrated with data mining and artificial intelligence play a crucial role and aim to become the new skin of cultural assets. 
  |  http://www.mdpi.com/resolver?pii=s20071909  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32235461/  |  
------------------------------------------- 
10.1111/den.13674  |   Clinical outcome of upper gastrointestinal bleeding has improved due to advances in endoscopic therapy and standardized peri-endoscopy care. Apart from validated clinical scores, artificial intelligence-assisted machine learning model may play an important role in risk stratification. While standard endoscopic treatments remain irreplaceable, novel endoscopic modalities have changed the landscape of management. Over-the-scope clips have high success rate as rescue or even first line treatment in difficult-to-treat cases. Hemostatic powder is safe and easy-to-use, which can be useful as temporary control with its high immediate hemostatic ability. After endoscopic hemostasis, Doppler endoscopic probe can offer an objective measure to guide treatment endpoint. In refractory bleeding, angiographic embolization should be considered before salvage surgery. In variceal hemorrhage, banding ligation and glue injection are first line treatment options. Endoscopic ultrasound-guided therapy is gaining popularity due to its capability of precise localization for treatment targets. Self-expandable metal stent may be considered as an alternative option of balloon tamponade in refractory bleeding. Transjugular intrahepatic portosystemic shunting should be reserved as salvage therapy. In this article, we aim to provide an evidence-based comprehensive review of the major advancements in endoscopic hemostatic techniques and clinical outcomes. 
  |  https://doi.org/10.1111/den.13674  |  
------------------------------------------- 
10.3389/fnint.2020.00010  |   Selective attention plays an essential role in information acquisition and utilization from the environment. In the past 50 years, research on selective attention has been a central topic in cognitive science. Compared with unimodal studies, crossmodal studies are more complex but necessary to solve real-world challenges in both human experiments and computational modeling. Although an increasing number of findings on crossmodal selective attention have shed light on humans' behavioral patterns and neural underpinnings, a much better understanding is still necessary to yield the same benefit for intelligent computational agents. This article reviews studies of selective attention in unimodal visual and auditory and crossmodal audiovisual setups from the multidisciplinary perspectives of psychology and cognitive neuroscience, and evaluates different ways to simulate analogous mechanisms in computational models and robotics. We discuss the gaps between these fields in this interdisciplinary review and provide insights about how to use psychological findings and theories in artificial intelligence from different perspectives. 
  |  https://doi.org//10.3389/fnint.2020.00010  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32174816/  |  
------------------------------------------- 
10.1093/nar/gkaa030  |   Chromatin interaction data from protocols such as ChIA-PET, HiChIP and Hi-C provide valuable insights into genome organization and gene regulation, but can include spurious interactions that do not reflect underlying genome biology. We introduce an extension of the Irreproducible Discovery Rate (IDR) method called IDR2D that identifies replicable interactions shared by chromatin interaction experiments. IDR2D provides a principled set of interactions and eliminates artifacts from single experiments. The method is available as a Bioconductor package for the R community, as well as an online service at https://idr2d.mit.edu. 
  |  https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkaa030  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32009147/  |  
------------------------------------------- 
10.34133/2020/1762107  |   Optimization problems especially in a dynamic environment is a hot research area that has attracted notable attention in the past decades. It is clear from the dynamic optimization literatures that most of the efforts have been devoted to continuous dynamic optimization problems although the majority of the real-life problems are combinatorial. Moreover, many algorithms shown to be successful in stationary combinatorial optimization problems commonly have mediocre performance in a dynamic environment. In this study, based on binary wolf pack algorithm (BWPA), combining with flexible population updating strategy, a flexible binary wolf pack algorithm (FWPA) is proposed. Then, FWPA is used to solve a set of static multidimensional knapsack benchmarks and several dynamic multidimensional knapsack problems, which have numerous practical applications. To the best of our knowledge, this paper constitutes the first study on the performance of WPA on a dynamic combinatorial problem. By comparing two state-of-the-art algorithms with the basic BWPA, the simulation experimental results demonstrate that FWPA can be considered as a feasibility and competitive algorithm for dynamic optimization problems. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32159160/  |  
------------------------------------------- 
10.1109/TCBB.2018.2870363  |   We build personalized relevance parameterization method (prep-ad) based on artificial intelligence (ai) techniques to compute Alzheimer's disease (ad) progression for patients at the mild cognitive impairment (mci) stage. Expressions of ad related genes, mini mental state examination (mmse) scores, and hippocampal volume measurements of mci patients are obtained from the Alzheimer's Disease Neuroimaging Initiative (adni) database. In evaluation of cognitive changes under pharmacological therapies, patients are grouped based on available clinical measurements and the type of therapy administered, namely donepezil monotherapy and polytherapy of donepezil with memantine. Average leave one out cross validation (loocv) error rates are calculated for prep-ad results as less than 8 percent when mmse scores are used to compute disease progression for a 60 month period, and 3 percent with hippocampal volume measurements for 12 months. Statistical significance is calculated as p = 0.003 for using ad related genes in disease progression and as for the results computed by prep-ad. These relatively small average loocv errors and p-values suggest that our prep-ad methods employing gene expressions, mmse scores and hippocampal volume loss measurements can be useful in supporting pharmacologic therapy decisions during early stages of ad. 
  |  https://dx.doi.org/10.1109/TCBB.2018.2870363  |  
------------------------------------------- 
10.1158/1055-9965.EPI-20-0074  |   Despite significant investment of funds and resources, few new cancer biomarkers have been introduced to the clinic in the last few decades. Even though many candidates produce promising results in the laboratory, deficiencies in sensitivity, specificity and predictive value make them less than desirable in a patient setting. This review will analyze these challenges in detail as well as discuss false discovery, problems with reproducibility and tumor heterogeneity. Circulating tumor DNA (ctDNA), an emerging cancer biomarker is also analyzed, particularly in the contexts of assay specificity, sensitivity, fragmentation, lead time, mutant allele fraction and clinical relevance. Emerging artificial intelligence technologies will likely be valuable tools in maximizing the clinical utility of ctDNA which is often found in very small quantities in patients with early stage tumors. Finally, the implications of challenging false discoveries are examined and some insights about improving cancer biomarker discovery are provided. 
  |  http://cebp.aacrjournals.org/cgi/pmidlookup?view=long&pmid=32277003  |  
------------------------------------------- 
10.1111/medu.14189  |   Despite advances in artificial intelligence-based diagnostics, ophthalmic clinical skills remain an important acquisition during medical school. Simple ophthalmic examination techniques allow future non-ophthalmic physicians to make timely referrals to ophthalmologists for sight-threatening disease. Currently, the coronavirus disease 2019 (COVID-19) pandemic poses a serious public health crisis worldwide and an immediate challenge to traditional methods of medical education. 
  |  https://doi.org/10.1111/medu.14189  |  
------------------------------------------- 
10.1097/ACO.0000000000000850  |    Purpose of review:  The goal of this review is to describe the recent improvements in clinical decision tools applied to the increasingly large and complex datasets in the pediatric ambulatory and inpatient setting. 
  Recent findings:  Clinical decision support has evolved beyond simple static alerts to complex dynamic alerts for: diagnosis, medical decision-making, monitoring of physiological, laboratory, and pharmacologic inputs, and adherence to institutional and national guidelines for both the patient and the healthcare team. Artificial intelligence and machine learning have enabled advances in predicting outcomes, such as sepsis and early deterioration, and assisting in procedural technique. 
  Summary:  With more than a decade of electronic medical data generation, clinical decision support tools have begun to evolve into more sophisticated and complex algorithms capable of transforming large datasets into succinct, timely, and pertinent summaries for treatment and management of pediatric patients. Future developments will need to leverage patient-generated health data, integrated device data, and provider-entered data to complete the continuum of patient care and will likely demonstrate improvements in patient outcomes. 
  |  http://dx.doi.org/10.1097/ACO.0000000000000850  |  
------------------------------------------- 
10.1016/j.neunet.2020.03.020  |   Graph based multi-view learning is well known due to its effectiveness and good clustering performance. However, most existing methods directly construct graph from original high-dimensional data which always contain redundancy, noise and outlying entries in real applications, resulting in unreliable and inaccurate graph. Moreover, they do not effectively select some useful features which are important for graph learning and clustering. To solve these limits, we propose a novel model that combines dimensionality reduction, manifold structure learning and feature selection into a framework. We map high-dimensional data into low-dimensional space to reduce the complexity of the algorithm and reduce the effect of noise and redundance. Therefore, we can adaptively learn a more accurate graph. Further more, ℓ<sub>21</sub>-norm regularization is adopted to adaptively select some important features which help improve clustering performance. Finally, an efficiently algorithm is proposed to solve the optimal solution. Extensive experimental results on some benchmark datasets demonstrate the superiority of the proposed method. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30097-6  |  
------------------------------------------- 
10.1021/acs.jcim.9b01121  |   Cross-validation is used to determine the validity of a model on unseen data by assessing if the model is overfitted to noise. It is widely used in many fields, from artificial intelligence to structural biology in X-ray crystallography and nuclear magnetic resonance. Although there are concerns of map overfitting in cryo-electron microscopy (cryo-EM), cross-validation is rarely used. The problem is that establishing a performance metric of the maps over unseen data (given by 2D-projection images) is difficult due to the low signal-to-noise ratios in the individual particles. Here, I present recent advances for cryo-EM map reconstruction. I highlight that the gold-standard procedure can fail to detect map overfitting in certain cases, showing the necessity of assessing the map quality on unbiased data. Finally, I describe the challenges and advantages of developing a robust cross-validation methodology for cryo-EM. 
  |  https://dx.doi.org/10.1021/acs.jcim.9b01121  |  
------------------------------------------- 
10.1109/TCYB.2019.2956975  |   Reasoning is one of the central topics in artificial intelligence. As an important reasoning paradigm, entailment recognition has attracted much research interest, which judges if a hypothesis can be inferred from given premises. However, existing research mainly focuses on text-based analysis, that is, recognizing textual entailment (RTE), which limits its depth and width. Actually, the knowledge and inference process of human are across different sensory organs like language and vision, with unique perspectives to represent complementary reasoning cues. It is significant to extend existing entailment recognition research to cross-media scenarios, that is, recognizing cross-media entailment (RCE). Therefore, this article focuses on one representative RCE task: visual-textual reasoning, and proposes the visual-textual hybrid sequence matching (VHSM) approach. VHSM can reason from image-text premises to text hypotheses, whose contributions are: 1) visual-textual hybrid multicontext inference is proposed to address RCE via matching with hybrid context embeddings, along with adaptive gated aggregation to obtain the final prediction results. It can fully exploit complementary visual-textual cue interaction during joint reasoning; 2) memory attention-based context embedding is proposed to sequentially encode hybrid context embeddings, with the memory attention networks to compare neighboring time-steps. This can capture the important memory dimensions by coefficient assignment, which fully exploits the visual-textual context correlation; and 3) cross-task and visual-textual transfer strategy is further proposed to enrich correlation training information for boosting reasoning accuracy, which transfers knowledge not only from cross-media retrieval task to RCE but also between corresponding text and image premises. The experimental results of recognizing visual-textual entailment task on the SNLI dataset verify the effectiveness of VHSM. 
  |  https://dx.doi.org/10.1109/TCYB.2019.2956975  |  
------------------------------------------- 
10.1109/JBHI.2020.2980262  |   The medical and machine learning communities are relying on the promise of artificial intelligence (AI) to transform medicine through enabling more accurate decisions and personalized treatment. However, progress is slow. Legal and ethical issues around unconsented patient data and privacy is one of the limiting factors in data sharing, resulting in a significant barrier in accessing routinely collected electronic health records (EHR) by the machine learning community. We propose a novel framework for generating synthetic data that closely approximates the joint distribution of variables in an original EHR dataset, providing a readily accessible, legally and ethically appropriate solution to support more open data sharing, enabling the development of AI solutions. In order to address issues around lack of clarity in defining sufficient anonymization, we created a quantifiable, mathematical definition for "identifiability". We used a conditional generative adversarial networks (GAN) framework to generate synthetic data while minimize patient identifiability that is defined based on the probability of re-identification given the combination of all data on any individual patient. We compared models fitted to our synthetically generated data to those fitted to the real data across four independent datasets to evaluate similarity in model performance, while assessing the extent to which original observations can be identified from the synthetic data. Our model, ADS-GAN, consistently outperformed state-of-the-art methods, and demonstrated reliability in the joint distributions. We propose that this method could be used to develop datasets that can be made publicly available while considerably lowering the risk of breaching patient confidentiality. 
  |  https://dx.doi.org/10.1109/JBHI.2020.2980262  |  
------------------------------------------- 
10.1097/RTI.0000000000000483  |   Coronary computed tomography angiography (cCTA) is a reliable and clinically proven method for the evaluation of coronary artery disease. cCTA data sets can be used to derive fractional flow reserve (FFR) as CT-FFR. This method has respectable results when compared in previous trials to invasive FFR, with the aim of detecting lesion-specific ischemia. Results from previous studies have shown many benefits, including improved therapeutic guidance to efficiently justify the management of patients with suspected coronary artery disease and enhanced outcomes and reduced health care costs. More recently, a technical approach to the calculation of CT-FFR using an artificial intelligence deep machine learning (ML) algorithm has been introduced. ML algorithms provide information in a more objective, reproducible, and rational manner and with improved diagnostic accuracy in comparison to cCTA. This review gives an overview of the technical background, clinical validation, and implementation of ML applications in CT-FFR. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000483  |  
------------------------------------------- 
10.1039/c9fo02076d  |   A number of major challenges facing modern society are related to the food supply. As the global population grows, it will be critical to feed everyone without damaging the environment. Advances in biotechnology, nanotechnology, structural design, and artificial intelligence are providing farmers and food manufacturers will new tools to address these problems. More and more people are migrating from rural to urban environments, leading to a change in their dietary habits, especially increasing consumption of animal-based products and highly-processed foods. Animal-based foods lead to more greenhouse gas production, land use, water use, and pollution than plant-based ones. Moreover, many animal-based and highly-processed foods have adverse effects on human health and wellbeing. Consumers are therefore being encouraged to consume more plant-based foods, such as fruits, vegetables, cereals, and legumes. Many people, however, do not have the time, money, or inclination to prepare foods from fresh produce. Consequently, there is a need for the food industry to create a new generation of processed foods that are desirable, tasty, inexpensive, and convenient, but that are also healthy and sustainable. This article highlights some of the main food-related challenges faced by modern society and how scientists are developing innovative technologies to address them. 
  |  https://doi.org/10.1039/c9fo02076d  |  
------------------------------------------- 
10.1097/YCO.0000000000000609  |    Purpose of review:  In recent years there has been interest in the use of machine learning in suicide research in reaction to the failure of traditional statistical methods to produce clinically useful models of future suicide. The current review summarizes recent prediction studies in the suicide literature including those using machine learning approaches to understand what value these novel approaches add. 
  Recent findings:  Studies using machine learning to predict suicide deaths report area under the curve that are only modestly greater than, and sensitivities that are equal to, those reported in studies using more conventional predictive methods. Positive predictive value remains around 1% among the cohort studies with a base rate that was not inflated by case-control methodology. 
  Summary:  Machine learning or artificial intelligence may afford opportunities in mental health research and in the clinical care of suicidal patients. However, application of such techniques should be carefully considered to avoid repeating the mistakes of existing methodologies. Prediction studies using machine-learning methods have yet to make a major contribution to our understanding of the field and are unproven as clinically useful tools. 
  |  http://dx.doi.org/10.1097/YCO.0000000000000609  |  
------------------------------------------- 
10.1007/s10278-019-00293-1  |   As resources in the healthcare environment continue to wane, leaders are seeking ways to continue to provide quality care bounded by the constraints of a reduced budget. This manuscript synthesizes the experience from a number of institutions to provide the healthcare leadership with an understanding of the value of an enterprise imaging program. The value of such a program extends across the entire health system. It leads to operational efficiencies through infrastructure and application consolidation and the creation of focused support capabilities with increased depth of skill. An enterprise imaging program provides a centralized foundation for all phases of image management from every image-producing specialty. Through centralization, standardized image exchange functions can be provided to all image producers. Telehealth services can be more tightly integrated into the electronic medical record. Mobile platforms can be utilized for image viewing and sharing by patients and providers. Mobile tools can also be utilized for image upload directly into the centralized image repository. Governance and data standards are more easily distributed, setting the stage for artificial intelligence and data analytics. Increased exposure to all image producers provides opportunities for cybersecurity optimization and increased awareness. 
  |  https://doi.org/10.1007/s10278-019-00293-1  |  
------------------------------------------- 
10.1701/3315.32853  |   Radiomics is a new frontier of medicine based on the extraction of quantitative data from radiological images which can not be seen by radiologist's naked eye and on the use of these data for the creation of clinical decision support systems. The long-term goal of radiomics is to improve the non-invasive diagnosis of focal and diffuse diseases of different organs by understanding links between extracted quantitative imaging data and the underlying molecular and pathological characteristics of lesions. In the last decade, several studies have highlighted the enormous potential of radiomics in both tumoral and non-tumoral diseases of many organs and systems including brain, lung, breast, gastrointestinal and genitourinary tracts. The enormous potential of radiomics needs to be pursued with the methodological rigor of scientific research and by integrating radiological data with other medical disciplines, in order to improve personalized patient management. 
  |  https://doi.org/10.1701/3315.32853  |  
------------------------------------------- 
10.1016/j.earlhumdev.2020.105024  |   Science fiction is all around us, manifesting in fiction, TV series, blockbuster movies etc. The Star Trek (ST) universe has become an integral element of popular culture and doctors play important roles. This paper introduces depictions of these individuals over the decades since the inception of the series in 1966. The doctors portrayed have reflected the shifting expectations of the general public in that medics have morphed successively from an old-style country doctor, to a single supermom, to a genetically engineered human, to a sentient, computer-generated hologram and to an alien who uses also uses natural healing methods. The doctor in the latest ST series has broken another barrier in that this is the first series to deliberately include homosexual couples within the Star Trek universe for the first time in its fifty-one odd year lifespan and the doctor is the first openly gay character. These doctors are expected to demonstrate total accessibility, the ability to utilise natural remedies when possible, compassion and unstinting commitment to their patients and their profession, infallibility and broad skills with flexibility that allows them to deal with virtually anything, in anyone/anything. These capacities appear desirable even if the traditional doctor is replaced by a machine, a warning for the medical profession. A second collection of papers will further explore the ST universe by analysing unethical medical experimentation, artificial intelligence (AI) and the institution of ethics in AI, the application of nanotechnology in biology and depictions of the nursing profession in this fictive future. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-3782(20)30187-0  |  
------------------------------------------- 
10.3389/fpsyt.2020.00045  |   Large-scale screening for depression has been using norms developed based on a given population at a given time. Researchers have attempted to adjust the cutoff scores over time and for different populations, but such efforts are too few and far in between to be sensitive to temporal and regional variations. In this study, we proposed an unsupervised machine learning approach to constructing depression classifications to overcome the limitations of the traditional norm-based method. Data were collected from 8,063 Chinese middle and high school students. Using k-means clustering, we generated four levels of depressive symptoms to match the norm-based classifications. We then evaluated the validity of the classifications by comparing them with the norm-based method (and its variations) in terms of their robustness, model performance (accuracy, AUC, and sensitivity), and convergent construct validity (i.e., associations with known correlates). The results showed that our automatic classification system performed well as compared to the norm-based method. 
  |  https://doi.org/10.3389/fpsyt.2020.00045  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32116859/  |  
------------------------------------------- 
10.2106/JBJS.19.01128  |    |  http://dx.doi.org/10.2106/JBJS.19.01128  |  
------------------------------------------- 
10.1542/peds.2019-2056G  |   As avid users of technology, adolescents are a key demographic to engage when designing and developing technology applications for health. There are multiple opportunities for improving adolescent health, from promoting preventive behaviors to providing guidance for adolescents with chronic illness in supporting treatment adherence and transition to adult health care systems. This article will provide a brief overview of current technologies and then highlight new technologies being used specifically for adolescent health, such as artificial intelligence, virtual and augmented reality, and machine learning. Because there is paucity of evidence in this field, we will make recommendations for future research. 
  |  http://pediatrics.aappublications.org/cgi/pmidlookup?view=long&pmid=32358210  |  
------------------------------------------- 
10.1016/j.hrthm.2020.02.023  |   The adoption of wearables in medicine has rapidly expanded worldwide. New generations of wearables are emerging, driven by consumers' demand to monitor their own health. With the ongoing development of new features capable of assessing real-time biometric data, the impact of wearables on cardiovascular management has become inevitable. Smartwatches, among other wearable devices, offer a user-friendly noninvasive approach to continuously monitor for health parameters. With advancements in artificial intelligence, the photoplethysmography-generated pulse waveform has the potential to accurately detect episodes of atrial fibrillation and one day could replace conventional diagnostic and long-term monitoring methods. Clinical benefits that could arise from the use of such devices include refining stroke prevention strategies, personalizing AF management, and optimizing the patient-physician relationship. Wearables are changing not only the way clinicians conduct research but also the future of cardiovascular preventive and therapeutic care. As such, wearables are here to stay. 
  |  None  |  
------------------------------------------- 
10.1007/978-1-0716-0282-9_1  |   Drug discovery is an expensive, time-consuming, and risky business. To avoid late-stage failure, learnings from past projects and the development of new approaches are crucial. New modalities and emerging new target spaces allow the exploration of unprecedented indications or to address so far undrugable targets. Late-stage attrition is usually attributed to the lack of efficacy or to compound-related safety issues. Efficacy has been shown to be related to a strong genetic link to human disease, a better understanding of the target biology, and the availability of biomarkers to bridge from animals to humans. Compound safety can be improved by ligand optimization, which is becoming increasingly demanding for difficult targets. Therefore, new strategies include the design of allosteric ligands, covalent binders, and other modalities. Design methods currently heavily rely on artificial intelligence and advanced computational methods such as free energy calculations and quantum chemistry. Especially for quantum chemical methods, a more detailed overview is given in this chapter. 
  |  https://dx.doi.org/10.1007/978-1-0716-0282-9_1  |  
------------------------------------------- 
10.1109/TPAMI.2020.2975796  |   Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Along this new direction, we investigate the behavior of our proposed convolutional classification layer and conduct detailed analysis. Based on our in-depth analysis, we further propose convolutional classification layers without weight-sharing. This new classification layer achieves a good trade-off between fully-connected classification layers and the convolutional classification layer. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods. 
  |  https://dx.doi.org/10.1109/TPAMI.2020.2975796  |  
------------------------------------------- 
10.3389/fnhum.2020.00070  |   Recent success stories in automated object or face recognition, partly fuelled by deep learning artificial neural network (ANN) architectures, have led to the advancement of biometric research platforms and, to some extent, the resurrection of Artificial Intelligence (AI). In line with this general trend, inter-disciplinary approaches have been taken to automate the recognition of emotions in adults or children for the benefit of various applications, such as identification of children's emotions prior to a clinical investigation. Within this context, it turns out that automating emotion recognition is far from being straightforward, with several challenges arising for both science (e.g., methodology underpinned by psychology) and technology (e.g., the iMotions biometric research platform). In this paper, we present a methodology and experiment and some interesting findings, which raise the following research questions for the recognition of emotions and attention in humans: (a) the adequacy of well-established techniques such as the International Affective Picture System (IAPS), (b) the adequacy of state-of-the-art biometric research platforms, (c) the extent to which emotional responses may be different in children and adults. Our findings and first attempts to answer some of these research questions are based on a mixed sample of adults and children who took part in the experiment, resulting in a statistical analysis of numerous variables. These are related to both automatically and interactively captured responses of participants to a sample of IAPS pictures. 
  |  https://doi.org/10.3389/fnhum.2020.00070  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32317947/  |  
------------------------------------------- 
10.2196/14122  |    Background:  With the increasing number of cancer treatments, the emergence of multidisciplinary teams (MDTs) provides patients with personalized treatment options. In recent years, artificial intelligence (AI) has developed rapidly in the medical field. There has been a gradual tendency to replace traditional diagnosis and treatment with AI. IBM Watson for Oncology (WFO) has been proven to be useful for decision-making in breast cancer and lung cancer, but to date, research on gastric cancer is limited. 
  Objective:  This study compared the concordance of WFO with MDT and investigated the impact on patient prognosis. 
  Methods:  This study retrospectively analyzed eligible patients (N=235) with gastric cancer who were evaluated by an MDT, received corresponding recommended treatment, and underwent follow-up. Thereafter, physicians inputted the information of all patients into WFO manually, and the results were compared with the treatment programs recommended by the MDT. If the MDT treatment program was classified as "recommended" or "considered" by WFO, we considered the results concordant. All patients were divided into a concordant group and a nonconcordant group according to whether the WFO and MDT treatment programs were concordant. The prognoses of the two groups were analyzed. 
  Results:  The overall concordance of WFO and the MDT was 54.5% (128/235) in this study. The subgroup analysis found that concordance was less likely in patients with human epidermal growth factor receptor 2 (HER2)-positive tumors than in patients with HER2-negative tumors (P=.02). Age, Eastern Cooperative Oncology Group performance status, differentiation type, and clinical stage were not found to affect concordance. Among all patients, the survival time was significantly better in concordant patients than in nonconcordant patients (P&lt;.001). Multivariate analysis revealed that concordance was an independent prognostic factor of overall survival in patients with gastric cancer (hazard ratio 0.312 [95% CI 0.187-0.521]). 
  Conclusions:  The treatment recommendations made by WFO and the MDT were mostly concordant in gastric cancer patients. If the WFO options are updated to include local treatment programs, the concordance will greatly improve. The HER2 status of patients with gastric cancer had a strong effect on the likelihood of concordance. Generally, survival was better in concordant patients than in nonconcordant patients. 
  |  https://www.jmir.org/2020/2/e14122/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32130123/  |  
------------------------------------------- 
10.4149/BLL_2020_013  |   Vaclav Trnka from Křovice (1739-1791, in Latin: Wenzel Trnka Krzowitz) was a remarkable physician whose life serves as an example in the history of medicine by connecting major capital cities of Central Europe. In view of current geographical layout, he was born and brought up in the Czech Republic, graduated from University of Vienna in Austria, and was appointed Professor of the Anatomy at the newly established Faculty of Medicine of University of Nagyszombat, presently Trnava in Slovak Republic. When the University moved to Buda and later to Pest (today Budapest, Hungary), he was the first educator to introduce anatomy as a medical subject to be taught in a Hungarian medical school. He also was elected the Dean of Faculty of Medicine three times and in 1786-1787 he acted as Rector of then the Royal University of Pest. During his life, he published twenty-seven monographs dealing with different areas of clinical medicine, such as malaria (intermittent fever), diabetes, and rickets. Based on these monographs we can proclaim that Václav Trnka was a co-founder of modern infectology, diabetology and ophthalmology in Central Europe. Nowadays, artificial intelligence and bioinformatics are inseparable parts of modern health care system which help the transformation of big data into valuable knowledge. In the 18th century, Professor Trnka owned more than 3,000 scientific books and had natural, innate intelligence and wisdom which made him a real "medical polymath". As a musician, Trnka also composed sixty-one canons, two of them long wrongly considered as Mozart's work. Despite the fact that Trnka is considered to be the founder of Hungarian anatomy education and a major medical figure of the eighteenth century Central Europe, no internationally acclaimed biographical record of his life or work has so far been published in English. Therefore, we would like to reintroduce Václav Trnka both as an anatomist and medical polymath, and to give an overview of the early days of anatomy teaching in present-day Slovakia and Hungary (Fig. 1, Ref. 27). Keywords: Trnka from Křovice, anatomist, medical polymath, history of medicine. 
  |  http://www.aepress.sk/_downloads/dl.php?from=pubmed&journal=BLL&file=2020_01_96.pdf  |  
------------------------------------------- 
10.1038/s41746-020-0249-z  |   Hospital systems, payers, and regulators have focused on reducing length of stay (LOS) and early readmission, with uncertain benefit. Interpretable machine learning (ML) may assist in transparently identifying the risk of important outcomes. We conducted a retrospective cohort study of hospitalizations at a tertiary academic medical center and its branches from January 2011 to May 2018. A consecutive sample of all hospitalizations in the study period were included. Algorithms were trained on medical, sociodemographic, and institutional variables to predict readmission, length of stay (LOS), and death within 48-72 h. Prediction performance was measured by area under the receiver operator characteristic curve (AUC), Brier score loss (BSL), which measures how well predicted probability matches observed probability, and other metrics. Interpretations were generated using multiple feature extraction algorithms. The study cohort included 1,485,880 hospitalizations for 708,089 unique patients (median age of 59 years, first and third quartiles (QI) [39, 73]; 55.6% female; 71% white). There were 211,022 30-day readmissions for an overall readmission rate of 14% (for patients ≥65 years: 16%). Median LOS, including observation and labor and delivery patients, was 2.94 days (QI [1.67, 5.34]), or, if these patients are excluded, 3.71 days (QI [2.15, 6.51]). Predictive performance was as follows: 30-day readmission (AUC 0.76/BSL 0.11); LOS &gt; 5 days (AUC 0.84/BSL 0.15); death within 48-72 h (AUC 0.91/BSL 0.001). Explanatory diagrams showed factors that impacted each prediction. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32285012/  |  
------------------------------------------- 
10.3390/s20051442  |   Cognitive Application Program Interface (API) is an API of emerging artificial intelligence (AI)-based cloud services, which extracts various contextual information from non-numerical multimedia data including image and audio. Our interest is to apply image-based cognitive APIs to implement flexible and efficient context sensing services in a smart home. In the existing approach with machine learning by us, with the complexity of recognition object and the number of the defined contexts increases by users, it still requires directly manually labeling a moderate scale of data for training and continually try to calling multiple cognitive APIs for feature extraction. In this paper, we propose a novel method that uses a small scale of labeled data to evaluate the capability of cognitive APIs in advance, before training features of the APIs with machine learning, for the flexible and efficient home context sensing. In the proposed method, we exploit document similarity measures and the concepts (i.e., internal cohesion and external isolation) integrate into clustering results, to see how the capability of different cognitive APIs for recognizing each context. By selecting the cognitive APIs that relatively adapt to the defined contexts and data based on the evaluation results, we have achieved the flexible integration and efficient process of cognitive APIs for home context sensing. 
  |  http://www.mdpi.com/resolver?pii=s20051442  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32155806/  |  
------------------------------------------- 
10.1016/j.stem.2020.02.013  |   The etiology of aortic aneurysms is poorly understood, but it is associated with atherosclerosis, hypercholesterolemia, and abnormal transforming growth factor β (TGF-β) signaling in smooth muscle. Here, we investigated the interactions between these different factors in aortic aneurysm development and identified a key role for smooth muscle cell (SMC) reprogramming into a mesenchymal stem cell (MSC)-like state. SMC-specific ablation of TGF-β signaling in Apoe<sup>-/-</sup> mice on a hypercholesterolemic diet led to development of aortic aneurysms exhibiting all the features of human disease, which was associated with transdifferentiation of a subset of contractile SMCs into an MSC-like intermediate state that generated osteoblasts, chondrocytes, adipocytes, and macrophages. This combination of medial SMC loss with marked increases in non-SMC aortic cell mass induced exuberant growth and dilation of the aorta, calcification and ossification of the aortic wall, and inflammation, resulting in aneurysm development. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1934-5909(20)30066-7  |  
------------------------------------------- 
10.1186/s41747-020-00150-9  |   PRIMAGE is one of the largest and more ambitious research projects dealing with medical imaging, artificial intelligence and cancer treatment in children. It is a 4-year European Commission-financed project that has 16 European partners in the consortium, including the European Society for Paediatric Oncology, two imaging biobanks, and three prominent European paediatric oncology units. The project is constructed as an observational in silico study involving high-quality anonymised datasets (imaging, clinical, molecular, and genetics) for the training and validation of machine learning and multiscale algorithms. The open cloud-based platform will offer precise clinical assistance for phenotyping (diagnosis), treatment allocation (prediction), and patient endpoints (prognosis), based on the use of imaging biomarkers, tumour growth simulation, advanced visualisation of confidence scores, and machine-learning approaches. The decision support prototype will be constructed and validated on two paediatric cancers: neuroblastoma and diffuse intrinsic pontine glioma. External validation will be performed on data recruited from independent collaborative centres. Final results will be available for the scientific community at the end of the project, and ready for translation to other malignant solid tumours. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32246291/  |  
------------------------------------------- 
10.31616/asj.2020.0147  |   Artificial neural networks (ANNs) have been used in a wide variety of real-world applications and it emerges as a promising field across various branches of medicine. This review aims to identify the role of ANNs in spinal diseases. Literature were searched from electronic databases of Scopus and Medline from 1993 to 2020 with English publications reported on the application of ANNs in spinal diseases. The search strategy was set as the combinations of the following keywords: "artificial neural networks," "spine," "back pain," "prognosis," "grading," "classification," "prediction," "segmentation," "biomechanics," "deep learning," and "imaging." The main findings of the included studies were summarized, with an emphasis on the recent advances in spinal diseases and its application in the diagnostic and prognostic procedures. According to the search strategy, a set of 3,653 articles were retrieved from Medline and Scopus databases. After careful evaluation of the abstracts, the full texts of 89 eligible papers were further examined, of which 79 articles satisfied the inclusion criteria of this review. Our review indicates several applications of ANNs in the management of spinal diseases including (1) diagnosis and assessment of spinal disease progression in the patients with low back pain, perioperative complications, and readmission rate following spine surgery; (2) enhancement of the clinically relevant information extracted from radiographic images to predict Pfirrmann grades, Modic changes, and spinal stenosis grades on magnetic resonance images automatically; (3) prediction of outcomes in lumbar spinal stenosis, lumbar disc herniation and patient-reported outcomes in lumbar fusion surgery, and preoperative planning and intraoperative assistance; and (4) its application in the biomechanical assessment of spinal diseases. The evidence suggests that ANNs can be successfully used for optimizing the diagnosis, prognosis and outcome prediction in spinal diseases. Therefore, incorporation of ANNs into spine clinical practice may improve clinical decision making. 
  |  http://asianspinejournal.org/journal/view.php?doi=10.31616/asj.2020.0147  |  
------------------------------------------- 
10.21037/atm.2020.02.183  |    Background:  Accurate thymoma staging via computed tomography (CT) images is difficult even for experienced thoracic doctors. Here we developed a preoperative staging tool differentiating Masaoka-Koga (MK) stage I patients from stage II patients using CT images. 
  Methods:  CT images of 174 thymoma patients were retrospectively selected. Two chest radiologists independently assessed the images. Variables with statistical differences in univariate analysis were adjusted for age, sex, and smoking history in multivariate logical regression to determine independent predictors of the thymoma stage. We established a deep learning (DL) 3D-DenseNet model to distinguish the MK stage I and stage II thymomas. Furthermore, we compared two different methods to label the regions of interest (ROI) in CT images. 
  Results:  In routine CT images, there were statistical differences (P&lt;0.05) in contour, necrosis, cystic components, and the degree of enhancement between stage I and II disease. Multivariate logical regression showed that only the degree of enhancement was an independent predictor of the thymoma stage. The area under the receiver operating characteristic curve (AUC) of routine CT images for classifying thymoma as MK stage I or II was low (AUC =0.639). The AUC of the 3D-DenseNet model showed better performance with a higher AUC (0.773). ROIs outlined by segmentation labels performed better (AUC =0.773) than those outlined by bounding box labels (AUC =0.722). 
  Conclusions:  Our DL 3D-DenseNet may aid thymoma stage classification, which may ultimately guide surgical treatment and improve outcomes. Compared with conventional methods, this approach provides improved staging accuracy. Moreover, ROIs labeled by segmentation is more recommendable when the sample size is limited. 
  |  https://doi.org/10.21037/atm.2020.02.183  |  
------------------------------------------- 
10.1016/j.ijmedinf.2020.104143  |    Objective:  The objective of this study is to apply machine learning algorithms for real-time and personalized waiting time prediction in emergency departments. We also aim to introduce the concept of systems thinking to enhance the performance of the prediction models. 
  Methods:  Four popular algorithms were applied: (i) stepwise multiple linear regression; (ii) artificial neural networks; (iii) support vector machines; and (iv) gradient boosting machines. A linear regression model served as a baseline model for comparison. We conducted computational experiments based on a dataset collected from an emergency department in Hong Kong. Model diagnostics were performed, and the results were cross-validated. 
  Results:  All the four machine learning algorithms with the use of systems knowledge outperformed the baseline model. The stepwise multiple linear regression reduced the mean-square error by almost 15%. The other three algorithms had similar performances, reducing the mean-square error by approximately 20%. Reductions of 17 - 22% in mean-square error due to the utilization of systems knowledge were observed. 
  Discussion:  The multi-dimensional stochasticity arising from the ED environment imposes a great challenge on waiting time prediction. The introduction of the concept of systems thinking led to significant enhancements of the models, suggesting that interdisciplinary efforts could potentially improve prediction performance. 
  Conclusion:  Machine learning algorithms with the utilization of the systems knowledge could significantly improve the performance of waiting time prediction. Waiting time prediction for less urgent patients is more challenging. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1386-5056(19)30965-7  |  
------------------------------------------- 
10.1002/mp.14114  |    Purpose:  In the treatment planning process of intensity-modulated radiation therapy (IMRT), a human planner operates the treatment planning system (TPS) to adjust treatment planning parameters, for example, dose volume histogram (DVH) constraints' locations and weights, to achieve a satisfactory plan for each patient. This process is usually time-consuming, and the plan quality depends on planer's experience and available planning time. In this study, we proposed to model the behaviors of human planners in treatment planning by a deep reinforcement learning (DRL)-based virtual treatment planner network (VTPN), such that it can operate the TPS in a human-like manner for treatment planning. 
  Methods and materials:  Using prostate cancer IMRT as an example, we established the VTPN using a deep neural network developed. We considered an in-house optimization engine with a weighted quadratic objective function. Virtual treatment planner network was designed to observe an intermediate plan DVHs and decide the action to improve the plan by changing weights and threshold dose in the objective function. We trained the VTPN in an end-to-end DRL process in 10 patient cases. A plan score was used to measure plan quality. We demonstrated the feasibility and effectiveness of the trained VTPN in another 64 patient cases. 
  Results:  Virtual treatment planner network was trained to spontaneously learn how to adjust treatment planning parameters to generate high-quality treatment plans. In the 64 testing cases, with initialized parameters, quality score was 4.97 (±2.02), with 9.0 being the highest possible score. Using VTPN to perform treatment planning improved quality score to 8.44 (±0.48). 
  Conclusions:  To our knowledge, this was the first time that intelligent treatment planning behaviors of human planner in external beam IMRT are autonomously encoded in an artificial intelligence system. The trained VTPN is capable of behaving in a human-like way to produce high-quality plans. 
  |  https://doi.org/10.1002/mp.14114  |  
------------------------------------------- 
10.1038/s41598-020-61409-0  |   Research at the intersection of computer vision and neuroscience has revealed hierarchical correspondence between layers of deep convolutional neural networks (DCNNs) and cascade of regions along human ventral visual cortex. Recently, studies have uncovered emergence of human interpretable concepts within DCNNs layers trained to identify visual objects and scenes. Here, we asked whether an artificial neural network (with convolutional structure) trained for visual categorization would demonstrate spatial correspondences with human brain regions showing central/peripheral biases. Using representational similarity analysis, we compared activations of convolutional layers of a DCNN trained for object and scene categorization with neural representations in human brain visual regions. Results reveal a brain-like topographical organization in the layers of the DCNN, such that activations of layer-units with central-bias were associated with brain regions with foveal tendencies (e.g. fusiform gyrus), and activations of layer-units with selectivity for image backgrounds were associated with cortical regions showing peripheral preference (e.g. parahippocampal cortex). The emergence of a categorical topographical correspondence between DCNNs and brain regions suggests these models are a good approximation of the perceptual representation generated by biological neural networks. 
  |  http://dx.doi.org/10.1038/s41598-020-61409-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32170209/  |  
------------------------------------------- 
10.1002/uog.21929  |    Objectives:  Operators performing fetal growth scans are usually aware of the gestational age of the pregnancy, which may lead to expected-value bias when performing biometric measurements. We aimed to evaluate the incidence of expected-value bias in routine fetal growth scans and assess its impact on standard biometric measurements. 
  Methods:  We collected prospectively full-length video recordings of routine ultrasound growth scans coupled with operator eye tracking. Expected value was defined as the gestational age at the time of the scan, based on the estimated due date that was established at the dating scan. Expected-value bias was defined as occurring when the operator looked at the measurement box on the screen during the process of caliper adjustment before saving a measurement. We studied the three standard biometric planes on which measurements of head circumference (HC), abdominal circumference (AC) and femur length (FL) are obtained. We evaluated the incidence of expected-value bias and quantified the impact of biased measurements. 
  Results:  We analyzed 272 third-trimester growth scans, performed by 16 operators, during which a total of 1409 measurements (354 HC, 703 AC and 352 FL; including repeat measurements) were obtained. Expected-value bias occurred in 91.4% of the saved standard biometric plane measurements (85.0% for HC, 92.9% for AC and 94.9% for FL). The operators were more likely to adjust the measurements towards the expected value than away from it (47.7% vs 19.7% of measurements; P &lt; 0.001). On average, measurements were corrected by 2.3 ± 5.6, 2.4 ± 10.4 and 3.2 ± 10.4 days of gestation towards the expected gestational age for the HC, AC, and FL measurements, respectively. Additionally, we noted a statistically significant reduction in measurement variance once the operator was biased (P = 0.026). Comparing the lowest and highest possible estimated fetal weight (using the smallest and largest biased HC, AC and FL measurements), we noted that the discordance, in percentage terms, was 10.1% ± 6.5%, and that in 17% (95% CI, 12-21%) of the scans, the fetus could be considered as small-for-gestational age or appropriate-for-gestational age if using the smallest or largest possible measurements, respectively. Similarly, in 13% (95% CI, 9-16%) of scans, the fetus could be considered as large-for-gestational age or appropriate-for-gestational age if using the largest or smallest possible measurements, respectively. 
  Conclusions:  During routine third-trimester growth scans, expected-value bias frequently occurs and significantly changes standard biometric measurements obtained. © 2019 the Authors. Ultrasound in Obstetrics &amp; Gynecology published by John Wiley &amp; Sons Ltd on behalf of the International Society of Ultrasound in Obstetrics and Gynecology. 
  |  https://doi.org/10.1002/uog.21929  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31763735/  |  
------------------------------------------- 
10.1016/j.preteyeres.2020.100829  |   The choroid is one of the most vascularized structures of the human body and plays an irreplaceable role in nourishing photoreceptors. As such, choroidal dysfunction is implicated in a multitude of ocular diseases. Studying the choroid can lead to a better understanding of disease pathogenesis, progression and discovery of novel management strategies. However, current research has produced inconsistent findings, partly due to the physical inaccessibility of the choroid and the lack of reliable biomarkers. With the advancements in optical coherence tomography technology, our group has developed a novel quantitative imaging biomarker known as the choroidal vascularity index (CVI), defined as the ratio of vascular area to the total choroidal area. CVI is a potential tool in establishing early diagnoses, monitoring disease progression and prognosticating patients. CVI has been reported in existing literature as a robust marker in numerous retinal and choroidal diseases. In this review, we will discuss the current role of CVI with reference to existing literature, and make postulations about its potential and future applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1350-9462(20)30001-X  |  
------------------------------------------- 
10.1161/STROKEAHA.119.027611  |   Background and Purpose- We aimed to investigate the ability of machine learning (ML) techniques analyzing diffusion-weighted imaging (DWI) and fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging to identify patients within the recommended time window for thrombolysis. Methods- We analyzed DWI and FLAIR images of consecutive patients with acute ischemic stroke within 24 hours of clear symptom onset by applying automatic image processing approaches. These processes included infarct segmentation, DWI, and FLAIR imaging registration and image feature extraction. A total of 89 vector features from each image sequence were captured and used in the ML. Three ML models were developed to estimate stroke onset time for binary classification (≤4.5 hours): logistic regression, support vector machine, and random forest. To evaluate the performance of ML models, the sensitivity and specificity for identifying patients within 4.5 hours were compared with the sensitivity and specificity of human readings of DWI-FLAIR mismatch. Results- Data from a total of 355 patients were analyzed. DWI-FLAIR mismatch from human readings identified patients within 4.5 hours of symptom onset with 48.5% sensitivity and 91.3% specificity. ML algorithms had significantly greater sensitivities than human readers (75.8% for logistic regression, <i>P</i>=0.020; 72.7% for support vector machine, <i>P</i>=0.033; 75.8% for random forest, <i>P</i>=0.013) in detecting patients within 4.5 hours, but their specificities were comparable (82.6% for logistic regression, <i>P</i>=0.157; 82.6% for support vector machine, <i>P</i>=0.157; 82.6% for random forest, <i>P</i>=0.157). Conclusions- ML algorithms using multiple magnetic resonance imaging features were feasible even more sensitive than human readings in identifying patients with stroke within the time window for acute thrombolysis. 
  |  http://www.ahajournals.org/doi/full/10.1161/STROKEAHA.119.027611?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.jfma.2020.03.024  |    Purpose:  To develop a deep learning image assessment software VeriSee™ and to validate its accuracy in grading the severity of diabetic retinopathy (DR). 
  Methods:  Diabetic patients who underwent single-field, nonmydriatic, 45-degree color retinal fundus photography at National Taiwan University Hospital between July 2007 and June 2017 were retrospectively recruited. A total of 7524 judgeable color fundus images were collected and were graded for the severity of DR by ophthalmologists. Among these pictures, 5649 along with another 31,612 color fundus images from the EyePACS dataset were used for model training of VeriSee™. The other 1875 images were used for validation and were graded for the severity of DR by VeriSee™, ophthalmologists, and internal physicians. Area under the receiver operating characteristic curve (AUC) for VeriSee™, and the sensitivities and specificities for VeriSee™, ophthalmologists, and internal physicians in diagnosing DR were calculated. 
  Results:  The AUCs for VeriSee™ in diagnosing any DR, referable DR and proliferative diabetic retinopathy (PDR) were 0.955, 0.955 and 0.984, respectively. VeriSee™ had better sensitivities in diagnosing any DR and PDR (92.2% and 90.9%, respectively) than internal physicians (64.3% and 20.6%, respectively) (P &lt; 0.001 for both). VeriSee™ also had better sensitivities in diagnosing any DR and referable DR (92.2% and 89.2%, respectively) than ophthalmologists (86.9% and 71.1%, respectively) (P &lt; 0.001 for both), while ophthalmologists had better specificities. 
  Conclusion:  VeriSee™ had good sensitivity and specificity in grading the severity of DR from color fundus images. It may offer clinical assistance to non-ophthalmologists in DR screening with nonmydriatic retinal fundus photography. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0929-6646(20)30118-2  |  
------------------------------------------- 
10.1200/CCI.19.00155  |    Purpose:  To develop an artificial intelligence (AI)-based model for identifying patients with lymph node (LN) metastasis based on digital evaluation of primary tumors and train the model using cystectomy specimens available from The Cancer Genome Atlas (TCGA) Project; patients from our institution were included for validation of the leave-out test cohort. 
  Methods:  In all, 307 patients were identified for inclusion in the study (TCGA, n = 294; in-house, n = 13). Deep learning models were trained from image patches at 2.5×, 5×, 10×, and 20× magnifications, and spatially resolved prediction maps were combined with microenvironment (lymphocyte infiltration) features to derive a final patient-level AI score (probability of LN metastasis). Training and validation included 219 patients (training, n = 146; validation, n = 73); 89 patients (TCGA, n = 75; in-house, n = 13) were reserved as an independent testing set. Multivariable logistic regression models for predicting LN status based on clinicopathologic features alone and a combined model with AI score were fit to training and validation sets. 
  Results:  Several patients were determined to have positive LN metastasis in TCGA (n = 105; 35.7%) and in-house (n = 3; 23.1%) cohorts. A clinicopathologic model that considered using factors such as age, T stage, and lymphovascular invasion demonstrated an area under the curve (AUC) of 0.755 (95% CI, 0.680 to 0.831) in the training and validation cohorts compared with the cross validation of the AI score (likelihood of positive LNs), which achieved an AUC of 0.866 (95% CI, 0.812 to 0.920; <i>P</i> = .021). Performance in the test cohort was similar, with a clinicopathologic model AUC of 0.678 (95% CI, 0.554 to 0.802) and an AI score of 0.784 (95% CI, 0.702 to 0.896; <i>P</i> = .21). In addition, the AI score remained significant after adjusting for clinicopathologic variables (<i>P</i> = 1.08 × 10<sup>-9</sup>), and the combined model significantly outperformed clinicopathologic features alone in the test cohort with an AUC of 0.807 (95% CI, 0.702 to 0.912; <i>P</i> = .047). 
  Conclusion:  Patients who are at higher risk of having positive LNs during cystectomy can be identified on primary tumor samples using novel AI-based methodologies applied to digital hematoxylin and eosin-stained slides. 
  |  http://ascopubs.org/doi/full/10.1200/CCI.19.00155?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1136/medethics-2019-105935  |   Artificial intelligence (AI) is expected to revolutionise the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI's progress in medicine, however, has led to concerns regarding the potential effects of this technology on relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied on, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely on AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice. 
  |  http://jme.bmj.com/cgi/pmidlookup?view=long&pmid=32220870  |  
------------------------------------------- 
10.1007/s00261-020-02485-8  |    Purpose:  To evaluate whether a three-phase dynamic contrast-enhanced CT protocol, when combined with a deep learning model, has similar accuracy in differentiating hepatocellular carcinoma (HCC) from other focal liver lesions (FLLs) compared with a four-phase protocol. 
  Methods:  Three hundred and forty-two patients (mean age 49.1 ± 10.5 years, range 19-86 years, 65.8% male) scanned with a four-phase CT protocol (precontrast, arterial, portal-venous and delayed phases) were retrospectively enrolled. A total of 449 FLLs were categorized into HCC and non-HCC groups based on the best available reference standard. Three convolutional dense networks (CDNs) with the input of four-phase CT images (model A), three-phase images without portal-venous phase (model B) and three-phase images without precontrast phase (model C) were trained on 80% of lesions and evaluated in the other 20% by receiver operating characteristics (ROC) and confusion matrix analysis. The DeLong test was performed to compare the areas under the ROC curves (AUCs) of A with B, B with C, and A with C. 
  Results:  The diagnostic accuracy in differentiating HCC from other FLLs on test sets was 83.3% for model A, 81.1% for model B and 85.6% for model C, and the AUCs were 0.925, 0.862 and 0.920, respectively. The AUCs of models A and C did not differ significantly (p = 0.765), but the AUCs of models A and B (p = 0.038) and of models B and C (p = 0.028) did. 
  Conclusions:  When combined with a CDN, a three-phase CT protocol without precontrast showed similar diagnostic accuracy as a four-phase protocol in differentiating HCC from other FLLs, suggesting that the multiphase CT protocol for HCC diagnosis might be optimized by removing the precontrast phase to reduce radiation dose. 
  |  https://dx.doi.org/10.1007/s00261-020-02485-8  |  
------------------------------------------- 
10.1057/s41271-019-00187-0  |   We undertook this study in light of an uncontrolled rise of melanoma incidence and mortality rates in the United Kingdom (UK). We aim to assess the effectiveness of prevention and early melanoma diagnosis in the UK's National Health Service (NHS) in comparison to the Australian system that has limited the melanoma rise. We compare the prevention campaigns against skin cancer and the stage at which melanoma is diagnosed. We analyse key drivers of early diagnosis. Overall, Australia has performed better than the UK and provides an example for the UK's NHS for better preventing melanoma and diagnosing it. Technologies under development, such as tele-dermatology and artificial intelligence applications, could aid in making melanoma early diagnosis easier, more cost-efficient, and lessen the burden on health systems. Diagnoses also provide the data to help public health officials target prevention programs. 
  |  http://dx.doi.org/10.1057/s41271-019-00187-0  |  
------------------------------------------- 
10.1148/radiol.2020200855  |    |  http://pubs.rsna.org/doi/10.1148/radiol.2020200855?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.compbiomed.2020.103699  |    Purpose:  To evaluate the impact of different supervision regimens on the training of artificial intelligence (AI) in the classification of chest radiographs as normal or abnormal in a moderately sized cohort of individuals more likely to be outpatients. 
  Materials and methods:  In a retrospective study, 7000 consecutive two-view chest radiographs obtained from 2012 to 2015 were labeled as normal or abnormal based on clinical reports. A convolutional neural network (CNN) was trained on this dataset and then evaluated with an unseen subset of 500 radiographs. Five different training approaches were tested: (1) weak supervision and four hybrid approaches combining weak supervision and extra supervision with annotation in (2) an unbalanced set of normal and abnormal cases, (3) a set of only abnormal cases, (4) a set of only normal cases, and (5) a balanced set of normal and abnormal cases. Standard binary classification metrics were assessed. 
  Results:  The weakly supervised model achieved an accuracy of 82%, but yielded 75 false negative cases, at a sensitivity of 70.0% and a negative predictive value (NPV) of 75.5%. Extra supervision increased NPV at the expense of the false positive rate and overall accuracy. Extra supervision with training using a balance of abnormal and normal radiographs resulted in the greatest increase in NPV (87.2%), improved sensitivity (92.8%), and reduced the number of false negatives by more than fourfold (18 compared to 75 cases). 
  Conclusion:  Extra supervision using a balance of annotated normal and abnormal cases applied to a weakly supervised model can minimize the number of false negative cases when classifying two-view chest radiographs. Further refinement of such hybrid training approaches for AI is warranted to refine models for practical clinical applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(20)30087-1  |  
------------------------------------------- 
10.18632/oncotarget.27541  |   [This corrects the article DOI: 10.18632/oncotarget.19393.]. 
  |  http://www.impactjournals.com/oncotarget/misc/linkedout.php?pii=27541  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284795/  |  
------------------------------------------- 
10.1007/978-1-0716-0389-5_19  |   Mycobacterium sp. is exhibiting complex evolution of antimicrobial resistance (AMR) and can therefore be considered as a serious human pathogen. Many strategies were employed earlier to evade the pathogenesis but AMR became threatened. Molecular tools employing bacteriophage can be an alternative to effective treatment against Mycobacterium. Phage treatment using phage-encoded products, such as lysins, causes lysis of cells; particularly bacteria could be used instead of direct use of these bacteriophages. Modern technologies along with bacteriophage strategies such as in silico immunoinformatics approach, machine learning, and artificial intelligence have been described thoroughly to escape the pathogenesis. Therefore, understanding the molecular mechanisms could be a possible alternative to evade the pathogenesis. 
  |  https://dx.doi.org/10.1007/978-1-0716-0389-5_19  |  
------------------------------------------- 
10.1109/TCBB.2020.2982142  |   The recent advances in wet-lab automation enable high-throughput experiments to be conducted seamlessly. In particular, the exhaustive enumeration of all possible conditions is always involved in high-throughput screening. Nonetheless, such a screening strategy is hardly believed to be optimal and cost-effective. By incorporating artificial intelligence, we design an open-source model based on categorical matrix completion and active machine learning to guide high throughput screening experiments. Specifically, we narrow our scope to the high-throughput screening for chemical compound effects on diverse protein sub-cellular locations. In the proposed model, we believe that exploration is more important than the exploitation in the long-run of high-throughput screening experiment, Therefore, we design several innovations to circumvent the existing limitations. In particular, categorical matrix completion is designed to accurately impute the missing experiments while margin sampling is also implemented for uncertainty estimation. The model is systematically tested on both simulated and real data. The simulation results reflect that our model can be robust to diverse scenarios, while the real data results demonstrate the wet-lab applicability of our model for high-throughput screening experiments. Lastly, we attribute the model success to its exploration ability by revealing the related matrix ranks and distinct experiment coverage comparisons. 
  |  https://dx.doi.org/10.1109/TCBB.2020.2982142  |  
------------------------------------------- 
10.1136/bmjsrh-2018-200271  |    Objectives:  Sexual and reproductive health (SRH) services are undergoing a digital transformation. This study explored the acceptability of three digital services, (i) video consultations via Skype, (ii) live webchats with a health advisor and (iii) artificial intelligence (AI)-enabled chatbots, as potential platforms for SRH advice. 
  Methods:  A pencil-and-paper 33-item survey was distributed in three clinics in Hampshire, UK for patients attending SRH services. Logistic regressions were performed to identify the correlates of acceptability. 
  Results:  In total, 257 patients (57% women, 50% aged &lt;25 years) completed the survey. As the first point of contact, 70% preferred face-to-face consultations, 17% telephone consultation, 10% webchats and 3% video consultations. Most would be willing to use video consultations (58%) and webchat facilities (73%) for ongoing care, but only 40% found AI chatbots acceptable. Younger age (&lt;25 years) (OR 2.43, 95% CI 1.35 to 4.38), White ethnicity (OR 2.87, 95% CI 1.30 to 6.34), past sexually transmitted infection (STI) diagnosis (OR 2.05, 95% CI 1.07 to 3.95), self-reported STI symptoms (OR 0.58, 95% CI 0.34 to 0.97), smartphone ownership (OR 16.0, 95% CI 3.64 to 70.5) and the preference for a SRH smartphone application (OR 1.95, 95% CI 1.13 to 3.35) were associated with video consultations, webchats or chatbots acceptability. 
  Conclusions:  Although video consultations and webchat services appear acceptable, there is currently little support for SRH chatbots. The findings demonstrate a preference for human interaction in SRH services. Policymakers and intervention developers need to ensure that digital transformation is not only cost-effective but also acceptable to users, easily accessible and equitable to all populations using SRH services. 
  |  http://jfprhc.bmj.com/cgi/pmidlookup?view=long&pmid=31964779  |  
------------------------------------------- 
10.1097/RTI.0000000000000498  |   The constantly increasing number of computed tomography (CT) examinations poses major challenges for radiologists. In this article, the additional benefits and potential of an artificial intelligence (AI) analysis platform for chest CT examinations in routine clinical practice will be examined. Specific application examples include AI-based, fully automatic lung segmentation with emphysema quantification, aortic measurements, detection of pulmonary nodules, and bone mineral density measurement. This contribution aims to appraise this AI-based application for value-added diagnosis during routine chest CT examinations and explore future development perspectives. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000498  |  
------------------------------------------- 
10.2463/mrms.mp.2019-0106  |    Purpose:  Idiopathic normal pressure hydrocephalus (iNPH) and Alzheimer's disease (AD) are geriatric diseases and common causes of dementia. Recently, many studies on the segmentation, disease detection, or classification of MRI using deep learning have been conducted. The aim of this study was to differentiate iNPH and AD using a residual extraction approach in the deep learning method. 
  Methods:  Twenty-three patients with iNPH, 23 patients with AD and 23 healthy controls were included in this study. All patients and volunteers underwent brain MRI with a 3T unit, and we used only whole-brain three-dimensional (3D) T<sub>1</sub>-weighted images. We designed a fully automated, end-to-end 3D deep learning classifier to differentiate iNPH, AD and control. We evaluated the performance of our model using a leave-one-out cross-validation test. We also evaluated the validity of the result by visualizing important areas in the process of differentiating AD and iNPH on the original input image using the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. 
  Results:  Twenty-one out of 23 iNPH cases, 19 out of 23 AD cases and 22 out of 23 controls were correctly diagnosed. The accuracy was 0.90. In the Grad-CAM heat map, brain parenchyma surrounding the lateral ventricle was highlighted in about half of the iNPH cases that were successfully diagnosed. The medial temporal lobe or inferior horn of the lateral ventricle was highlighted in many successfully diagnosed cases of AD. About half of the successful cases showed nonspecific heat maps. 
  Conclusions:  Residual extraction approach in a deep learning method achieved a high accuracy for the differential diagnosis of iNPH, AD, and healthy controls trained with a small number of cases. 
  |  https://dx.doi.org/10.2463/mrms.mp.2019-0106  |  
------------------------------------------- 
10.1080/14737159.2020.1758067  |   <b>Background:</b> A key objective in glaucoma is to identify those at risk of rapid progression and blindness. Recently, a novel first-in-man method for visualising apoptotic retinal cells called DARC (Detection-of-Apoptosing-Retinal-Cells) was reported. The aim was to develop an automatic CNN-aided method of DARC spot detection to enable prediction of glaucoma progression.<b>Methods:</b> Anonymised DARC images were acquired from healthy control (n=40) and glaucoma (n=20) Phase 2 clinical trial subjects (ISRCTN10751859) from which 5 observers manually counted spots. The CNN-aided algorithm was trained and validated using manual counts from control subjects, and then tested on glaucoma eyes.<b>Results:</b> The algorithm had 97.0% accuracy, 91.1% sensitivity and 97.1% specificity to spot detection when compared to manual grading of 50% controls. It was next tested on glaucoma patient eyes defined as progressing or stable based on a significant (p&lt;0.05) rate of progression using OCT-retinal nerve fibre layer measurements at 18 months. It demonstrated 85.7% sensitivity, 91.7% specificity with AUC of 0.89, and a significantly (p=0.0044) greater DARC count in those patients who later progressed.<b>Conclusion:</b> This CNN-enabled algorithm provides an automated and objective measure of DARC, promoting its use as an AI-aided biomarker for predicting glaucoma progression and testing new drugs. 
  |  http://www.tandfonline.com/doi/full/10.1080/14737159.2020.1758067  |  
------------------------------------------- 
10.1177/1745691620902467  |   Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science. 
  |  http://journals.sagepub.com/doi/full/10.1177/1745691620902467?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41598-020-58178-1  |   Artificial intelligence provides the opportunity to reveal important information buried in large amounts of complex data. Electronic health records (eHRs) are a source of such big data that provide a multitude of health related clinical information about patients. However, text data from eHRs, e.g., discharge summary notes, are challenging in their analysis because these notes are free-form texts and the writing formats and styles vary considerably between different records. For this reason, in this paper we study deep learning neural networks in combination with natural language processing to analyze text data from clinical discharge summaries. We provide a detail analysis of patient phenotyping, i.e., the automatic prediction of ten patient disorders, by investigating the influence of network architectures, sample sizes and information content of tokens. Importantly, for patients suffering from Chronic Pain, the disorder that is the most difficult one to classify, we find the largest performance gain for a combined word- and sentence-level input convolutional neural network (ws-CNN). As a general result, we find that the combination of data quality and data quantity of the text data is playing a crucial role for using more complex network architectures that improve significantly beyond a word-level input CNN model. From our investigations of learning curves and token selection mechanisms, we conclude that for such a transition one requires larger sample sizes because the amount of information per sample is quite small and only carried by few tokens and token categories. Interestingly, we found that the token frequency in the eHRs follow a Zipf law and we utilized this behavior to investigate the information content of tokens by defining a token selection mechanism. The latter addresses also issues of explainable AI. 
  |  http://dx.doi.org/10.1038/s41598-020-58178-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31996705/  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105354  |    Background:  Mental disorders, according to the definition of World Health Organization, consist of a wide range of signs, which are generally specified by a combination of unusual thoughts, feelings, behavior, and relationships with others. Social anxiety disorder (SAD) is one of the most prevalent mental disorders, described as permanent and severe fear or feeling of embarrassment in social situations. Considering the imprecise nature of SAD symptoms, the main objective of this study was to generate an intelligent decision support system for SAD diagnosis, using Adaptive neuro-fuzzy inference system (ANFIS) technique and to conduct an evaluation method, using sensitivity, specificity and accuracy metrics. 
  Method:  In this study, a real-world dataset with the sample size of 214 was selected and used to generate the model. The method comprised a multi-stage procedure named preprocessing, classification, and evaluation. The preprocessing stage, itself, consists of three steps called normalization, feature selection, and anomaly detection, using the Self-Organizing Map (SOM) clustering method. The ANFIS technique with 5-fold cross-validation was used for the classification of social anxiety disorder. 
  Results and conclusion:  The preprocessed dataset with seven input features were used to train the ANFIS model. The hybrid optimization learning algorithm and 41 epochs were used as optimal learning parameters. The accuracy, sensitivity, and specificity metrics were reported 98.67%, 97.14%, and 100%, respectively. The results revealed that the proposed model was quite appropriate for SAD diagnosis and in line with findings of other studies. Further research study addressing the design of a decision support system for diagnosing the severity of SAD is recommended. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31507-X  |  
------------------------------------------- 
10.1098/rsif.2019.0616  |   This research focuses on the signal processing required for a sensory system that can simultaneously localize multiple moving underwater objects in a three-dimensional (3D) volume by simulating the hydrodynamic flow caused by these objects. We propose a method for localization in a simulated setting based on an established hydrodynamic theory founded in fish lateral line organ research. Fish neurally concatenate the information of multiple sensors to localize sources. Similarly, we use the sampled fluid velocity via two parallel lateral lines to perform source localization in three dimensions in two steps. Using a convolutional neural network, we first estimate a two-dimensional image of the probability of a present source. Then we determine the position of each source, via an automated iterative 3D-aware algorithm. We study various neural network architectural designs and different ways of presenting the input to the neural network; multi-level amplified inputs and merged convolutional streams are shown to improve the imaging performance. Results show that the combined system can exhibit adequate 3D localization of multiple sources. 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rsif.2019.0616?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31964270/  |  
------------------------------------------- 
10.1093/bioinformatics/btz544  |    Motivation:  Protein structure refinement is an important step of protein structure prediction. Existing approaches have generally used a single scoring function combined with Monte Carlo method or Molecular Dynamics algorithm. The one-dimension optimization of a single energy function may take the structure too far away without a constraint. The basic motivation of our study is to reduce the bias problem caused by minimizing only a single energy function due to the very diversity of different protein structures. 
  Results:  We report a new Artificial Intelligence-based protein structure Refinement method called AIR. Its fundamental idea is to use multiple energy functions as multi-objectives in an effort to correct the potential inaccuracy from a single function. A multi-objective particle swarm optimization algorithm-based structure refinement is designed, where each structure is considered as a particle in the protocol. With the refinement iterations, the particles move around. The quality of particles in each iteration is evaluated by three energy functions, and the non-dominated particles are put into a set called Pareto set. After enough iteration times, particles from the Pareto set are screened and part of the top solutions are outputted as the final refined structures. The multi-objective energy function optimization strategy designed in the AIR protocol provides a different constraint view of the structure, by extending the one-dimension optimization to a new three-dimension space optimization driven by the multi-objective particle swarm optimization engine. Experimental results on CASP11, CASP12 refinement targets and blind tests in CASP 13 turn to be promising. 
  Availability and implementation:  The AIR is available online at: www.csbio.sjtu.edu.cn/bioinf/AIR/. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz544  |  
------------------------------------------- 
10.1055/s-0040-1701980  |    Objective:  To create practical recommendations for the curation of routinely collected health data and artificial intelligence (AI) in primary care with a focus on ensuring their ethical use. 
  Methods:  We defined data curation as the process of management of data throughout its lifecycle to ensure it can be used into the future. We used a literature review and Delphi exercises to capture insights from the Primary Care Informatics Working Group (PCIWG) of the International Medical Informatics Association (IMIA). 
  Results:  We created six recommendations: (1) Ensure consent and formal process to govern access and sharing throughout the data life cycle; (2) Sustainable data creation/collection requires trust and permission; (3) Pay attention to Extract-Transform-Load (ETL) processes as they may have unrecognised risks; (4) Integrate data governance and data quality management to support clinical practice in integrated care systems; (5) Recognise the need for new processes to address the ethical issues arising from AI in primary care; (6) Apply an ethical framework mapped to the data life cycle, including an assessment of data quality to achieve effective data curation. 
  Conclusions:  The ethical use of data needs to be integrated within the curation process, hence running throughout the data lifecycle. Current information systems may not fully detect the risks associated with ETL and AI; they need careful scrutiny. With distributed integrated care systems where data are often used remote from documentation, harmonised data quality assessment, management, and governance is important. These recommendations should help maintain trust and connectedness in contemporary information systems and planned developments. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0040-1701980  |  
------------------------------------------- 
10.1016/S1473-3099(20)30287-5  |    Background:  Rapid spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) in Wuhan, China, prompted heightened surveillance in Shenzhen, China. The resulting data provide a rare opportunity to measure key metrics of disease course, transmission, and the impact of control measures. 
  Methods:  From Jan 14 to Feb 12, 2020, the Shenzhen Center for Disease Control and Prevention identified 391 SARS-CoV-2 cases and 1286 close contacts. We compared cases identified through symptomatic surveillance and contact tracing, and estimated the time from symptom onset to confirmation, isolation, and admission to hospital. We estimated metrics of disease transmission and analysed factors influencing transmission risk. 
  Findings:  Cases were older than the general population (mean age 45 years) and balanced between males (n=187) and females (n=204). 356 (91%) of 391 cases had mild or moderate clinical severity at initial assessment. As of Feb 22, 2020, three cases had died and 225 had recovered (median time to recovery 21 days; 95% CI 20-22). Cases were isolated on average 4·6 days (95% CI 4·1-5·0) after developing symptoms; contact tracing reduced this by 1·9 days (95% CI 1·1-2·7). Household contacts and those travelling with a case were at higher risk of infection (odds ratio 6·27 [95% CI 1·49-26·33] for household contacts and 7·06 [1·43-34·91] for those travelling with a case) than other close contacts. The household secondary attack rate was 11·2% (95% CI 9·1-13·8), and children were as likely to be infected as adults (infection rate 7·4% in children &lt;10 years vs population average of 6·6%). The observed reproductive number (R) was 0·4 (95% CI 0·3-0·5), with a mean serial interval of 6·3 days (95% CI 5·2-7·6). 
  Interpretation:  Our data on cases as well as their infected and uninfected close contacts provide key insights into the epidemiology of SARS-CoV-2. This analysis shows that isolation and contact tracing reduce the time during which cases are infectious in the community, thereby reducing the R. The overall impact of isolation and contact tracing, however, is uncertain and highly dependent on the number of asymptomatic cases. Moreover, children are at a similar risk of infection to the general population, although less likely to have severe symptoms; hence they should be considered in analyses of transmission and control. 
  Funding:  Emergency Response Program of Harbin Institute of Technology, Emergency Response Program of Peng Cheng Laboratory, US Centers for Disease Control and Prevention. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1473-3099(20)30287-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32353347/  |  
------------------------------------------- 
10.1371/journal.pone.0228989  |   Prediction and early detection of kidney damage induced by nonsteroidal anti-inflammatories (NSAIDs) would provide the best chances of maximizing the anti-inflammatory effects while minimizing the risk of kidney damage. Unfortunately, biomarkers for detecting NSAID-induced kidney damage in cats remain to be discovered. To identify potential urinary biomarkers for monitoring NSAID-based treatments, we applied an untargeted metabolomics approach to urine collected from cats treated repeatedly with meloxicam or saline for up to 17 days. Applying multivariate analysis, this study identified a panel of seven metabolites that discriminate meloxicam treated from saline treated cats. Combining artificial intelligence machine learning algorithms and an independent testing urinary metabolome data set from cats with meloxicam-induced kidney damage, a panel of metabolites was identified and validated. The panel of metabolites including tryptophan, tyrosine, taurine, threonic acid, pseudouridine, xylitol and lyxitol, successfully distinguish meloxicam-treated and saline-treated cats with up to 75-100% sensitivity and specificity. This panel of urinary metabolites may prove a useful and non-invasive diagnostic tool for monitoring potential NSAID induced kidney injury in feline patients and may act as the framework for identifying urine biomarkers of NSAID induced injury in other species. 
  |  http://dx.plos.org/10.1371/journal.pone.0228989  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32053695/  |  
------------------------------------------- 
10.1016/j.arbres.2019.11.019  |    Introduction:  Mortality risk prediction for Intermediate Respiratory Care Unit's (IRCU) patients can facilitate optimal treatment in high-risk patients. While Intensive Care Units (ICUs) have a long term experience in using algorithms for this purpose, due to the special features of the IRCUs, the same strategics are not applicable. The aim of this study is to develop an IRCU specific mortality predictor tool using machine learning methods. 
  Methods:  Vital signs of patients were recorded from 1966 patients admitted from 2007 to 2017 in the Jiménez Díaz Foundation University Hospital's IRCU. A neural network was used to select the variables that better predict mortality status. Multivariate logistic regression provided us cut-off points that best discriminated the mortality status for each of the parameters. A new guideline for risk assessment was applied and mortality was recorded during one year. 
  Results:  Our algorithm shows that thrombocytopenia, metabolic acidosis, anemia, tachypnea, age, sodium levels, hypoxemia, leukocytopenia and hyperkalemia are the most relevant parameters associated with mortality. First year with this decision scene showed a decrease in failure rate of a 50%. 
  Conclusions:  We have generated a neural network model capable of identifying and classifying mortality predictors in the IRCU of a general hospital. Combined with multivariate regression analysis, it has provided us with an useful tool for the real-time monitoring of patients to detect specific mortality risks. The overall algorithm can be scaled to any type of unit offering personalized results and will increase accuracy over time when more patients are included to the cohorts. 
  |  http://www.archbronconeumol.org/en/linksolver/ft/pii/S0300-2896(19)30594-0  |  
------------------------------------------- 
10.1016/j.ophtha.2020.03.008  |    Purpose:  To develop an artificial intelligence (AI) dashboard for monitoring glaucomatous functional loss. 
  Design:  Retrospective, cross-sectional, longitudinal cohort study. 
  Participants:  Of 31 591 visual fields (VFs) on 8077 subjects, 13 231 VFs from the most recent visit of each patient were included to develop the AI dashboard. Longitudinal VFs from 287 eyes with glaucoma were used to validate the models. 
  Method:  We entered VF data from the most recent visit of glaucomatous and nonglaucomatous patients into a "pipeline" that included principal component analysis (PCA), manifold learning, and unsupervised clustering to identify eyes with similar global, hemifield, and local patterns of VF loss. We visualized the results on a map, which we refer to as an "AI-enabled glaucoma dashboard." We used density-based clustering and the VF decomposition method called "archetypal analysis" to annotate the dashboard. Finally, we used 2 separate benchmark datasets-one representing "likely nonprogression" and the other representing "likely progression"-to validate the dashboard and assess its ability to portray functional change over time in glaucoma. 
  Main outcome measures:  The severity and extent of functional loss and characteristic patterns of VF loss in patients with glaucoma. 
  Results:  After building the dashboard, we identified 32 nonoverlapping clusters. Each cluster on the dashboard corresponded to a particular global functional severity, an extent of VF loss into different hemifields, and characteristic local patterns of VF loss. By using 2 independent benchmark datasets and a definition of stability as trajectories not passing through over 2 clusters in a left or downward direction, the specificity for detecting "likely nonprogression" was 94% and the sensitivity for detecting "likely progression" was 77%. 
  Conclusions:  The AI-enabled glaucoma dashboard, developed using a large VF dataset containing a broad spectrum of visual deficit types, has the potential to provide clinicians with a user-friendly tool for determination of the severity of glaucomatous vision deficit, the spatial extent of the damage, and a means for monitoring the disease progression. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0161-6420(20)30238-4  |  
------------------------------------------- 
10.1016/j.carbpol.2020.116018  |   The stretchable hydrogels provide potential alternatives to bionic skins. However, skin simulation remains seriously challenging due to its complex nature, including mechanical property, protective effect, and sensory capability. Herein, conductive gels toughened by sodium alginate fibers in oil-water system were developed for preparation of skin-like ionic sensors. The dynamic network was constructed by polyvinyl alcohol and sodium alginate fibers, providing a wide scope of mechanical properties, such as high toughness, anti-fatigue fracture and remodelability. Moreover, salts imparted good conductivity to gels. As a result, gels exhibited sensory capability toward stress and strain, so they were considered sensors to monitor various movements of human body. In particular, gels demonstrated temperture tolerance ranging from -20 °C to 40 °C and non-drying for 6 days at 25 °C. In this study, gels showed complex intelligence similar to natural skin, and might find applications in artificial intelligence, human-mechanial interactions, and smart wearable devices. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0144-8617(20)30192-2  |  
------------------------------------------- 
10.1111/bjd.18880  |   In the past, the skills required to make an accurate dermatological diagnosis have required exposure to thousands of patients over many years. However, in recent years, artificial intelligence (AI) has made enormous advances, particularly in the area of image classification. This has led computer scientists to apply these techniques to develop algorithms that are able to recognize skin lesions, particularly melanoma. Since 2017, there have been numerous studies assessing the accuracy of algorithms, with some reporting that the accuracy matches or surpasses that of a dermatologist. While the principles underlying these methods are relatively straightforward, it can be challenging for the practising dermatologist to make sense of a plethora of unfamiliar terms in this domain. Here we explain the concepts of AI, machine learning, neural networks and deep learning, and explore the principles of how these tasks are accomplished. We critically evaluate the studies that have assessed the efficacy of these methods and discuss limitations and potential ethical issues. The burden of skin cancer is growing within the Western world, with major implications for both population skin health and the provision of dermatology services. AI has the potential to assist in the diagnosis of skin lesions and may have particular value at the interface between primary and secondary care. The emerging technology represents an exciting opportunity for dermatologists, who are the individuals best informed to explore the utility of this powerful novel diagnostic tool, and facilitate its safe and ethical implementation within healthcare systems. What is already known about this topic? There is considerable interest in the application of artificial intelligence to medicine. Several publications in recent years have described computer algorithms that can diagnose melanoma or skin lesions. Multiple groups have independently evaluated algorithms for the diagnosis of melanoma and skin lesions. What does this study add? We combine an introduction to the field with a summary of studies comparing dermatologists against artificial intelligence algorithms with the aim of providing a comprehensive resource for clinicians. This review will equip clinicians with the relevant knowledge to critically appraise future studies, and also assess the clinical utility of this technology. A better informed and engaged cohort of clinicians will ensure that the technology is applied effectively and ethically. 
  |  https://doi.org/10.1111/bjd.18880  |  
------------------------------------------- 
10.3390/s20051356  |   We propose a log analysis framework for test driving of autonomous vehicles. The log of a vehicle is a fundamental source to detect and analyze events during driving. A set of dumped logs are, however, usually mixed and fragmented since they are generated concurrently by a number of modules such as sensors, actuators and programs. This makes it hard to analyze them to discover latent errors that could occur due to complex chain reactions among those modules. Our framework provides a logging architecture based on formal specifications, which hierarchically organizes them to find out a priori relationships between them. Then, algorithmic or implementation errors can be detected by examining a posteriori relationships. However, a test in a situation of certain parameters, so called an oracle test, does not necessarily trigger latent violations of the relationships. In our framework, this is remedied by adopting metamorphic testing to quantitatively verify the formal specification. As a working proof, we define three metamorphic relations critical for testing autonomous vehicles and verify them in a quantitative manner based on our logging system. 
  |  http://www.mdpi.com/resolver?pii=s20051356  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32121632/  |  
------------------------------------------- 
10.2196/16912  |    Background:  Clinical decision support systems (CDSS) are an integral component of health information technologies and can assist disease interpretation, diagnosis, treatment, and prognosis. However, the utility of CDSS in the clinic remains controversial. 
  Objective:  The aim is to assess the effects of CDSS integrated with British Medical Journal (BMJ) Best Practice-aided diagnosis in real-world research. 
  Methods:  This was a retrospective, longitudinal observational study using routinely collected clinical diagnosis data from electronic medical records. A total of 34,113 hospitalized patient records were successively selected from December 2016 to February 2019 in six clinical departments. The diagnostic accuracy of the CDSS was verified before its implementation. A self-controlled comparison was then applied to detect the effects of CDSS implementation. Multivariable logistic regression and single-group interrupted time series analysis were used to explore the effects of CDSS. The sensitivity analysis was conducted using the subgroup data from January 2018 to February 2019. 
  Results:  The total accuracy rates of the recommended diagnosis from CDSS were 75.46% in the first-rank diagnosis, 83.94% in the top-2 diagnosis, and 87.53% in the top-3 diagnosis in the data before CDSS implementation. Higher consistency was observed between admission and discharge diagnoses, shorter confirmed diagnosis times, and shorter hospitalization days after the CDSS implementation (all P&lt;.001). Multivariable logistic regression analysis showed that the consistency rates after CDSS implementation (OR 1.078, 95% CI 1.015-1.144) and the proportion of hospitalization time 7 days or less (OR 1.688, 95% CI 1.592-1.789) both increased. The interrupted time series analysis showed that the consistency rates significantly increased by 6.722% (95% CI 2.433%-11.012%, P=.002) after CDSS implementation. The proportion of hospitalization time 7 days or less significantly increased by 7.837% (95% CI 1.798%-13.876%, P=.01). Similar results were obtained in the subgroup analysis. 
  Conclusions:  The CDSS integrated with BMJ Best Practice improved the accuracy of clinicians' diagnoses. Shorter confirmed diagnosis times and hospitalization days were also found to be associated with CDSS implementation in retrospective real-world studies. These findings highlight the utility of artificial intelligence-based CDSS to improve diagnosis efficiency, but these results require confirmation in future randomized controlled trials. 
  |  https://medinform.jmir.org/2020/1/e16912/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31958069/  |  
------------------------------------------- 
10.1038/s41467-020-15582-5  |   The unpredictable elements involved in a vehicular traffic system, like human interaction and weather, lead to a very complicated, high-dimensional, nonlinear dynamical system. Therefore, it is difficult to develop a mathematical or artificial intelligence model that describes the time evolution of traffic systems. All the while, the ever-increasing demands on transportation systems has left traffic agencies in dire need of a robust method for analyzing and forecasting traffic. Here we demonstrate how the Koopman mode decomposition can offer a model-free, data-driven approach for analyzing and forecasting traffic dynamics. By obtaining a decomposition of data sets collected by the Federal Highway Administration and the California Department of Transportation, we are able to reconstruct observed data, distinguish any growing or decaying patterns, and obtain a hierarchy of previously identified and never before identified spatiotemporal patterns. Furthermore, it is demonstrated how this methodology can be utilized to forecast highway network conditions. 
  |  http://dx.doi.org/10.1038/s41467-020-15582-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32350245/  |  
------------------------------------------- 
10.1038/s41746-020-0261-3  |   The diagnosis of heart failure can be difficult, even for heart failure specialists. Artificial Intelligence-Clinical Decision Support System (AI-CDSS) has the potential to assist physicians in heart failure diagnosis. The aim of this work was to evaluate the diagnostic accuracy of an AI-CDSS for heart failure. AI-CDSS for cardiology was developed with a hybrid (expert-driven and machine-learning-driven) approach of knowledge acquisition to evolve the knowledge base with heart failure diagnosis. A retrospective cohort of 1198 patients with and without heart failure was used for the development of AI-CDSS (training dataset, <i>n</i> = 600) and to test the performance (test dataset, <i>n</i> = 598). A prospective clinical pilot study of 97 patients with dyspnea was used to assess the diagnostic accuracy of AI-CDSS compared with that of non-heart failure specialists. The concordance rate between AI-CDSS and heart failure specialists was evaluated. In retrospective cohort, the concordance rate was 98.3% in the test dataset. The concordance rate for patients with heart failure with reduced ejection fraction, heart failure with mid-range ejection fraction, heart failure with preserved ejection fraction, and no heart failure was 100%, 100%, 99.6%, and 91.7%, respectively. In a prospective pilot study of 97 patients presenting with dyspnea to the outpatient clinic, 44% had heart failure. The concordance rate between AI-CDSS and heart failure specialists was 98%, whereas that between non-heart failure specialists and heart failure specialists was 76%. In conclusion, AI-CDSS showed a high diagnostic accuracy for heart failure. Therefore, AI-CDSS may be useful for the diagnosis of heart failure, especially when heart failure specialists are not available. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32285014/  |  
------------------------------------------- 
10.3390/antibiotics9020050  |   Hospital-acquired infections, particularly in the critical care setting, have become increasingly common during the last decade, with Gram-negative bacterial infections presenting the highest incidence among them. Multi-drug-resistant (MDR) Gram-negative infections are associated with high morbidity and mortality with significant direct and indirect costs resulting from long hospitalization due to antibiotic failure. Time is critical to identifying bacteria and their resistance to antibiotics due to the critical health status of patients in the intensive care unit (ICU). As common antibiotic resistance tests require more than 24 h after the sample is collected to determine sensitivity in specific antibiotics, we suggest applying machine learning (ML) techniques to assist the clinician in determining whether bacteria are resistant to individual antimicrobials by knowing only a sample's Gram stain, site of infection, and patient demographics. In our single center study, we compared the performance of eight machine learning algorithms to assess antibiotic susceptibility predictions. The demographic characteristics of the patients are considered for this study, as well as data from cultures and susceptibility testing. Applying machine learning algorithms to patient antimicrobial susceptibility data, readily available, solely from the Microbiology Laboratory without any of the patient's clinical data, even in resource-limited hospital settings, can provide informative antibiotic susceptibility predictions to aid clinicians in selecting appropriate empirical antibiotic therapy. These strategies, when used as a decision support tool, have the potential to improve empiric therapy selection and reduce the antimicrobial resistance burden. 
  |  http://www.mdpi.com/resolver?pii=antibiotics9020050  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023854/  |  
------------------------------------------- 
10.3389/fbioe.2020.00093  |   Osteoarthritis (OA), a degenerative joint disease, is the most common chronic condition of the joints, which cannot be prevented effectively. Computational modeling of joint degradation allows to estimate the patient-specific progression of OA, which can aid clinicians to estimate the most suitable time window for surgical intervention in osteoarthritic patients. This paper gives an overview of the different approaches used to model different aspects of joint degeneration, thereby focusing mostly on the knee joint. The paper starts by discussing how OA affects the different components of the joint and how these are accounted for in the models. Subsequently, it discusses the different modeling approaches that can be used to answer questions related to OA etiology, progression and treatment. These models are ordered based on their underlying assumptions and technologies: musculoskeletal models, Finite Element models, (gene) regulatory models, multiscale models and data-driven models (artificial intelligence/machine learning). Finally, it is concluded that in the future, efforts should be made to integrate the different modeling techniques into a more robust computational framework that should not only be efficient to predict OA progression but also easily allow a patient's individualized risk assessment as screening tool for use in clinical practice. 
  |  https://doi.org/10.3389/fbioe.2020.00093  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32185167/  |  
------------------------------------------- 
10.3390/s20030726  |   An understanding of marine ecosystems and their biodiversity is relevant to sustainable use of the goods and services they offer. Since marine areas host complex ecosystems, it is important to develop spatially widespread monitoring networks capable of providing large amounts of multiparametric information, encompassing both biotic and abiotic variables, and describing the ecological dynamics of the observed species. In this context, imaging devices are valuable tools that complement other biological and oceanographic monitoring devices. Nevertheless, large amounts of images or movies cannot all be manually processed, and autonomous routines for recognizing the relevant content, classification, and tagging are urgently needed. In this work, we propose a pipeline for the analysis of visual data that integrates video/image annotation tools for defining, training, and validation of datasets with video/image enhancement and machine and deep learning approaches. Such a pipeline is required to achieve good performance in the recognition and classification tasks of mobile and sessile megafauna, in order to obtain integrated information on spatial distribution and temporal dynamics. A prototype implementation of the analysis pipeline is provided in the context of deep-sea videos taken by one of the fixed cameras at the LoVe Ocean Observatory network of Lofoten Islands (Norway) at 260 m depth, in the Barents Sea, which has shown good classification results on an independent test dataset with an accuracy value of 76.18% and an area under the curve (AUC) value of 87.59%. 
  |  http://www.mdpi.com/resolver?pii=s20030726  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012976/  |  
------------------------------------------- 
10.1097/RTI.0000000000000484  |    Purpose:  The purpose of this study was to validate the accuracy of an artificial intelligence (AI) prototype application in determining bone mineral density (BMD) from chest computed tomography (CT), as compared with dual-energy x-ray absorptiometry (DEXA). 
  Materials and methods:  In this Institutional Review Board-approved study, we analyzed the data of 65 patients (57 female, mean age: 67.4 y) who underwent both DEXA and chest CT (mean time between scans: 1.31 y). From the DEXA studies, T-scores for L1-L4 (lumbar vertebrae 1 to 4) were recorded. Patients were then divided on the basis of their T-scores into normal control, osteopenic, or osteoporotic groups. An AI algorithm based on wavelet features, AdaBoost, and local geometry constraints independently localized thoracic vertebrae from chest CT studies and automatically computed average Hounsfield Unit (HU) values with kVp-dependent spectral correction. The Pearson correlation evaluated the correlation between the T-scores and HU values. Mann-Whitney U test was implemented to compare the HU values of normal control versus osteoporotic patients. 
  Results:  Overall, the DEXA-determined T-scores and AI-derived HU values showed a moderate correlation (r=0.55; P&lt;0.001). This 65-patient population was divided into 3 subgroups on the basis of their T-scores. The mean T-scores for the 3 subgroups (normal control, osteopenic, osteoporotic) were 0.77±1.50, -1.51±0.04, and -3.26±0.59, respectively. The mean DEXA-determined L1-L4 BMD measures were 1.13±0.16, 0.88±0.06, and 0.68±0.06 g/cm, respectively. The mean AI-derived attenuation values were 145±42.5, 136±31.82, and 103±16.28 HU, respectively. Using these AI-derived HU values, a significant difference was found between the normal control patients and osteoporotic group (P=0.045). 
  Conclusion:  Our results show that this AI prototype can successfully determine BMD in moderate correlation with DEXA. Combined with other AI algorithms directed at evaluating cardiac and lung diseases, this prototype may contribute to future comprehensive preventative care based on a single chest CT. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000484  |  
------------------------------------------- 
10.1166/jnn.2020.17798  |   Amidst the considerable attention artificial intelligence (AI) has attracted in recent years, a neuromorphic chip that mimics the biological neuron has emerged as a promising technology. Memristor or Resistive random-access memory (RRAM) is widely used to implement a synaptic device. Recently, 3D vertical RRAM (VRRAM) has become a promising candidate to reducing resistive memory bit cost. This study investigates the operation principle of synapse in 3D VRRAM architecture. In these devices, the classification response current through a vertical pillar is set by applying a training algorithm to the memristors. The accuracy of neural networks with 3D VRRAM synapses was verified by using the HSPICE simulator to classify the alphabet in 7×7 character images. This simulation demonstrated that 3D VRRAMs are usable as synapses in a neural network system and that a 3D VRRAM synapse should be designed to consider the initial value of the memristor to prepare the training conditions for high classification accuracy. These results mean that a synaptic circuit using 3D VRRAM will become a key technology for implementing neural computing hardware. 
  |  https://www.ingentaconnect.com/openurl?genre=article&issn=&volume=20&issue=8&spage=4730&aulast=Sun  |  
------------------------------------------- 
10.3390/ijms21082856  |   The therapeutic concept of unleashing a pre-existing immune response against the tumor by the application of immune-checkpoint inhibitors (ICI) has resulted in long-term survival in advanced cancer patient subgroups. However, the majority of patients do not benefit from single-agent ICI and therefore new combination strategies are eagerly necessitated. In addition to conventional chemotherapy, kinase inhibitors as well as tumor-specific vaccinations are extensively investigated in combination with ICI to augment therapy responses. An unprecedented clinical outcome with chimeric antigen receptor (CAR-)T cell therapy has led to the approval for relapsed/refractory diffuse large B cell lymphoma and B cell acute lymphoblastic leukemia whereas response rates in solid tumors are unsatisfactory. Immune-checkpoints negatively impact CAR-T cell therapy in hematologic and solid malignancies and as a consequence provide a therapeutic target to overcome resistance. Established biomarkers such as programmed death ligand 1 (PD-L1) and tumor mutational burden (TMB) help to select patients who will benefit most from ICI, however, biomarker negativity does not exclude responses. Investigating alterations in the antigen presenting pathway as well as radiomics have the potential to determine tumor immunogenicity and response to ICI. Within this review we summarize the literature about specific combination partners for ICI and the applicability of artificial intelligence to predict ICI therapy responses. 
  |  http://www.mdpi.com/resolver?pii=ijms21082856  |  
------------------------------------------- 
10.3346/jkms.2020.35.e1  |    |  https://jkms.org/DOIx.php?id=10.3346/jkms.2020.35.e1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31898430/  |  
------------------------------------------- 
10.2471/BLT.19.237560  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284655/  |  
------------------------------------------- 
10.4155/fmc-2019-0307  |   A new medicine will take an average of 10-15 years and more than US$2 billion before it can reach the pharmacy shelf. Traditionally, drug discovery relied on natural products as the main source of new drug entities, but was later shifted toward high-throughput synthesis and combinatorial chemistry-based development. New technologies such as ultra-high-throughput drug screening and artificial intelligence are being heavily employed to reduce the cost and the time of early drug discovery, but they remain relatively unchanged. However, are there other potentially faster and cheaper means of drug discovery? Is drug repurposing a viable alternative? In this review, we discuss the different means of drug discovery including their advantages and disadvantages. 
  |  https://www.future-science.com/doi/full/10.4155/fmc-2019-0307?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1111/bjd.18875  |    |  https://doi.org/10.1111/bjd.18875  |  
------------------------------------------- 
10.1172/JCI133821  |   The advent of human induced pluripotent stem cells (iPSCs) provided a means for avoiding ethical concerns associated with the use of cells isolated from human embryos. The number of labs now using iPSCs to generate photoreceptor, retinal pigmented epithelial (RPE), and-more recently-choroidal endothelial cells has grown exponentially. However, for autologous cell replacement to be effective, manufacturing strategies will need to change. Many tasks carried out by hand will need simplifying and automating. In this issue of the JCI, Schaub and colleagues combined quantitative bright-field microscopy and artificial intelligence (deep neural networks and traditional machine learning) to noninvasively monitor iPSC-derived graft maturation, predict donor cell identity, and evaluate graft function prior to transplantation. This approach allowed the authors to preemptively identify and remove abnormal grafts. Notably, the method is (a) transferable, (b) cost and time effective, (c) high throughput, and (d) useful for primary product validation. 
  |  https://doi.org/10.1172/JCI133821  |  
------------------------------------------- 
10.1177/1745691619896255  |   My goal in searching for the big pictures is to discover novel ways of organizing information in psychology that will have both theoretical and practical significance. The first section lists my reasons for writing each of five articles. The second section discusses an additional five articles that integrate advancements in artificial intelligence and cognitive psychology. The following two sections elaborate on my collaboration with ontologists to use formal ontologies to organize psychological knowledge, including the National Institute of Mental Health Research Domain Criteria, for formulating a biological basis for mental illness. I next discuss strategies for writing integrative articles. The following section describes the helpfulness of the integrations for making psychology relevant to a general audience. I conclude with recommendations for creating breadth in doctoral training. 
  |  http://journals.sagepub.com/doi/full/10.1177/1745691619896255?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1109/TVCG.2020.2973063  |   The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual, auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insects, fish, and mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and rnulti-agent artificial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not previously been reported in the technical literature related to XR. Biologists and computer scientists can benefit greatly from deepening interdisciplinary research in this emerging field and together we can develop new methods for conducting fundamental research in behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior experiments conducted in virtual environments. 
  |  https://dx.doi.org/10.1109/TVCG.2020.2973063  |  
------------------------------------------- 
10.1097/WCO.0000000000000790  |    Purpose of review:  Epilepsy surgery is the therapy of choice for 30-40% of people with focal drug-resistant epilepsy. Currently only ∼60% of well selected patients become postsurgically seizure-free underlining the need for better tools to identify the epileptogenic zone. This article reviews the latest neurophysiological advances for EZ localization with emphasis on ictal EZ identification, interictal EZ markers, and noninvasive neurophysiological mapping procedures. 
  Recent findings:  We will review methods for computerized EZ assessment, summarize computational network approaches for outcome prediction and individualized surgical planning. We will discuss electrical stimulation as an option to reduce the time needed for presurgical work-up. We will summarize recent research regarding high-frequency oscillations, connectivity measures, and combinations of multiple markers using machine learning. This latter was shown to outperform single markers. The role of NREM sleep for best identification of the EZ interictally will be discussed. We will summarize recent large-scale studies using electrical or magnetic source imaging for clinical decision-making. 
  Summary:  New approaches based on technical advancements paired with artificial intelligence are on the horizon for better EZ identification. They are ultimately expected to result in a more efficient, less invasive, and less time-demanding presurgical investigation. 
  |  http://dx.doi.org/10.1097/WCO.0000000000000790  |  
------------------------------------------- 
10.3390/life10020018  |   David Deamer has written another book, <i>Assembling Life,</i> on the origin of life. It is unapologetically polemic, presenting Deamer's view that life originated in fresh water hydrothermal fields on volcanic islands on early Earth, arguing that this provided a unique environment not just for organic chemistry but for the self-assembling structure that drive that chemistry and form the basis of structure in life. It is worth reading, it is an advance in the field, but is it convincing? I argue that the Origin of Life field as a whole is unconvincing, generating results in Toy Domains that cannot be scaled to any real world scenario. I suggest that, by analogy with the history of artificial intelligence and solar astronomy, we need much more scale, and fundamentally new ideas, to take the field forward. 
  |  http://www.mdpi.com/resolver?pii=life10020018  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32085425/  |  
------------------------------------------- 
10.1103/PhysRevE.101.022302  |   We consider the synchronization of oscillators in complex networks where there is an interplay between the oscillator dynamics and the network topology. Through a remarkable transformation in parameter space and the introduction of virtual frequencies we show that Kuramoto oscillators on annealed networks, with or without frequency-degree correlation, and Kuramoto oscillators on complete graphs with frequency-weighted coupling can be transformed to Kuramoto oscillators on complete graphs with a rearranged, virtual frequency distribution and uniform coupling. The virtual frequency distribution encodes both the natural frequency distribution (dynamics) and the degree distribution (topology). We apply this transformation to give direct explanations to a variety of phenomena that have been observed in complex networks, such as explosive synchronization and vanishing synchronization onset. 
  |  None  |  
------------------------------------------- 
10.1038/s41380-020-0748-y  |   The current diagnostic practices are linked to a 20-fold increase in the reported prevalence of ASD over the last 30 years. Fragmenting the autism phenotype into dimensional "autistic traits" results in the alleged recognition of autism-like symptoms in any psychiatric or neurodevelopemental condition and in individuals decreasingly distant from the typical population, and prematurely dismisses the relevance of a diagnostic threshold. Non-specific socio-communicative and repetitive DSM 5 criteria, combined with four quantitative specifiers as well as all their possible combinations, render limitless variety of presentations consistent with the categorical diagnosis of ASD. We propose several remedies to this problem: maintain a line of research on prototypical autism; limit the heterogeneity compatible with a categorical diagnosis to situations with a phenotypic overlap and a validated etiological link with prototypical autism; reintroduce the qualitative properties of autism presentations and of current dimensional specifiers, language, intelligence, comorbidity, and severity in the criteria used to diagnose autism in replacement of quantitative "social" and "repetitive" criteria; use these qualitative features combined with the clinical intuition of experts and machine-learning algorithms to differentiate coherent subgroups in today's autism spectrum; study these subgroups separately, and then compare them; and question the autistic nature of "autistic traits". 
  |  http://dx.doi.org/10.1038/s41380-020-0748-y  |  
------------------------------------------- 
10.1001/jamanetworkopen.2020.0841  |   This cross-sectional study calculates the increase in clinical cancer knowledge represented in the National Comprehensive Cancer Network (NCCN) Clinical Practice Guidelines from 1996 to 2019. 
  |  https://jamanetwork.com/journals/jamanetworkopen/fullarticle/10.1001/jamanetworkopen.2020.0841  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32167566/  |  
------------------------------------------- 
10.1038/s41598-020-63887-8  |   Ménière's Disease (MD) is difficult to diagnose and evaluate objectively over the course of treatment. Recently, several studies have reported MD diagnoses by MRI-based endolymphatic hydrops (EH) analysis. However, this method is time-consuming and complicated. Therefore, a fast, objective, and accurate evaluation tool is necessary. The purpose of this study was to develop an algorithm that can accurately analyze EH on intravenous (IV) gadolinium (Gd)-enhanced inner-ear MRI using artificial intelligence (AI) with deep learning. In this study, we developed a convolutional neural network (CNN)-based deep-learning model named INHEARIT (INner ear Hydrops Estimation via ARtificial InTelligence) for the automatic segmentation of the cochlea and vestibule, and calculation of the EH ratio in the segmented region. Measurement of the EH ratio was performed manually by a neuro-otologist and neuro-radiologist and by estimation with the INHEARIT model and were highly consistent (intraclass correlation coefficient = 0.971). This is the first study to demonstrate that automated EH ratio measurements are possible, which is important in the current clinical context where the usefulness of IV-Gd inner-ear MRI for MD diagnosis is increasing. 
  |  http://dx.doi.org/10.1038/s41598-020-63887-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32332804/  |  
------------------------------------------- 
10.3389/fbioe.2020.00041  |   Enhancement of activity is one major topic related to the aging society. Therefore, it is necessary to understand people's motion and identify possible risk factors during activity. Technology can be used to monitor motion patterns during daily life. Especially the use of artificial intelligence combined with wearable sensors can simplify measurement systems and might at some point replace the standard motion capturing using optical measurement technologies. Therefore, this study aims to analyze the estimation of 3D joint angles and joint moments of the lower limbs based on IMU data using a feedforward neural network. The dataset summarizes optical motion capture data of former studies and additional newly collected IMU data. Based on the optical data, the acceleration and angular rate of inertial sensors was simulated. The data was augmented by simulating different sensor positions and orientations. In this study, gait analysis was undertaken with 30 participants using a conventional motion capture set-up based on an optoelectronic system and force plates in parallel with a custom IMU system consisting of five sensors. A mean correlation coefficient of 0.85 for the joint angles and 0.95 for the joint moments was achieved. The RMSE for the joint angle prediction was smaller than 4.8° and the nRMSE for the joint moment prediction was below 13.0%. Especially in the sagittal motion plane good results could be achieved. As the measured dataset is rather small, data was synthesized to complement the measured data. The enlargement of the dataset improved the prediction of the joint angles. While size did not affect the joint moment prediction, the addition of noise to the dataset resulted in an improved prediction accuracy. This indicates that research on appropriate augmentation techniques for biomechanical data is useful to further improve machine learning applications. 
  |  https://doi.org/10.3389/fbioe.2020.00041  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32117923/  |  
------------------------------------------- 
10.1016/j.gene.2020.144475  |    Introduction:  In this article, we utilized Ingenuity® Pathway Analysis (IPA®) bioinformatics analysis software and Metascape® bioinformatics analysis website tools to analyse the possible mechanism of ERH affecting tumourigenesis (proliferation and apoptosis) in bladder cancer (BC) T24 cells. 
  Methods:  The ERH gene was knocked down, and BC T24 cells were divided into ERH normal and knockdown groups. Affymetrix® gene expression microarrays were performed to obtain a differentially expressed gene list (DEGL) between the 2 groups. IPA® data analyses contain five modules: disease and function analysis, upstream analysis, regulator effects analysis, canonical pathway analysis and molecular network analysis. The results of Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were analysed by Metascape®. 
  Results:  The results of the gene expression profiling chip and the DEGL showed that 344 genes were upregulated and 254 genes were downregulated. The IPA® and Metascape® pathway analyses showed that the ERH gene may affect proliferation and apoptosis by affecting the apoptosis, cell cycle, Toll-like receptor (TLR), NF-κB or TGF-beta signalling pathways. Upstream analysis determined that the ERH gene may regulate TNF and NK-κB in the BC T24 cell lines. The ERH gene may be involved in the "cell death and survival" molecular network in BC T24 cells. ERH may be a regulator of KITLG through TNF. 
  Conclusions:  The ERH gene may affect apoptosis through the TLR, NF-κB, TNF or TGF-beta signalling pathways in BC T24 cells, and may be a regulator of KITLG to ultimately activate the growth of malignant tumours. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30144-X  |  
------------------------------------------- 
10.1038/s41467-019-14081-6  |   The Bruton tyrosine kinase (BTK) inhibitor ibrutinib provides effective treatment for patients with chronic lymphocytic leukemia (CLL), despite extensive heterogeneity in this disease. To define the underlining regulatory dynamics, we analyze high-resolution time courses of ibrutinib treatment in patients with CLL, combining immune-phenotyping, single-cell transcriptome profiling, and chromatin mapping. We identify a consistent regulatory program starting with a sharp decrease of NF-κB binding in CLL cells, which is followed by reduced activity of lineage-defining transcription factors, erosion of CLL cell identity, and acquisition of a quiescence-like gene signature. We observe patient-to-patient variation in the speed of execution of this program, which we exploit to predict patient-specific dynamics in the response to ibrutinib based on the pre-treatment patient samples. In aggregate, our study describes time-dependent cellular, molecular, and regulatory effects for therapeutic inhibition of B cell receptor signaling in CLL, and it establishes a broadly applicable method for epigenome/transcriptome-based treatment monitoring. 
  |  http://dx.doi.org/10.1038/s41467-019-14081-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31996669/  |  
------------------------------------------- 
10.1002/mc.23165  |   Colorectal cancer (CRC) is a kind of malignant cancer with high morbidity and mortality. The purpose of this study was to explore potential regulated key genes involved in CRC through bioinformatics analysis and experimental verification. The gene expression profile data were downloaded from the Gene Expression Omnibus, and the differential expression genes were detected in cancerous and paracancerous samples of CRC patients, respectively. Then functional enrichment analysis, such as the Kyoto Encyclopedia of Genes and Genomes pathway analysis as well as the protein-protein interaction network were constructed, and the highly related genes were clustered by Molecular COmplex DEtection algorithm to find out the core interaction in different genes' crosstalk. The genes affecting CRC prognosis were screened by the Human Protein Atlas database. In addition, the expression level of core genes was detected by GEPIA database, and the core genes' changes in large-scale cancer genome data set were directly analyzed by cBioPortal database. The expression of the predicted hub genes DSN1, AHCY, and ERCC6L was verified by reverse-transcription quantitative polymerase chain reaction in CRC cells. The gene function of DSN1 was analyzed by wound healing and colony formation assays. The results showed that silencing of DSN1 could significantly reduce the migration and proliferation of CRC cells. Further, BUB1B, the potential interacting protein of DSN1, was also predicted via bioinformatics analysis. Above all, this study shows that bioinformatics analysis combined with experimental method verification provide more potential vital genes for the prevention and therapy of CRC. 
  |  https://doi.org/10.1002/mc.23165  |  
------------------------------------------- 
10.21037/atm.2020.01.107  |    Background:  Neoadjuvant chemoradiotherapy (NCRT) followed by surgery is the standard treatment for patients with locally advanced rectal cancer. This study developed a random forest (RF) model to predict pathological complete response (pCR) based on radiomics derived from baseline <sup>18</sup>F-fluorodeoxyglucose ([<sup>18</sup>F]FDG)-positron emission tomography (PET)/computed tomography (CT). 
  Methods:  This study included 169 patients with newly diagnosed rectal cancer. All patients received <sup>18</sup>F[FDG]-PET/CT, NCRT, and surgery. In total, 68 radiomic features were extracted from the metabolic tumor volume. The numbers of splits in a decision tree and trees in an RF were determined based on their effects on predictive performance. Receiver operating characteristic curve analysis was performed to evaluate predictive performance and ascertain the optimal threshold for maximizing prediction accuracy. 
  Results:  After NCRT, 22 patients (13%) achieved pCR, and 42 features that could differentiate tumors with pCR were used to construct the RF model. Six decision trees and seven splits were suitable. Accordingly, the sensitivity, specificity, positive predictive value, negative predictive value, and accuracy were 81.8%, 97.3%, 81.8%, 97.3%, and 95.3%, respectively. 
  Conclusions:  By using an RF, we determined that radiomics derived from baseline <sup>18</sup>F[FDG]-PET/CT could accurately predict pCR in patients with rectal cancer. Highly accurate and predictive values can be achieved but should be externally validated. 
  |  https://doi.org/10.21037/atm.2020.01.107  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32309354/  |  
------------------------------------------- 
10.1016/j.cub.2020.02.062  |   An extension of the prediction error theory of dopamine, imported from artificial intelligence, represents the full distribution over future rewards rather than only the average and better explains dopamine responses. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9822(20)30269-4  |  
------------------------------------------- 
10.1016/j.gene.2020.144454  |   Parkinson's disease (PD) is a common neurodegenerative disorder which affects dopaminergic neurons leading to alteration of numerous cellular pathways. Several reports highlight that PD disturbs also other cells than CNS neurons including PBMCs, which could lead, among other things, to dysfunctions of immune functions. Because autophagy could be altered in PD, a monocentric pilot study was performed to quantify the transcripts levels of several autophagy genes in blood cells. MAP1LC3B, GABARAP, GABARAPL1, GABARAPL2 and P62/SQSTM1 were found to be overexpressed in patients. On the contrary, transcripts for HSPA8 and GAPDH were both decreased. Expression of MAP1LC3B and GABARAP was able to successfully segregate PD patients from healthy controls. The accuracy of this segregation was substantially increased when combined expressions of MAP1LC3B and GAPDH or GABARAP and GAPDH were used as categorical variables. This pilot study suggests that autophagy genes expression is dysregulated in PD patients and may open new perspectives for the characterisation of prediction markers. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30123-2  |  
------------------------------------------- 
10.1093/nar/gkaa191  |   Recent advances in high-throughput single-cell RNA-seq have enabled us to measure thousands of gene expression levels at single-cell resolution. However, the transcriptomic profiles are high-dimensional and sparse in nature. To address it, a deep learning framework based on auto-encoder, termed DeepAE, is proposed to elucidate high-dimensional transcriptomic profiling data in an encode-decode manner. Comparative experiments were conducted on nine transcriptomic profiling datasets to compare DeepAE with four benchmark methods. The results demonstrate that the proposed DeepAE outperforms the benchmark methods with robust performance on uncovering the key dimensions of single-cell RNA-seq data. In addition, we also investigate the performance of DeepAE in other contexts and platforms such as mass cytometry and metabolic profiling in a comprehensive manner. Gene ontology enrichment and pathology analysis are conducted to reveal the mechanisms behind the robust performance of DeepAE by uncovering its key dimensions. 
  |  https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkaa191  |  
------------------------------------------- 
10.1016/j.neulet.2020.134752  |   Event-related potential (ERP)-based brain-computer interface (BCI) has been widely used in robot control. Increasing the amplitude of the ERPs is key for improving the performance of ERP-based BCI. However, using images of robot motion as visual stimuli has not been studied widely. The aim of this study is to explore the concreteness of robot motion images on ERPs. Fifteen subjects used five kinds of visual spellers employing different images as visual stimuli: squares, arrows, a single kind of robot motion, multiple kinds of robot motions, and multiple kinds of robot motions with arrows. The three robot motion stimuli induced larger N200 and P300 potentials than non-robot motion stimuli. The topography shows that robot motion stimuli also evoke stronger negativities in the anterior and occipital areas. Concrete images provide more information to the subject about the robot motion, which might help the brain extract the meaning of the image more automatically. We use a support vector machine to detect the subject's intentions. There is substantial improvement in the classification performance when using robot motion images as visual stimuli, which implies that concrete visual stimuli improve the performance of the ERP-based BCI. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0304-3940(20)30022-7  |  
------------------------------------------- 
10.1088/1361-6528/ab7252  |   Artificial intelligence devices that can mimic human brains are the foundation for building future artificial neural networks. A key step in mimicking biological neural systems is the modulation of synaptic weight, which is mainly achieved by various engineering approaches using material design, or modification of the device structure. Here, we realize the modulation of the synaptic weight of a Ta<sub>2</sub>O<sub>5</sub>/ITO-based all-metal oxide synaptic transistor via laser irradiation. Prior to the deposition of the active layer and electrodes, a femtosecond laser was used to irradiate the surface of the insulator layer. Typical synaptic characteristics such as excitatory postsynaptic current, paired pulse facilitation and long-term potentiation were successfully simulated under different laser intensities and scanning rates. In particular, we demonstrate for the first time that laser irradiation could control the quantity of oxygen vacancies in the Ta<sub>2</sub>O<sub>5</sub> thin film, leading to precise modulation of the synaptic weight. Our research provides an instantaneous (&lt;1 s), convenient and low-temperature approach to improving synaptic behaviors, which could be promising for neuromorphic computing hardware design. 
  |  https://doi.org/10.1088/1361-6528/ab7252  |  
------------------------------------------- 
10.1097/SAP.0000000000002252  |    Background:  Artificial Intelligent Virtual Assistants (AIVA) is a segment of artificial intelligence that is rapidly developing. However, its utilization to address patients' frequently asked questions remains unexplored. 
  Methods:  We developed an AIVA to answer questions related to 10 frequent topics asked by plastic surgery patients in our institution. Between July 27, 2018, and August 10 of 2018, we recruited subjects with administrative positions at our health care institution to chat with the virtual assistant. They asked, with their own words, 1 question for each topic and filled out a satisfaction questionnaire. Postsurvey analysis of questions and answers allowed assessment of the virtual assistant's accuracy. 
  Results:  Thirty participants completed the survey. The majority was female (70%), and the mean age was 27.76 years (SD, 8.68 [19-51] years). The overall accuracy of the plastic surgery AIVA was 92.3% (277/294 questions), and participants considered the answer correct in 83.3% of the time (250/294 answers). Most of the participants considered the AIVA easy to use, answered adequately, and could be helpful for patients. However, when asked if this technology could replace a human assistant, they stayed neutral. 
  |  http://dx.doi.org/10.1097/SAP.0000000000002252  |  
------------------------------------------- 
10.1093/bioinformatics/btaa157  |    Motivation:  Predicting potential links in biomedical bipartite networks can provide useful insights into the diagnosis and treatment of complex diseases and the discovery of novel drug targets. Computational methods have been proposed recently to predict potential links for various biomedical bipartite networks. However, existing methods are usually rely on the coverage of known links, which may encounter difficulties when dealing with new nodes without any known link information. 
  Results:  In this study, we propose a new link prediction method, named graph regularized generalized matrix factorization (GRGMF), to identify potential links in biomedical bipartite networks. First, we formulate a generalized matrix factorization model to exploit the latent patterns behind observed links. In particular, it can take into account the neighborhood information of each node when learning the latent representation for each node, and the neighborhood information of each node can be learned adaptively. Second, we introduce two graph regularization terms to draw support from affinity information of each node derived from external databases to enhance the learning of latent representations. We conduct extensive experiments on six real datasets. Experiment results show that GRGMF can achieve competitive performance on all these datasets, which demonstrate the effectiveness of GRGMF in prediction potential links in biomedical bipartite networks. 
  Availability and implementation:  The package is available at https://github.com/happyalfred2016/GRGMF. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btaa157  |  
------------------------------------------- 
10.3390/ijerph17020381  |   This article serves as the introduction to this special issue on Mobile Health and Mobile Rehabilitation for People with Disabilities. Social, technological and policy trends are reviewed. Needs, opportunities and challenges for the emerging fields of mobile health (mHealth, aka eHealth) and mobile rehabilitation (mRehab) are discussed. Healthcare in the United States (U.S.) is at a critical juncture characterized by: (1) a growing need for healthcare and rehabilitation services; (2) maturing technological capabilities to support more effective and efficient health services; (3) evolving public policies designed, by turns, to contain cost and support new models of care; and (4) a growing need to ensure acceptance and usability of new health technologies by people with disabilities and chronic conditions, clinicians and health delivery systems. Discussion of demographic and population health data, healthcare service delivery and a public policy primarily focuses on the U.S. However, trends identified (aging populations, growing prevalence of chronic conditions and disability, labor shortages in healthcare) apply to most countries with advanced economies and others. Furthermore, technologies that enable mRehab (wearable sensors, in-home environmental monitors, cloud computing, artificial intelligence) transcend national boundaries. Remote and mobile healthcare delivery is needed and inevitable. Proactive engagement is critical to ensure acceptance and effectiveness for all stakeholders. 
  |  http://www.mdpi.com/resolver?pii=ijerph17020381  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31936006/  |  
------------------------------------------- 
10.1007/s10047-020-01161-4  |   Future development of innovative artificial organs is closely related with cutting edge emerging technology. These technologies include brain machine or computer interface, organs made by three dimensional bioprinting, organs designed from induced-pluripotent stem cell for personalized tissue or organ, and xenotransplantation. To bridge the gap between scientific innovation and regulatory product review, Pharmaceuticals and Medical Devices Agency of Japan (PMDA) started the science board to discuss about the new scientific topics regarding medical products including medical device and regenerative products with external experts since 2012. Topics which PMDA raised for science board included cellular and tissue-based products from iPS cells, artificial intelligence and genome editing technology. In addition, PMDA started the horizon scanning to identify a new cutting edge technology which could potentially lead to innovative health technology or product, which has a strong impact on clinical medicine. Although the effectiveness and safety of the medical products must be reasonably assured before clinical use, PMDA introduced Sakigake review assignment (a review partner of device development) and conditional approval system to balance between pre-market and post-market evaluation. 
  |  https://dx.doi.org/10.1007/s10047-020-01161-4  |  
------------------------------------------- 
10.1093/cid/ciaa383  |    Background:  A locally developed Case-Based Reasoning (CBR) algorithm, designed to augment antimicrobial prescribing in secondary care was evaluated. 
  Methods:  Prescribing recommendations made by a CBR algorithm were compared to decisions made by physicians in clinical practice. Comparisons were examined in two patient populations. Firstly, in patients with confirmed Escherichia coli blood stream infections ('E.coli patients'), and secondly in ward-based patients presenting with a range of potential infections ('ward patients'). Prescribing recommendations were compared against the Antimicrobial Spectrum Index (ASI) and the WHO Essential Medicine List Access, Watch, Reserve (AWaRe) classification system. Appropriateness of a prescription was defined as the spectrum of the prescription covering the known, or most-likely organism antimicrobial sensitivity profile. 
  Results:  In total, 224 patients (145 E.coli patients and 79 ward patients) were included. Mean (SD) age was 66 (18) years with 108/224 (48%) female gender. The CBR recommendations were appropriate in 202/224 (90%) compared to 186/224 (83%) in practice (OR: 1.24 95%CI:0.392-3.936;p=0.71). CBR recommendations had a smaller ASI compared to practice with a median (range) of 6 (0-13) compared to 8 (0-12) (p&lt;0.01). CBR recommendations were more likely to be classified as Access class antimicrobials compared to physicians' prescriptions at 110/224 (49%) vs. 79/224 (35%) (OR: 1.77 95%CI:1.212-2.588 p&lt;0.01). Results were similar for E.coli and ward patients on subgroup analysis. 
  Conclusions:  A CBR-driven decision support system provided appropriate recommendations within a narrower spectrum compared to current clinical practice. Future work must investigate the impact of this intervention on prescribing behaviours more broadly and patient outcomes. 
  |  https://academic.oup.com/cid/article-lookup/doi/10.1093/cid/ciaa383  |  
------------------------------------------- 
10.1016/j.copbio.2020.02.014  |   Systems metabolic engineering attempts to engineer a production host's biological network to overproduce valuable chemicals and materials in a sustainable manner. In contrast to genome-scale metabolic models that are well established, regulatory network models have not been sufficiently considered in systems metabolic engineering despite their importance and recent notable advances. In this paper, recent studies on inferring and characterizing regulatory networks at both transcriptional and translational levels are reviewed. The recent studies discussed herein suggest that their corresponding computational methods and models can be effectively applied to optimize a production host's regulatory networks for the enhanced biological production. For the successful application of regulatory network models, datasets on biological sequence-phenotype relationship need to be more generated. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0958-1669(20)30031-8  |  
------------------------------------------- 
10.1007/s00330-020-06677-0  |    Objectives:  To investigate whether a deep learning model can predict the bone mineral density (BMD) of lumbar vertebrae from unenhanced abdominal computed tomography (CT) images. 
  Methods:  In this Institutional Review Board-approved retrospective study, patients who received both unenhanced CT examinations and dual-energy X-ray absorptiometry (DXA) of the lumbar vertebrae, in two institutions (1 and 2), were included. Supervised deep learning was employed to obtain a convolutional neural network (CNN) model using axial CT images, including the lumbar vertebrae as input data and BMD values obtained with DXA as reference data. For this purpose, 1665 CT images from 183 patients in institution 1, which were augmented to 99,900 (= 1665 × 60) images (noise adding, parallel shift and rotation were performed), were used. Internal (by using data of 45 other patients in institution 1) and external validations (by using data of 50 patients in institution 2) were performed to evaluate the performance of the trained CNN model. Correlations and diagnostic performances were evaluated with Pearson's correlation coefficient (r) and area under the receiver operating characteristic curve (AUC), respectively. 
  Results:  The estimated BMD values, according to the CNN model (BMD<sub>CNN</sub>), were significantly correlated with the BMD values obtained with DXA (r = 0.852 (p &lt; 0.001) and 0.840 (p &lt; 0.001) for the internal and external validation datasets, respectively). Using BMD<sub>CNN</sub>, osteoporosis was diagnosed with AUCs of 0.965 and 0.970 for the internal and external validation datasets, respectively. 
  Conclusions:  Using deep learning, the BMD of lumbar vertebrae could be predicted from unenhanced abdominal CT images. 
  Key points:  • By applying a deep learning technique, the bone mineral density (BMD) of lumbar vertebrae can be estimated from unenhanced abdominal CT images. • A strong correlation was observed between the estimated BMD and the BMD obtained with DXA. • By using the estimated BMD, osteoporosis could be diagnosed with high performance. 
  |  https://dx.doi.org/10.1007/s00330-020-06677-0  |  
------------------------------------------- 
10.1016/j.dib.2020.105314  |   The data presented in this article is part of the whole slide imaging (WSI) datasets generated in European project AIDPATH This data is also related to the research paper entitle "Glomerulosclerosis Identification in Whole Slide Images using Semantic Segmentation", published in Computer Methods and Programs in Biomedicine Journal [1]. In that article, different methods based on deep learning for glomeruli segmentation and their classification into normal and sclerotic glomerulous are presented and discussed. The raw data used is described and provided here. In addition, the detected glomeruli are also provided as individual image files. These data will encourage research on artificial intelligence (AI) methods, create and compare fresh algorithms, and measure their usability in quantitative nephropathology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3409(20)30208-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32154349/  |  
------------------------------------------- 
10.1016/j.chaos.2020.109828  |   A worldwide multi-scale interplay among a plethora of factors, ranging from micro-pathogens and individual or population interactions to macro-scale environmental, socio-economic and demographic conditions, entails the development of highly sophisticated mathematical models for robust representation of the contagious disease dynamics that would lead to the improvement of current outbreak control strategies and vaccination and prevention policies. Due to the complexity of the underlying interactions, both deterministic and stochastic epidemiological models are built upon incomplete information regarding the infectious network. Hence, rigorous mathematical epidemiology models can be utilized to combat epidemic outbreaks. We introduce a new spatiotemporal approach (SBDiEM) for modeling, forecasting and nowcasting infectious dynamics, particularly in light of recent efforts to establish a global surveillance network for combating pandemics with the use of artificial intelligence. This model can be adjusted to describe past outbreaks as well as COVID-19. Our novel methodology may have important implications for national health systems, international stakeholders and policy makers. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-0779(20)30228-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32327901/  |  
------------------------------------------- 
10.2147/DMSO.S242585  |    Introduction:  The research of auxiliary diagnosis has always been one of the hotspots in the world. The implementation of auxiliary diagnosis support algorithm for medical text data faces challenges with interpretability and creditability. The improvement of clinical diagnostic techniques means not only the improvement of diagnostic accuracy but also the further study of diagnostic basis. Traditional research methods for diagnostic markers often require a large amount of time and economic costs. Research objects are often dozens of samples, and it is, therefore, difficult to synthesize large amounts of data. Therefore, the comprehensiveness and reliability of traditional methods have yet to be improved. Therefore, the establishment of a model that can automatically diagnose diseases and automatically provide a diagnostic basis at the same time has a positive effect on the improvement of medical diagnostic techniques. 
  Methods:  Here, we established an auxiliary diagnostic tool based on attention deep learning algorithm to diagnostic hyperlipemia and automatically predict the corresponding diagnostic markers using hematological parameters. In this paper, we not only demonstrated the ability of the proposed model to automatically diagnose diseases using text-based medical data, such as physiological parameters, but also demonstrated its ability to forecast disease diagnostic markers. Human physiological parameters are used as input to the model, and the doctor's diagnosis results as an output. Through the attention layer, the degree of attention of the model to different physiological parameters can be obtained, that is, the model provides a diagnostic basis. 
  Results:  It achieved 94% ACC, 97.48% AUC, 96% sensitivity and 92% specificity with the test dataset. All the above samples are drawn from clinical practice. Moreover, the model predicted the diagnostic markers of hyperlipidemia by the attention mechanism, and the results were fully agreeable to the golden criteria. 
  Discussion:  The auxiliary diagnosis system proposed in this paper not only achieves the accurate and robust performance, and can be used for the preliminary diagnosis of patients, but also showing its great potential to discover new diagnostic markers. Therefore, it not only can improve the efficiency of clinical diagnosis but also shorten the research period of researching a diagnosis basis to an extent. It has a positive significance to the development of the medical diagnosis level. 
  |  https://dx.doi.org/10.2147/DMSO.S242585  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210601/  |  
------------------------------------------- 
10.1177/1352458519888178  |   Advances in wearable and wireless biosensing technology pave the way for a brave new world of novel multiple sclerosis (MS) outcome measures. Our current tools for examining patients date back to the 19th century and while invaluable to the neurologist invite accompaniment from these new technologies and artificial intelligence (AI) analytical methods. While the most common biosensor tool used in MS publications to date is the accelerometer, the landscape is changing quickly with multi-sensor applications, electrodermal sensors, and wireless radiofrequency waves. Some caution is warranted to ensure novel outcomes have clear clinical relevance and stand-up to the rigors of reliability, reproducibility, and precision, but the ultimate implementation of biosensing in the MS clinical setting is inevitable. 
  |  http://journals.sagepub.com/doi/full/10.1177/1352458519888178?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1109/TNNLS.2020.2975035  |   Open-domain dialog generation, which is a crucial component of artificial intelligence, is an essential and challenging problem. In this article, we present a personalized dialog system, which leverages the advantages of multitask learning and reinforcement learning for personalized dialogue generation (MRPDG). Specifically, MRPDG consists of two subtasks: 1) an author profiling module that recognizes user characteristics from the input sentence (auxiliary task) and 2) a personalized dialog generation system that generates informative, grammatical, and coherent responses with reinforcement learning algorithms (primary task). Three kinds of rewards are proposed to generate high-quality conversations. We investigate the effectiveness of three widely used reinforcement learning methods [i.e., Q-learning, policy gradient, and actor-critic (AC) algorithm] in a personalized dialog generation system and demonstrate that the AC algorithm achieves the best results on the underlying framework. Comprehensive experiments are conducted to evaluate the performance of the proposed model on two real-life data sets. Experimental results illustrate that MRPDG is able to produce high-quality personalized dialogs for users with different characteristics. Quantitatively, the proposed model can achieve better performance than the compared methods across different evaluation metrics, such as the human evaluation, BiLingual Evaluation Understudy (BLEU), and perplexity. 
  |  https://dx.doi.org/10.1109/TNNLS.2020.2975035  |  
------------------------------------------- 
10.1063/1.5133405  |   We propose a novel type of neural networks known as "attention-based sequence-to-sequence architecture" for a model-free prediction of spatiotemporal systems. This architecture is composed of an encoder and a decoder in which the encoder acts upon a given input sequence and then the decoder yields another output sequence to make a multistep prediction at a time. In order to demonstrate the potential of this approach, we train the neural network using data numerically sampled from the Korteweg-de Vries equation-which describes the interaction between solitary waves-and then predict its future evolution. Furthermore, we validate the applicability of the approach on datasets sampled from the chaotic Lorenz system and three other partial differential equations. The results show that the proposed method can achieve good performance in predicting the evolutionary behavior of studied spatiotemporal dynamics. To the best of our knowledge, this work is the first attempt at applying attention-based sequence-to-sequence architecture to the prediction task of solitary waves. 
  |  https://dx.doi.org/10.1063/1.5133405  |  
------------------------------------------- 
10.1093/rpd/ncaa018  |   We presented an artificial intelligence-based model to predict annual effective dose (AED) value of health workers. Potential factors affecting AED and the results of annual blood tests were collected from 91 radiation workers. Filter-based feature selection strategy revealed that the eight factors plate, red cell distribution width (RDW), educational degree, nonacademic course in radiation protection (hour), working hours per month, department and the number of procedures done per year and work in radiology department or not (0,1) were the most important predictors for AED. The prediction model was developed using a multilayer perceptron neural network and these prediction parameters as inputs. The model provided favorable accuracy in predicting AED value while a regression model did not. There was a strong linear relationship between the predicted AED values and the measured doses (R-value =0.89 for training samples and 0.86 for testing samples). These results are promising and show that artificial neural networks can be used to improve/facilitate dose estimation process. 
  |  https://academic.oup.com/rpd/article-lookup/doi/10.1093/rpd/ncaa018  |  
------------------------------------------- 
10.1021/acs.jpcb.0c01618  |   Amphiphilic molecules, forming self-assembled nanoarchitectures, are typically composed of hydrophobic and hydrophilic domains. Peptide amphiphiles can be designed from two, three or four building blocks imparting novel structural and functional properties and affinities for interaction with cellular membranes or intracellular organelles. Here we present a combined numerical approach to design amphiphilic peptide scaffolds that are derived from the human nuclear Ki-67 protein. Ki-67 acts, like a biosurfactant, as a steric and electrostatic charge barrier against the collapse of mitotic chromosomes. The proposed predictive design of new Ki-67 protein-derived amphiphilic aminoacid sequences exploits the computational outcomes of a set of web-accessible predictors, which are based on machine learning methods. The ensemble of such artificial intelligence algorithms, involving support vector machine (SVM), random forest (RF) classifiers and neural networks (NN), enables the nano-engineering of a broad range of innovative peptide materials for therapeutic delivery in various applications. Amphiphilic cell-penetrating peptides (CPP), derived from natural protein sequences, may spontaneously form self-assembled nanocarriers characterized by enhanced cellular uptake. Thanks to their inherent low immunogenicity, they may enable the safe delivery of therapeutic molecules across the biological barriers. 
  |  https://dx.doi.org/10.1021/acs.jpcb.0c01618  |  
------------------------------------------- 
10.1016/j.mric.2019.12.001  |   MR imaging is the standard diagnostic modality that provides a comprehensive and accurate assessment for both osseous and soft-tissue pathologic conditions of the shoulder. This article discusses standard MR imaging and arthrography protocols used routinely in clinical practice, as well as more innovative sequences and reconstruction techniques, facilitated by the increasing availability of high-field-strength magnets and multichannel phased array surface coils and incorporation of artificial intelligence. These exciting innovations allow for a more detailed and diagnostic imaging assessment, improvements in image quality, and more rapid image acquisition. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1064-9689(19)30110-2  |  
------------------------------------------- 
10.1097/RTI.0000000000000500  |    Objectives:  The objective of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for the fully automated per lobe segmentation and emphysema quantification (EQ) on chest-computed tomography as it compares to the Global Initiative for Chronic Obstructive Lung Disease (GOLD) severity classification of chronic obstructive pulmonary disease (COPD) patients. 
  Methods:  Patients (n=137) who underwent chest-computed tomography acquisition and spirometry within 6 months were retrospectively included in this Institutional Review Board-approved and Health Insurance Portability and Accountability Act-compliant study. Patient-specific spirometry data, which included forced expiratory volume in 1 second, forced vital capacity, and the forced expiratory volume in 1 second/forced vital capacity ratio (Tiffeneau-Index), were used to assign patients to their respective GOLD stage I to IV. Lung lobe segmentation was carried out using AI-RAD Companion software prototype (Siemens Healthineers), a deep convolution image-to-image network and emphysema was quantified in each lung lobe to detect the low attenuation volume. 
  Results:  A strong correlation between the whole-lung-EQ and the GOLD stages was found (ρ=0.88, P&lt;0.0001). The most significant correlation was noted in the left upper lobe (ρ=0.85, P&lt;0.0001), and the weakest in the left lower lobe (ρ=0.72, P&lt;0.0001) and right middle lobe (ρ=0.72, P&lt;0.0001). 
  Conclusions:  AI-based per lobe segmentation and its EQ demonstrate a very strong correlation with the GOLD severity stages of COPD patients. Furthermore, the low attenuation volume of the left upper lobe not only showed the strongest correlation to GOLD severity but was also able to most clearly distinguish mild and moderate forms of COPD. This is particularly relevant due to the fact that early disease processes often elude conventional pulmonary function diagnostics. Earlier detection of COPD is a crucial element for positively altering the course of disease progression through various therapeutic options. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000500  |  
------------------------------------------- 
10.1093/neuros/nyz403  |    Background:  Although survival statistics in patients with glioblastoma multiforme (GBM) are well-defined at the group level, predicting individual patient survival remains challenging because of significant variation within strata. 
  Objective:  To compare statistical and machine learning algorithms in their ability to predict survival in GBM patients and deploy the best performing model as an online survival calculator. 
  Methods:  Patients undergoing an operation for a histopathologically confirmed GBM were extracted from the Surveillance Epidemiology and End Results (SEER) database (2005-2015) and split into a training and hold-out test set in an 80/20 ratio. Fifteen statistical and machine learning algorithms were trained based on 13 demographic, socioeconomic, clinical, and radiographic features to predict overall survival, 1-yr survival status, and compute personalized survival curves. 
  Results:  In total, 20 821 patients met our inclusion criteria. The accelerated failure time model demonstrated superior performance in terms of discrimination (concordance index = 0.70), calibration, interpretability, predictive applicability, and computational efficiency compared to Cox proportional hazards regression and other machine learning algorithms. This model was deployed through a free, publicly available software interface (https://cnoc-bwh.shinyapps.io/gbmsurvivalpredictor/). 
  Conclusion:  The development and deployment of survival prediction tools require a multimodal assessment rather than a single metric comparison. This study provides a framework for the development of prediction tools in cancer patients, as well as an online survival calculator for patients with GBM. Future efforts should improve the interpretability, predictive applicability, and computational efficiency of existing machine learning algorithms, increase the granularity of population-based registries, and externally validate the proposed prediction tool. 
  |  https://academic.oup.com/neurosurgery/article-lookup/doi/10.1093/neuros/nyz403  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31586211/  |  
------------------------------------------- 
10.1016/j.gie.2020.03.3759  |    Background and aims:  Artificial intelligence (AI) is being implemented into colonoscopy practice, but no study has investigated whether AI is cost-saving. We quantified the cost reduction from using AI as an aid in the optical diagnosis of colorectal polyps. 
  Methods:  This study is an add-on analysis of a clinical trial that investigated the performance of AI for differentiating colorectal polyps (ie, neoplastic versus non-neoplastic). We included all patients with diminutive (≤5 mm) rectosigmoid polyp for analyses. The average colonoscopy cost was compared for 2 scenarios: (1) a diagnose-and-leave strategy supported by the AI prediction (ie, diminutive rectosigmoid polyps were not removed when predicted as non-neoplastic), and (2) a resect-all-polyps strategy. Gross annual costs for colonoscopies were also calculated based on numbers and reimbursement of colonoscopies conducted under public health insurances in 4 countries. 
  Results:  Overall, 207 patients with 250 diminutive rectosigmoid polyps (104 neoplastic, 144 non-neoplastic, and 2 indeterminate) were included. AI correctly differentiated neoplastic polyps with 93.3% sensitivity, 95.2% specificity, and 95.2% negative predictive value. Thus, 105 polyps were removed whereas 145 were left under the diagnose-and-leave strategy, which was estimated to reduce the average colonoscopy cost and the gross annual reimbursement for colonoscopies by 18.9% and 149.2 million dollars in Japan, 6.9% and 12.3 million dollars in England, 7.6% and 1.1 million dollars in Norway, and 10.9% and 85.2 million dollars in the United States, respectively, compared to the resect-all-polyps strategy. 
  Conclusions:  The use of AI to enable the diagnose-and-leave strategy results in substantial cost reductions for colonoscopy. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)34034-7  |  
------------------------------------------- 
10.1016/j.dib.2019.105098  |   This data article reports on a new set of 234 competency questions for ontology development and their formalisation into a set of 131 SPARQL-OWL queries. This is the largest set of competency questions with their linked queries to date, covering several ontologies of different type in different subject domains developed by different groups of question authors and ontology developers. The dataset is focused specifically on the ontology TBox (terminological part). The dataset may serve as a manually created gold standard for testing and benchmarking, research into competency questions and querying ontologies, and tool development. The data is available in Mendeley Data. Its analysis is presented in "Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL" [15]. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3409(19)31454-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31989008/  |  
------------------------------------------- 
10.1016/j.neunet.2020.01.024  |   This paper investigates the event-triggered synchronization control of discrete-time neural networks. The main highlights are threefold: (1) a new event-triggered mechanism (ETM) is presented, which can be regarded as a switching between the discrete-time periodic sampled-data control and a continuous ETM; (2) a saturating controller which is equipped with two switching gains is designed to match the switching property of the proposed ETM; (3) a dedicated switching Lyapunov-Krasovskii functional is constructed, which takes the sawtooth constraints of control input into account. Based on these ingredients, the synchronization criteria are derived such that the considered error systems are locally stable. Whereafter, two co-design problems are discussed to maximize the set of admissible initial conditions and the triggering threshold, respectively. Finally, the effectiveness and advantages of the proposed method are validated by two numerical examples. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30034-4  |  
------------------------------------------- 
10.1080/21507740.2020.1740351  |   Deep fakes have rapidly emerged as one of the most ominous concerns within modern society. The ability to easily and cheaply generate convincing images, audio, and video via artificial intelligence will have repercussions within politics, privacy, law, security, and broadly across all of society. In light of the widespread apprehension, numerous technological efforts aim to develop tools to distinguish between reliable audio/video and the fakes. These tools and strategies will be particularly effective for consumers when their guard is naturally up, for example during election cycles. However, recent research suggests that not only can deep fakes create credible representations of reality, but they can also be employed to create false memories. Memory malleability research has been around for some time, but it relied on doctored photographs or text to generate fraudulent recollections. These recollected but fake memories take advantage of our cognitive miserliness that favors selecting those recalled memories that evoke our preferred weltanschauung. Even responsible consumers can be duped when false but belief-consistent memories, implanted when we are least vigilant can, like a Trojan horse, be later elicited at crucial dates to confirm our pre-determined biases and influence us to accomplish nefarious goals. This paper seeks to understand the process of how such memories are created, and, based on that, proposing ethical and legal guidelines for the legitimate use of fake technologies. 
  |  None  |  
------------------------------------------- 
10.1097/MOU.0000000000000741  |    Purpose of review:  Patients with nonmuscle invasive bladder cancer (NMIBC) have a high risk of recurrent tumors, even in spite of contemporary guideline recommended therapy. Follow-up recommendations are also clear (cystoscopy with cytology and upper urinary tract imaging in high-risk patients), but frequency and duration of follow-up are well defined. However, recent developments in follow-up tools might be of interest for clinical practice. 
  Recent findings:  Enhanced endoscopy improves detection and treatment of recurrences, and it can help in tailoring follow-up. However, it remains an invasive procedure. Most recently cystoscopy augmented with artificial intelligence has shown some promising results. Active surveillance, frequently done in prostate cancer patients, is also gaining attention in NMIBC follow-up. Finally markers are being studied and launched. Although not recommended by guidelines, and not used in clinical practice, recent studies have shown marker combinations with very high negative predictive values for (high risk) recurrences in follow-up of NMIBC patients. 
  Summary:  New tools for follow-up such as enhanced cystoscopy and urinary markers might help to individualize follow-up, which will result in decreasing patient discomfort, workload and costs while quality of care is maintained. 
  |  http://dx.doi.org/10.1097/MOU.0000000000000741  |  
------------------------------------------- 
10.1016/j.breast.2020.01.042  |    Background:  Persistent pain in breast cancer survivors is common. Psychological and sleep-related factors modulate perception, interpretation and coping with pain and may contribute to the clinical phenotype. The present analysis pursued the hypothesis that breast cancer survivors form subgroups, based on psychological and sleep-related parameters that are relevant to the impact of pain on the patients' life. 
  Methods:  We analysed 337 women treated for breast cancer, in whom psychological and sleep-related parameters as well as parameters related to pain intensity and interference had been acquired. Data were analysed by using supervised and unsupervised machine-learning techniques (i) to detect patient subgroups based on the pattern of psychological or sleep-related parameters, (ii) to interpret the detected cluster structure and (iii) to relate this data structure to pain interference and impact on life. 
  Results:  Artificial intelligence-based detection of data structure, implemented as self-organizing neuronal maps, identified two different clusters of patients. A smaller cluster (11.5% of the patients) had comparatively lower resilience, more depressive symptoms and lower extraversion than the other patients. In these patients, life-satisfaction, mood, and life in general were comparatively more impeded by persistent pain. 
  Conclusions:  The results support the initial hypothesis that psychological and sleep-related parameter patterns are meaningful for subgrouping patients with respect to how persistent pain after breast cancer treatments interferes with their life. This indicates that management of pain should address more complex features than just pain intensity. Artificial intelligence is a useful tool in the identification of subgroups of patients based on psychological factors. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9776(20)30057-6  |  
------------------------------------------- 
10.1016/j.ajo.2020.03.035  |    Purpose:  To assess the performance of deep learning approaches for differentiating nonglaucomatous optic neuropathy versus glaucomatous optic neuropathy (GON) on color fundus photographs by the use of image recognition. 
  Design:  Development of an Artificial Intelligence Classification algorithm METHODS: Setting: Institutional. 
  Subjects:  An analysis including 3,815 fundus images from the PACS system of Seoul National University Bundang Hospital consisting of 2,883 normal optic disc images, 446 nonglaucomatous optic neuropathy with optic disc pallor (NGON) and 486 GON. 
  Observations:  The presence of NGON and GON was interpreted by two expert neuro-ophthalmologists and had corroborate evidence on visual field testing and optical coherence tomography. Images were preprocessed in size and color enhancement before input. We applied the convolutional neural network (CNN) of ResNet-50 architecture. The area under the Precision-Recall curve (average precision, AP) was evaluated for the efficacy of deep learning algorithms to assess the performance of classifying nonglaucomatous optic disc pallor and GON. 
  Results:  The diagnostic accuracy of the ResNet-50 model to detect GON among NGON images showed a sensitivity of 93.4% and specificity of 81.8%. The area under the Precision-Recall curve for differentiating NGON vs GON showed an AP value of 0.874. False positive cases were found with extensive areas of peripapillary atrophy and tilted optic discs. 
  Conclusion:  Artificial intelligence-based deep learning algorithms for detecting optic disc diseases showed excellent performance in differentiating nonglaucomatous and glaucomatous optic neuropathy on color fundus photographs, necessitating further research for clinical application. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9394(20)30146-X  |  
------------------------------------------- 
10.1021/acsami.9b22925  |   Neuromorphic computing inspired by the neural systems in human brain will overcome the issue of independent information processing and storage. An artificial synaptic device as a basic unit of a neuromorphic computing system can perform signal processing with low power consumption, and exploring different types of synaptic transistors is essential to provide suitable artificial synaptic devices for artificial intelligence. Hence, for the first time, an electret-based synaptic transistor (EST) is presented, which successfully shows synaptic behaviors including excitatory/inhibitory postsynaptic current, paired-pulse facilitation/depression, long-term memory, and high-pass filtering. Moreover, a neuromorphic computing simulation based on our EST is performed using the handwritten artificial neural network, which exhibits an excellent recognition accuracy (85.88%) after 120 learning epochs, higher than most reported organic synaptic transistors and close to the ideal accuracy (92.11%). Such a novel synaptic device enriches the diversity of synaptic transistors, laying the foundation for the diversified development of the next generation of neuromorphic computing systems. 
  |  https://dx.doi.org/10.1021/acsami.9b22925  |  
------------------------------------------- 
10.3389/fimmu.2020.00380  |   Sepsis is defined as dysregulated host response caused by systemic infection, leading to organ failure. It is a life-threatening condition, often requiring admission to an intensive care unit (ICU). The causative agents and processes involved are multifactorial but are characterized by an overarching inflammatory response, sharing elements in common with severe inflammatory response syndrome (SIRS) of non-infectious origin. Sepsis presents with a range of pathophysiological and genetic features which make clinical differentiation from SIRS very challenging. This may reflect a poor understanding of the key gene inter-activities and/or pathway associations underlying these disease processes. Improved understanding is critical for early differential recognition of sepsis and SIRS and to improve patient management and clinical outcomes. Judicious selection of gene biomarkers suitable for development of diagnostic tests/testing could make differentiation of sepsis and SIRS feasible. Here we describe a methodologic framework for the identification and validation of biomarkers in SIRS, sepsis and septic shock patients, using a 2-tier gene screening, artificial neural network (ANN) data mining technique, using previously published gene expression datasets. Eight key hub markers have been identified which may delineate distinct, core disease processes and which show potential for informing underlying immunological and pathological processes and thus patient stratification and treatment. These do not show sufficient fold change differences between the different disease states to be useful as primary diagnostic biomarkers, but are instrumental in identifying candidate pathways and other associated biomarkers for further exploration. 
  |  https://doi.org/10.3389/fimmu.2020.00380  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32318053/  |  
------------------------------------------- 
10.1111/1759-7714.13391  |    Background:  IBM Watson for Oncology (WFO) provides physicians with evidence-based treatment options. This study was designed to explore the concordance of the suggested therapeutic regimen for advanced non-small cell lung (NSCLC) cancer patients between the updated version of WFO and physicians in our department, in order to reflect the differences of cancer treatment between China and the United States. 
  Methods:  Retrospective data from 165 patients with advanced NSCLC from September 2014 to March 2018 were entered manually into WFO. WFO recommendations were provided in three categories: recommended, for consideration, and not recommended. Concordance was analyzed by comparing the treatment decisions proposed by WFO with the real treatment. Potential influenced factors were also analyzed. 
  Results:  Overall, the treatment recommendations were concordant in 73.3% (121/165) of cases. When two alternative drugs such as icotinib and nedaplatin were included as "for consideration," the total consistency could be elevated from 73.3% to 90.3%(149/165). The logistic regression analysis showed that gender (P = 0.096), ECOG (P = 0.0.502), smoking (P = 0.455), and pathology (P = 0.633) had no effect on consistency, but stages (P = 0.019), including stage ≤III (77.8%, 21/27) and stage IV (93.5%, 129/138) had significant effects on consistency. 
  Conclusions:  In China, most of the treatment recommendations of WFO are consistent with the real world treatment. Factors such as patient preferences, prices, drug approval and medical insurance are also taken into consideration, and they ultimately affect the inconsistency. To be comprehensively and rapidly applied in China, localization needs to be accelerated by WFO. 
  |  https://doi.org/10.1111/1759-7714.13391  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32191394/  |  
------------------------------------------- 
10.1007/s10278-019-00282-4  |   The explosion of medical imaging data along with the advent of big data analytics has launched an exciting era for clinical research. One factor affecting the ability to aggregate large medical image collections for research is the lack of infrastructure for automated data annotation. Among all imaging modalities, annotation of magnetic resonance (MR) images is particularly challenging due to the non-standard labeling of MR image types. In this work, we aimed to train a deep neural network to annotate MR image sequence type for scans of brain tumor patients. We focused on the four most common MR sequence types within neuroimaging: T1-weighted (T1W), T1-weighted post-gadolinium contrast (T1Gd), T2-weighted (T2W), and T2-weighted fluid-attenuated inversion recovery (FLAIR). Our repository contains images acquired using a variety of pulse sequences, sequence parameters, field strengths, and scanner manufacturers. Image selection was agnostic to patient demographics, diagnosis, and the presence of tumor in the imaging field of view. We used a total of 14,400 two-dimensional images, each visualizing a different part of the brain. Data was split into train, validation, and test sets (9600, 2400, and 2400 images, respectively) and sets consisted of equal-sized groups of image types. Overall, the model reached an accuracy of 99% on the test set. Our results showed excellent performance of deep learning techniques in predicting sequence types for brain tumor MR images. We conclude deep learning models can serve as tools to support clinical research and facilitate efficient database management. 
  |  https://doi.org/10.1007/s10278-019-00282-4  |  
------------------------------------------- 
10.1097/MD.0000000000019563  |   Thrombotic thrombocytopenic purpura (TTP) is a life-threatening disease, and its mortality rate is 10% to 20%. However, there are currently only a few markers to predict the prognosis in patients with TTP. We aimed to identify several clinical indices and laboratory parameters for predicting the prognosis of TTP at admission.A single-centre observational cohort study that included patients with TTP from the First Affiliated Hospital of Zhengzhou University in China was conducted from January 1, 2012 to November 30, 2018. The primary outcome was prognosis, including in-hospital mortality, major thromboembolic events, or failure to achieve remission at discharge. We used the random forest method to identify the best set of predictors.Eighty-seven patients with TTP were identified, of whom 12 died during the treatment. The total number of patients within-hospital mortality, major thromboembolic events, and failure to achieve remission at discharge was 58. The machine learning method showed that the D-dimer level was the strongest predictor of the primary outcome. Receiver operating characteristic (ROC) analysis demonstrated that the sensitivity and specificity of the D-dimer level alone for identifying high-risk patients were 78% and 81%, respectively, with an optimum diagnostic cut-off value of 770 ng/mL. The area under the ROC curve (AUC) was 0.80, and the 95% confidence interval (CI) was 0.70 to 0.90.This study found that the D-dimer level exhibited a good predictive ability for prognosis in patients with TTP. These findings may aid in the development of new and intensive treatment strategies to achieve remission among high-risk patients. However, external validation is necessary to confirm the generalizability of our approach across populations and treatment practices. 
  |  http://dx.doi.org/10.1097/MD.0000000000019563  |  
------------------------------------------- 
10.3390/s20061751  |   Measuring biodiversity simultaneously in different locations, at different temporal scales, and over wide spatial scales is of strategic importance for the improvement of our understanding of the functioning of marine ecosystems and for the conservation of their biodiversity. Monitoring networks of cabled observatories, along with other docked autonomous systems (e.g., Remotely Operated Vehicles [ROVs], Autonomous Underwater Vehicles [AUVs], and crawlers), are being conceived and established at a spatial scale capable of tracking energy fluxes across benthic and pelagic compartments, as well as across geographic ecotones. At the same time, optoacoustic imaging is sustaining an unprecedented expansion in marine ecological monitoring, enabling the acquisition of new biological and environmental data at an appropriate spatiotemporal scale. At this stage, one of the main problems for an effective application of these technologies is the processing, storage, and treatment of the acquired complex ecological information. Here, we provide a conceptual overview on the technological developments in the multiparametric generation, storage, and automated hierarchic treatment of biological and environmental information required to capture the spatiotemporal complexity of a marine ecosystem. In doing so, we present a pipeline of ecological data acquisition and processing in different steps and prone to automation. We also give an example of population biomass, community richness and biodiversity data computation (as indicators for ecosystem functionality) with an Internet Operated Vehicle (a mobile crawler). Finally, we discuss the software requirements for that automated data processing at the level of cyber-infrastructures with sensor calibration and control, data banking, and ingestion into large data portals. 
  |  http://www.mdpi.com/resolver?pii=s20061751  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245204/  |  
------------------------------------------- 
10.1097/MD.0000000000019820  |   Acute respiratory distress syndrome (ARDS) is characterized as a neutrophil-dominant disorder without effective pharmacological interventions. Knowledge of neutrophils in ARDS patients at the transcriptome level is still limited. We aimed to identify the hub genes and key pathways in neutrophils of patients with ARDS. The transcriptional profiles of neutrophils from ARDS patients and healthy volunteers were obtained from the GSE76293 dataset. The differentially expressed genes (DEGs) between ARDS and healthy samples were screened using the limma R package. Subsequently, functional and pathway enrichment analyses were performed based on the database for annotation, visualization, and integrated discovery (DAVID). The construction of a protein-protein interaction network was carried out using the search tool for the retrieval of interacting genes (STRING) database and the network was visualized by Cytoscape software. The Cytoscape plugins cytoHubba and MCODE were used to identify hub genes and significant modules. Finally, 136 upregulated genes and 95 downregulated genes were identified. Gene ontology analyses revealed MHC class II plays a major role in functional annotations. SLC11A1, ARG1, CHI3L1, HP, LCN2, and MMP8 were identified as hub genes, and they were all involved in the neutrophil degranulation pathway. The MAPK and neutrophil degranulation pathways in neutrophils were considered as key pathways in the pathogenesis of ARDS. This study improves our understanding of the biological characteristics of neutrophils and the mechanisms underlying ARDS, and key pathways and hub genes identified in this work can serve as targets for novel ARDS treatment strategies. 
  |  http://dx.doi.org/10.1097/MD.0000000000019820  |  
------------------------------------------- 
10.1016/j.cub.2020.01.059  |   Ancient Chinese poetry is constituted by structured language that deviates from ordinary language usage [1, 2]; its poetic genres impose unique combinatory constraints on linguistic elements [3]. How does the constrained poetic structure facilitate speech segmentation when common linguistic [4-8] and statistical cues [5, 9] are unreliable to listeners in poems? We generated artificial Jueju, which arguably has the most constrained structure in ancient Chinese poetry, and presented each poem twice as an isochronous sequence of syllables to native Mandarin speakers while conducting magnetoencephalography (MEG) recording. We found that listeners deployed their prior knowledge of Jueju to build the line structure and to establish the conceptual flow of Jueju. Unprecedentedly, we found a phase precession phenomenon indicating predictive processes of speech segmentation-the neural phase advanced faster after listeners acquired knowledge of incoming speech. The statistical co-occurrence of monosyllabic words in Jueju negatively correlated with speech segmentation, which provides an alternative perspective on how statistical cues facilitate speech segmentation. Our findings suggest that constrained poetic structures serve as a temporal map for listeners to group speech contents and to predict incoming speech signals. Listeners can parse speech streams by using not only grammatical and statistical cues but also their prior knowledge of the form of language. VIDEO ABSTRACT. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9822(20)30103-2  |  
------------------------------------------- 
10.1371/journal.pone.0227991  |   This study reports complete plastome sequences for six species of Neotropical Cranichideae and focuses on identification of the most variable regions (hotspots) in this group of orchids. These structure of these six plastomes is relatively conserved, exhibiting lengths ranging between 142,599 to 154,562 bp with 36.7% GC on average and exhibiting typical quadripartite arrangement (LSC, SSC and two IRs). Variation detected in the LSC/IR and SSC/IR junctions is explained by the loss of ndhF and ycf1 length variation. For the two genera of epiphytic clade in Spiranthinae, almost whole sets of the ndh-gene family were missing. Eight mutation hotspots were identified based on nucleotide diversity, sequence variability and parsimony-informative sites. Three of them (rps16-trnQ, trnT-trnL, rpl32-trnL) seem to be universal hotspots in the family, and the other five (trnG-trnR, trnR-atpA, trnP-psaJ, rpl32-infA, and rps15-ycf1) are described for the first time as orchid molecular hotspots. These regions have much more variation than all those used previously in phylogenetics of the group and offer useful plastid markers for phylogenetic, barcoding and population genetic studies. The use of whole plastomes or exclusive no-gap matrices also positioned with high support the holomycotrophic Rhizanthella among Orchidoideae plastomes in model-based analyses, showing the utility of plastomes for phylogenetic placement of this unusual genus. 
  |  http://dx.plos.org/10.1371/journal.pone.0227991  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31990943/  |  
------------------------------------------- 
10.3390/s20072018  |   School bullying is a serious problem among teenagers. School violence is one type of school bullying and considered to be the most harmful. As AI (Artificial Intelligence) techniques develop, there are now new methods to detect school violence. This paper proposes a video-based school violence detecting algorithm. This algorithm first detects foreground moving targets via the KNN (K-Nearest Neighbor) method and then preprocesses the detected targets via morphological processing methods. Then, this paper proposes a circumscribed rectangular frame integrating method to optimize the circumscribed rectangular frame of moving targets. Rectangular frame features and optical-flow features were extracted to describe the differences between school violence and daily-life activities. We used the Relief-F and Wrapper algorithms to reduce the feature dimension. SVM (Support Vector Machine) was applied as the classifier, and 5-fold cross validation was performed. The accuracy was 89.6%, and the precision was 94.4%. To further improve the recognition performance, we developed a DT-SVM (Decision Tree-SVM) two-layer classifier. We used boxplots to determine some features of the DT layer that are able to distinguish between typical physical violence and daily-life activities and between typical daily-life activities and physical violence. For the remainder of activities, the SVM layer performed a classification. For this DT-SVM classifier, the accuracy reached 97.6%, and the precision reached 97.2%, thus showing a significant improvement. 
  |  http://www.mdpi.com/resolver?pii=s20072018  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32260274/  |  
------------------------------------------- 
10.1371/journal.pone.0228459  |    Background:  Carbapenem-resistant Klebsiella pneumoniae (CRKP) is emerging as a significant pathogen causing healthcare-associated infections. Matrix-assisted laser desorption/ionisation mass spectrometry time-of-flight mass spectrometry (MALDI-TOF MS) is used by clinical microbiology laboratories to address the need for rapid, cost-effective and accurate identification of microorganisms. We evaluated application of machine learning methods for differentiation of drug resistant bacteria from susceptible ones directly using the profile spectra of whole cells MALDI-TOF MS in 46 CRKP and 49 CSKP isolates. 
  Methods:  We developed a two-step strategy for data preprocessing consisting of peak matching and a feature selection step before supervised machine learning analysis. Subsequently, five machine learning algorithms were used for classification. 
  Results:  Random forest (RF) outperformed other four algorithms. Using RF algorithm, we correctly identified 93% of the CRKP and 100% of the CSKP isolates with an overall classification accuracy rate of 97% when 80 peaks were selected as input features. 
  Conclusions:  We conclude that CRKPs can be differentiated from CSKPs through RF analysis. We used direct colony method, and only one spectrum for an isolate for analysis, without modification of current protocol. This allows the technique to be easily incorporated into clinical practice in the future. 
  |  http://dx.plos.org/10.1371/journal.pone.0228459  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32027671/  |  
------------------------------------------- 
10.1097/CRD.0000000000000294  |   The computer science technology trend called artificial intelligence (AI) is not new. Both machine learning and deep learning AI applications have recently begun to impact cardiovascular medicine. Scientists working in the AI domain have long recognized the importance of data quality and provenance to AI algorithm efficiency and accuracy. A diverse array of cardiovascular raw data sources of variable quality-electronic medical records, radiological picture archiving and communication systems, laboratory results, omics, etc.-are available to train AI algorithms for predictive modeling of clinical outcomes (in-hospital mortality, acute coronary syndrome risk stratification, etc.), accelerated image interpretation (edge detection, tissue characterization, etc.) and enhanced phenotyping of heterogeneous conditions (heart failure with preserved ejection fraction, hypertension, etc.). A number of software as medical device narrow AI products for cardiac arrhythmia characterization and advanced image deconvolution are now Food and Drug Administration approved, and many others are in the pipeline. Present and future health professionals using AI-infused analytics and wearable devices have 3 critical roles to play in their informed development and ethical application in practice: (1) medical domain experts providing clinical context to computer and data scientists, (2) data stewards assuring the quality, relevance and provenance of data inputs, and (3) real-time and post-hoc interpreters of AI black box solutions and recommendations to patients. The next wave of so-called contextual adaption AI technologies will more closely approximate human decision-making, potentially augmenting cardiologists' real-time performance in emergency rooms, catheterization laboratories, imaging suites, and clinics. However, before such higher order AI technologies are adopted in the clinical setting and by healthcare systems, regulatory agencies, and industry must jointly develop robust AI standards of practice and transparent technology insertion rule sets. 
  |  http://dx.doi.org/10.1097/CRD.0000000000000294  |  
------------------------------------------- 
10.1038/s41467-020-14562-z  |   Neoantigen burden is regarded as a fundamental determinant of response to immunotherapy. However, its predictive value remains in question because some tumours with high neoantigen load show resistance. Here, we investigate our patient cohort together with a public cohort by our algorithms for the modelling of peptide-MHC binding and inter-cohort genomic prediction of therapeutic resistance. We first attempt to predict MHC-binding peptides at high accuracy with convolutional neural networks. Our prediction outperforms previous methods in &gt; 70% of test cases. We then develop a classifier that can predict resistance from functional mutations. The predictive genes are involved in immune response and EGFR signalling, whereas their mutation patterns reflect positive selection. When integrated with our neoantigen profiling, these anti-immunogenic mutations reveal higher predictive power than known resistance factors. Our results suggest that the clinical benefit of immunotherapy can be determined by neoantigens that induce immunity and functional mutations that facilitate immune evasion. 
  |  http://dx.doi.org/10.1038/s41467-020-14562-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32075964/  |  
------------------------------------------- 
10.1016/j.bbamem.2020.183319  |   SecA is an essential component of the Sec protein secretion pathway in bacteria. Secretory proteins targeted to the Sec pathway by their N-terminal signal peptide bind to SecA, which couples binding and hydrolysis of adenosine triphosphate with movement of the secretory protein across the membrane-embedded SecYEG protein translocon. The phylogenetic diversity of bacteria raises the important question as to whether the region of SecA where the pre-protein binds has organism-specific sequence features that might impact the reaction mechanism of SecA. To address this question we established a large data set of SecA protein sequences and implemented a protocol to cluster and analyze these sequences according to features of two of the SecA functional domains, the protein binding domain and the nucleotide-binding domain 1. We identify remarkable sequence diversity of the protein binding domain, but also conserved motifs with potential role in protein binding. The N-terminus of SecA has sequence motifs that could help anchor SecA to the membrane. The overall sequence length and net estimated charge of SecA sequences depend on the organism. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0005-2736(20)30150-4  |  
------------------------------------------- 
10.1097/MAO.0000000000002440  |    Objective:  The use of machine learning technology to automate intellectual processes and boost clinical process efficiency in medicine has exploded in the past 5 years. Machine learning excels in automating pattern recognition and in adapting learned representations to new settings. Moreover, machine learning techniques have the advantage of incorporating complexity and are free from many of the limitations of traditional deterministic approaches. Cochlear implants (CI) are a unique fit for machine learning techniques given the need for optimization of signal processing to fit complex environmental scenarios and individual patients' CI MAPping. However, there are many other opportunities where machine learning may assist in CI beyond signal processing. The objective of this review was to synthesize past applications of machine learning technologies for pediatric and adult CI and describe novel opportunities for research and development. 
  Data sources:  The PubMed/MEDLINE, EMBASE, Scopus, and ISI Web of Knowledge databases were mined using a directed search strategy to identify the nexus between CI and artificial intelligence/machine learning literature. 
  Study selection:  Non-English language articles, articles without an available abstract or full-text, and nonrelevant articles were manually appraised and excluded. Included articles were evaluated for specific machine learning methodologies, content, and application success. 
  Data synthesis:  The database search identified 298 articles. Two hundred fifty-nine articles (86.9%) were excluded based on the available abstract/full-text, language, and relevance. The remaining 39 articles were included in the review analysis. There was a marked increase in year-over-year publications from 2013 to 2018. Applications of machine learning technologies involved speech/signal processing optimization (17; 43.6% of articles), automated evoked potential measurement (6; 15.4%), postoperative performance/efficacy prediction (5; 12.8%), and surgical anatomy location prediction (3; 7.7%), and 2 (5.1%) in each of robotics, electrode placement performance, and biomaterials performance. 
  Conclusion:  The relationship between CI and artificial intelligence is strengthening with a recent increase in publications reporting successful applications. Considerable effort has been directed toward augmenting signal processing and automating postoperative MAPping using machine learning algorithms. Other promising applications include augmenting CI surgery mechanics and personalized medicine approaches for boosting CI patient performance. Future opportunities include addressing scalability and the research and clinical communities' acceptance of machine learning algorithms as effective techniques. 
  |  http://dx.doi.org/10.1097/MAO.0000000000002440  |  
------------------------------------------- 
10.2214/AJR.19.22346  |   <b>OBJECTIVE.</b> The purpose of this study was to assess, by analyzing features of the primary tumor with <sup>18</sup>F-FDG PET, the utility of deep machine learning with a convolutional neural network (CNN) in predicting the potential of newly diagnosed non-small cell lung cancer (NSCLC) to metastasize to lymph nodes or distant sites. <b>MATERIALS AND METHODS.</b> Consecutively registered patients with newly diagnosed, untreated NSCLC were retrospectively included in a single-center study. PET images were segmented with local image features extraction software, and data were used for CNN training and validation after data augmentation strategies were used. The standard of reference for designation of N category was invasive lymph node sampling or 6-month follow-up imaging. Distant metastases developing during the study follow-up period were assessed by imaging (CT or PET/CT), in tissue obtained from new suspected sites of disease, and according to the treating oncologist's designation. <b>RESULTS.</b> A total of 264 patients with NSCLC participated in follow-up for a median of 25.2 months (range, 6-43 months). N category designations were available for 223 of 264 (84.5%) patients, and M category for all 264. The sensitivity, specificity, and accuracy of CNN for predicting node positivity were 0.74 ± 0.32, 0.84 ± 0.16, and 0.80 ± 0.17. The corresponding values for predicting distant metastases were 0.45 ± 0.08, 0.79 ± 0.06, and 0.63 ± 0.05. <b>CONCLUSION.</b> This study showed that using a CNN to analyze segmented PET images of patients with previously untreated NSCLC can yield moderately high accuracy for designation of N category, although this may be insufficient to preclude invasive lymph node sampling. The sensitivity of the CNN in predicting distant metastases is fairly poor, although specificity is moderately high. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.22346  |  
------------------------------------------- 
10.1016/j.jtbi.2020.110278  |    Motivation:  Interactions between proteins and peptides influence biological functions. Predicting such bio-molecular interactions can lead to faster disease prevention and help in drug discovery. Experimental methods for determining protein-peptide binding sites are costly and time-consuming. Therefore, computational methods have become prevalent. However, existing models show extremely low detection rates of actual peptide binding sites in proteins. To address this problem, we employed a two-stage technique - first, we extracted the relevant features from protein sequences and transformed them into images applying a novel method and then, we applied a convolutional neural network to identify the peptide binding sites in proteins. 
  Results:  We found that our approach achieves 67% sensitivity or recall (true positive rate) surpassing existing methods by over 35%. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-5193(20)30133-8  |  
------------------------------------------- 
10.1136/bmjopen-2019-033096  |    Objectives:  Socially assistive humanoid robots are considered a promising technology to tackle the challenges in health and social care posed by the growth of the ageing population. The purpose of our study was to explore the current evidence on barriers and enablers for the implementation of humanoid robots in health and social care. 
  Design:  Systematic review of studies entailing hands-on interactions with a humanoid robot. 
  Setting:  From April 2018 to June 2018, databases were searched using a combination of the same search terms for articles published during the last decade. Data collection was conducted by using the <i>Rayyan</i> software, a standardised predefined grid, and a risk of bias and a quality assessment tool. 
  Participants:  Post-experimental data were collected and analysed for a total of 420 participants. Participants comprised: older adults (n=307) aged ≥60 years, with no or some degree of age-related cognitive impairment, residing either in residential care facilities or at their home; care home staff (n=106); and informal caregivers (n=7). 
  Primary outcomes:  Identification of enablers and barriers to the implementation of socially assistive humanoid robots in health and social care, and consequent insights and impact. Future developments to inform further research. 
  Results:  Twelve studies met the eligibility criteria and were included. None of the selected studies had an experimental design; hence overall quality was low, with high risks of biases. Several studies had no comparator, no baseline, small samples, and self-reported measures only. Within this limited evidence base, the enablers found were enjoyment, usability, personalisation and familiarisation. Barriers were related to technical problems, to the robots' limited capabilities and the negative preconceptions towards the use of robots in healthcare. Factors which produced mixed results were the robot's human-like attributes, previous experience with technology and views of formal and informal carers. 
  Conclusions:  The available evidence related to implementation factors of socially assistive humanoid robots for older adults is limited, mainly focusing on aspects at individual level, and exploring acceptance of this technology. Investigation of elements linked to the environment, organisation, societal and cultural milieu, policy and legal framework is necessary. 
  Prospero registration number:  CRD42018092866. 
  |  http://bmjopen.bmj.com/cgi/pmidlookup?view=long&pmid=31924639  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31924639/  |  
------------------------------------------- 
10.3390/biom10050665  |   A proper echocardiographic study requires several video clips recorded from different acquisition angles for observation of the complex cardiac anatomy. However, these video clips are not necessarily labeled in a database. Identification of the acquired view becomes the first step of analyzing an echocardiogram. Currently, there is no consensus whether the mislabeled samples can be used to create a feasible clinical prediction model of ejection fraction (EF). The aim of this study was to test two types of input methods for the classification of images, and to test the accuracy of the prediction model for EF in a learning database containing mislabeled images that were not checked by observers. We enrolled 340 patients with five standard views (long axis, short axis, 3-chamber view, 4-chamber view and 2-chamber view) and 10 images in a cycle, used for training a convolutional neural network to classify views (total 17,000 labeled images). All DICOM images were rigidly registered and rescaled into a reference image to fit the size of echocardiographic images. We employed 5-fold cross validation to examine model performance. We tested models trained by two types of data, averaged images and 10 selected images. Our best model (from 10 selected images) classified video views with 98.1% overall test accuracy in the independent cohort. In our view classification model, 1.9% of the images were mislabeled. To determine if this 98.1% accuracy was acceptable for creating the clinical prediction model using echocardiographic data, we tested the prediction model for EF using learning data with a 1.9% error rate. The accuracy of the prediction model for EF was warranted, even with training data containing 1.9% mislabeled images. The CNN algorithm can classify images into five standard views in a clinical setting. Our results suggest that this approach may provide a clinically feasible accuracy level of view classification for the analysis of echocardiographic data. 
  |  http://www.mdpi.com/resolver?pii=biom10050665  |  
------------------------------------------- 
10.1038/s41467-019-14108-y  |   The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards. 
  |  http://dx.doi.org/10.1038/s41467-019-14108-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31932590/  |  
------------------------------------------- 
10.3758/s13428-019-01272-8  |   We present a new dataset of English word recognition times for a total of 62 thousand words, called the English Crowdsourcing Project. The data were collected via an internet vocabulary test in which more than one million people participated. The present dataset is limited to native English speakers. Participants were asked to indicate which words they knew. Their response times were registered, although at no point were the participants asked to respond as quickly as possible. Still, the response times correlate around .75 with the response times of the English Lexicon Project for the shared words. Also, the results of virtual experiments indicate that the new response times are a valid addition to the English Lexicon Project. This not only means that we have useful response times for some 35 thousand extra words, but we now also have data on differences in response latencies as a function of education and age. 
  |  https://dx.doi.org/10.3758/s13428-019-01272-8  |  
------------------------------------------- 
10.1002/jrsm.1398  |    Background:  Evidence from new health technologies is growing, along with demands for evidence to inform policy decisions, creating challenges in completing health technology assessments (HTAs)/systematic reviews (SRs) in a timely manner. Software can decrease the time and burden by automating the process, but evidence validating such software is limited. We tested the accuracy of RobotReviewer, a semi-autonomous risk of bias (RoB) assessment tool, and its agreement with human reviewers. 
  Methods:  Two reviewers independently conducted RoB assessments on a sample of randomized controlled trials (RCTs), and their consensus ratings were compared with those generated by RobotReviewer. Agreement with the human reviewers was assessed using percent agreement and weighted kappa (κ). The accuracy of RobotReviewer was also assessed by calculating the sensitivity, specificity, and area under the curve in comparison to the consensus agreement of the human reviewers. 
  Results:  The study included 372 RCTs. Inter-rater reliability ranged from κ = -0.06 (no agreement) for blinding of participants and personnel to κ = 0.62 (good agreement) for random sequence generation (excluding overall RoB). RobotReviewer was found to use a high percentage of "irrelevant supporting quotations" to complement RoB assessments for blinding of participants and personnel (72.6%), blinding of outcome assessment (70.4%), and allocation concealment (54.3%). 
  Conclusion:  RobotReviewer can help with risk of bias assessment of RCTs but cannot replace human evaluations. Thus, reviewers should check and validate RoB assessments from RobotReviewer by consulting the original article when not relevant supporting quotations are provided by RobotReviewer. This consultation is in line with the recommendation provided by the developers. 
  |  https://doi.org/10.1002/jrsm.1398  |  
------------------------------------------- 
10.1159/000505021  |   In fetal cardiology, imaging (especially echocardiography) has demonstrated to help in the diagnosis and monitoring of fetuses with a compromised cardiovascular system potentially associated with several fetal conditions. Different ultrasound approaches are currently used to evaluate fetal cardiac structure and function, including conventional 2-D imaging and M-mode and tissue Doppler imaging among others. However, assessment of the fetal heart is still challenging mainly due to involuntary movements of the fetus, the small size of the heart, and the lack of expertise in fetal echocardiography of some sonographers. Therefore, the use of new technologies to improve the primary acquired images, to help extract measurements, or to aid in the diagnosis of cardiac abnormalities is of great importance for optimal assessment of the fetal heart. Machine leaning (ML) is a computer science discipline focused on teaching a computer to perform tasks with specific goals without explicitly programming the rules on how to perform this task. In this review we provide a brief overview on the potential of ML techniques to improve the evaluation of fetal cardiac function by optimizing image acquisition and quantification/segmentation, as well as aid in improving the prenatal diagnoses of fetal cardiac remodeling and abnormalities. 
  |  https://www.karger.com?DOI=10.1159/000505021  |  
------------------------------------------- 
10.1016/j.oraloncology.2020.104727  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1368-8375(20)30163-9  |  
------------------------------------------- 
10.1080/10503307.2020.1741047  |   <b>ABSTRACT</b><b>Objective:</b> Therapist interpersonal skills are foundational to psychotherapy. However, assessment is labor intensive and infrequent. This study evaluated if machine learning (ML) tools can automatically assess therapist interpersonal skills.<b>Method:</b> Data were drawn from a previous study in which 164 undergraduate students (i.e., not clinical trainees) completed the Facilitative Interpersonal Skills (FIS) task. This task involves responding to video vignettes depicting interpersonally challenging moments in psychotherapy. Trained raters scored the responses. We used an elastic net model on top of a term frequency-inverse document frequency representation to predict FIS scores.<b>Results:</b> Models predicted FIS total and item-level scores above chance (<i>rho</i>s = .27-.53, <i>p</i>s &lt; .001), achieving 31-60% of human reliability. Models explained 13-24% of the variance in FIS total and item-level scores on a held out set of data (<i>R</i><sup>2</sup>), with the exception of the two items most reliant on vocal cues (verbal fluency, emotional expression), for which models explained ≤1% of variance.<b>Conclusion:</b> ML may be a promising approach for automating assessment of constructs like interpersonal skill previously coded by humans. ML may perform best when the standardized stimuli limit the "space" of potential responses (vs. naturalistic psychotherapy) and when models have access to the same data available to raters (i.e., transcripts). 
  |  None  |  
------------------------------------------- 
10.3233/JAD-191169  |    Background:  Machine learning (ML) is a promising technique for patient-specific prediction of mild cognitive impairment (MCI) and dementia development. Neuropsychiatric symptoms (NPS) might improve the accuracy of ML models but have barely been used for this purpose. 
  Objectives:  To investigate if baseline mild behavioral impairment (MBI) status used for NPS quantification along with brain morphology features are predictive of follow-up diagnosis, median 40 months later in patients with normal cognition (NC) or MCI. 
  Method:  Baseline neuroimaging, neuropsychiatric, and clinical data from 102 individuals with NC and 239 with MCI were extracted from the Alzheimer's Disease Neuroimaging Initiative database. Neuropsychiatric inventory questionnaire items were transformed to MBI domains using a published algorithm. Diagnosis at latest follow-up was used as the outcome variable and ground truth classification. A logistic model tree classifier combined with information gain feature selection was trained to predict follow-up diagnosis. 
  Results:  In the binary classification (NC versus MCI/AD), the optimal ML model required only two features from over 200, MBI total score and left hippocampal volume. These features correctly classified participants as remaining normal or developing cognitive impairment with 84.4% accuracy (area under the receiver operating characteristics curve [ROC-AUC] = 0.86). Seven features were selected for the three-class model (NC versus MCI versus dementia) achieving an accuracy of 58.8% (ROC-AUC=0.73). 
  Conclusion:  Baseline NPS, categorized for MBI domain and duration, have prognostic utility in addition to brain morphology measures for predicting diagnosis change using ML. MBI total score, followed by impulse dyscontrol and affective dysregulation were most predictive of future diagnosis. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/JAD-191169  |  
------------------------------------------- 
10.3390/diagnostics10020079  |   Accurate clinical evaluation of renal osteodystrophy (ROD) is currently accomplished using invasive in vivo transiliac bone biopsy, followed by in vitro histomorphometry. In this study, we demonstrate that an alternative method for ROD assessment is through a fast, label-free Raman recording of multiple biomarkers combined with computational analysis for predicting the minimally required number of spectra for sample classification at defined accuracies. Four clinically relevant biomarkers: the mineral-to-matrix ratio, the carbonate-to-matrix ratio, phenylalanine, and calcium contents were experimentally determined and simultaneously considered as input to a linear discriminant analysis (LDA). Additionally, sample evaluation was performed with a linear support vector machine (LSVM) algorithm, with a 300 variable input. The computed probabilities based on a single spectrum were only marginally different (~80% from LDA and ~87% from LSVM), both providing an unacceptable classification power for a correct sample assignment. However, the Type I and Type II assignment errors confirm that a relatively small number of independent spectra (7 spectra for Type I and 5 spectra for Type II) is necessary for a <i>p</i> &lt; 0.05 error probability. This low number of spectra supports the practicality of future in vivo Raman translation for a fast and accurate ROD detection in clinical settings. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10020079  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023980/  |  
------------------------------------------- 
10.1111/ics.12602  |    Objective:  These were two folds: at first, to develop an automatic grading system specifically dedicated to some facial signs of men, similar to the one previously validated on women of different ethnic ancestry and second, to assess its potential in detecting and grading the possible impacts of a severe aerial urban pollution on some facial signs of Chinese men. 
  Methods:  In both studies, selfie images were obtained from differently aged men. Nine facial signs were automatically graded through a specific A.I-based algorithm and clinically assessed by a panel of experts and dermatologists. Selfie pictures were taken from individual smartphones of variable optical properties. The first study, designed for developing an automatic grading system, involved three comparable cohorts of men from three different regional ancestries (African, Asian, Caucasian, 110 each) the selfie images of which were acquired under four different lighting conditions. As a second use case study, the facial signs of two cohorts of Chinese men (101 and 100, each), differently aged, regularly exposed to very different aerial urban pollution conditions (UP) were analysed by the same algorithm, selfies being taken under only one lighting condition. 
  Results:  -The new automatic grading system of facial signs suits well to men, showing comparable results than that the one dedicated to women and provides data in close agreement with experts' assessments. -In both cases (expert's or automatic methodology), the accuracy of the scores appeared ethnic-dependent. -The applied case confirmed previous results obtained clinically, that is, that many facial signs were found of an increased severity among men exposed to a severe urban pollution, as compared to those living in a less polluted city. -In both studies, statistical agreements between the automatic grading system and expert's assessments were reached. In some facial signs, the automatic grading system seems offering a slightly better accuracy than the assessments made by the experts. 
  Conclusion:  Apart from some minor limitations, this A.I-based automatic grading system, free from human intervention, performed as well as the one previously developed in women, in close agreement with expert's assessments. In epidemiological studies, this system offers an easy, fast, affordable and confidential approach in the detection and quantification of male facial signs. 
  Objectif:  Il était double: (i) de développer d’un système automatique de scorage spécifique de plusieurs signes faciaux pour les hommes, similaire à celui précédemment validé sur des femmes de différentes origines. Et (ii), de jauger ses capacités pour la détection et l’évaluation des possibles impacts d’une pollution aérienne urbaine sévère sur le visage d’hommes chinois. MÉTHODES: Dans chacune des deux études des images de type selfies d’hommes de différents âges ont été obtenues. Neuf signes faciaux ont été automatiquement évalués grâce à un algorithme spécifique basé sur l’Intelligence Artificielle (IA) puis scorés cliniquement par un panel d’experts et de dermatologues. Des selfies ont été acquis à partir de téléphones portables individuels possédants des optiques et des résolutions différentes. L’étude N°1, conçue pour développer un système de scorage automatique du visage, a regroupé trois cohortes comparables d’hommes d’origines géographiques différentes (Africain, Asiatique et Caucasien, 110 volontaires par ethnies) et a requis l’acquisition sous 4 conditions d’éclairage. L’étude N°2, comme cas pratique, a nécessité le recrutement de deux cohortes d’hommes chinois d’âges différents (101 et 101 volontaires chacune) exposés régulièrement à de très différentes conditions de pollution aérienne urbaine et pour lesquels des selfies ont été enregistrés sous une seule condition d’éclairage. RÉSULTATS: -Le nouveau système de scorage automatique de signes faciaux des hommes performe de manière satisfaisante et montre des résultats comparables à celui précédemment conçu pour les femmes et donne des mesures très proches des évaluations cliniques des experts et dermatologues. -Dans les deux cas (experts ou mesures automatiques), l’acuité des scores apparaît dépendante à l’origine ethnique. -Le cas pratique confirme nos résultats précédents obtenues cliniquement, c’est à dire que de nombreux signes faciaux ont été trouvés d’amplitude plus importante pour les hommes exposés à une pollution aérienne urbaine sévère en comparaison de ceux vivant dans une ville moins polluée. -Les deux études ont démontré l’adéquation statistique entre le système automatique et les évaluations des experts et dermatologues. Pour certains signes, une certaine supériorité de système automatique a pu être observée vis-à-vis de l’évaluation des experts. 
  Conclusion:  A l’exception de quelques limitations mineures, le nouveau système de scorage automatique, basé sur l’IA, du visage des hommes - ne nécessitant aucune intervention humaine - fonctionne aussi bien que celui dédié aux femmes et toujours en parfaite adéquation avec les dermatologues. Pour des études épidémiologiques, ce système offre une approche rapide, aisée, confidentielle et d’un coût très abordable pour la détection et la quantification des signes faciaux masculins. 
  |  https://doi.org/10.1111/ics.12602  |  
------------------------------------------- 
10.1053/j.gastro.2020.02.012  |    Background &amp; aims:  There are intra- and inter-observer variations in endoscopic assessment of ulcerative colitis (UC) and biopsies are often collected for histologic evaluation. We sought to develop a deep neural network system for consistent, objective, and real-time analysis of endoscopic images from patients with UC. 
  Methods:  We constructed the deep neural network for evaluation of UC (DNUC) algorithm using 40,758 images of colonoscopies and 6885 biopsy results from 2012 patients with UC who underwent colonoscopy from January 2014 through March 2018 at a single center in Japan (the training set). We validated the accuracy of the DNUC algorithm in a prospective study of 875 patients with UC who underwent colonoscopy from April 2018 through April 2019, with 4187 endoscopic images and 4104 biopsy specimens. Endoscopic remission was defined as an UC endoscopic index of severity (UCEIS) score of 0; histologic remission was defined as a Geboes score of 3 points or less. 
  Results:  In the prospective study, the DNUC identified patients with endoscopic remission with 90.1% accuracy (95% CI, 89.2%-90.9%) and a kappa coefficient of 0.798 (95% CI, 0.780-0.814), using findings reported by endoscopists as the reference standard. The intraclass correlation coefficient between the DNUC and the endoscopists for UCEIS scoring was 0.917 (95% CI, 0.911-0.921). The DNUC identified patients in histologic remission with 92.9% accuracy (95% CI, 92.1%-93.7%); the kappa coefficient between the DNUC and the biopsy result was 0.859 (95% CI, 0.841-0.875). 
  Conclusions:  We developed a deep neural network for evaluation of endoscopic images from patients with UC that identified those in endoscopic remission with 90.1% accuracy and histologic remission with 92.9% accuracy. The DNUC can therefore identify patients in remission without the need for mucosal biopsy collection and analysis. Trial number: UMIN000031430. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5085(20)30212-2  |  
------------------------------------------- 
10.1016/j.neunet.2020.02.007  |   As a famous multivariable analysis technique, regression methods, such as ridge regression, are widely used for image representation and dimensionality reduction. However, the metric of ridge regression and its variants is always the Frobenius norm (F-norm), which is sensitive to outliers and noise in data. At the same time, the performance of the ridge regression and its extensions is limited by the class number of the data. To address these problems, we propose a novel regression learning method which named low-rank discriminative regression learning (LDRL) for image representation. LDRL assumes that the input data is corrupted and thus the L<sub>1</sub> norm can be used as a sparse constraint on the noised matrix to recover the clean data for regression, which can improve the robustness of the algorithm. Due to learn a novel project matrix that is not limited by the number of classes, LDRL is suitable for classifying the data set no matter whether there is a small or large number of classes. The performance of the proposed LDRL is evaluated on six public image databases. The experimental results prove that LDRL obtains better performance than existing regression methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30052-6  |  
------------------------------------------- 
10.4103/jpi.jpi_30_19  |    Background:  Free-text sections of pathology reports contain the most important information from a diagnostic standpoint. However, this information is largely underutilized for computer-based analytics. The vast majority of NLP-based methods lack a capacity to accurately extract complex diagnostic entities and relationships among them as well as to provide an adequate knowledge representation for downstream data-mining applications. 
  Methods:  In this paper, we introduce a novel informatics pipeline that extends open information extraction (openIE) techniques with artificial intelligence (AI) based modeling to extract and transform complex diagnostic entities and relationships among them into Knowledge Graphs (KGs) of relational triples (RTs). 
  Results:  Evaluation studies have demonstrated that the pipeline's output significantly differs from a random process. The semantic similarity with original reports is high (Mean Weighted Overlap of 0.83). The <i>precision</i> and <i>recall</i> of extracted RTs based on experts' assessment were 0.925 and 0.841 respectively (<i>P</i> &lt;0.0001). Inter-rater agreement was significant at 93.6% and inter-rated reliability was 81.8%. 
  Conclusion:  The results demonstrated important properties of the pipeline such as <i>high accuracy,</i> <i>minimality</i> and <i>adequate knowledge representation</i>. Therefore, we conclude that the pipeline can be used in various downstream data-mining applications to assist diagnostic medicine. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32166042/  |  
------------------------------------------- 
10.1148/ryai.2020190007  |   A publicly available dataset containing k-space data as well as Digital Imaging and Communications in Medicine image data of knee images for accelerated MR image reconstruction using machine learning is presented. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32076662/  |  
------------------------------------------- 
10.1007/s10278-019-00299-9  |   Pneumothorax is a potentially life-threatening condition that requires prompt recognition and often urgent intervention. In the ICU setting, large numbers of chest radiographs are performed and must be interpreted on a daily basis which may delay diagnosis of this entity. Development of artificial intelligence (AI) techniques to detect pneumothorax could help expedite detection as well as localize and potentially quantify pneumothorax. Open image analysis competitions are useful in advancing state-of-the art AI algorithms but generally require large expert annotated datasets. We have annotated and adjudicated a large dataset of chest radiographs to be made public with the goal of sparking innovation in this space. Because of the cumbersome and time-consuming nature of image labeling, we explored the value of using AI models to generate annotations for review. Utilization of this machine learning annotation (MLA) technique appeared to expedite our annotation process with relatively high sensitivity at the expense of specificity. Further research is required to confirm and better characterize the value of MLAs. Our adjudicated dataset is now available for public consumption in the form of a challenge. 
  |  https://doi.org/10.1007/s10278-019-00299-9  |  
------------------------------------------- 
10.1016/j.ejrad.2020.108992  |    Purpose:  We aimed to propose a highly automatic and objective model named deep learning Radiomics of thyroid (DLRT) for the differential diagnosis of benign and malignant thyroid nodules from ultrasound (US) images. 
  Methods:  We retrospectively enrolled and finally include US images and fine-needle aspiration biopsies from 1734 patients with 1750 thyroid nodules. A basic convolutional neural network (CNN) model, a transfer learning (TL) model, and a newly designed model named deep learning Radiomics of thyroid (DLRT) were used for the investigation. Their diagnostic accuracy was further compared with human observers (one senior and one junior US radiologist). Moreover, the robustness of DLRT over different US instruments was also validated. Analysis of receiver operating characteristic (ROC) curves were performed to calculate optimal area under it (AUC) for benign and malignant nodules. One observer helped to delineate the nodules. 
  Results:  AUCs of DLRT were 0.96 (95% confidence interval [CI]: 0.94-0.98), 0.95 (95% confidence interval [CI]: 0.93-0.97) and 0.97 (95% confidence interval [CI]: 0.95-0.99) in the training, internal and external validation cohort, respectively, which were significantly better than other deep learning models (P &lt; 0.01) and human observers (P &lt; 0.001). No significant difference was found when applying DLRT on thyroid US images acquired from different US instruments. 
  Conclusions:  DLRT shows the best overall performance comparing with other deep learning models and human observers. It holds great promise for improving the differential diagnosis of benign and malignant thyroid nodules. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0720-048X(20)30181-9  |  
------------------------------------------- 
10.3390/cancers12020507  |   Pathologic diagnosis of nasopharyngeal carcinoma (NPC) can be challenging since most cases are nonkeratinizing carcinoma with little differentiation and many admixed lymphocytes. Our aim was to evaluate the possibility to identify NPC in nasopharyngeal biopsies using deep learning. A total of 726 nasopharyngeal biopsies were included. Among them, 100 cases were randomly selected as the testing set, 20 cases as the validation set, and all other 606 cases as the training set. All three datasets had equal numbers of NPC cases and benign cases. Manual annotation was performed. Cropped square image patches of 256 × 256 pixels were used for patch-level training, validation, and testing. The final patch-level algorithm effectively identified NPC patches, with an area under the receiver operator characteristic curve (AUC) of 0.9900. Using gradient-weighted class activation mapping, we demonstrated that the identification of NPC patches was based on morphologic features of tumor cells. At the second stage, whole-slide images were sequentially cropped into patches, inferred with the patch-level algorithm, and reconstructed into images with a smaller size for training, validation, and testing. Finally, the AUC was 0.9848 for slide-level identification of NPC. Our result shows for the first time that deep learning algorithms can identify NPC. 
  |  http://www.mdpi.com/resolver?pii=cancers12020507  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32098314/  |  
------------------------------------------- 
10.3389/fonc.2020.00143  |   <b>Background:</b> Artificial Intelligence (AI) frameworks have emerged as a novel approach in medicine. However, information regarding its applicability and effectiveness in a clinical prognostic factor setting remains unclear. <b>Methods:</b> The AI framework was derived from a pooled dataset of intrahepatic cholangiocarcinoma (ICC) patients from three clinical centers (<i>n</i> = 1,421) by applying the TensorFlow deep learning algorithm to Cox-indicated pathologic (four), serologic (six), and etiologic (two) factors; this algorithm was validated using a dataset of ICC patients from an independent clinical center (<i>n</i> = 234). The model was compared to the commonly used staging system (American Joint Committee on Cancer; AJCC) and methodology (Cox regression) by evaluating the brier score (BS), integrated discrimination improvement (IDI), net reclassification improvement (NRI), and area under curve (AUC) values. <b>Results:</b> The framework (BS, 0.17; AUC, 0.78) was found to be more accurate than the AJCC stage (BS, 0.48; AUC, 0.60; IDI, 0.29; NRI, 11.85; <i>P</i> &lt; 0.001) and the Cox model (BS, 0.49; AUC, 0.70; IDI, 0.46; NRI, 46.11; <i>P</i> &lt; 0.001). Furthermore, hazard ratios greater than three were identified in both overall survival (HR; 3.190; 95% confidence interval [CI], 2.150-4.733; <i>P</i> &lt; 0.001) and disease-free survival (HR, 3.559; 95% CI, 2.500-5.067; <i>P</i> &lt; 0.001) between latent risk and stable groups in validation. In addition, the latent risk subgroup was found to be significantly benefited from adjuvant treatment (HR, 0.459; 95% CI, 0.360-0.586; <i>P</i> &lt; 0.001). <b>Conclusions:</b> The AI framework seems promising in the prognostic estimation and stratification of susceptible individuals for adjuvant treatment in patients with ICC after resection. Future prospective validations are needed for the framework to be applied in clinical practice. 
  |  https://doi.org/10.3389/fonc.2020.00143  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32140448/  |  
------------------------------------------- 
10.1186/s12890-020-1089-y  |    Background:  Community-acquired pneumonia (CAP) requires urgent and specific antimicrobial therapy. However, the causal pathogen is typically unknown at the point when anti-infective therapeutics must be initiated. Physicians synthesize information from diverse data streams to make appropriate decisions. Artificial intelligence (AI) excels at finding complex relationships in large volumes of data. We aimed to evaluate the abilities of experienced physicians and AI to answer this question at patient admission: is it a viral or a bacterial pneumonia? 
  Methods:  We included patients hospitalized for CAP and recorded all data available in the first 3-h period of care (clinical, biological and radiological information). For this proof-of-concept investigation, we decided to study only CAP caused by a singular and identified pathogen. We built a machine learning model prediction using all collected data. Finally, an independent validation set of samples was used to test the pathogen prediction performance of: (i) a panel of three experts and (ii) the AI algorithm. Both were blinded regarding the final microbial diagnosis. Positive likelihood ratio (LR) values &gt; 10 and negative LR values &lt; 0.1 were considered clinically relevant. 
  Results:  We included 153 patients with CAP (70.6% men; 62 [51-73] years old; mean SAPSII, 37 [27-47]), 37% had viral pneumonia, 24% had bacterial pneumonia, 20% had a co-infection and 19% had no identified respiratory pathogen. We performed the analysis on 93 patients as co-pathogen and no-pathogen cases were excluded. The discriminant abilities of the AI approach were low to moderate (LR+ = 2.12 for viral and 6.29 for bacterial pneumonia), and the discriminant abilities of the experts were very low to low (LR+ = 3.81 for viral and 1.89 for bacterial pneumonia). 
  Conclusion:  Neither experts nor an AI algorithm can predict the microbial etiology of CAP within the first hours of hospitalization when there is an urgent need to define the anti-infective therapeutic strategy. 
  |  https://bmcpulmmed.biomedcentral.com/articles/10.1186/s12890-020-1089-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32143620/  |  
------------------------------------------- 
10.3390/ma13030656  |   Electrochemical actuators have attracted tremendous attention worldwide because of their critical significance to artificial intelligence. The development of electrochemical actuators-with the merits of low driven-voltage, lightweight, flexibility and large deformation-is an urgent task in the development of smart technologies. Nanomaterials with special structures and superior properties provide the opportunity for the development and application of smart actuators. Here, we report an electrochemical actuator based on an ionogel graphene composite, which is assembled with simple casting methodology and can be driven with a low voltage of 2.5 V. The flexible sandwich-structured actuator operates under a capacitive mechanism based on asymmetrical volume expansion of active ions under electrical stimulus. It shows a high specific capacitance of 39 F g<sup>-1</sup> at current density of 1 A g<sup>-1</sup> under potential of 2.5 V. The specific capacitance is calculated on the weight of graphene. The device presents a large actuation peak-to-peak displacement of 24 mm at a frequency of 0.1 Hz under the stimulus potential of 2.5 V, and it can still reach a large value of 12 mm at a high frequency of 1 Hz. The free length of the device is 25 mm. Notably, the device exhibits excellent air-working stability at frequency of 1 Hz under 2.5 V with the actuation displacement retention of 98%, even after 10,000 cycles. This study presents insights into the design of smart actuators based on nanomaterials, and will accelerate the development of artificial intelligence. 
  |  http://www.mdpi.com/resolver?pii=ma13030656  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32024186/  |  
------------------------------------------- 
10.3390/jcm9041018  |    Background and objective:  Accurate retinal vessel segmentation is often considered to be a reliable biomarker of diagnosis and screening of various diseases, including cardiovascular diseases, diabetic, and ophthalmologic diseases. Recently, deep learning (DL) algorithms have demonstrated high performance in segmenting retinal images that may enable fast and lifesaving diagnoses. To our knowledge, there is no systematic review of the current work in this research area. Therefore, we performed a systematic review with a meta-analysis of relevant studies to quantify the performance of the DL algorithms in retinal vessel segmentation. 
  Methods:  A systematic search on EMBASE, PubMed, Google Scholar, Scopus, and Web of Science was conducted for studies that were published between 1 January 2000 and 15 January 2020. We followed the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) procedure. The DL-based study design was mandatory for a study's inclusion. Two authors independently screened all titles and abstracts against predefined inclusion and exclusion criteria. We used the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool for assessing the risk of bias and applicability. 
  Results:  Thirty-one studies were included in the systematic review; however, only 23 studies met the inclusion criteria for the meta-analysis. DL showed high performance for four publicly available databases, achieving an average area under the ROC of 0.96, 0.97, 0.96, and 0.94 on the DRIVE, STARE, CHASE_DB1, and HRF databases, respectively. The pooled sensitivity for the DRIVE, STARE, CHASE_DB1, and HRF databases was 0.77, 0.79, 0.78, and 0.81, respectively. Moreover, the pooled specificity of the DRIVE, STARE, CHASE_DB1, and HRF databases was 0.97, 0.97, 0.97, and 0.92, respectively. 
  Conclusion:  The findings of our study showed the DL algorithms had high sensitivity and specificity for segmenting the retinal vessels from digital fundus images. The future role of DL algorithms in retinal vessel segmentation is promising, especially for those countries with limited access to healthcare. More compressive studies and global efforts are mandatory for evaluating the cost-effectiveness of DL-based tools for retinal disease screening worldwide. 
  |  http://www.mdpi.com/resolver?pii=jcm9041018  |  
------------------------------------------- 
10.1002/hast.1079  |   In the much-celebrated book Deep Medicine, Eric Topol argues that the development of artificial intelligence for health care will lead to a dramatic shift in the culture and practice of medicine. In the next several decades, he suggests, AI will become sophisticated enough that many of the everyday tasks of physicians could be delegated to it. Topol is perhaps the most articulate advocate of the benefits of AI in medicine, but he is hardly alone in spruiking its potential to allow physicians to dedicate more of their time and attention to providing empathetic care for their patients in the future. Unfortunately, several factors suggest a radically different picture for the future of health care. Far from facilitating a return to a time of closer doctor-patient relationships, the use of medical AI seems likely to further erode therapeutic relationships and threaten professional and patient satisfaction. 
  |  https://doi.org/10.1002/hast.1079  |  
------------------------------------------- 
10.3389/fnbot.2020.00005  |   Traditionally investigated in philosophy, body ownership and agency-two main components of the minimal self-have recently gained attention from other disciplines, such as brain, cognitive and behavioral sciences, and even robotics and artificial intelligence. In robotics, intuitive human interaction in natural and dynamic environments becomes more and more important, and requires skills such as self-other distinction and an understanding of agency effects. In a previous review article, we investigated studies on mechanisms for the development of motor and cognitive skills in robots (Schillaci et al., 2016). In this review article, we argue that these mechanisms also build the foundation for an understanding of an artificial self. In particular, we look at developmental processes of the minimal self in biological systems, transfer principles of those to the development of an artificial self, and suggest metrics for agency and body ownership in an artificial self. 
  |  https://doi.org/10.3389/fnbot.2020.00005  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32153380/  |  
------------------------------------------- 
10.1176/appi.ps.201900464  |    |  https://ps.psychiatryonline.org/doi/full/10.1176/appi.ps.201900464?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1556/650.2020.31703  |   The <i>one disease - one target - one drug</i> paradigm was an almost dominant principle in drug discovery from the 1960s to the 2000s. The stagnation and even decline in the productivity of drug innovation around the turn of the millennium and beyond, the realization of limitations of the one-target approach, especially in the treatment of multifactorial diseases, have drawn attention considerably to the <i>one disease</i> - <i>multiple target - one drug</i> multi-targeting drug concept. In this review, we outline old and new molecular design strategies and their practical implementation with own and other examples that also demonstrate unique therapeutic and diagnostic values and benefits of the multi-targeting approach. Finally, we point out that the full potential of the multi-targeting concept can emerge through data analytics and association methods (such as artificial intelligence) and system-based approach, preferably by linking it to quantitative systems pharmacology. This new <i>systems pharmacology drug</i> approach may also lead to novel breakthrough drugs, drug combinations and drug repositioning. Orv Hetil. 2020; 161(14): 523-531. 
 Absztrakt: Az <i>egy betegség – egy célpont – egy gyógyszer</i> paradigma az 1960-as évektől egészen a 2000-es évekig a gyógyszerkutatás meghatározó koncepciója volt. A gyógyszer-innováció eredményességének megtorpanása, sőt visszaesése, az egy támadáspontú megközelítés különösen multifaktoriális betegségek terápiájára való alkalmazhatóságának elvi korlátai azonban ráirányították a figyelmet az <i>egy betegség – több célpont – egy gyógyszer</i> több támadáspontú gyógyszer <i>koncepciójára</i>. Áttekintő közleményünkben a több támadáspontú gyógyszerek régi és új molekulatervezési stratégiáit és azok gyakorlati megvalósítását ismertetjük saját és mások példáin keresztül, melyek a több támadáspontú megközelítés különleges terápiás és diagnosztikai értékeit és előnyeit is illusztrálják. Végül rámutatunk arra, hogy a több támadáspontú koncepció új lehetőségeket is nyújtó teljes potenciálja rendszerszemléletű megközelítéssel, célszerűen kvantitatív rendszerfarmakológiával és adatelemzési, adatasszociációs (például mesterséges intelligenciát alkalmazó) módszerekkel bontakozhat ki. A <i>rendszerfarmakológiai gyógyszer</i> koncepciója új áttörést jelentő hatóanyagokhoz, kombinációs készítményekhez és gyógyszer-repozíciós készítményekhez is vezethet. Orv Hetil. 2020; 161(14): 523–531. 
  |  https://akjournals.com/doi/10.1556/650.2020.31703  |  
------------------------------------------- 
10.1007/s10278-019-00267-3  |   Collecting and curating large medical-image datasets for deep neural network (DNN) algorithm development is typically difficult and resource-intensive. While transfer learning (TL) decreases reliance on large data collections, current TL implementations are tailored to two-dimensional (2D) datasets, limiting applicability to volumetric imaging (e.g., computed tomography). Targeting performance enhancement of a DNN algorithm based on a small image dataset, we assessed incremental impact of 3D-to-2D projection methods, one supporting novel data augmentation (DA); photometric grayscale-to-color conversion (GCC); and/or TL on training of an algorithm from a small coronary computed tomography angiography (CCTA) dataset (200 examinations, 50% with atherosclerosis and 50% atherosclerosis-free) producing 245 diseased and 1127 normal coronary arteries/branches. Volumetric CCTA data was converted to a 2D format creating both an Aggregate Projection View (APV) and a Mosaic Projection View (MPV), supporting DA per vessel; both grayscale and color-mapped versions of each view were also obtained. Training was performed both without and with TL, and algorithm performance of all permutations was compared using area under the receiver operating characteristics curve. Without TL, APV performance was 0.74 and 0.87 on grayscale and color images, respectively, compared to 0.90 and 0.87 for MPV. With TL, APV performance was 0.78 and 0.88 on grayscale and color images, respectively, compared with 0.93 and 0.91 for MPV. In conclusion, TL enhances performance of a DNN algorithm from a small volumetric dataset after proposed 3D-to-2D reformatting, but additive gain is achieved with application of either GCC to APV or the proposed novel MPV technique for DA. 
  |  https://doi.org/10.1007/s10278-019-00267-3  |  
------------------------------------------- 
10.1002/mp.14033  |    Purpose:  The integral quality monitor (IQM) is a real-time radiotherapy beam monitoring system, which consists of a spatially sensitive large-area ion chamber, mounted at the collimator of the linear accelerator (linac), and a calculation algorithm to predict the detector signal for each beam segment. By comparing the measured and predicted signals the system validates the beam delivery. The current commercial version of IQM uses an analytic method to predict the signal, which requires a semi-empirical approach to determine and optimize various calculation parameters. The process of developing the calculation model is complex and time consuming, and moreover, the model cannot be easily generalized across various beam delivery platforms with different combinations of beam energy, beam flattening, beam shaping elements, and Linac models. Therefore, as an alternative solution, we investigated the feasibility of developing a machine learning (ML) method, using an artificial neural network (ANN), to predict the ion chamber signal. In developing an ANN, it is not necessary to explicitly account for each of the elements of beam interactions with various structures in the beam path to the ion chamber. 
  Methods:  The ANN was designed with multilayer perceptron (MLP). The input layer consisted of multiple features, derived from the geometrical characteristics of beam segments. Gradient descent error backpropagation technique was used to train the ANN. The combined training dataset included 270 rectangular fields, and 801 clinical IMRT fields delivered using 6 MV beams on Varian TrueBeam<sup>TM</sup> and Elekta Infinity<sup>TM</sup> . Each of 12 different ANN configurations (3 different sets of input features × 4 different sets of number of hidden nodes) was simulated 10 times with randomly selected 80% of data for training and the remaining data for validation. 
  Results:  Artificial neural networks with one hidden layer, consisting of 10 nodes, and 10 input features provided optimum results. Once the feature sets were extracted, the time required for the network training was on the order of a few minutes, and the time required to perform an output calculation per field was only fraction of a second. More than 95% of clinical intensity-modulated radiation therapy (IMRT) segments were calculated within ± 3.0% modeling error for Varian Truebeam (90% and ±3.3% for Elekta Infinity). A total of 3320 volumetric-modulated arc therapy (VMAT) segments from Truebeam were calculated using the ANN trained with IMRT fields. More than 95% of the cumulative VMAT beam segments were within 3.6% modeling error, similar to the performance for IMRT segments. In general the modeling error was found to be inversely proportional to the size and intensity of the beam segment. 
  Conclusions:  A prototype ANN has been developed for predicting the signals of the IQM system, with substantially less efforts compared to the analytic model. The performance of the ANN was found to be at least equivalent to that of the analytic method, in terms of average and maximum error, for 6 MV beams on both Varian TrueBeam and Elekta Infinity platforms. 
  |  https://doi.org/10.1002/mp.14033  |  
------------------------------------------- 
10.1007/s00464-020-07447-1  |    Background:  Operating room (OR) fires are uncommon but disastrous events. Inappropriate handling of OR fires can result in injuries, even death. Aiming to simulate OR fire emergencies and effectively train clinicians to react appropriately, we have developed an artificial intelligence (AI)-based OR fire virtual trainer based on the principle of the "fire triangle" and SAGES FUSE curriculum. The simulator can predict the user's actions in the virtual OR and provide them with timely feedback to assist with training. We conducted a study investigating the validity of the AI-assisted OR fire trainer at the 2019 SAGES Learning Center. 
  Methods:  Fifty-three participants with varying medical experience were voluntarily recruited to participate in our Institutional Review Board approved study. All participants were asked to contain a fire within the virtual OR. Participants were then asked to fill out a 7-point Likert questionnaire consisting of ten questions regarding the face validation of the AI-assisted OR fire simulator. Shapiro-Wilk tests were conducted to test normality of the scores for each trial. A Friedman's ANOVA with post hoc tests was used to evaluate the effect of multiple trials on performance. 
  Results:  On a 7-point scale, eight of the ten questions were rated a mean of 6 or greater (72.73%), especially those relating to the usefulness of the simulator for OR fire-containing training. 79.25% of the participants rated the degree of usefulness of AI guidance over 6 out of 7. The performance of individuals improved significantly over the five trials, χ<sup>2</sup>(4) = 119.89, p &lt; .001, and there was a significant linear trend of performance r = .97, p = 0.006. A pairwise analysis showed that only after the introduction of AI did performance improve significantly. 
  Conclusions:  The AI-guided OR fire trainer offers the potential to assess OR personnel and teach the proper response to an iatrogenic fire scenario in a safe, repeatable, immersive environment. 
  |  https://doi.org/10.1007/s00464-020-07447-1  |  
------------------------------------------- 
10.3390/s20071951  |   Point-of-care (PoC) diagnostics is promising for early detection of a number of diseases, including cancer, diabetes, and cardiovascular diseases, in addition to serving for monitoring health conditions. To be efficient and cost-effective, portable PoC devices are made with microfluidic technologies, with which laboratory analysis can be made with small-volume samples. Recent years have witnessed considerable progress in this area with "epidermal electronics", including miniaturized wearable diagnosis devices. These wearable devices allow for continuous real-time transmission of biological data to the Internet for further processing and transformation into clinical knowledge. Other approaches include bluetooth and WiFi technology for data transmission from portable (non-wearable) diagnosis devices to cellphones or computers, and then to the Internet for communication with centralized healthcare structures. There are, however, considerable challenges to be faced before PoC devices become routine in the clinical practice. For instance, the implementation of this technology requires integration of detection components with other fluid regulatory elements at the microscale, where fluid-flow properties become increasingly controlled by viscous forces rather than inertial forces. Another challenge is to develop new materials for environmentally friendly, cheap, and portable microfluidic devices. In this review paper, we first revisit the progress made in the last few years and discuss trends and strategies for the fabrication of microfluidic devices. Then, we discuss the challenges in lab-on-a-chip biosensing devices, including colorimetric sensors coupled to smartphones, plasmonic sensors, and electronic tongues. The latter ones use statistical and big data analysis for proper classification. The increasing use of big data and artificial intelligence methods is then commented upon in the context of wearable and handled biosensing platforms for the Internet of things and futuristic healthcare systems. 
  |  http://www.mdpi.com/resolver?pii=s20071951  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32244343/  |  
------------------------------------------- 
10.1111/cyt.12829  |    Introduction:  Distinguishing small cell lung carcinoma (SCLC) from large cell neuroendocrine carcinoma (LCNEC) in cytology is challenging. Our aim was to design a deep learning algorithm for classifying high-grade neuroendocrine carcinomas in fine-needle aspirations (FNA). 
  Materials and methods:  Archival cytology cases of high-grade neuroendocrine carcinoma (17 small cell, 13 large cell, 10 mixed/unclassifiable) were retrieved. Each case included smears (Diff-Quik and Pap stains) and cell block or concomitant core biopsies (H&amp;E stain). All slides (N=114) were scanned at 40x magnification, randomized and split into training (11 large, 9 small) and test (2 large, 8 small, 10 mixed) groups. Tumor was annotated using QuPath and exported as JPEG image tiles. Three distinct deep learning convolutional neural networks, one for each preparation/stain, were designed to classify each tile and provide an overall diagnosis for each slide. 
  Results:  The H&amp;E-trained algorithm correctly classified 7/8 (87.5%) SCLC cases and 2/2 (100%) LCNEC cases. The Pap stain algorithm correctly classified 6/7 (85.7%) SCNEC and 1/1 (100%) LCNEC cases. The algorithm trained on Diff-Quik stained images correctly classified 7/8 (87.5%) SCLC and 1/1 (100%) LCNEC cases. 
  Conclusion:  Using open source software it was feasible to design a deep learning algorithm to distinguish between SCLC and LCNEC. The algorithm showed high precision in distinguishing between these two categories on H&amp;E sectioned material and direct smears. Although the dataset was limited, our deep learning models show promising results in the classification of LCNEC and SCLC. Additional work using a larger dataset is necessary to improve the algorithm's performance. 
  |  https://doi.org/10.1111/cyt.12829  |  
------------------------------------------- 
10.1016/j.wneu.2020.04.135  |    Introduction:  Preoperative prognostication of adverse events (AEs) for patients undergoing surgery for lumbar degenerative spondylolisthesis (LDS) can improve risk stratification and help guide the surgical decision-making process. The aim of this study was to develop and validate a set of predictive variables for 30-day AEs following surgery for LDS. 
  Methods:  The American College of Surgeons National Surgical Quality Improvement Program (NSQIP) was used for this study (2005-2016). Logistic regression (enter, stepwise and forward) and least absolute shrinkage and selection operator (LASSO) methods were performed to identify and select variables for analyses, which resulted in 26 potential models. The final model was selected based upon clinical criteria and numerical results. 
  Results:  The overall 30-day rate of AEs for 80,610 patients who underwent surgery for LDS in this database was 4.9% (n=3,965). The median age of the cohort was 58.0 years (range, 18-89 years). The model with the following 10-predictive factors: age, gender, American Society of Anesthesiologists grade, autogenous iliac bone graft, instrumented fusion, levels of surgery, surgical approach, functional status, preoperative serum albumin (g/dl) and serum alkaline phosphatase (IU/L) performed well on the discrimination, calibration, Brier score and decision analyses to develop machine learning algorithms. Logistic regression showed higher AUCs than LASSO methods across the different models. The predictive probability derived from the best model is uploaded on an open access web application which can be found at: https://spine.massgeneral.org/drupal/Lumbar-Degenerative-AdverseEvents CONCLUSION: It is feasible to develop machine learning algorithms from large datasets to provide useful tools for patient-counseling and surgical risk assessment. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1878-8750(20)30842-1  |  
------------------------------------------- 
10.3390/jcm9041117  |   In the absence of accurate medical records, it is critical to correctly classify implant fixture systems using periapical radiographs to provide accurate diagnoses and treatments to patients or to respond to complications. The purpose of this study was to evaluate whether deep neural networks can identify four different types of implants on intraoral radiographs. In this study, images of 801 patients who underwent periapical radiographs between 2005 and 2019 at Yonsei University Dental Hospital were used. Images containing the following four types of implants were selected: Brånemark Mk TiUnite, Dentium Implantium, Straumann Bone Level, and Straumann Tissue Level. SqueezeNet, GoogLeNet, ResNet-18, MobileNet-v2, and ResNet-50 were tested to determine the optimal pre-trained network architecture. The accuracy, precision, recall, and F1 score were calculated for each network using a confusion matrix. All five models showed a test accuracy exceeding 90%. SqueezeNet and MobileNet-v2, which are small networks with less than four million parameters, showed an accuracy of approximately 96% and 97%, respectively. The results of this study confirmed that convolutional neural networks can classify the four implant fixtures with high accuracy even with a relatively small network and a small number of images. This may solve the inconveniences associated with unnecessary treatments and medical expenses caused by lack of knowledge about the exact type of implant. 
  |  http://www.mdpi.com/resolver?pii=jcm9041117  |  
------------------------------------------- 
10.1007/s13304-020-00771-0  |   Gastric cancer is the fifth malignancy and the third cause of cancer death worldwide, according to the global cancer statistics presented in 2018. Its definition and staging have been revised in the eight edition of the AJCC/TNM classification, which took effect in 2018. Novel molecular classifications for GC have been recently established and the process of translating these classifications into clinical practice is ongoing. The cornerstone of GC treatment is surgical, in a context of multimodal therapy. Surgical treatment is being standardized, and is evolving according to new anatomical concepts and to the recent technological developments. This is leading to a massive improvement in the use of mini-invasive techniques. Mini-invasive techniques aim to be equivalent to open surgery from an oncologic point of view, with better short-term outcomes. The persecution of better short-term outcomes also includes the optimization of the perioperative management, which is being implemented on large scale according to the enhanced recovery after surgery principles. In the era of precision medicine, multimodal treatment is also evolving. The long-time-awaited results of many trials investigating the role for preoperative and postoperative management have been published, changing the clinical practice. Novel investigations focused both on traditional chemotherapeutic regimens and targeted therapies are currently ongoing. Modern platforms increase the possibility for further standardization of the different treatments, promote the use of big data and open new possibilities for surgical learning. This systematic review in two parts assesses all the current updates in GC treatment. 
  |  https://dx.doi.org/10.1007/s13304-020-00771-0  |  
------------------------------------------- 
10.1097/RTI.0000000000000482  |   Artificial intelligence (AI) algorithms are dependent on a high amount of robust data and the application of appropriate computational power and software. AI offers the potential for major changes in cardiothoracic imaging. Beyond image processing, machine learning and deep learning have the potential to support the image acquisition process. AI applications may improve patient care through superior image quality and have the potential to lower radiation dose with AI-driven reconstruction algorithms and may help avoid overscanning. This review summarizes recent promising applications of AI in patient and scan preparation as well as contrast medium and radiation dose optimization. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000482  |  
------------------------------------------- 
10.1016/j.artmed.2020.101816  |    Aim:  A new automatic method for detecting specific points and lines (straight and curves) in dental panoramic radiographies (orthopantomographies) is proposed, where the human knowledge is mapped to the automatic system. The goal is to compute relevant mandibular indices (Mandibular Cortical Width, Panoramic Mandibular Index, Mandibular Ratio, Mandibular Cortical Index) in order to detect the thinning and deterioration of the mandibular bone. Data can be stored for posterior massive analysis. 
  Methods:  Panoramic radiographies are intrinsically complex, including: artificial structures, unclear limits in bony structures, jawbones with irregular curvatures and intensity levels, irregular shapes and borders of the mental foramen, irregular teeth alignments or missing dental pieces. An intelligent sequence of linked imaging segmentation processes is proposed to cope with the above situations towards the design of the automatic segmentation, making the following contributions: (i) Fuzzy K-means classification for identifying artificial structures; (ii) adjust a tangent line to the lower border of the lower jawbone (lower cortex), based on texture analysis, grey scale dilation, binarization and labelling; (iii) identification of the mental foramen region and its centre, based on multi-thresholding, binarization, morphological operations and labelling; (iv) tracing a perpendicular line to the tangent passing through the centre of the mental foramen region and two parallel lines to the tangent, passing through borders on the mental foramen intersected by the perpendicular; (v) following the perpendicular line, a sweep is made moving up the tangent for detecting accumulation of binary points after applying adaptive filtering; (vi) detection of the lower mandible alveolar crest line based on the identification of inter-teeth gaps by saliency and interest points feature description. 
  Results:  The performance of the proposed approach was quantitatively compared against the criteria of expert dentists, verifying also its validity with statistical studies based on the analysis of deterioration of bone structures with different levels of osteoporosis. All indices are computed inside two regions of interest, which tolerate flexibility in sizes and locations, making this process robust enough. 
  Conclusions:  The proposed approach provides an automatic procedure able to process with efficiency and reliability panoramic X-Ray images for early osteoporosis detection. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(19)30421-X  |  
------------------------------------------- 
10.1016/j.jhqr.2019.07.009  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2603-6479(19)30115-0  |  
------------------------------------------- 
10.1136/thoraxjnl-2019-214104  |    Background:  Estimation of the risk of malignancy in pulmonary nodules detected by CT is central in clinical management. The use of artificial intelligence (AI) offers an opportunity to improve risk prediction. Here we compare the performance of an AI algorithm, the lung cancer prediction convolutional neural network (LCP-CNN), with that of the Brock University model, recommended in UK guidelines. 
  Methods:  A dataset of incidentally detected pulmonary nodules measuring 5-15 mm was collected retrospectively from three UK hospitals for use in a validation study. Ground truth diagnosis for each nodule was based on histology (required for any cancer), resolution, stability or (for pulmonary lymph nodes only) expert opinion. There were 1397 nodules in 1187 patients, of which 234 nodules in 229 (19.3%) patients were cancer. Model discrimination and performance statistics at predefined score thresholds were compared between the Brock model and the LCP-CNN. 
  Results:  The area under the curve for LCP-CNN was 89.6% (95% CI 87.6 to 91.5), compared with 86.8% (95% CI 84.3 to 89.1) for the Brock model (p≤0.005). Using the LCP-CNN, we found that 24.5% of nodules scored below the lowest cancer nodule score, compared with 10.9% using the Brock score. Using the predefined thresholds, we found that the LCP-CNN gave one false negative (0.4% of cancers), whereas the Brock model gave six (2.5%), while specificity statistics were similar between the two models. 
  Conclusion:  The LCP-CNN score has better discrimination and allows a larger proportion of benign nodules to be identified without missing cancers than the Brock model. This has the potential to substantially reduce the proportion of surveillance CT scans required and thus save significant resources. 
  |  http://thorax.bmj.com/cgi/pmidlookup?view=long&pmid=32139611  |  
------------------------------------------- 
10.1016/j.earlhumdev.2020.105014  |   The first collection of Star Trek (ST) papers in this journal concentrated on the various doctors that appeared in ST in chronological screening order. This second collection will demonstrate that depictions of certain characters in ST harkens back to the high-ranking Nazis Adolf Eichmann and Josef Mengele, cautionary tales, lest we allow history to repeat itself and such atrocities to be relived. Artificial Intelligence (AI) is also explored as well as issues pertaining to AI in the medical field and ST's introduction of "ethical subroutines" that attempt to ensure that artificial beings (including doctors) are created with the machine analogues of a conscience. Nanotechnology in ST is also shown to be a potential field that can be applied not only for medical benefit but also with malevolent intentions. These narratives also serve as cautionary tales with regard to the potential unintended consequences of completely unfettered research. Finally, another paper will review the role of nurses in the Star Trek universe and will show that while nursing as a profession has striven for autonomy, in ST, the nurse continues to be overshadowed by the medical doctor. It is hoped that this collection of papers may help us to understand where it is that medicine may be heading and what are our best options are for averting problems, tragedy and outright disaster/s. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-3782(20)30145-6  |  
------------------------------------------- 
10.1038/s41746-020-0229-3  |   Autoimmune diseases are chronic, multifactorial conditions. Through machine learning (ML), a branch of the wider field of artificial intelligence, it is possible to extract patterns within patient data, and exploit these patterns to predict patient outcomes for improved clinical management. Here, we surveyed the use of ML methods to address clinical problems in autoimmune disease. A systematic review was conducted using MEDLINE, embase and computers and applied sciences complete databases. Relevant papers included "machine learning" or "artificial intelligence" and the autoimmune diseases search term(s) in their title, abstract or key words. Exclusion criteria: studies not written in English, no real human patient data included, publication prior to 2001, studies that were not peer reviewed, non-autoimmune disease comorbidity research and review papers. 169 (of 702) studies met the criteria for inclusion. Support vector machines and random forests were the most popular ML methods used. ML models using data on multiple sclerosis, rheumatoid arthritis and inflammatory bowel disease were most common. A small proportion of studies (7.7% or 13/169) combined different data types in the modelling process. Cross-validation, combined with a separate testing set for more robust model evaluation occurred in 8.3% of papers (14/169). The field may benefit from adopting a best practice of validation, cross-validation and independent testing of ML models. Many models achieved good predictive results in simple scenarios (e.g. classification of cases and controls). Progression to more complex predictive models may be achievable in future through integration of multiple data types. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32195365/  |  
------------------------------------------- 
10.1038/s41591-020-0793-8  |   Mounting evidence suggests that function and connectivity of the striatum is disrupted in schizophrenia<sup>1-5</sup>. We have developed a new hypothesis-driven neuroimaging biomarker for schizophrenia identification, prognosis and subtyping based on functional striatal abnormalities (FSA). FSA scores provide a personalized index of striatal dysfunction, ranging from normal to highly pathological. Using inter-site cross-validation on functional magnetic resonance images acquired from seven independent scanners (n = 1,100), FSA distinguished individuals with schizophrenia from healthy controls with an accuracy exceeding 80% (sensitivity, 79.3%; specificity, 81.5%). In two longitudinal cohorts, inter-individual variation in baseline FSA scores was significantly associated with antipsychotic treatment response. FSA revealed a spectrum of severity in striatal dysfunction across neuropsychiatric disorders, where dysfunction was most severe in schizophrenia, milder in bipolar disorder, and indistinguishable from healthy individuals in depression, obsessive-compulsive disorder and attention-deficit hyperactivity disorder. Loci of striatal hyperactivity recapitulated the spatial distribution of dopaminergic function and the expression profiles of polygenic risk for schizophrenia. In conclusion, we have developed a new biomarker to index striatal dysfunction and established its utility in predicting antipsychotic treatment response, clinical stratification and elucidating striatal dysfunction in neuropsychiatric disorders. 
  |  http://dx.doi.org/10.1038/s41591-020-0793-8  |  
------------------------------------------- 
10.1111/bju.15035  |    Objectives:  To assess the recall of a deep learning (DL) method to automatically detect kidney stones composition from digital photographs of stones. 
  Materials and methods:  A total of 63 human kidney stones of varied compositions were obtained from a stone laboratory including calcium oxalate monohydrate (COM), uric acid (UA), magnesium ammonium phosphate hexahydrate (MAPH/struvite), calcium hydrogen phosphate dihydrate (CHPD/brushite), and cystine stones. At least two images of the stones, both surface and inner core, were captured on a digital camera for all stones. A deep convolutional neural network (CNN), ResNet-101 (ResNet, Microsoft), was applied as a multi-class classification model, to each image. This model was assessed using leave-one-out cross-validation with the primary outcome being network prediction recall. 
  Results:  The composition prediction recall for each composition was as follows: UA 94% (n = 17), COM 90% (n = 21), MAPH/struvite 86% (n = 7), cystine 75% (n = 4), CHPD/brushite 71% (n = 14). The overall weighted recall of the CNNs composition analysis was 85% for the entire cohort. Specificity and precision for each stone type were as follows: UA (97.83%, 94.12%), COM (97.62%, 95%), struvite (91.84%, 71.43%), cystine (98.31%, 75%), and brushite (96.43%, 75%). 
  Conclusion:  Deep CNNs can be used to identify kidney stone composition from digital photographs with good recall. Future work is needed to see if DL can be used for detecting stone composition during digital endoscopy. This technology may enable integrated endoscopic and laser systems that automatically provide laser settings based on stone composition recognition with the goal to improve surgical efficiency. 
  |  https://doi.org/10.1111/bju.15035  |  
------------------------------------------- 
10.1007/978-1-4939-9857-9_21  |   Research on Toxoplasma gondii and its interplay with the host is often performed using fluorescence microscopy-based imaging experiments combined with manual quantification of acquired images. We present here an accurate and unbiased quantification method for host-pathogen interactions. We describe how to plan experiments and prepare, stain and image infected specimens and analyze them with the program HRMAn (Host Response to Microbe Analysis). HRMAn is a high-content image analysis method based on KNIME Analytics Platform. Users of this guide will be able to perform infection studies in high-throughput volume and to a greater level of detail. Relying on cutting edge machine learning algorithms, HRMAn can be trained and tailored to many experimental settings and questions. 
  |  https://dx.doi.org/10.1007/978-1-4939-9857-9_21  |  
------------------------------------------- 
10.1016/j.isci.2020.100886  |   Electrocardiograms (ECGs) are widely used to clinically detect cardiac arrhythmias (CAs). They are also being used to develop computer-assisted methods for heart disease diagnosis. We have developed a convolution neural network model to detect and classify CAs, using a large 12-lead ECG dataset (6,877 recordings) provided by the China Physiological Signal Challenge (CPSC) 2018. Our model, which was ranked first in the challenge competition, achieved a median overall F1-score of 0.84 for the nine-type CA classification of CPSC2018's hidden test set of 2,954 ECG recordings. Further analysis showed that concurrent CAs were adequately predictive for 476 patients with multiple types of CA diagnoses in the dataset. Using only single-lead data yielded a performance that was only slightly worse than using the full 12-lead data, with leads aVR and V1 being the most prominent. We extensively consider these results in the context of their agreement with and relevance to clinical observations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2589-0042(20)30070-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32062420/  |  
------------------------------------------- 
10.1007/s00256-020-03410-2  |    Objective:  To clinically validate a fully automated deep convolutional neural network (DCNN) for detection of surgically proven meniscus tears. 
  Materials and methods:  One hundred consecutive patients were retrospectively included, who underwent knee MRI and knee arthroscopy in our institution. All MRI were evaluated for medial and lateral meniscus tears by two musculoskeletal radiologists independently and by DCNN. Included patients were not part of the training set of the DCNN. Surgical reports served as the standard of reference. Statistics included sensitivity, specificity, accuracy, ROC curve analysis, and kappa statistics. 
  Results:  Fifty-seven percent (57/100) of patients had a tear of the medial and 24% (24/100) of the lateral meniscus, including 12% (12/100) with a tear of both menisci. For medial meniscus tear detection, sensitivity, specificity, and accuracy were for reader 1: 93%, 91%, and 92%, for reader 2: 96%, 86%, and 92%, and for the DCNN: 84%, 88%, and 86%. For lateral meniscus tear detection, sensitivity, specificity, and accuracy were for reader 1: 71%, 95%, and 89%, for reader 2: 67%, 99%, and 91%, and for the DCNN: 58%, 92%, and 84%. Sensitivity for medial meniscus tears was significantly different between reader 2 and the DCNN (p = 0.039), and no significant differences existed for all other comparisons (all p ≥ 0.092). The AUC-ROC of the DCNN was 0.882, 0.781, and 0.961 for detection of medial, lateral, and overall meniscus tear. Inter-reader agreement was very good for the medial (kappa = 0.876) and good for the lateral meniscus (kappa = 0.741). 
  Conclusion:  DCNN-based meniscus tear detection can be performed in a fully automated manner with a similar specificity but a lower sensitivity in comparison with musculoskeletal radiologists. 
  |  https://dx.doi.org/10.1007/s00256-020-03410-2  |  
------------------------------------------- 
10.1097/RMR.0000000000000237  |   This manuscript will review emerging applications of artificial intelligence, specifically deep learning, and its application to glioblastoma multiforme (GBM), the most common primary malignant brain tumor. Current deep learning approaches, commonly convolutional neural networks (CNNs), that take input data from MR images to grade gliomas (high grade from low grade) and predict overall survival will be shown. There will be more in-depth review of recent articles that have applied different CNNs to predict the genetics of glioma on pre-operative MR images, specifically 1p19q codeletion, MGMT promoter, and IDH mutations, which are important criteria for the diagnosis, treatment management, and prognostication of patients with GBM. Finally, there will be a brief mention of current challenges with DL techniques and their application to image analysis in GBM. 
  |  http://dx.doi.org/10.1097/RMR.0000000000000237  |  
------------------------------------------- 
10.1007/s10803-019-04242-3  |   People punish transgressors with different intensity depending if they are members of their group or not. We explore this in a cross-sectional analytical study with paired samples in children with developmental disorders who watched two videos and expressed their opinion. In Video-1, a football-player from the participant's country scores a goal with his hand. In Video-2, a player from another country does the same against the country of the participant. Each subject watched the two videos and their answers were compared. The autism spectrum disorder (ASD) group showed negative feelings in Video 1 (M = - .1; CI 95% - .51 to .31); and in Video 2 (M = - .43; CI 95% .77 to - .09; t(8) = 1.64, p = .13), but the attention deficit hyperactivity disorder, learning disabilities, intellectual disability groups showed positive opinion in Video-1 and negative in Video-2. This suggests that children with ASD respect rules regardless of whether those who break them belong or not to their own group, possibly due to lower degrees of empathy. 
  |  https://doi.org/10.1007/s10803-019-04242-3  |  
------------------------------------------- 
10.1016/j.cognition.2020.104243  |   The processes and mechanisms of human learning are central to inquiries in a number of fields including psychology, cognitive science, development, education, and artificial intelligence. Arguments, debates, and controversies linger over the questions of human learning with one of the most contentious being whether simple associative processes could explain human children's prodigious learning, and in doing so, could lead to artificial intelligence that parallels human learning. One phenomenon at the center of these debates concerns a form of far generalization, sometimes referred to as "generative learning", because the learner's behavior seems to reflect more than co-occurrences among specifically experienced instances and to be based on principles through which new instances may be generated. In two experimental studies (N = 148) of preschool children's learning of how multi-digit number names map to their written forms and in a computational modeling experiment using a deep learning neural network, we show that data sets with a suite of inter-correlated imperfect predictive components yield far and systematic generalizations that accord with generative principles and do so despite limited examples and exceptions in the training data. Implications for human cognition, cognitive development, education, and machine learning are discussed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-0277(20)30062-7  |  
------------------------------------------- 
10.1016/j.clinimag.2020.01.008  |    Purpose:  To validate a machine learning model trained on an open source dataset and subsequently optimize it to chest X-rays with large pneumothoraces from our institution. 
  Methods:  The study was retrospective in nature. The open-source chest X-ray (CXR8) dataset was dichotomized to cases with pneumothorax (PTX) and all other cases (non-PTX), resulting in 41,946 non-PTX and 4696 PTX cases for the training set and 11,120 non-PTX and 541 PTX cases for the validation set. A limited supervision machine learning model was constructed to incorporate both localized and unlocalized pathology. Cases were then queried from our health system from 2013 to 2017. A total of 159 pneumothorax and 682 non-pneumothorax cases were available for the training set. For the validation set, 48 pneumothorax and 1287 non-pneumothorax cases were available. The model was trained, a receiver operator curve (ROC) was created, and output metrics, including area under the curve (AUC), sensitivity and specificity were calculated. 
  Results:  Initial training of the model using the CXR8 dataset resulted in an AUC of 0.90 for pneumothorax detection. Naively inferring our own validation dataset on the CXR8 trained model output an AUC of 0.59. After re-training the model with our own training dataset, the validation dataset inference output an AUC of 0.90. 
  Conclusion:  Our study showed that even though you may get great results on open-source datasets, those models may not translate well to real world data without an intervening retraining process. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0899-7071(20)30019-X  |  
------------------------------------------- 
10.1021/acs.jctc.9b00781  |   Over the past several decades, atomistic simulations of biomolecules, whether carried out using molecular dynamics or Monte Carlo techniques, have provided detailed insights into their function. Comparing the results of such simulations for a few closely related systems has guided our understanding of the mechanisms by which changes such as ligand binding or mutation can alter the function. The general problem of detecting and interpreting such mechanisms from simulations of many related systems, however, remains a challenge. This problem is addressed here by applying supervised and unsupervised machine learning techniques to a variety of thermodynamic observables extracted from molecular dynamics simulations of different systems. As an important test case, these methods are applied to understand the evasion by human immunodeficiency virus type-1 (HIV-1) protease of darunavir, a potent inhibitor to which resistance can develop via the simultaneous mutation of multiple amino acids. Complex mutational patterns have been observed among resistant strains, presenting a challenge to developing a mechanistic picture of resistance in the protease. In order to dissect these patterns and gain mechanistic insight into the role of specific mutations, molecular dynamics simulations were carried out on a collection of HIV-1 protease variants, chosen to include highly resistant strains and susceptible controls, in complex with darunavir. Using a machine learning approach that takes advantage of the hierarchical nature in the relationships among the sequence, structure, and function, an integrative analysis of these trajectories reveals key details of the resistance mechanism, including changes in the protein structure, hydrogen bonding, and protein-ligand contacts. 
  |  None  |  
------------------------------------------- 
10.1371/journal.pone.0227921  |   Low temperature induces changes in plants at physiological and molecular levels, thus affecting growth and development. The Lanzhou lily (Lilium davidii, var. unicolor) is an important medicinal plant with high economic value. However, the molecular mechanisms underlying its photosynthetic and antioxidation responses to low temperature still remain poorly understood. This study subjected the Lanzhou lily to the two temperatures of 20°C (control) and 4°C (low temperature) for 24 h. Physiological parameters related to membrane integrity, photosynthesis, antioxidant system, and differentially expressed genes were investigated. Compared with control, low temperature increased the relative electrical conductivity by 43.2%, while it decreased net photosynthesis rate, ratio of variable to maximal fluorescence, and catalase activity by 47.3%, 10.1%, and 11.1%, respectively. In addition, low temperature significantly increased the content of soluble protein, soluble sugar, and proline, as well as the activity of superoxide dismutase and peroxidase. Comparative transcriptome profiling showed that a total of 238,109 differentially expressed genes were detected. Among these, 3,566 were significantly upregulated while 2,982 were significantly downregulated in response to low temperature. Gene Ontology enrichment analysis indicated that in response to low temperature, the mostly significantly enriched differentially expressed genes were mainly involved in phosphorylation, membrane and protein kinase activity, as well as photosynthesis, light harvesting, light reaction, and alpha,alpha-trehalose-phosphate synthase activity. Kyoto Encyclopedia of Genes and Genomes enrichment analysis also indicated that the most significantly enriched pathways involved ribosome biogenesis in eukaryotes, phenylalanine metabolism, circadian rhythm, porphyrin and chlorophyll metabolism, photosynthesis of antenna proteins, photosynthesis, and carbon fixation in photosynthetic organisms. Moreover, the expression patterns of 10 randomly selected differentially expressed genes confirmed the RNA-Seq results. These results expand the understanding of the physiological and molecular mechanisms underlying the response of the Lanzhou lily to low temperature stress. 
  |  http://dx.plos.org/10.1371/journal.pone.0227921  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31971962/  |  
------------------------------------------- 
10.1007/s11103-020-00971-7  |   RNA-seq was used to analyze the transcriptional changes in sugar beet (Beta vulgaris L.) triggered by alkaline solution to elucidate the molecular mechanism underlying alkaline tolerance in sugar beet. Several differentially expressed genes related to stress tolerance were identified. Our results provide a valuable resource for the breeding of new germplasms with high alkaline tolerance. Alkalinity is a highly stressful environmental factor that limits plant growth and production. Sugar beet own the ability to acclimate to various abiotic stresses, especially salt and alkaline stress. Although substantial previous studies on response of sugar beet to saline stress has been conducted, the expressions of alkali-responsive genes in sugar beet have not been comprehensively investigated. In this study, we conducted transcriptome analysis of leaves in sugar beet seedlings treated with alkaline solutions for 0 day (control, C), 3 days (short-term alkaline treatment, ST) and 7 days (long-term alkaline treatment, LT). The clean reads were obtained and assembled into 25,507 unigenes. Among them, 975 and 383 differentially expressed genes (DEGs) were identified in the comparison groups ST_vs_C and LT_vs_C, respectively. Gene ontology (GO) analysis revealed that oxidation-reduction process and lipid metabolic process were the most enriched GO term among the DEGs in ST_vs_C and LT_vs_C, respectively. According to Kyoto Encyclopedia of Genes and Genomes pathway, carbon fixation in photosynthetic organisms pathway were significantly enriched under alkaline stress. Besides, expression level of genes encoding D-3-phosphoglycerate dehydrogenase 1, glutamyl-tRNA reductase 1, fatty acid hydroperoxide lyase, ethylene-insensitive protein 2, metal tolerance protein 11 and magnesium-chelatase subunit ChlI, etc., were significantly altered under alkaline stress. Additionally, among the DEGs, 136 were non-annotated genes and 24 occurred with differential alternative splicing. Our results provide a valuable resource on alkali-responsive genes and should benefit the improvement of alkaline stress tolerance in sugar beet. 
  |  https://doi.org/10.1007/s11103-020-00971-7  |  
------------------------------------------- 
10.1016/j.jns.2020.116667  |    Objective:  To develop artificial neural network (ANN)-based functional outcome prediction models for patients with acute ischemic stroke (AIS) receiving intravenous thrombolysis based on immediate pretreatment parameters. 
  Methods:  The derived cohort consisted of 196 patients with AIS treated with intravenous thrombolysis between 2009 and 2017 at Shuang Ho Hospital in Taiwan. We evaluated the predictive value of parameters associated with major neurologic improvement (MNI) at 24 h after thrombolysis as well as the 3-month outcome. ANN models were applied for outcome prediction. The generalizability of the model was assessed through 5-fold cross-validation. The performance of the models was assessed according to the accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), RESULTS: The parameters associated with MNI were blood pressure (BP), heart rate, glucose level, consciousness level, National Institutes of Health Stroke Scale (NIHSS) score, and history of diabetes mellitus (DM). The parameters associated with the 3-month outcome were age, consciousness level, BP, glucose level, hemoglobin A1c, history of DM, stroke subtype, and NIHSS score. After adequate training, ANN Model 1 to predict MNI achieved an AUC of 0.944. Accuracy, sensitivity, and specificity were 94.6%, 89.8%, and 95.9%, respectively. ANN Model 2 to predict the 3-month outcome achieved an AUC of 0.933, with accuracy, sensitivity, and specificity of 88.8%, 94.7%, and 86.5%, respectively. 
  Conclusions:  The ANN-based models achieved reliable performance to predict MNI and 3-month outcomes after thrombolysis for AIS. The models proposed have clinical value to assist in decision-making, especially when invasive adjuvant strategies are considered. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-510X(20)30003-4  |  
------------------------------------------- 
10.1371/journal.pone.0226595  |   Standard treatment for locally advanced rectal adenocarcinoma (LARC) includes a combination of chemotherapy with pyrimidine analogues, such as capecitabine, and radiation therapy, followed by surgery. Currently no clinically useful genomic predictors of benefit from neoadjuvant chemoradiotherapy (nCRT) exist for LARC. In this study we assessed the expression of 8,127 long noncoding RNAs (lncRNAs), poorly studied in LARC, to infer their ability in classifying patients' pathological complete response (pCR). We collected and analyzed, using lncRNA-specific Agilent microarrays a consecutive series of 61 LARC cases undergoing nCRT. Potential lncRNA predictors in responders and non-responders to nCRT were identified with LASSO regression, and a model was optimized using k-fold cross-validation after selection of the three most informative lncRNA. 11 lncRNAs were differentially expressed with false discovery rate &lt; 0.01 between responders and non-responders to NACT. We identified lnc-KLF7-1, lnc-MAB21L2-1, and LINC00324 as the most promising variable subset for classification building. Overall sensitivity and specificity were 0.91 and 0.94 respectively, with an AUC of our ROC curve = 0.93. Our study shows for the first time that lncRNAs can accurately predict response in LARC undergoing nCRT. Our three-lncRNA based signature must be independently validated and further analyses must be conducted to fully understand the biological role of the identified signature, but our results suggest lncRNAs may be an ideal biomarker for response prediction in the studied setting. 
  |  http://dx.plos.org/10.1371/journal.pone.0226595  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023246/  |  
------------------------------------------- 
10.1055/a-1036-6114  |   <b>Background and study aims </b> Artificial intelligence (AI)-assisted image classification has been shown to have high accuracy on endoscopic diagnosis. We evaluated the potential effects of use of an AI-assisted image classifier on training of junior endoscopists for histological prediction of gastric lesions. <b>Methods </b> An AI image classifier was built on a convolutional neural network with five convolutional layers and three fully connected layers A Resnet backbone was trained by 2,000 non-magnified endoscopic gastric images. The independent validation set consisted of another 1,000 endoscopic images from 100 gastric lesions. The first part of the validation set was reviewed by six junior endoscopists and the prediction of AI was then disclosed to three of them (Group A) while the remaining three (Group B) were not provided this information. All endoscopists reviewed the second part of the validation set independently. <b>Results </b> The overall accuracy of AI was 91.0 % (95 % CI: 89.2-92.7 %) with 97.1 % sensitivity (95 % CI: 95.6-98.7%), 85.9 % specificity (95 % CI: 83.0-88.4 %) and 0.91 area under the ROC (AUROC) (95 % CI: 0.89-0.93). AI was superior to all junior endoscopists in accuracy and AUROC in both validation sets. The performance of Group A endoscopists but not Group B endoscopists improved on the second validation set (accuracy 69.3 % to 74.7 %; <i>P</i> = 0.003). <b>Conclusion </b> The trained AI image classifier can accurately predict presence of neoplastic component of gastric lesions. Feedback from the AI image classifier can also hasten the learning curve of junior endoscopists in predicting histology of gastric lesions. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/a-1036-6114  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32010746/  |  
------------------------------------------- 
10.1016/j.oraloncology.2020.104634  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1368-8375(20)30070-1  |  
------------------------------------------- 
10.1016/j.cmi.2020.03.006  |    Objectives:  Inconsistent results have been found between pneumonia and meteorological factors. We aimed to identify principal meteorological factors associated with pneumonia, and to estimate the effect size and lag time. 
  Methods:  This was nationwide population-based study used a healthcare claims database merged with a weather database in eight metropolitan cities in Korea. We applied a stepwise approach using the Granger causality test and generalized additive model to elucidate the association between weekly pneumonia incidence (WPI) and meteorological factors/air pollutants (MFAP). Impulse response function was used to examine the time lag. 
  Results:  In total, 2 011 424 cases of pneumonia were identified from 2007 to 2017. Among MFAP, diurnal temperature range (DTR), humidity and particulate matter ≤2.5 μm in diameter (PM<sub>2.5</sub>) showed statistically significant associations with WPI (p &lt; 0.001 for all 3 MFAPs). The association of DTR and WPI showed an inverted U pattern for bacterial and unspecified pneumonia, whereas for viral pneumonia, WPI increased gradually in a more linear manner with DTR and no substantial decline. Humidity showed a consistent pattern in all three pneumonia categories. WPI steeply increased up to 10 to 20 μg/m³ of PM<sub>2.5</sub> but did not show a further increase in higher concentrations. On the basis of the result, we examined the effect of MFAP in different lag times up to 3 weeks. 
  Conclusions:  DTR, humidity and PM<sub>2.5</sub> were identified as MFAP most closely associated with WPI. With the model, we were able to visualize the effect-time association of MFAP and WPI. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1198-743X(20)30148-8  |  
------------------------------------------- 
10.1007/s10554-020-01853-1  |   Beginning with the discovery of X-rays to the development of three-dimensional (3D) imaging, improvements in acquisition, post-processing, and visualization have provided clinicians with detailed information for increasingly accurate medical diagnosis and clinical management. This paper highlights advances in imaging technologies for congenital heart disease (CHD), medical adoption, and future developments required to improve pre-procedural and intra-procedural guidance. 
  |  https://doi.org/10.1007/s10554-020-01853-1  |  
------------------------------------------- 
10.1111/nph.16621  |   Though substantial effort has gone into predicting how global climate change will impact biodiversity patterns, the scarcity of taxon-specific information has hampered the efficacy of these endeavors. Further, most studies analyzing spatiotemporal patterns of biodiversity focus narrowly on species richness. We apply machine learning approaches to a comprehensive vascular plant database for the United States and generate predictive models of regional plant taxonomic and phylogenetic diversity in response to a wide range of environmental variables. We demonstrate differences in predicted patterns and potential drivers of native versus non-native biodiversity. In particular, native phylogenetic diversity is likely to decrease over the next half century despite increases in species richness. We also identify that patterns of taxonomic diversity can be incongruent with those of phylogenetic diversity. The combination of macro-environmental factors that determine diversity likely vary at continental scales; thus, as climate change alters the combinations of these factors across the landscape, the collective effect on regional diversity will also vary. Our study represents one of the most comprehensive examination of plant diversity patterns to date and demonstrates that our ability to predict future diversity may benefit tremendously from the application of machine learning. 
  |  https://doi.org/10.1111/nph.16621  |  
------------------------------------------- 
10.1080/02640414.2020.1735983  |   Swimmers with limb deficiency are a core population within Para Swimming, accordingly this study examined the contribution of limb segments to race performance in these swimmers. Data were obtained for 174 male Para swimmers with limb deficiency. Ensemble partial least squares regression showed accurate predictions when using relative limb segment lengths to estimate Para swimmers' personal best race performances. The contribution of limb segments to performance in swim events was estimated using these regression models. The analysis found swim stroke and event distance to influence the contributions of limb segments to performance. For freestyle swim events, these changes were primarily due to the increased importance of the hand, and decreased importance of the foot and shank, as the distance of the event increased. When comparing swim strokes, higher importance of the thigh and shank in the 100 m breaststroke compared with other swim strokes confirms the separate SB class. Varied contributions of the hand, upper arm and foot suggest that freestyle could also be separated from backstroke and butterfly events to promote fairer classification. This study shows that swim stroke and event distance influence the activity limitation of Para swimmers with limb deficiency suggesting classification should account for these factors. 
  |  None  |  
------------------------------------------- 
10.1097/01.APO.0000656988.16221.04  |   The rising popularity of artificial intelligence (AI) in ophthalmology is fuelled by the ever-increasing clinical "big data" that can be used for algorithm development. Cataract is one of the leading causes of visual impairment worldwide. However, compared with other major age-related eye diseases, such as diabetic retinopathy, age-related macular degeneration, and glaucoma, AI development in the domain of cataract is still relatively underexplored. In this regard, several previous studies explored algorithms for automated cataract assessment using either slit lamp of color fundus photographs. However, several other study groups proposed or derived new AI-based calculation for pre-cataract surgery intraocular lens power. Along with advancements in digitization of clinical data, data curation for future cataract-related AI developmental work is bound to undergo significant improvements in the foreseeable future. Even though most of these previous studies reported early promising performances, limitations such as lack of robust, high-quality training data, and lack of external validations remain. In the next phase of work, apart from algorithm's performance, it will also be pertinent to evaluate deployment angles, feasibility, efficiency, and cost-effectiveness of these new cataract-related AI systems. 
  |  http://dx.doi.org/10.1097/01.APO.0000656988.16221.04  |  
------------------------------------------- 
10.1371/journal.pone.0229354  |   Systematic exposure to social media causes social comparisons, especially among women who compare their image to others; they are particularly vulnerable to mood decrease, self-objectification, body concerns, and lower perception of themselves. This study first investigates the possible links between life satisfaction, self-esteem, anxiety, depression, and the intensity of Instagram use with a social comparison model. In the study, 974 women age 18-49 who were Instagram users voluntarily participated, completing a questionnaire. The results suggest associations between the analyzed psychological data and social comparison types. Then, artificial neural networks models were implemented to predict the type of such comparison (positive, negative, equal) based on the aforementioned psychological traits. The models were able to properly predict between 71% and 82% of cases. As human behavior analysis has been a subject of study in various fields of science, this paper contributes towards understanding the role of artificial intelligence methods for analyzing behavioral data in psychology. 
  |  http://dx.plos.org/10.1371/journal.pone.0229354  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32097446/  |  
------------------------------------------- 
10.1007/s11102-020-01032-4  |    Purpose:  This study was designed to develop a computer-aided diagnosis (CAD) system based on a convolutional neural network (CNN) to diagnose patients with pituitary tumors. 
  Methods:  We included adult patients clinically diagnosed with pituitary adenoma (pituitary adenoma group), or adult individuals without pituitary adenoma (control group). After pre-processing, all the MRI data were randomly divided into training or testing datasets in a ratio of 8:2 to create or evaluate the CNN model. Multiple CNNs with the same structure were applied for different types of MR images respectively, and a comprehensive diagnosis was performed based on the classification results of different types of MR images using an equal-weighted majority voting strategy. Finally, we assessed the diagnostic performance of the CAD system by accuracy, sensitivity, specificity, positive predictive value, and F1 score. 
  Results:  We enrolled 149 participants with 796 MR images and adopted the data augmentation technology to create 7960 new images. The proposed CAD method showed remarkable diagnostic performance with an overall accuracy of 91.02%, sensitivity of 92.27%, specificity of 75.70%, positive predictive value of 93.45%, and F1-score of 92.67% in separate MRI type. In the comprehensive diagnosis, the CAD achieved better performance with accuracy, sensitivity, and specificity of 96.97%, 94.44%, and 100%, respectively. 
  Conclusion:  The CAD system could accurately diagnose patients with pituitary tumors based on MR images. Further, we will improve this CAD system by augmenting the amount of dataset and evaluate its performance by external dataset. 
  |  https://doi.org/10.1007/s11102-020-01032-4  |  
------------------------------------------- 
10.1016/j.compmedimag.2020.101711  |   Plaque rupture and subsequent thrombosis are major processes of acute cardiovascular events. The Vulnerability Index is a very important indicator of whether a plaque is ruptured, and these easily ruptured or fragile plaques can be detected early. The higher the general vulnerability index, the higher the instability of the plaque. Therefore, determining a clear vulnerability index classification point can effectively reduce unnecessary interventional therapy. However, the current critical value of the vulnerability index has not been well defined. In this study, we proposed a neural network-based method to determine the critical point of vulnerability index that distinguishes vulnerable plaques from stable ones. Firstly, based on MatConvNet, the intravascular ultrasound images under different vulnerability index labels are classified. Different vulnerability indexes can obtain different accuracy rates for the demarcation points. The corresponding data points are fitted to find the existing relationship to judge the highest classification. In this way, the vulnerability index corresponding to the highest classification accuracy rate is judged. Then the article is based on the same experiment of different components of the aortic artery in the artificial neural network, and finally the vulnerability index corresponding to the highest classification accuracy can be obtained. The results show that the best vulnerability index point is 1.716 when the experiment is based on the intravascular ultrasound image, and the best vulnerability index point is 1.607 when the experiment is based on the aortic artery component data. Moreover, the vulnerability index and classification accuracy rate has a periodic relationship within a certain range, and finally the highest AUC is 0.7143 based on the obtained vulnerability index point on the verification set. In this paper, the convolution neural network is used to find the best vulnerability index classification points. The experimental results show that this method has the guiding significance for the classification and diagnosis of vulnerable plaques, further reduce interventional treatment of cardiovascular disease. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0895-6111(20)30014-8  |  
------------------------------------------- 
10.1007/s10278-019-00295-z  |   Breast cancer is the most common type of malignancy diagnosed in women. Through early detection and diagnosis, there is a great chance of recovery and thereby reduce the mortality rate. Many preliminary tests like non-invasive radiological diagnosis using ultrasound, mammography, and MRI are widely used for the diagnosis of breast cancer. However, histopathological analysis of breast biopsy specimen is inevitable and is considered to be the golden standard for the affirmation of cancer. With the advancements in the digital computing capabilities, memory capacity, and imaging modalities, the development of computer-aided powerful analytical techniques for histopathological data has increased dramatically. These automated techniques help to alleviate the laborious work of the pathologist and to improve the reproducibility and reliability of the interpretation. This paper reviews and summarizes digital image computational algorithms applied on histopathological breast cancer images for nuclear atypia scoring and explores the future possibilities. The algorithms for nuclear pleomorphism scoring of breast cancer can be widely grouped into two categories: handcrafted feature-based and learned feature-based. Handcrafted feature-based algorithms mainly include the computational steps like pre-processing the images, segmenting the nuclei, extracting unique features, feature selection, and machine learning-based classification. However, most of the recent algorithms are based on learned features, that extract high-level abstractions directly from the histopathological images utilizing deep learning techniques. In this paper, we discuss the various algorithms applied for the nuclear pleomorphism scoring of breast cancer, discourse the challenges to be dealt with, and outline the importance of benchmark datasets. A comparative analysis of some prominent works on breast cancer nuclear atypia scoring is done using a benchmark dataset which enables to quantitatively measure and compare the different features and algorithms used for breast cancer grading. Results show that improvements are still required, to have an automated cancer grading system suitable for clinical applications. 
  |  https://doi.org/10.1007/s10278-019-00295-z  |  
------------------------------------------- 
10.1038/s41598-019-56927-5  |   Tracking the fluctuations in blood glucose levels is important for healthy subjects and crucial diabetic patients. Tight glucose monitoring reduces the risk of hypoglycemia, which can result in a series of complications, especially in diabetic patients, such as confusion, irritability, seizure and can even be fatal in specific conditions. Hypoglycemia affects the electrophysiology of the heart. However, due to strong inter-subject heterogeneity, previous studies based on a cohort of subjects failed to deploy electrocardiogram (ECG)-based hypoglycemic detection systems reliably. The current study used personalised medicine approach and Artificial Intelligence (AI) to automatically detect nocturnal hypoglycemia using a few heartbeats of raw ECG signal recorded with non-invasive, wearable devices, in healthy individuals, monitored 24 hours for 14 consecutive days. Additionally, we present a visualisation method enabling clinicians to visualise which part of the ECG signal (e.g., T-wave, ST-interval) is significantly associated with the hypoglycemic event in each subject, overcoming the intelligibility problem of deep-learning methods. These results advance the feasibility of a real-time, non-invasive hypoglycemia alarming system using short excerpts of ECG signal. 
  |  http://dx.doi.org/10.1038/s41598-019-56927-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31932608/  |  
------------------------------------------- 
10.1093/jnci/djaa017  |    Background:  To forecast survival and enhance treatment decisions for patients with colorectal cancer liver metastases (mCRC) by using on-treatment radiomics signature to predict tumor sensitiveness to FOLFIRI±cetuximab. 
  Methods:  We retrospectively analyzed 667 mCRC patients treated with FOLFIRI alone [F] or in combination with cetuximab [FC]. CT quality was classified as high (HQ) or standard (SD). Four datasets were created using the nomenclature [treatment]-[quality]. Patients were randomly assigned (2:1) to training or validation sets: FCHQ: 78:38, FCSD: 124:62, FHQ: 78:51, FSD: 158:78. Four tumor imaging biomarkers measured quantitative radiomics changes between standard of care CT scans at baseline and 8 weeks. Using machine learning, the performance of the signature to classify tumors as treatment-sensitive or treatment-insensitive was trained and validated using ROC curves. Hazard Ratio (HR) and Cox Regression models evaluated association with overall survival (OS). 
  Results:  The signature (AUC[95CI]) used temporal decrease in tumor spatial heterogeneity plus boundary infiltration to successfully predict sensitivity to anti-EGFR therapy (FCHQ: 0.80 [0.69-0.94], FCSD: 0.72 [0.59-0.83]) but failed with chemotherapy (FHQ: 0.59 [0.44-0.72], FSD: 0.55 [0.43-0.66]). In cetuximab-containing sets, radiomics signature outperformed existing biomarkers (KRAS-mutational status, and tumor shrinkage by RECIST 1.1) for detection of treatment-sensitivity and was strongly associated with OS (two-sided P &lt; 0.005). 
  Conclusions:  Radiomics response signature can serve as an intermediate surrogate marker of overall survival. The signature outperformed known biomarkers in providing an early prediction of treatment-sensitivity and could be used to guide cetuximab treatment continuation decisions. 
  |  https://academic.oup.com/jnci/article-lookup/doi/10.1093/jnci/djaa017  |  
------------------------------------------- 
10.1016/j.jss.2020.03.046  |   Surgeons perform two primary tasks: operating and engaging patients and caregivers in shared decision-making. Human dexterity and decision-making are biologically limited. Intelligent, autonomous machines have the potential to augment or replace surgeons. Rather than regarding this possibility with denial, ire, or indifference, surgeons should understand and steer these technologies. Closer examination of surgical innovations and lessons learned from the automotive industry can inform this process. Innovations in minimally invasive surgery and surgical decision-making follow classic S-shaped curves with three phases: (1) introduction of a new technology, (2) achievement of a performance advantage relative to existing standards, and (3) arrival at a performance plateau, followed by replacement with an innovation featuring greater machine autonomy and less human influence. There is currently no level I evidence demonstrating improved patient outcomes using intelligent, autonomous machines for performing operations or surgical decision-making tasks. History suggests that if such evidence emerges and if the machines are cost effective, then they will augment or replace humans, initially for simple, common, rote tasks under close human supervision and later for complex tasks with minimal human supervision. This process poses ethical challenges in assigning liability for errors, matching decisions to patient values, and displacing human workers, but may allow surgeons to spend less time gathering and analyzing data and more time interacting with patients and tending to urgent, critical-and potentially more valuable-aspects of patient care. Surgeons should steer these technologies toward optimal patient care and net social benefit using the uniquely human traits of creativity, altruism, and moral deliberation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-4804(20)30178-5  |  
------------------------------------------- 
10.1089/end.2019.0475  |   <b><i>Purpose:</i></b> To validate the output of a machine learning-based software as an intelligible interface for predicting multiple outcomes after percutaneous nephrolithotomy (PCNL). We compared the performance of this system with Guy's stone score (GSS) and the Clinical Research Office of Endourological Society (CROES) nomogram. <b><i>Patients and Methods:</i></b> Data from 146 adult patients (87 males, 59%) who underwent PCNL at our institute were used. To validate the system, accuracy of the software for predicting each postoperative outcome was compared with the actual outcome. Similarly, preoperative data were analyzed with GSS and CROES nomograms to determine stone-free status as predicted by these nomograms. A receiver operating characteristic (ROC) curve was generated for each scoring system, and the area under the ROC curve (AUC) was calculated and used to assess the predictive performance of all three models. <b><i>Results:</i></b> Overall stone-free rate was 72.6% (106/146). Forty of 146 patients (27.4%) were scheduled for 42 ancillary procedures (extracorporeal shockwave lithotripsy [SWL] [<i>n</i> = 31] or repeat PCNL [<i>n</i> = 11]) to manage residual renal stones. Overall, the machine learning system predicted the PCNL outcomes with an accuracy ranging between 80% and 95.1%. For predicting the stone-free status, the AUC for the software (0.915) was significantly larger than the AUC for GSS (0.615) or CROES nomograms (0.621) (<i>p</i> &lt; 0.001). <b><i>Conclusion:</i></b> At the internal institutional level, the machine learning-based software was a promising tool for recording, processing, and predicting outcomes after PCNL. Validation of this system against an external dataset is highly recommended before its widespread application. 
  |  https://www.liebertpub.com/doi/full/10.1089/end.2019.0475?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.scitotenv.2020.138858  |   The emergence of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) in China at December 2019 had led to a global outbreak of coronavirus disease 2019 (COVID-19) and the disease started to spread all over the world and became an international public health issue. The entire humanity has to fight in this war against the unexpected and each and every individual role is important. Healthcare system is doing exceptional work and the government is taking various measures that help the society to control the spread. Public, on the other hand, coordinates with the policies and act accordingly in most state of affairs. But the role of technologies in assisting different social bodies to fight against the pandemic remains hidden. The intention of our study is to uncover the hidden roles of technologies that ultimately help for controlling the pandemic. On investigating, it is found that the strategies utilizing potential technologies would yield better benefits and these technological strategies can be framed either to control the pandemic or to support the confinement of the society during pandemic which in turn aids in controlling the spreading of infection. This study enlightens the various implemented technologies that assists the healthcare systems, government and public in diverse aspects for fighting against COVID-19. Furthermore, the technological swift that happened during the pandemic and their influence in the environment and society is discussed. Besides the implemented technologies, this work also deals with untapped potential technologies that have prospective applications in controlling the pandemic circumstances. Alongside the various discussion, our suggested solution for certain situational issues is also presented. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(20)32375-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32336562/  |  
------------------------------------------- 
10.1186/s12880-020-0409-2  |   MR images (MRIs) accurate segmentation of brain lesions is important for improving cancer diagnosis, surgical planning, and prediction of outcome. However, manual and accurate segmentation of brain lesions from 3D MRIs is highly expensive, time-consuming, and prone to user biases. We present an efficient yet conceptually simple brain segmentation network (referred as Brain SegNet), which is a 3D residual framework for automatic voxel-wise segmentation of brain lesion. Our model is able to directly predict dense voxel segmentation of brain tumor or ischemic stroke regions in 3D brain MRIs. The proposed 3D segmentation network can run at about 0.5s per MRIs - about 50 times faster than previous approaches Med Image Anal 43: 98-111, 2018, Med Image Anal 36:61-78, 2017. Our model is evaluated on the BRATS 2015 benchmark for brain tumor segmentation, where it obtains state-of-the-art results, by surpassing recently published results reported in Med Image Anal 43: 98-111, 2018, Med Image Anal 36:61-78, 2017. We further applied the proposed Brain SegNet for ischemic stroke lesion outcome prediction, with impressive results achieved on the Ischemic Stroke Lesion Segmentation (ISLES) 2017 database. 
  |  https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-020-0409-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046685/  |  
------------------------------------------- 
10.1016/j.jns.2020.116730  |    Objective:  This study was aimed to discuss the application of radiomics using CT analysis in basal ganglia infarction (BGI) for determining the time since stroke onset (TSS) which could provide critical information to clinicians in deciding stroke treatment options such as thrombolysis. 
  Methods:  This study involved 316 patients with BGI (237 in the training cohort and 79 in the independent validation cohort). Region of interest segmentation and feature extraction was done by ITK-SNAP software. We used the existing medical history to binarize the TSS into two categories: positive (&lt; 4.5 h) and negative (≥ 4.5 h). The key radiomic signature features were retrieved by the least absolute shrinkage and selection operator multiple logistic regression model. Receiver operating characteristic curve and AUC analysis were used to evaluate the performance of the radiomic signature in both the training and validation cohorts. 
  Results:  295 features were extracted from a manually outlined infarction region. Five features were selected to construct the radiomic signature for TSS classification purposes. The performance of the radiomic signature to distinguish between positive and negative in the training cohort was good, with an AUC of 0.982, a sensitivity of 0.929, and a specificity of 0.959. In the validation cohort, the radiomic signature showed an AUC of 0.974, a sensitivity of 0.951, and a specificity of 0.961. 
  Conclusion:  A unique radiomic signature was constructed for use as a diagnostic tool for discriminating the TSS in BGI and may guide decisions to use thrombolysis in patients with unknown times of BGI onset. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-510X(20)30066-6  |  
------------------------------------------- 
10.3233/JAD-190952  |   Due to the high costs of providing long-term care to older adults with cognitive impairment, artificial companions are increasingly considered as a cost-efficient way to provide support. Artificial companions can comfort, entertain, and inform, and even induce a sense of being in a close relationship. Sensors and algorithms are increasingly leading to applications that exude a life-like feel. We focus on a case study of an artificial companion for people with cognitive impairment. This companion is an avatar on an electronic tablet that is displayed as a dog or a cat. Whereas artificial intelligence guides most artificial companions, this application also relies on technicians "behind" the on-screen avatar, who via surveillance, interact with users. This case is notable because it particularly illustrates the tension between the endless opportunities offered by technology and the ethical issues stemming from limited regulations. Reviewing the case through the lens of biomedical ethics, concerns of deception, monitoring and tracking, as well as informed consent and social isolation are raised by the introduction of this technology to users with cognitive impairment. We provide a detailed description of the case, review the main ethical issues and present two theoretical frameworks, the "human-driven technology" platform and the emancipatory gerontology framework, to inform the design of future applications. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/JAD-190952  |  
------------------------------------------- 
10.1001/jamadermatol.2019.5014  |    Importance:  The use of artificial intelligence (AI) is expanding throughout the field of medicine. In dermatology, researchers are evaluating the potential for direct-to-patient and clinician decision-support AI tools to classify skin lesions. Although AI is poised to change how patients engage in health care, patient perspectives remain poorly understood. 
  Objective:  To explore how patients conceptualize AI and perceive the use of AI for skin cancer screening. 
  Design, setting, and participants:  A qualitative study using a grounded theory approach to semistructured interview analysis was conducted in general dermatology clinics at the Brigham and Women's Hospital and melanoma clinics at the Dana-Farber Cancer Institute. Forty-eight patients were enrolled. Each interview was independently coded by 2 researchers with interrater reliability measurement; reconciled codes were used to assess code frequency. The study was conducted from May 6 to July 8, 2019. 
  Main outcomes and measures:  Artificial intelligence concept, perceived benefits and risks of AI, strengths and weaknesses of AI, AI implementation, response to conflict between human and AI clinical decision-making, and recommendation for or against AI. 
  Results:  Of 48 patients enrolled, 26 participants (54%) were women; mean (SD) age was 53.3 (21.7) years. Sixteen patients (33%) had a history of melanoma, 16 patients (33%) had a history of nonmelanoma skin cancer only, and 16 patients (33%) had no history of skin cancer. Twenty-four patients were interviewed about a direct-to-patient AI tool and 24 patients were interviewed about a clinician decision-support AI tool. Interrater reliability ratings for the 2 coding teams were κ = 0.94 and κ = 0.89. Patients primarily conceptualized AI in terms of cognition. Increased diagnostic speed (29 participants [60%]) and health care access (29 [60%]) were the most commonly perceived benefits of AI for skin cancer screening; increased patient anxiety was the most commonly perceived risk (19 [40%]). Patients perceived both more accurate diagnosis (33 [69%]) and less accurate diagnosis (41 [85%]) to be the greatest strength and weakness of AI, respectively. The dominant theme that emerged was the importance of symbiosis between humans and AI (45 [94%]). Seeking biopsy was the most common response to conflict between human and AI clinical decision-making (32 [67%]). Overall, 36 patients (75%) would recommend AI to family members and friends. 
  Conclusions and relevance:  In this qualitative study, patients appeared to be receptive to the use of AI for skin cancer screening if implemented in a manner that preserves the integrity of the human physician-patient relationship. 
  |  https://jamanetwork.com/journals/jamadermatology/fullarticle/10.1001/jamadermatol.2019.5014  |  
------------------------------------------- 
10.1186/s12911-019-1008-4  |    Background:  We used the Surveillance, Epidemiology, and End Results (SEER) database to develop and validate deep survival neural network machine learning (ML) algorithms to predict survival following a spino-pelvic chondrosarcoma diagnosis. 
  Methods:  The SEER 18 registries were used to apply the Risk Estimate Distance Survival Neural Network (RED_SNN) in the model. Our model was evaluated at each time window with receiver operating characteristic curves and areas under the curves (AUCs), as was the concordance index (c-index). 
  Results:  The subjects (n = 1088) were separated into training (80%, n = 870) and test sets (20%, n = 218). The training data were randomly sorted into training and validation sets using 5-fold cross validation. The median c-index of the five validation sets was 0.84 (95% confidence interval 0.79-0.87). The median AUC of the five validation subsets was 0.84. This model was evaluated with the previously separated test set. The c-index was 0.82 and the mean AUC of the 30 different time windows was 0.85 (standard deviation 0.02). According to the estimated survival probability (by 62 months), we divided the test group into five subgroups. The survival curves of the subgroups showed statistically significant separation (p &lt; 0.001). 
  Conclusions:  This study is the first to analyze population-level data using artificial neural network ML algorithms for the role and outcomes of surgical resection and radiation therapy in spino-pelvic chondrosarcoma. 
  |  https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-1008-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31907039/  |  
------------------------------------------- 
10.1016/j.ejvs.2020.02.030  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1078-5884(20)30165-9  |  
------------------------------------------- 
10.1016/j.jacr.2020.04.007  |    |  None  |  
------------------------------------------- 
10.1002/hep.31171  |   We thank Gawrieh et al. for their interest in our work and agree with them that computational methods leveraging machine learning and artificial intelligence approaches offer promise to enhance histopathological assessment of liver biopsies. 
  |  https://doi.org/10.1002/hep.31171  |  
------------------------------------------- 
10.1016/j.jacr.2020.04.010  |    |  None  |  
------------------------------------------- 
10.1016/j.acra.2019.12.015  |    Rationale and objectives:  This study aimed to investigate whether benign and malignant renal solid masses could be distinguished through machine learning (ML)-based computed tomography (CT) texture analysis. 
  Materials and methods:  Seventy-nine patients with 84 solid renal masses (21 benign; 63 malignant) from a single center were included in this retrospective study. Malignant masses included common renal cell carcinoma (RCC) subtypes: clear cell RCC, papillary cell RCC, and chromophobe RCC. Benign masses are represented by oncocytomas and fat-poor angiomyolipomas. Following preprocessing steps, a total of 271 texture features were extracted from unenhanced and contrast-enhanced CT images. Dimension reduction was done with a reliability analysis and then with a feature selection algorithm. A nested-approach was used for feature selection, model optimization, and validation. Eight ML algorithms were used for the classifications: decision tree, locally weighted learning, k-nearest neighbors, naive Bayes, logistic regression, support vector machine, neural network, and random forest. 
  Results:  The number of features with good reproducibility was 198 for unenhanced CT and 244 for contrast-enhanced CT. Random forest algorithm demonstrated the best predictive performance using five selected contrast-enhanced CT texture features. The accuracy and area under the curve metrics were 90.5% and 0.915, respectively. Having eliminated the highly collinear features from the analysis, the accuracy and area under the curve values slightly increased to 91.7% and 0.916, respectively. 
  Conclusion:  ML-based contrast-enhanced CT texture analysis might be a potential method for distinguishing benign and malignant solid renal masses with satisfactory performance. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30003-9  |  
------------------------------------------- 
10.1111/medu.14131  |   'COLD' TECHNOLOGIES AND 'WARM' HANDS-ON MEDICINE NEED TO WALK HAND-IN-HAND: Technologies, such as deep learning artificial intelligence (AI), promise benign solutions to thorny, complex problems; but this view is misguided. Though AI has revolutionised aspects of technical medicine, it has brought in its wake practical, conceptual, pedagogical and ethical conundrums. For example, widespread adoption of technologies threatens to shift emphasis from 'hands-on' embodied clinical work to disembodied 'technology enhanced' fuzzy scenarios muddying ethical responsibilities. Where AI can offer a powerful sharpening of diagnostic accuracy and treatment options, 'cold' technologies and 'warm' hands-on medicine need to walk hand-in-hand. This presents a pedagogical challenge grounded in historical precedent: in the wake of Vesalian anatomy introducing the dominant metaphor of 'body as machine,' a medicine of qualities was devalued through the rise of instrumental scientific medicine. The AI age in medicine promises to redouble the machine metaphor, reducing complex patient experiences to linear problem-solving interventions promising 'solutionism.' As an instrumental intervention, AI can objectify patients, frustrating the benefits of dialogue, as patients' complex and often unpredictable fleshly experiences of illness are recalculated in solution-focused computational terms. SUSPICIONS ABOUT SOLUTIONS: The rate of change in numbers and sophistication of new technologies is daunting; they include surgical robotics, implants, computer programming and genetic interventions such as clustered regularly interspaced short palindromic repeats (CRISPR). Contributing to the focus of this issue on 'solutionism,' we explore how AI is often promoted as an all-encompassing answer to complex problems, including the pedagogical, where learning 'hands-on' bedside medicine has proven benefits beyond the technical. Where AI and embodied medicine have differing epistemological, ontological and axiological roots, we must not imagine that they will readily walk hand-in-hand down the aisle towards a happy marriage. Their union will be fractious, requiring lifelong guidance provided by a perceptive medical education suspicious of 'smart' solutions to complex problems. 
  |  https://doi.org/10.1111/medu.14131  |  
------------------------------------------- 
10.1016/j.acra.2019.09.008  |    Objectives:  Noise, commonly encountered on computed tomography (CT) images, can impact diagnostic accuracy. To reduce the image noise, we developed a deep-learning reconstruction (DLR) method that integrates deep convolutional neural networks into image reconstruction. In this phantom study, we compared the image noise characteristics, spatial resolution, and task-based detectability on DLR images and images reconstructed with other state-of-the art techniques. 
  Methods:  We scanned a phantom harboring cylindrical modules with different contrast on a 320-row detector CT scanner. Phantom images were reconstructed with filtered back projection, hybrid iterative reconstruction, model-based iterative reconstruction, and DLR. The standard deviation of the CT number and the noise power spectrum were calculated for noise characterization. The 10% modulation-transfer function (MTF) level was used to evaluate spatial resolution; task-based detectability was assessed using the model observer method. 
  Results:  On images reconstructed with DLR, the noise was lower than on images subjected to other reconstructions, especially at low radiation dose settings. Noise power spectrum measurements also showed that the noise amplitude was lower, especially for low-frequency components, on DLR images. Based on the MTF, spatial resolution was higher on model-based iterative reconstruction image than DLR image, however, for lower-contrast objects, the MTF on DLR images was comparable to images reconstructed with other methods. The machine observer study showed that at reduced radiation-dose settings, DLR yielded the best detectability. 
  Conclusion:  On DLR images, the image noise was lower, and high-contrast spatial resolution and task-based detectability were better than on images reconstructed with other state-of-the art techniques. DLR also outperformed other methods with respect to task-based detectability. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30434-9  |  
------------------------------------------- 
10.1007/s11517-020-02155-3  |   This study outlines the first investigation of application of machine learning to distinguish "skilled" and "novice" psychomotor performance during a virtual reality (VR) brain tumor resection task. Tumor resection task participants included 23 neurosurgeons and senior neurosurgery residents as the "skilled" group and 92 junior neurosurgery residents and medical students as the "novice" group. The task involved removing a series of virtual brain tumors without causing injury to surrounding tissue. Originally, 150 features were extracted followed by statistical and forward feature selection. The selected features were provided to 4 classifiers, namely, K-Nearest Neighbors, Parzen Window, Support Vector Machine, and Fuzzy K-Nearest Neighbors. Sets of 5 to 30 selected features were provided to the classifiers. A working point of 15 premium features resulted in accuracy values as high as 90% using the Supprt Vector Machine. The obtained results highlight the potentials of machine learning, applied to VR simulation data, to help realign the traditional apprenticeship educational paradigm to a more objective model, based on proven performance standards. Graphical abstract Using several scenarios of virtual reality neurosurgical tumor resection together with machine learning classifiers to distinguish skill level. 
  |  https://dx.doi.org/10.1007/s11517-020-02155-3  |  
------------------------------------------- 
10.1021/acs.est.9b04251  |   The so-called fourth revolution in the water sector will encounter the Big data and Artificial Intelligence (AI) revolution. The current data surplus stemming from all types of devices together with the relentless increase in computer capacity is revolutionizing almost all existing sectors, and the water sector will not be an exception. Combining the power of Big data analytics (including AI) with existing and future urban water infrastructure represents a significant untapped opportunity for the operation, maintenance, and rehabilitation of urban water infrastructure to achieve economic and environmental sustainability. However, such progress may catalyze socio-economic changes and cross sector boundaries (e.g., water service, health, business) as the appearance of new needs and business models will influence the job market. Such progress will impact the academic sector as new forms of research based on large amounts of data will be possible, and new research needs will be requested by the technology industrial sector. Research and development enabling new technological approaches and more effective management strategies are needed to ensure that the emerging framework for the water sector will meet future societal needs. The feature further elucidates the complexities and possibilities associated with such collaborations. 
  |  https://dx.doi.org/10.1021/acs.est.9b04251  |  
------------------------------------------- 
10.1186/s13059-020-1933-7  |   Microbial populations exhibit functional changes in response to different ambient environments. Although whole metagenome sequencing promises enough raw data to study those changes, existing tools are limited in their ability to directly compare microbial metabolic function across samples and studies. We introduce Carnelian, an end-to-end pipeline for metabolic functional profiling uniquely suited to finding functional trends across diverse datasets. Carnelian is able to find shared metabolic pathways, concordant functional dysbioses, and distinguish Enzyme Commission (EC) terms missed by existing methodologies. We demonstrate Carnelian's effectiveness on type 2 diabetes, Crohn's disease, Parkinson's disease, and industrialized and non-industrialized gut microbiome cohorts. 
  |  https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1933-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093762/  |  
------------------------------------------- 
10.1016/j.jaad.2020.04.019  |    Background:  Early detection of melanoma is crucial to improving the detection of thin curable melanomas. Non-invasive, computer-assisted methods have been developed to use at the bedside to aid in diagnoses but have not been compared directly in a clinical setting. 
  Objective:  We conducted a prospective diagnostic accuracy study comparing a dermatologist's clinical examination at the bedside, teledermatology, and non-invasive imaging techniques (FotoFinder®, Melafind®, Verisante Aura<sup>TM</sup>). 
  Methods:  184 patients were recruited prospectively from an outpatient dermatology clinic, with lesions imaged, assessed and excised. Skin specimens were assessed by 2 blinded pathologists, providing the gold standard comparison. 
  Results:  59 lesions from 56 patients had a histopathological diagnosis of melanoma, while 150 lesions from 128 patients were diagnosed as benign. Sensitivities and specificities were, respectively: MelaFind® (82.5%, 52.4%), Verisante Aura<sup>TM</sup> (21.4%, 86.2%), FotoFinder® Moleanalyzer Pro (88.1%, 78.8%). The sensitivity and specificity of the teledermoscopist (84.5%, 82.6%) and local dermatologist (96.6%, 32.2%) were also compared. 
  Limitations:  There are inherent limitations in using pathology as the gold standard to compare sensitivities and specificities. 
  Conclusion:  This study demonstrates that the highest sensitivity and specificity of the instruments was established with the FotoFinder® Moleanalyzer Pro, which could be a valuable tool to assist with, but not replace clinical decision making. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0190-9622(20)30559-4  |  
------------------------------------------- 
10.3390/s20082250  |   In this article, we propose a multi-label convolution neural network (MLCNN)-aided transmit antenna selection (AS) scheme for end-to-end multiple-input multiple-output (MIMO) Internet of Things (IoT) communication systems in correlated channel conditions. In contrast to the conventional single-label multi-class classification ML schemes, we opt for using the concept of multi-label in the proposed MLCNN-aided transmit AS MIMO IoT system, which may greatly reduce the length of training labels in the case of multi-antenna selection. Additionally, applying multi-label concept may significantly improve the prediction accuracy of the trained MLCNN model under correlated large-scale MIMO channel conditions with less training data. The corresponding simulation results verified that the proposed MLCNN-aided AS scheme may be capable of achieving near-optimal capacity performance in real time, and the performance is relatively insensitive to the effects of imperfect CSI. 
  |  http://www.mdpi.com/resolver?pii=s20082250  |  
------------------------------------------- 
10.1016/j.jacr.2019.08.016  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(19)31010-5  |  
------------------------------------------- 
10.1111/cts.12784  |   Recent progress in the Internet of Things and artificial intelligence has made it possible to utilize the vast quantity of personal health records, clinical data, and scientific findings for prognosis, diagnosis, and therapy. These innovative technologies provide new possibilities with the development of medical devices (MDs), whose behaviors can be continuously modified. A novel regulatory framework covering these MDs is now under discussion in Japan. In this review, we introduce the regulatory initiative for MDs and the importance of a paradigm shift from regulation to innovation regarding MDs. 
  |  https://doi.org/10.1111/cts.12784  |  
------------------------------------------- 
10.1109/JBHI.2020.2990529  |   Powered by the technologies that have originated from manufacturing, the fourth revolution of healthcare technologies is happening (Healthcare 4.0). As an example of such revolution, new generation homecare robotic systems (HRS) based on the cyber-physical systems (CPS) with higher speed and more intelligent execution are emerging. In this article, the new visions and features of the CPS-based HRS are proposed. The latest progress in related enabling technologies is reviewed, including artificial intelligence, sensing fundamentals, materials and machines, cloud computing and communication, as well as motion capture and mapping. Finally, the future perspectives of the CPS-based HRS and the technical challenges faced in each technical area are discussed. 
  |  https://dx.doi.org/10.1109/JBHI.2020.2990529  |  
------------------------------------------- 
10.1111/bju.14993  |    |  https://doi.org/10.1111/bju.14993  |  
------------------------------------------- 
10.3389/fnins.2020.00113  |   [This corrects the article DOI: 10.3389/fnins.2018.00276.]. 
  |  https://doi.org/10.3389/fnins.2020.00113  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32153353/  |  
------------------------------------------- 
10.1213/ANE.0000000000004751  |    |  http://dx.doi.org/10.1213/ANE.0000000000004751  |  
------------------------------------------- 
10.1016/j.earlhumdev.2020.105023  |   Science fiction (SF) is ubiquitous and it has also been utilised for the purposes of teaching since it has replaced legend, myth and fable. This is especially the case for Star Trek (ST) which has become an integral part of popular culture, even for those who do not follow SF. In this Best Practice Guideline (BPG) we will engage topics that ordinary readers of EHD might not normally come across, but may well find interesting. We will review the individual doctors in ST from the viewpoint of a medical doctor, and will demonstrate the ways in which the medic in the various series (which spans decades, since 1966) reflects the zeitgeist. A second BPG will provide an assortment of ST cautionary tales which range from nanotechnology, to The Holocaust, to artificial intelligence. SF is famously "extravagant fiction today, cold fact tomorrow" and takes us "where no man has gone before". The imaginings of SF authors (some of whom are scientists and doctors) should be taken seriously for potential detrimental effects/events that may befall the medical profession or the human race, and that might be avoided with the foresight provided by SF. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-3782(20)30181-X  |  
------------------------------------------- 
10.1053/j.gastro.2019.12.035  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5085(20)30081-0  |  
------------------------------------------- 
10.1038/s41598-020-63755-5  |   Attempting to imitate the brain's functionalities, researchers have bridged between neuroscience and artificial intelligence for decades; however, experimental neuroscience has not directly advanced the field of machine learning (ML). Here, using neuronal cultures, we demonstrate that increased training frequency accelerates the neuronal adaptation processes. This mechanism was implemented on artificial neural networks, where a local learning step-size increases for coherent consecutive learning steps, and tested on a simple dataset of handwritten digits, MNIST. Based on our on-line learning results with a few handwriting examples, success rates for brain-inspired algorithms substantially outperform the commonly used ML algorithms. We speculate this emerging bridge from slow brain function to ML will promote ultrafast decision making under limited examples, which is the reality in many aspects of human activity, robotic control, and network optimization. 
  |  http://dx.doi.org/10.1038/s41598-020-63755-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32327697/  |  
------------------------------------------- 
10.1016/j.jelectrocard.2019.12.018  |    Background:  Coronary artery disease (CAD) is a leading cause of death and disability. Conventional non-invasive diagnostic modalities for the detection of stable CAD at rest are subject to significant limitations: low sensitivity, and personal expertise. We aimed to develop a reliable and time-cost efficient screening tool for the detection of coronary ischemia using machine learning. 
  Methods:  We developed a supervised artificial intelligence algorithm combined with a five lead vectorcardiography (VCG) approach (i.e. Cardisiography, CSG) for the diagnosis of CAD. Using vectorcardiography, the excitation process of the heart can be described as a three-dimensional signal. A diagnosis can be received, by first, calculating specific physical parameters from the signal, and subsequently, analyzing them with a machine learning algorithm containing neuronal networks. In this multi-center analysis, the primary evaluated outcome was the accuracy of the CSG Diagnosis System, validated by a five-fold nested cross-validation in comparison to angiographic findings as the gold standard. Individuals with 1, 2, or 3- vessel disease were defined as being affected. 
  Results:  Of the 595 patients, 62·0% (n = 369) had 1, 2 or 3- vessel disease identified by coronary angiography. CSG identified a CAD at rest with a sensitivity of 90·2 ± 4·2% for female patients (male: 97·2 ± 3·1%), specificity of 74·4 ± 9·8% (male: 76·1 ± 8·5%), and overall accuracy of 82·5 ± 6·4% (male: 90·7 ± 3·3%). 
  Conclusion:  These findings demonstrate that supervised artificial intelligence-enabled vectorcardiography can overcome limitations of conventional non-invasive diagnostic modalities for the detection of coronary ischemia at rest and is capable as a highly valid screening tool. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-0736(19)30695-8  |  
------------------------------------------- 
10.1186/s40662-020-00184-5  |    Background:  To analyze the clinical results of an artificial neural network (ANN) that has been processed in order to improve the predictability of intracorneal ring segments (ICRS) implantation in keratoconus. 
  Methods:  This retrospective, comparative, nonrandomized, pilot, clinical study included a cohort of 20 keratoconic eyes implanted with intracorneal ring segments KeraRing (Mediphacos, Belo Horizonte, Brazil) using the ANN (ANN group) and 20 keratoconic eyes implanted with KeraRing using the manufacturer's nomograms (nomogram group). Uncorrected distance visual acuity (UDVA), corrected distance visual acuity (CDVA) (visual acuity is expressed in decimal value and in LogMAR value in brackets), manifest refraction, corneal topography, tomography, aberrometry, pachymetry and volume analysis (Sirius System. CSO, Firenze, Italy) were performed during the preoperative visit; and the two groups, ANN group and nomogram group, did not differ significantly preoperatively in all of the parameters evaluated. These preoperative values were compared with the results obtained at the third-month visit. Mann-Whitney test and Wilcoxon test were used for the statistical analyses. 
  Results:  The spherical equivalent and the keratometric values decreased significantly in both groups. The CDVA improved from 0.60 ± 0.23 (0.22 LogMAR) pre-operatively to 0.73 ± 0.21 (0.14 LogMAR) post-operatively in the ANN group (<i>p</i> &lt; 0.005), and from 0.54 ± 0.19 (0.27 LogMAR) pre-operatively to 0.62 ± 0.19 (0.21 LogMAR) post-operatively in the nomogram group (<i>p</i> &lt; 0.01), with statistically significant difference between the two groups (<i>p</i> &lt; 0.05), being better in the ANN group. Coma-like aberrations decreased significantly in the ANN group, while in the nomogram group they did not change significantly, but no statistically significant difference was found between the two groups. 
  Conclusions:  ANN to guide ICRS provides an increase in the visual acuity, reduction in the spherical equivalent and improvement in the optical quality of keratoconus patients. ANN gives better results when compared with the manufacturer's nomograms in terms of better corrected vision and reduction of the coma-like aberrations. The constant inclusion of new cases will make the predictability of ANN increasingly better as the software finetunes its learning. 
  |  https://eandv.biomedcentral.com/articles/10.1186/s40662-020-00184-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32292796/  |  
------------------------------------------- 
10.1007/s00330-019-06593-y  |    Objectives:  Patients with multiple sclerosis (MS) regularly undergo MRI for assessment of disease burden. However, interpretation may be time consuming and prone to intra- and interobserver variability. Here, we evaluate the potential of artificial neural networks (ANN) for automated volumetric assessment of MS disease burden and activity on MRI. 
  Methods:  A single-institutional dataset with 334 MS patients (334 MRI exams) was used to develop and train an ANN for automated identification and volumetric segmentation of T2/FLAIR-hyperintense and contrast-enhancing (CE) lesions. Independent testing was performed in a single-institutional longitudinal dataset with 82 patients (266 MRI exams). We evaluated lesion detection performance (F1 scores), lesion segmentation agreement (DICE coefficients), and lesion volume agreement (concordance correlation coefficients [CCC]). Independent evaluation was performed on the public ISBI-2015 challenge dataset. 
  Results:  The F1 score was maximized in the training set at a detection threshold of 7 mm<sup>3</sup> for T2/FLAIR lesions and 14 mm<sup>3</sup> for CE lesions. In the training set, mean F1 scores were 0.867 for T2/FLAIR lesions and 0.636 for CE lesions, as compared to 0.878 for T2/FLAIR lesions and 0.715 for CE lesions in the test set. Using these thresholds, the ANN yielded mean DICE coefficients of 0.834 and 0.878 for segmentation of T2/FLAIR and CE lesions in the training set (fivefold cross-validation). Corresponding DICE coefficients in the test set were 0.846 for T2/FLAIR lesions and 0.908 for CE lesions, and the CCC was ≥ 0.960 in each dataset. 
  Conclusions:  Our results highlight the capability of ANN for quantitative state-of-the-art assessment of volumetric lesion load on MRI and potentially enable a more accurate assessment of disease burden in patients with MS. 
  Key points:  • Artificial neural networks (ANN) can accurately detect and segment both T2/FLAIR and contrast-enhancing MS lesions in MRI data. • Performance of the ANN was consistent in a clinically derived dataset, with patients presenting all possible disease stages in MRI scans acquired from standard clinical routine rather than with high-quality research sequences. • Computer-aided evaluation of MS with ANN could streamline both clinical and research procedures in the volumetric assessment of MS disease burden as well as in lesion detection. 
  |  https://dx.doi.org/10.1007/s00330-019-06593-y  |  
------------------------------------------- 
10.1038/s41598-020-62945-5  |   In today's data-driven world, the ability to process large data volumes is crucial. Key tasks, such as pattern recognition and image classification, are well suited for artificial neural networks (ANNs) inspired by the brain. Neuromorphic computing approaches aimed towards physical realizations of ANNs have been traditionally supported by micro-electronic platforms, but recently, photonic techniques for neuronal emulation have emerged given their unique properties (e.g. ultrafast operation, large bandwidths, low cross-talk). Yet, hardware-friendly systems of photonic spiking neurons able to perform processing tasks at high speeds and with continuous operation remain elusive. This work provides a first experimental report of Vertical-Cavity Surface-Emitting Laser-based spiking neurons demonstrating different functional processing tasks, including coincidence detection and pattern recognition, at ultrafast rates. Furthermore, our approach relies on simple hardware implementations using off-the-shelf components. These results therefore hold exciting prospects for novel, compact and high-speed neuromorphic photonic platforms for future computing and Artificial Intelligence systems. 
  |  http://dx.doi.org/10.1038/s41598-020-62945-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32269249/  |  
------------------------------------------- 
10.3390/s20061669  |   Tracking the rheological properties of the drilling fluid is a key factor for the success of the drilling operation. The main objective of this paper is to relate the most frequent mud measurements (every 15 to 20 min) as mud weight (MWT) and Marsh funnel viscosity (MFV) to the less frequent mud rheological measurements (twice a day) as plastic viscosity (PV), yield point (YP), behavior index (n), and apparent viscosity (AV) for fully automating the process of retrieving rheological properties. The adaptive neuro-fuzzy inference system (ANFIS) was used to develop new models to determine the mud rheological properties using real field measurements of 741 data points. The data were collected from 99 different wells during drilling operations of 12 ¼ inches section. The ANFIS clustering technique was optimized by using training to a testing ratio of 80% to 20% as 591 data points for training and 150 points, cluster radius value of 0.1, and 200 epochs. The results of the prediction models showed a correlation coefficient (R) that exceeded 0.9 between the actual and predicted values with an average absolute percentage error (AAPE) below 5.7% for the training and testing data sets. ANFIS models will help to track in real-time the rheological properties for invert emulsion mud that allows better control for the drilling operation problems. 
  |  http://www.mdpi.com/resolver?pii=s20061669  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32192144/  |  
------------------------------------------- 
10.1007/s00345-020-03080-8  |    Purpose:  The delivery of precision medicine is a primary objective for both clinical and translational investigators. Patients with newly diagnosed prostate cancer (PCa) face the challenge of deciding among multiple initial treatment modalities. The purpose of this study is to utilize artificial neural network (ANN) modeling to predict survival outcomes according to initial treatment modality and to develop an online decision-making support system. 
  Methods:  Data were collected retrospectively from 7267 patients diagnosed with PCa between January 1988 and December 2017. The analyses included 19 pretreatment clinicopathological covariates. Multilayer perceptron (MLP), MLP for N-year survival prediction (MLP-N), and long short-term memory (LSTM) ANN models were used to analyze progression to castration-resistant PCa (CRPC)-free survival, cancer-specific survival (CSS), and overall survival (OS), according to initial treatment modality. The performances of the ANN and the Cox-proportional hazards regression models were compared using Harrell's C-index. 
  Results:  The ANN models provided higher predictive power for 5- and 10-year progression to CRPC-free survival, CSS, and OS compared to the Cox-proportional hazards regression model. The LSTM model achieved the highest predictive power, followed by the MLP-N, and MLP models. We developed an online decision-making support system based on the LSTM model to provide individualized survival outcomes at 5 and 10 years, according to the initial treatment strategy. 
  Conclusion:  The LSTM ANN model may provide individualized survival outcomes of PCa according to initial treatment strategy. Our online decision-making support system can be utilized by patients and health-care providers to determine the optimal initial treatment modality and to guide survival predictions. 
  |  https://dx.doi.org/10.1007/s00345-020-03080-8  |  
------------------------------------------- 
10.3390/diagnostics10050261  |   The use of deep-learning-based artificial intelligence (AI) is emerging in ophthalmology, with AI-mediated differential diagnosis of neovascular age-related macular degeneration (AMD) and dry AMD a promising methodology for precise treatment strategies and prognosis. Here, we developed deep learning algorithms and predicted diseases using 399 images of fundus. Based on feature extraction and classification with fully connected layers, we applied the Visual Geometry Group with 16 layers (VGG16) model of convolutional neural networks to classify new images. Image-data augmentation in our model was performed using Keras ImageDataGenerator, and the leave-one-out procedure was used for model cross-validation. The prediction and validation results obtained using the AI AMD diagnosis model showed relevant performance and suitability as well as better diagnostic accuracy than manual review by first-year residents. These results suggest the efficacy of this tool for early differential diagnosis of AMD in situations involving shortages of ophthalmology specialists and other medical devices. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10050261  |  
------------------------------------------- 
10.1080/21688370.2020.1712177  |   The 1<sup>st</sup> International Symposium on Mucosal Health and Aquaculture (MHA2019) was held on 11-13 September 2019 in Oslo, Norway. This was the first platform of its kind that gathered people from academia, R&amp;D institutes, and industry to discuss the state-of-the-art and future directions of mucosal health research in aquaculture. The symposium was divided into four scientific sessions: Session 1, Mucosal structures and functions; Session 2, Mucosal health and nutrition; Session 3, Mucosal health and microbiome; and Session 4, Mucosal health and the changing environment. A plenary talk from a prominent authority in the field opened each session. The papers presented at MHA2019 exemplified the dynamic evolution of the field of mucosal health in aquaculture - from an area solely explored from an immunological aspect about 20 years ago, to the multidisciplinary field it is today. Moreover, papers presented highlighted the complementary application of both classical (e.g., histology) and modern platforms (e.g., <i>omics</i>, artificial intelligence) to characterize mucosal health. The interactive discussion during the meeting underscored the importance of mucosal health research in modern aquaculture and collectively emphasized the role of both fundamental and applied approaches in advancing this timely and highly relevant field. The symposium was organized by Nofima, The Norwegian Institute of Food, Fisheries, and Aquaculture Research, with support from the Research Council of Norway. 
  |  http://www.tandfonline.com/doi/full/10.1080/21688370.2020.1712177  |  
------------------------------------------- 
10.1148/radiol.2020192536  |   In this article, the authors propose an ethical framework for using and sharing clinical data for the development of artificial intelligence (AI) applications. The philosophical premise is as follows: when clinical data are used to provide care, the primary purpose for acquiring the data is fulfilled. At that point, clinical data should be treated as a form of public good, to be used for the benefit of future patients. In their 2013 article, Faden et al argued that all who participate in the health care system, including patients, have a moral obligation to contribute to improving that system. The authors extend that framework to questions surrounding the secondary use of clinical data for AI applications. Specifically, the authors propose that all individuals and entities with access to clinical data become data stewards, with fiduciary (or trust) responsibilities to patients to carefully safeguard patient privacy, and to the public to ensure that the data are made widely available for the development of knowledge and tools to benefit future patients. According to this framework, the authors maintain that it is unethical for providers to "sell" clinical data to other parties by granting access to clinical data, especially under exclusive arrangements, in exchange for monetary or in-kind payments that exceed costs. The authors also propose that patient consent is not required before the data are used for secondary purposes when obtaining such consent is prohibitively costly or burdensome, as long as mechanisms are in place to ensure that ethical standards are strictly followed. Rather than debate whether patients or provider organizations "own" the data, the authors propose that clinical data are not owned at all in the traditional sense, but rather that all who interact with or control the data have an obligation to ensure that the data are used for the benefit of future patients and society. 
  |  http://pubs.rsna.org/doi/10.1148/radiol.2020192536?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1186/s13690-020-00409-y  |    Background:  This article describes the design of an intervention study that focuses on whether and to what degree culturally competent social robots can improve health and well-being related outcomes among older adults residing long-term care homes. The trial forms the final stage of the international, multidisciplinary CARESSES project aimed at designing, developing and evaluating culturally competent robots that can assist older people according to the culture of the individual they are supporting. The importance of cultural competence has been demonstrated in previous nursing literature to be key towards improving health outcomes among patients. 
  Method:  This study employed a mixed-method, single-blind, parallel-group controlled before-and-after experimental trial design that took place in England and Japan. It aimed to recruit 45 residents of long-term care homes aged ≥65 years, possess sufficient cognitive and physical health and who self-identify with the English, Indian or Japanese culture (<i>n</i> = 15 each). Participants were allocated to either the experimental group, control group 1 or control group 2 (all n = 15). Those allocated to the experimental group or control group 1 received a Pepper robot programmed with the CARESSES culturally competent artificial intelligence (experimental group) or a limited version of this software (control group 1) for 18 h across 2 weeks. Participants in control group 2 did not receive a robot and continued to receive care as usual. Participants could also nominate their informal carer(s) to participate. Quantitative data collection occurred at baseline, after 1 week of use, and after 2 weeks of use with the latter time-point also including qualitative semi-structured interviews that explored their experience and perceptions further. Quantitative outcomes of interest included perceptions of robotic cultural competence, health-related quality of life, loneliness, user satisfaction, attitudes towards robots and caregiver burden. 
  Discussion:  This trial adds to the current preliminary and limited pool of evidence regarding the benefits of socially assistive robots for older adults which to date indicates considerable potential for improving outcomes. It is the first to assess whether and to what extent cultural competence carries importance in generating improvements to well-being. 
  Trial registration:  Name of the registry: ClinicalTrials.govTrial registration number: <a href="http://clinicaltrials.gov/show/NCT03756194" title="See in ClinicalTrials.gov">NCT03756194</a>.Date of registration: 28 November 2018. URL of trial registry record. 
  |  https://archpublichealth.biomedcentral.com/articles/10.1186/s13690-020-00409-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32206312/  |  
------------------------------------------- 
10.1016/j.jpsychires.2020.01.013  |   Shuganjieyu capsule (Shugan) is a combined extract of Hypericum perforatum (HP) and Eleutherococcus senticosus (ES). Both HP and ES have been proven effective in the treatment of depression and impaired cognition. However, for mild to moderate depression (MMD), the treatment effect and underlying mechanism by combining both HP and ES are largely unknown. Here, we aim to evaluate the therapeutic effects on impaired cognition using Shugan, a combined medication of HP and ES. Resting-state magnetic resonance imaging (MRI) data and cognitive assessment have been collected from 54 healthy controls and 55 MMD patients that have been undergoing 8-week Shugan-treatment. The functional connectivity (FC) and brain region volume changes of the basal ganglia seeded circuit have been measured, and their relation with the cognitive assessment score was calculated. After that, a literature-based pathway analysis has been conducted to explore the biological relations among Shugan, brain regions, and depression. Compared to healthy controls, MMD patients demonstrated a significantly higher FC (P= 0.0025) between right ventral caudate (vCa) and left orbitofrontal cortex (OFC), which was decreased after the treatment (P &lt; 0.001). A volume of the right caudate, which is increased in MMD, has also been reduced by Shugan treatment (P= 0.017). Importantly, the cognitive scores were strongly correlated with both Shugan treatment and the FC between vCa and OFC (r= 0.321, P= 0.02). Besides, we identified multiple signaling pathways, through which Shugan might improve the cognition of MMD patients. Our results support the therapeutic effects of Shugan on cognition in MMD, which may be realized partly through the regulation within two brain regions, vCa and OFC. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-3956(19)31176-8  |  
------------------------------------------- 
10.3390/jcm9030871  |   Automatic chest anatomy segmentation plays a key role in computer-aided disease diagnosis, such as for cardiomegaly, pleural effusion, emphysema, and pneumothorax. Among these diseases, cardiomegaly is considered a perilous disease, involving a high risk of sudden cardiac death. It can be diagnosed early by an expert medical practitioner using a chest X-Ray (CXR) analysis. The cardiothoracic ratio (CTR) and transverse cardiac diameter (TCD) are the clinical criteria used to estimate the heart size for diagnosing cardiomegaly. Manual estimation of CTR and other diseases is a time-consuming process and requires significant work by the medical expert. Cardiomegaly and related diseases can be automatically estimated by accurate anatomical semantic segmentation of CXRs using artificial intelligence. Automatic segmentation of the lungs and heart from the CXRs is considered an intensive task owing to inferior quality images and intensity variations using nonideal imaging conditions. Although there are a few deep learning-based techniques for chest anatomy segmentation, most of them only consider single class lung segmentation with deep complex architectures that require a lot of trainable parameters. To address these issues, this study presents two multiclass residual mesh-based CXR segmentation networks, X-RayNet-1 and X-RayNet-2, which are specifically designed to provide fine segmentation performance with a few trainable parameters compared to conventional deep learning schemes. The proposed methods utilize semantic segmentation to support the diagnostic procedure of related diseases. To evaluate X-RayNet-1 and X-RayNet-2, experiments were performed with a publicly available Japanese Society of Radiological Technology (JSRT) dataset for multiclass segmentation of the lungs, heart, and clavicle bones; two other publicly available datasets, Montgomery County (MC) and Shenzhen X-Ray sets (SC), were evaluated for lung segmentation. The experimental results showed that X-RayNet-1 achieved fine performance for all datasets and X-RayNet-2 achieved competitive performance with a 75% parameter reduction. 
  |  http://www.mdpi.com/resolver?pii=jcm9030871  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32209991/  |  
------------------------------------------- 
10.1111/ane.13216  |    Objective:  People with epilepsy are at increased risk for mental health comorbidities. Machine-learning methods based on spoken language can detect suicidality in adults. This study's purpose was to use spoken words to create machine-learning classifiers that identify current or lifetime history of comorbid psychiatric conditions in teenagers and young adults with epilepsy. 
  Materials and methods:  Eligible participants were &gt;12 years old with epilepsy. All participants were interviewed using the Mini International Neuropsychiatric Interview (MINI) or the MINI Kid Tracking and asked five open-ended conversational questions. N-grams and Linguistic Inquiry and Word Count (LIWC) word categories were used to construct machine learning classification models from language harvested from interviews. Data were analyzed for four individual MINI identified disorders and for three mutually exclusive groups: participants with no psychiatric disorders, participants with non-suicidal psychiatric disorders, and participants with any degree of suicidality. Performance was measured using areas under the receiver operating characteristic curve (AROCs). 
  Results:  Classifiers were constructed from 227 interviews with 122 participants (7.5 ± 3.1 minutes and 454 ± 299 words). AROCs for models differentiating the non-overlapping groups and individual disorders ranged 57%-78% (many with P &lt; .02). 
  Discussion and conclusion:  Machine-learning classifiers of spoken language can reliably identify current or lifetime history of suicidality and depression in people with epilepsy. Data suggest identification of anxiety and bipolar disorders may be achieved with larger data sets. Machine-learning analysis of spoken language can be promising as a useful screening alternative when traditional approaches are unwieldy (eg, telephone calls, primary care offices, school health clinics). 
  |  https://doi.org/10.1111/ane.13216  |  
------------------------------------------- 
PMID:31797645  |   Translational bioinformatics (TBI) is focused on the integration of biomedical data science and informatics. This combination is extremely powerful for scientific discovery as well as translation into clinical practice. Several topics where TBI research is at the leading edge are 1) the use of large-scale biobanks linked to electronic health records, 2) pharmacogenomics, and 3) artificial intelligence and machine learning. This perspective discusses these three topics and points to the important elements for driving precision medicine into the future. 
  |  https://doi.org/10.1142/9789811215636_0067  |  
------------------------------------------- 
10.9778/cmajo.20190151  |    Background:  As artificial intelligence (AI) approaches in research increase and AI becomes more integrated into medicine, there is a need to understand perspectives from members of the Canadian public and medical community. The aim of this project was to investigate current perspectives on ethical issues surrounding AI in health care. 
  Methods:  In this qualitative study, adult patients with meningioma and their caregivers were recruited consecutively (August 2018-February 2019) from a neurosurgical clinic in Toronto. Health care providers caring for these patients were recruited through snowball sampling. Based on a nonsystematic literature search, we constructed 3 vignettes that sought participants' views on hypothetical issues surrounding potential AI applications in health care. The vignettes were presented to participants in interviews, which lasted 15-45 minutes. Responses were transcribed and coded for concepts, frequency of response types and larger concepts emerging from the interview. 
  Results:  We interviewed 30 participants: 18 patients, 7 caregivers and 5 health care providers. For each question, a variable number of responses were recorded. The majority of participants endorsed nonconsented use of health data but advocated for disclosure and transparency. Few patients and caregivers felt that allocation of health resources should be done via computerized output, and a majority stated that it was inappropriate to delegate such decisions to a computer. Almost all participants felt that selling health data should be prohibited, and a minority stated that less privacy is acceptable for the goal of improving health. Certain caveats were identified, including the desire for deidentification of data and use within trusted institutions. 
  Interpretation:  In this preliminary study, patients and caregivers reported a mixture of hopefulness and concern around the use of AI in health care research, whereas providers were generally more skeptical. These findings provide a point of departure for institutions adopting health AI solutions to consider the ethical implications of this work by understanding stakeholders' perspectives. 
  |  http://cmajopen.ca/cgi/pmidlookup?view=long&pmid=32071143  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32071143/  |  
------------------------------------------- 
10.3390/ijerph17062078  |   Triple-negative breast cancer (TNBC) cells are deficient in estrogen, progesterone and ERBB2 receptor expression, presenting a particularly challenging therapeutic target due to their highly invasive nature and relatively low response to therapeutics. There is an absence of specific treatment strategies for this tumor subgroup, and hence TNBC is managed with conventional therapeutics, often leading to systemic relapse. In terms of histology and transcription profile these cancers have similarities to BRCA-1-linked breast cancers, and it is hypothesized that BRCA1 pathway is non-functional in this type of breast cancer. In this review article, we discuss the different receptors expressed by TNBC as well as the diversity of different signaling pathways targeted by TNBC therapeutics, for example, Notch, Hedgehog, Wnt/b-Catenin as well as TGF-beta signaling pathways. Additionally, many epidermal growth factor receptor (EGFR), poly (ADP-ribose) polymerase (PARP) and mammalian target of rapamycin (mTOR) inhibitors effectively inhibit the TNBCs, but they face challenges of either resistance to drugs or relapse. The resistance of TNBC to conventional therapeutic agents has helped in the advancement of advanced TNBC therapeutic approaches including hyperthermia, photodynamic therapy, as well as nanomedicine-based targeted therapeutics of drugs, miRNA, siRNA, and aptamers, which will also be discussed. Artificial intelligence is another tool that is presented to enhance the diagnosis of TNBC. 
  |  http://www.mdpi.com/resolver?pii=ijerph17062078  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245065/  |  
------------------------------------------- 
10.1159/000506014  |    |  https://www.karger.com?DOI=10.1159/000506014  |  
------------------------------------------- 
10.1016/j.copbio.2020.01.010  |   Human population growth and accelerated climate change necessitate agricultural improvements using designer crop ideotypes (idealized plants that can grow in niche environments). Diverse and highly skilled research groups must integrate efforts to bridge the gaps needed to achieve international goals toward sustainable agriculture. Given the scale of global agricultural needs and the breadth of multiple types of omics data needed to optimize these efforts, explainable artificial intelligence (AI with a decipherable decision making process that provides a meaningful explanation to humans) and exascale computing (computers that can perform 10<sup>18</sup> floating-point operations per second, or exaflops) are crucial. Accurate phenotyping and daily-resolution climatype associations are equally important for refining ideotype production to specific environments at various levels of granularity. We review advances toward tackling technological hurdles to solve multiple United Nations Sustainable Development Goals and discuss a vision to overcome gaps between research and policy. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0958-1669(20)30010-0  |  
------------------------------------------- 
10.1007/s40123-020-00252-y  |   Corneal ectasia is a complication of refractive surgery, and keratoconus is a contraindication to this type of procedure. Surface ablation may be an option for selected cases of mild keratoconus, with patient education being fundamental to this treatment as well as a complete evaluation of the cornea and optical properties of the patient. Here we report the clinical outcome of a patient 15 years after advanced surface ablation in a case of mild (fruste) keratoconus. 
  |  https://dx.doi.org/10.1007/s40123-020-00252-y  |  
------------------------------------------- 
10.1186/s12911-020-1091-6  |    Background:  Trials often struggle to achieve their target sample size with only half doing so. Some researchers have turned to Electronic Health Records (EHRs), seeking a more efficient way of recruitment. The Scottish Health Research Register (SHARE) obtained patients' consent for their EHRs to be used as a searching base from which researchers can find potential participants. However, due to the fact that EHR data is not complete, sufficient or accurate, a database search strategy may not generate the best case-finding result. The current study aims to evaluate the performance of a case-based reasoning method in identifying participants for population-based clinical studies recruiting through SHARE, and assess the difference between its resultant cohort and the original one deriving from searching EHRs. 
  Methods:  A case-based reasoning framework was applied to 119 participants in nine projects using two-fold cross-validation, with records from a further 86,292 individuals used for testing. A prediction score for study participation was derived from the diagnosis, procedure, pharmaceutical prescription, and laboratory test results attributes of each participant. Evaluation was conducted by calculating Area Under the ROC Curve and information retrieval metrics for the ranking list of the test set by prediction score. We compared the most likely participants as identified by searching a database to those ranked highest by our model. 
  Results:  The average ROCAUC for nine projects was 81% indicating strong predictive ability for these data. However, the derived ranking lists showed lower predictive performance, with only 21% of the persons ranked within top 50 positions being the same as identified by searching databases. 
  Conclusions:  Case-based reasoning is may be more effective than a database search strategy for participant identification for clinical studies using population EHRs. The lower performance of ranking lists derived from case-based reasoning means that patients identified as highly suitable for study participation may still not be recruited. This suggests that further study is needed into improvements in the collection and curation of population EHRs, such as use of free text data to aid reliable identification of people more likely to be recruited to clinical trials. 
  |  https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1091-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32306964/  |  
------------------------------------------- 
10.1016/j.eclinm.2020.100281  |    Background:  Suicide is a leading cause of death worldwide and results in a large number of person years of life lost. There is an opportunity to evaluate whether administrative health care system data and machine learning can quantify suicide risk in a clinical setting. 
  Methods:  The objective was to compare the performance of prediction models that quantify the risk of death by suicide within 90 days of an ED visit for parasuicide with predictors available in administrative health care system data.The modeling dataset was assembled from 5 administrative health care data systems. The data systems contained nearly all of the physician visits, ambulatory care visits, inpatient hospitalizations, and community pharmacy dispenses, of nearly the entire 4.07 million persons in Alberta, Canada. 101 predictors were selected, and these were assembled for each of the 8 quarters (2 years) prior to the quarter of death, resulting in 808 predictors in total for each person. Prediction model performance was validated with 10-fold cross-validation. 
  Findings:  The optimal gradient boosted trees prediction model achieved promising discrimination (AUC: 0.88) and calibration that could lead to clinical applications. The 5 most important predictors in the optimal gradient boosted trees model each came from a different administrative health care data system. 
  Interpretation:  The combination of predictors from multiple administrative data systems and the combination of personal and ecologic predictors resulted in promising prediction performance. Further research is needed to develop prediction models optimized for implementation in clinical settings. 
  Funding:  There was no funding for this study. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2589-5370(20)30025-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32300738/  |  
------------------------------------------- 
10.2214/AJR.19.22332  |   <b>OBJECTIVE.</b> The purpose of this study was to perform quantitative and qualitative evaluation of a deep learning image reconstruction (DLIR) algorithm in contrast-enhanced oncologic CT of the abdomen. <b>MATERIALS AND METHODS.</b> Retrospective review (April-May 2019) of the cases of adults undergoing oncologic staging with portal venous phase abdominal CT was conducted for evaluation of standard 30% adaptive statistical iterative reconstruction V (30% ASIR-V) reconstruction compared with DLIR at low, medium, and high strengths. Attenuation and noise measurements were performed. Two radiologists, blinded to examination details, scored six categories while comparing reconstructions for overall image quality, lesion diagnostic confidence, artifacts, image noise and texture, lesion conspicuity, and resolution. <b>RESULTS.</b> DLIR had a better contrast-to-noise ratio than 30% ASIR-V did; high-strength DLIR performed the best. High-strength DLIR was associated with 47% reduction in noise, resulting in a 92-94% increase in contrast-to-noise ratio compared with that of 30% ASIR-V. For overall image quality and image noise and texture, DLIR scored significantly higher than 30% ASIR-V with significantly higher scores as DLIR strength increased. A total of 193 lesions were identified. The lesion diagnostic confidence, conspicuity, and artifact scores were significantly higher for all DLIR levels than for 30% ASIR-V. There was no significant difference in perceived resolution between the reconstruction methods. <b>CONCLUSION.</b> Compared with 30% ASIR-V, DLIR improved CT evaluation of the abdomen in the portal venous phase. DLIR strength should be chosen to balance the degree of desired denoising for a clinical task relative to mild blurring, which increases with progressively higher DLIR strengths. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.22332  |  
------------------------------------------- 
10.3390/s20030649  |   The increasing interest in the effects of emotion on cognitive, social, and neural processes creates a constant need for efficient and reliable techniques for emotion elicitation. Emotions are important in many areas, especially in advertising design and video production. The impact of emotions on the audience plays an important role. This paper analyzes the physical elements in a two-dimensional emotion map by extracting the physical elements of a video (color, light intensity, sound, etc.). We used k-nearest neighbors (K-NN), support vector machine (SVM), and multilayer perceptron (MLP) classifiers in the machine learning method to accurately predict the four dimensions that express emotions, as well as summarize the relationship between the two-dimensional emotion space and physical elements when designing and producing video. 
  |  http://www.mdpi.com/resolver?pii=s20030649  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31991587/  |  
------------------------------------------- 
10.1080/17434440.2020.1755257  |   <b>Introduction</b>: Prostate magnetic resonance imaging (MRI) is commonly used for localized disease mainly to detect intraprostatic lesions and to guide biopsies. Despite its documented success in clinical practice, limitations still exist for prostate MRI. In this review, we discuss common clinical uses of prostate MRI, its limitations, and potential solutions for those limitations.<b>Areas covered</b>: Current uses of prostate MRI and challenges discussed. Literature search in PubMed was completed using the keywords "prostate MRI, prostate cancer."<b>Expert opinion</b>: Prostate MRI is a useful method for localization, biopsy, and treatment guidance of prostate cancer. Certain limitations of prostate MRI such as false negatives due to spatial resolution and relatively low repeatability between different radiologists can potentially be solved by investing more on education training and artificial intelligence technology. 
  |  http://www.tandfonline.com/doi/full/10.1080/17434440.2020.1755257  |  
------------------------------------------- 
10.1063/1.5132945  |   The social force model (SFM) can be applied to characterize pedestrian dynamics in normal scenarios. However, its model of interactions among pedestrians deviates from actual scenarios to some extent. Thus, we propose an improved SFM where pedestrians consider avoiding potential conflicts in advance during the walking process. Meanwhile, the response range of potential conflicts is related to the response time and relative velocity vector. Simulation results demonstrate that the conflict avoidance force plays an important role in guiding pedestrian dynamics. Conflict avoidance makes pedestrian trajectories smoother and more realistic. Moreover, for high pedestrian density (without congestion), moderate values of response time may exist, resulting in the minimum evacuation efficiency. We hope to provide some insights into how to better model interactions among pedestrians during normal evacuation. 
  |  https://dx.doi.org/10.1063/1.5132945  |  
------------------------------------------- 
10.1016/j.biopsych.2020.02.016  |   The neuroimaging community has witnessed a paradigm shift in biomarker discovery from using traditional univariate brain mapping approaches to multivariate predictive models, allowing the field to move toward a translational neuroscience era. Regression-based multivariate models (hereafter "predictive modeling") provide a powerful and widely used approach to predict human behavior with neuroimaging features. These studies maintain a focus on decoding individual differences in a continuously behavioral phenotype from neuroimaging data, opening up an exciting opportunity to describe the human brain at the single-subject level. In this survey, we provide an overview of recent studies that utilize machine learning approaches to identify neuroimaging predictors over the past decade. We first review regression-based approaches and highlight connectome-based predictive modeling, which has grown in popularity in recent years. Next, we systematically describe recent representative studies using these tools in the context of cognitive function, symptom severity, personality traits, and emotion processing. Finally, we highlight a few challenges related to combining multimodal data, longitudinal prediction, external validations, and the employment of deep learning methods that have emerged from our review of the existing literature, as well as present some promising and challenging future directions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0006-3223(20)30111-6  |  
------------------------------------------- 
10.1093/gigascience/giaa011  |    Background:  Color vision is the ability to detect, distinguish, and analyze the wavelength distributions of light independent of the total intensity. It mediates the interaction between an organism and its environment from multiple important aspects. However, the physicochemical basis of color coding has not been explored completely, and how color perception is integrated with other sensory input, typically odor, is unclear. 
  Results:  Here, we developed an artificial intelligence platform to train algorithms for distinguishing color and odor based on the large-scale physicochemical features of 1,267 and 598 structurally diverse molecules, respectively. The predictive accuracies achieved using the random forest and deep belief network for the prediction of color were 100% and 95.23% ± 0.40% (mean ± SD), respectively. The predictive accuracies achieved using the random forest and deep belief network for the prediction of odor were 93.40% ± 0.31% and 94.75% ± 0.44% (mean ± SD), respectively. Twenty-four physicochemical features were sufficient for the accurate prediction of color, while 39 physicochemical features were sufficient for the accurate prediction of odor. A positive correlation between the color-coding and odor-coding properties of the molecules was predicted. A group of descriptors was found to interlink prominently in color and odor perceptions. 
  Conclusions:  Our random forest model and deep belief network accurately predicted the colors and odors of structurally diverse molecules. These findings extend our understanding of the molecular and structural basis of color vision and reveal the interrelationship between color and odor perceptions in nature. 
  |  https://academic.oup.com/gigascience/article-lookup/doi/10.1093/gigascience/giaa011  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32101298/  |  
------------------------------------------- 
10.3389/fnhum.2020.00100  |   As a full-blown research topic, numerical cognition is investigated by a variety of disciplines including cognitive science, developmental and educational psychology, linguistics, anthropology and, more recently, biology and neuroscience. However, despite the great progress achieved by such a broad and diversified scientific inquiry, we are still lacking a comprehensive theory that could explain how numerical concepts are learned by the human brain. In this perspective, I argue that computer simulation should have a primary role in filling this gap because it allows identifying the finer-grained computational mechanisms underlying complex behavior and cognition. Modeling efforts will be most effective if carried out at cross-disciplinary intersections, as attested by the recent success in simulating human cognition using techniques developed in the fields of artificial intelligence and machine learning. In this respect, deep learning models have provided valuable insights into our most basic quantification abilities, showing how numerosity perception could emerge in multi-layered neural networks that learn the statistical structure of their visual environment. Nevertheless, this modeling approach has not yet scaled to more sophisticated cognitive skills that are foundational to higher-level mathematical thinking, such as those involving the use of symbolic numbers and arithmetic principles. I will discuss promising directions to push deep learning into this uncharted territory. If successful, such endeavor would allow simulating the acquisition of numerical concepts in its full complexity, guiding empirical investigation on the richest soil and possibly offering far-reaching implications for educational practice. 
  |  https://doi.org/10.3389/fnhum.2020.00100  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32265678/  |  
------------------------------------------- 
10.1016/j.jid.2019.12.029  |   Deep learning is a branch of artificial intelligence that uses computational networks inspired by the human brain to extract patterns from raw data. Development and application of deep learning methods for image analysis, including classification, segmentation, and restoration, have accelerated in the last decade. These tools have been progressively incorporated into several research fields, opening new avenues in the analysis of biomedical imaging. Recently, the application of deep learning to dermatological images has shown great potential. Deep learning algorithms have shown performance comparable with humans in classifying skin lesion images into different skin cancer categories. The potential relevance of deep learning to the clinical realm created the need for researchers in disciplines other than computer science to understand its fundamentals. In this paper, we introduce the basics of a deep learning architecture for image classification, the convolutional neural network, in a manner accessible to nonexperts. We explain its fundamental operation, the convolution, and describe the metrics for the evaluation of its performance. These concepts are important to interpret and evaluate scientific publications involving these tools. We also present examples of recent applications for dermatology. We further discuss the capabilities and limitations of these artificial intelligence-based methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-202X(20)30063-4  |  
------------------------------------------- 
10.1513/AnnalsATS.201908-613OC  |   <b>Rationale:</b> Interpretation of spirometry is influenced by inherent limitations and by the normal or predicted reference values used. For example, traditional spirometric parameters such as "distal" airflows do not provide sufficient differentiating capacity, especially for mixed patterns or small airway disease.<b>Objectives:</b> We assessed the utility of an alternative spirometric parameter (area under the expiratory flow-volume curve [AEX]) in differentiating between normal, obstruction, restriction, and mixed patterns, as well as in severity stratification of traditional functional impairments.<b>Methods:</b> We analyzed 15,308 spirometry tests in subjects who had same-day lung volume assessments in a pulmonary function laboratory. Using Global Lung Initiative predicted values and standard criteria for pulmonary function impairment, we assessed the diagnostic performance of AEX in best-split partition and artificial neural network models.<b>Results:</b> The average square root AEX values were 3.32, 1.81, 2.30, and 1.64 L⋅s<sup>-0.5</sup> in normal, obstruction, restriction, and mixed patterns, respectively. As such, in combination with traditional spirometric measurements, the square root of AEX differentiated well between normal, obstruction, restriction, and mixed defects. Using forced expiratory volume in 1 second (FEV<sub>1</sub>), forced vital capacity (FVC), and FEV<sub>1</sub>/FVC <i>z-</i>scores plus the square root of AEX in a machine learning algorithm, diagnostic categorization of ventilatory impairments was accomplished with very low rates of misclassification (&lt;9%). Especially for mixed ventilatory patterns, the neural network model performed best in improving the rates of diagnostic misclassification.<b>Conclusions:</b> Using a novel approach to lung function assessment in combination with traditional spirometric measurements, AEX differentiates well between normal, obstruction, restriction and mixed impairments, potentially obviating the need for more complex lung volume-based determinations. 
  |  http://www.atsjournals.org/doi/full/10.1513/AnnalsATS.201908-613OC?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.diii.2020.03.006  |    Purpose:  The second edition of the artificial intelligence (AI) data challenge was organized by the French Society of Radiology with the aim to: (i), work on relevant public health issues; (ii), build large, multicentre, high quality databases; and (iii), include three-dimensional (3D) information and prognostic questions. 
  Materials and methods:  Relevant clinical questions were proposed by French subspecialty colleges of radiology. Their feasibility was assessed by experts in the field of AI. A dedicated platform was set up for inclusion centers to safely upload their anonymized examinations in compliance with general data protection regulation. The quality of the database was checked by experts weekly with annotations performed by radiologists. Multidisciplinary teams competed between September 11<sup>th</sup> and October 13<sup>th</sup> 2019. 
  Results:  Three questions were selected using different imaging and evaluation modalities, including: pulmonary nodule detection and classification from 3D computed tomography (CT), prediction of expanded disability status scale in multiple sclerosis using 3D magnetic resonance imaging (MRI) and segmentation of muscular surface for sarcopenia estimation from two-dimensional CT. A total of 4347 examinations were gathered of which only 6% were excluded. Three independent databases from 24 individual centers were created. A total of 143 participants were split into 20 multidisciplinary teams. 
  Conclusion:  Three data challenges with over 1200 general data protection regulation compliant CT or MRI examinations each were organized. Future challenges should be made with more complex situations combining histopathological or genetic information to resemble real life situations faced by radiologists in routine practice. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2211-5684(20)30083-8  |  
------------------------------------------- 
10.1007/s00428-020-02775-y  |   Mesothelin (MSLN) is a cell surface glycoprotein present in many cancer types. Its expression is generally associated with an unfavorable prognosis. This study examined the prognostic significance of MSLN expression in different areas of individual colorectal cancers (CRCs) using tissue microarrays (TMAs) by enrolling 314 patients with stage II (T3-T4, N0, M0) CRCs. Using formalin-fixed paraffin-embedded tissue blocks from patients, TMA blocks were constructed. Tissue core specimens were obtained from submucosal invasive front (Fr-sm), subserosal invasive front (Fr-ss), central area (Ce), and rolled edge (Ro) of each tumor. Using these four-point TMA sets, MSLN expression was immunohistochemically surveyed. The area-specific prognostic significance of MSLN expression was evaluated. A deep learning convolutional neural network algorithm was used for imaging analysis and evaluating our judgment's objectivity. MSLN staining ratio was positively correlated between the manual and machine-learning analyses (r = 0.71). The correlation coefficient between Ro and Ce, Ro and Fr-sm, and Ro and Fr-ss was r = 0.63, r = 0.54, and r = 0.61, respectively. Disease-specific survival curves for the MSLN-positive and MSLN-negative groups in Fr-sm, Fr-ss, and Ro were significantly different (five-year survival rates 88.1% and 95.5% (P = 0.024), 85.0 and 96.2% (P = 0.0087), 87.8 and 95.5% (P = 0.051), and 77.9 and 95.8% (P = 0.046) for Fr-sm, Fr-ss, Ce, and Ro, respectively). The analysis performed using area-specific four-point TMAs clearly demonstrated that MSLN expression in stage II CRC was relatively homogeneous within tumors. Additionally, high MSLN expression showed or tended to show unfavorable prognostic significance regardless of the tumor area. 
  |  https://dx.doi.org/10.1007/s00428-020-02775-y  |  
------------------------------------------- 
10.1007/s00383-020-04655-7  |    Introduction:  There is a tendency toward nonoperative management of appendicitis resulting in an increasing need for preoperative diagnosis and classification. For medical purposes, simple conceptual decision-making models that can learn are widely used. Decision trees are reliable and effective techniques which provide high classification accuracy. We tested if we could detect appendicitis and differentiate uncomplicated from complicated cases using machine learning algorithms. 
  Materials and methods:  We analyzed all cases admitted between 2010 and 2016 that fell into the following categories: healthy controls (Group 1); sham controls (Group 2); sham disease (Group 3), and acute abdomen (Group 4). The latter group was further divided into four groups: false laparotomy; uncomplicated appendicitis; complicated appendicitis without abscess, and complicated appendicitis with abscess. Patients with comorbidities and whose complete blood count and/or pathology results were lacking were excluded. Data were collected for demographics, preoperative blood analysis, and postoperative diagnosis. Various machine learning algorithms were applied to detect appendicitis patients. 
  Results:  There were 7244 patients with a mean age of 6.84 ± 5.31 years, of whom 82.3% (5960/7244) were male. Most algorithms tested, especially linear methods, provided similar performance measures. We preferred the decision tree model due to its easier interpretability. With this algorithm, we detected appendicitis patients with 93.97% area under the curve (AUC), 94.69% accuracy, 93.55% sensitivity, and 96.55% specificity, and uncomplicated appendicitis with 79.47% AUC, 70.83% accuracy, 66.81% sensitivity, and 81.88% specificity. 
  Conclusions:  Machine learning is a novel approach to prevent unnecessary operations and decrease the burden of appendicitis both for patients and health systems. 
  Levels of evidence:  III. 
  |  https://doi.org/10.1007/s00383-020-04655-7  |  
------------------------------------------- 
10.1093/bioinformatics/btz576  |    Motivation:  Sequence alignment remains fundamental in bioinformatics. Pair-wise alignment is traditionally based on ad hoc scores for substitutions, insertions and deletions, but can also be based on probability models (pair hidden Markov models: PHMMs). PHMMs enable us to: fit the parameters to each kind of data, calculate the reliability of alignment parts and measure sequence similarity integrated over possible alignments. 
  Results:  This study shows how multiple models correspond to one set of scores. Scores can be converted to probabilities by partition functions with a 'temperature' parameter: for any temperature, this corresponds to some PHMM. There is a special class of models with balanced length probability, i.e. no bias toward either longer or shorter alignments. The best way to score alignments and assess their significance depends on the aim: judging whether whole sequences are related versus finding related parts. This clarifies the statistical basis of sequence alignment. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz576  |  
------------------------------------------- 
10.1186/s41747-020-00148-3  |    Background:  To evaluate whether machine learning algorithms allow the prediction of Child-Pugh classification on clinical multiphase computed tomography (CT). 
  Methods:  A total of 259 patients who underwent diagnostic abdominal CT (unenhanced, contrast-enhanced arterial, and venous phases) were included in this retrospective study. Child-Pugh scores were determined based on laboratory and clinical parameters. Linear regression (LR), Random Forest (RF), and convolutional neural network (CNN) algorithms were used to predict the Child-Pugh class. Their performances were compared to the prediction of experienced radiologists (ERs). Spearman correlation coefficients and accuracy were assessed for all predictive models. Additionally, a binary classification in low disease severity (Child-Pugh class A) and advanced disease severity (Child-Pugh class ≥ B) was performed. 
  Results:  Eleven imaging features exhibited a significant correlation when adjusted for multiple comparisons with Child-Pugh class. Significant correlations between predicted and measured Child-Pugh classes were observed (ρ<sub>LA</sub> = 0.35, ρ<sub>RF</sub> = 0.32, ρ<sub>CNN</sub> = 0.51, ρ<sub>ERs</sub> = 0.60; p &lt; 0.001). Significantly better accuracies for the prediction of Child-Pugh classes versus no-information rate were found for CNN and ERs (p ≤ 0.034), not for LR and RF (p ≥ 0.384). For binary severity classification, the area under the curve at receiver operating characteristic analysis was significantly lower (p ≤ 0.042) for LR (0.71) and RF (0.69) than for CNN (0.80) and ERs (0.76), without significant differences between CNN and ERs (p = 0.144). 
  Conclusions:  The performance of a CNN in assessing Child-Pugh class based on multiphase abdominal CT images is comparable to that of ERs. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32249336/  |  
------------------------------------------- 
10.1111/liv.14353  |   Liver tumours are very common and malignant tumours represent a major cause of cancer-related death. Imaging plays an important role at many different stages of the care pathway. This review discusses new aspects and new roles for imaging and for MRI, in particular. MRI is already the best tool for the characterization and staging of benign and malignant liver tumours and it could also become a useful screening tool, especially for hepatocellular carcinoma. Liver imaging will be increasingly quantitative in the future, integrating new approaches such as those of artificial intelligence. 
  |  https://doi.org/10.1111/liv.14353  |  
------------------------------------------- 
10.1093/europace/euz249  |   This review aims to provide a comprehensive recapitulation of the evolution in the field of cardiac rhythm monitoring, shedding light in recent progress made in multilead ECG systems and wearable devices, with emphasis on the promising role of the artificial intelligence and computational techniques in the detection of cardiac abnormalities. 
  |  https://academic.oup.com/europace/article-lookup/doi/10.1093/europace/euz249  |  
------------------------------------------- 
10.1093/jamia/ocz206  |    |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz206  |  
------------------------------------------- 
10.1016/j.gaitpost.2020.01.002  |    Background:  Walking and mobility are essential for a satisfactory quality of life. However, individuals with transfemoral amputations have difficulties in preventing falls due to prosthetic knee buckling, defined as the sudden loss of postural support during weight-bearing activities. The risk of prosthetic knee buckling can be evaluated by determining the prosthetic knee angular impulse (PKAI) during the early stance phase. However, little is known about the factors associated with PKAI in individuals with unilateral transfemoral amputations. 
  Research question:  What are the demographic factors that can be associated with the risk of prosthetic knee buckling, quantified by PKAI, during walking in individuals with unilateral transfemoral amputations? 
  Methods:  Thirteen individuals with unilateral transfemoral amputations were instructed to perform level walking at a comfortable, self-selected speed on a straight, 10-m walkway. PKAI was calculated as the time integral of the prosthetic knee external flexion-extension moment during the initial 40 % of the prosthetic gait cycle. We used Pearson's correlation coefficients to examine the relationship of PKAI with the following variables: the subject's body height, body mass, and age; the time since amputation; and the current prosthesis use history. Furthermore, an independentt-test was used to compare PKAI according to the sex (male vs. female) and etiology (trauma vs. nontrauma). 
  Results:  PKAI exhibited a significant negative linear relationship with the subject's body height and body mass. However, it showed no significant correlation with age, the time since amputation, and the current prosthesis use history. It was also significantly greater in women than in men and was not significantly influenced by the etiology. 
  Significance:  Awareness about demographic factors associated with PKAI during walking can contribute to fall assessments in gait rehabilitation programs for individuals with unilateral transfemoral amputations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0966-6362(20)30003-5  |  
------------------------------------------- 
10.1016/j.chaos.2020.109794  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0960-0779(20)30196-X  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32288357/  |  
------------------------------------------- 
10.1016/j.isatra.2020.03.001  |   This paper examines two approaches in tuning fractional order proportional-integral-differential (FOPID) control named as neuro-based FOPID (NNFOPID) and particle swarm-based FOPID (PSOFOPID) for pitch control of a Twin Rotor Aerodynamic System (TRAS). For the neuro-based FOPID control, the innovations are the modification of output equation in the artificial neural network and the implementation of the Rectified Linear Unit (ReLU) activation function. The advantages of the proposed approach are a lighter network and the ability to tune more practical controller parameters without a deep knowledge of the system to achieve a satisfying pitch tracking response. As for the particle swarm-based FOPID control, the application of PSO with spreading factor algorithm is extended for tuning the FOPID controller gains and the innovation here is a new procedure in setting the initial search range. The important advantages of this proposed swarm-based algorithm are the avoidance of being trapped in local optima and reduction of the search area respectively. The performances of the proposed controllers are proven by extensive simulations and experimental verifications based on five standard criteria: square-wave characteristics, reference to disturbance ratio, evaluation time, energy consumption of the control signal and tracking performance. The performances of the proposed controllers are compared against an optimised PID control in three system conditions, namely Case I) without coupling effect and wind disturbance, Case II) with coupling effect only and Case III) with wind disturbance only. Together, this study finds that NNFOPID control offers an accurate system positioning by a 34% reduction in steady-state error with the lowest energy consumption and minimum evaluation time in Case II. In terms of the tracking performance and robustness for Case II, the superiority of PSOFOPID control is confirmed by a 27% reduction in the tracking error and the lowest oscillation value. The experimental results also validate the robustness and energy consumption of both controllers in Case III. It is envisaged that the proposed control designs can be very useful in tuning FOPID controller gains for high performance, low energy, and robust aerodynamics systems. 
  |  None  |  
------------------------------------------- 
10.1590/0034-7167-2018-0421  |    Objectives:  to present the nurses' experience with technological tools to support the early identification of sepsis. 
  Methods:  experience report before and after the implementation of artificial intelligence algorithms in the clinical practice of a philanthropic hospital, in the first half of 2018. 
  Results:  describe the motivation for the creation and use of the algorithm; the role of the nurse in the development and implementation of this technology and its effects on the nursing work process. 
  Final considerations:  technological innovations need to contribute to the improvement of professional practices in health. Thus, nurses must recognize their role in all stages of this process, in order to guarantee safe, effective and patient-centered care. In the case presented, the participation of the nurses in the technology incorporation process enables a rapid decision-making in the early identification of sepsis. 
  |  http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0034-71672020000300502&lng=en&nrm=iso&tlng=en  |  
------------------------------------------- 
10.1002/adma.201905522  |   Living things rely on various physical, chemical, and biological interfaces, e.g., somatosensation, olfactory/gustatory perception, and nervous system response. They help organisms to perceive the world, adapt to their surroundings, and maintain internal and external balance. Interfacial information exchanges are complicated but efficient, delicate but precise, and multimodal but unisonous, which has driven researchers to study the science of such interfaces and develop techniques with potential applications in health monitoring, smart robotics, future wearable devices, and cyber physical/human systems. To understand better the issues in these interfaces, a cyber-physiochemical interface (CPI) that is capable of extracting biophysical and biochemical signals, and closely relating them to electronic, communication, and computing technology, to provide the core for aforementioned applications, is proposed. The scientific and technical progress in CPI is summarized, and the challenges to and strategies for building stable interfaces, including materials, sensor development, system integration, and data processing techniques are discussed. It is hoped that this will result in an unprecedented multi-disciplinary network of scientific collaboration in CPI to explore much uncharted territory for progress, providing technical inspiration-to the development of the next-generation personal healthcare technology, smart sports-technology, adaptive prosthetics and augmentation of human capability, etc. 
  |  https://doi.org/10.1002/adma.201905522  |  
------------------------------------------- 
10.1186/s12859-020-3418-9  |    Background:  MicroRNA (miRNA) regulation is associated with several diseases, including neurodegenerative diseases. Several approaches can be used for modeling miRNA regulation. However, their precision may be limited for analyzing multidimensional data. Here, we addressed this question by integrating shape analysis and feature selection into miRAMINT, a methodology that we used for analyzing multidimensional RNA-seq and proteomic data from a knock-in mouse model (Hdh mice) of Huntington's disease (HD), a disease caused by CAG repeat expansion in huntingtin (htt). This dataset covers 6 CAG repeat alleles and 3 age points in the striatum and cortex of Hdh mice. 
  Results:  Remarkably, compared to previous analyzes of this multidimensional dataset, the miRAMINT approach retained only 31 explanatory striatal miRNA-mRNA pairs that are precisely associated with the shape of CAG repeat dependence over time, among which 5 pairs with a strong change of target expression levels. Several of these pairs were previously associated with neuronal homeostasis or HD pathogenesis, or both. Such miRNA-mRNA pairs were not detected in cortex. 
  Conclusions:  These data suggest that miRNA regulation has a limited global role in HD while providing accurately-selected miRNA-target pairs to study how the brain may compute molecular responses to HD over time. These data also provide a methodological framework for researchers to explore how shape analysis can enhance multidimensional data analytics in biology and disease. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3418-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093602/  |  
------------------------------------------- 
10.1016/j.gene.2020.144461  |   Down syndrome is one of the most common chromosomal disorders and yet our understanding about the dysregulated genes in this disease is limited. Through this case study, we investigated the gene expression profile of primary amniotic fluid mesenchymal stem cells (AFMSCs) isolated from the amniotic sac of monozygotic twins discordant for trisomy 21 with one fetal hydrops at 17 weeks of gestation. AFMSCs were cultured to analyze the gene expression profiles for the human transcriptome array. Gene ontology was used to evaluate dysregulated gene functions. Total 25,799 genes were identified such that 65 were up-regulated (0.25%) and 111 were down-regulated (0.43%) with a log<sub>2</sub> fold change trisomy 21/euploidy (log<sub>2</sub> [FC]) &gt; 1, p &lt; 0.01). 16 genes were selected and verified by qRT-PCR, which showed compatible result with transcriptome array. At the chromosome level, chromosome 21 was found to carry the highest percentage of up-regulated genes (2.13%, 7/329 genes) with the highest mean log<sub>2</sub> [FC] (0.23, p &lt; 10<sup>-5</sup>), particularly on 21q22.3. There were eight segments with significant mean log<sub>2</sub> [FC] on chromosomes 1, 6, 11, and 21 for upregulation, and on chromosomes 16, 17, and 19 for downregulation, indicating a pattern of dysregulated genes clustering in domains along the genome. Gene ontology showed the identified genes associated with extracellular matrix organization (11 genes, p = 5.1 × 10<sup>-6</sup>) and central nervous system development (8 genes, p = 6.0 × 10<sup>-5</sup>). Using transcriptome analysis of the AFMSCs of monozygotic twins discordant for trisomy 21, we report the dysregulated genes involved in Down syndrome, their predominance on chromosome 21, and the cluster pattern on the whole genome. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30130-X  |  
------------------------------------------- 
10.1016/j.ad.2019.09.002  |    Background:  Automated image classification is a promising branch of machine learning (ML) useful for skin cancer diagnosis, but little has been determined about its limitations for general usability in current clinical practice. 
  Objective:  To determine limitations in the selection of skin cancer images for ML analysis, particularly in melanoma. 
  Methods:  Retrospective cohort study design, including 2,849 consecutive high-quality dermoscopy images of skin tumors from 2010 to 2014, for evaluation by a ML system. Each dermoscopy image was assorted according to its eligibility for ML analysis. 
  Results:  Of the 2,849 images chosen from our database, 968 (34%) met the inclusion criteria for analysis by the ML system. Only 64.7% of nevi and 36.6% of melanoma met the inclusion criteria. Of the 528 melanomas, 335 (63.4%) were excluded. An absence of normal surrounding skin (40.5% of all melanomas from our database) and absence of pigmentation (14.2%) were the most common reasons for exclusion from ML analysis. 
  Discussion:  Only 36.6% of our melanomas were admissible for analysis by state-of-the-art ML systems. We conclude that future ML systems should be trained on larger datasets which include relevant non-ideal images from lesions evaluated in real clinical practice. Fortunately, many of these limitations are being overcome by the scientific community as recent works show. 
  |  http://www.elsevier.es/en/linksolver/pdf/pii/S0001-7310(20)30004-1  |  
------------------------------------------- 
10.1148/radiol.2020200058  |    |  http://pubs.rsna.org/doi/10.1148/radiol.2020200058?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41587-020-0417-3  |    |  https://dx.doi.org/10.1038/s41587-020-0417-3  |  
------------------------------------------- 
10.1016/j.jacr.2020.02.018  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(20)30239-8  |  
------------------------------------------- 
10.1080/14756366.2019.1693702  |   Cyclin-dependent kinase 2 (CDK2) is the family of Ser/Thr protein kinases that has emerged as a highly selective with low toxic cancer therapy target. A multistage virtual screening method combined by SVM, protein-ligand interaction fingerprints (PLIF) pharmacophore and docking was utilised for screening the CDK2 inhibitors. The evaluation of the validation set indicated that this method can be used to screen large chemical databases because it has a high hit-rate and enrichment factor (80.1% and 332.83 respectively). Six compounds were screened out from NCI, Enamine and Pubchem database. After molecular dynamics and binding free energy calculation, two compounds had great potential as novel CDK2 inhibitors and they also showed selective inhibition against CDK2 in the kinase activity assay. 
  |  http://www.tandfonline.com/doi/full/10.1080/14756366.2019.1693702  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31760818/  |  
------------------------------------------- 
10.1136/ebmental-2019-300136  |    Background:  All patients admitted to an acute inpatient mental health unit must have nursing observations carried out at night either hourly or every 15 minutes, to ascertain that they are safe and breathing. However, while this practice ensures patient safety, it can also disturb patients' sleep, which in turn can impact negatively on their recovery. 
  Objective:  This article describes the process of introducing artificial intelligence ('digitally assisted nursing observations') in an acute mental health inpatient ward, to enable staff to carry out the hourly and the 15 minutes observations, minimising disruption of patients' sleep while maintaining their safety. 
  Findings:  The preliminary data obtained indicate that the digitally assisted nursing observations agreed with the observations without sensors when both were carried out in parallel and that over an estimated 755 patient nights, the new system has not been associated with any untoward incidents. Preliminary qualitative data suggest that the new technology improves patients' and staff's experience at night. 
  Discussion:  This project suggests that the digitally assisted nursing observations could maintain patients' safety while potentially improving patients' and staff's experience in an acute psychiatric ward. The limitations of this study, namely, its narrative character and the fact that patients were not randomised to the new technology, suggest taking the reported findings as qualitative and preliminary. 
  Clinical implications:  These results suggest that the care provided at night in acute inpatient psychiatric units could be substantially improved with this technology. This warrants a more thorough and stringent evaluation. 
  |  https://ebmh.bmj.com/cgi/pmidlookup?view=long&pmid=32046991  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046991/  |  
------------------------------------------- 
10.3389/fneur.2020.00001  |   Although a variety of imaging modalities are used or currently being investigated for patients with brain tumors including brain metastases, clinical image interpretation to date uses only a fraction of the underlying complex, high-dimensional digital information from routinely acquired imaging data. The growing availability of high-performance computing allows the extraction of quantitative imaging features from medical images that are usually beyond human perception. Using machine learning techniques and advanced statistical methods, subsets of such imaging features are used to generate mathematical models that represent characteristic signatures related to the underlying tumor biology and might be helpful for the assessment of prognosis or treatment response, or the identification of molecular markers. The identification of appropriate, characteristic image features as well as the generation of predictive or prognostic mathematical models is summarized under the term radiomics. This review summarizes the current status of radiomics in patients with brain metastases. 
  |  https://doi.org/10.3389/fneur.2020.00001  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32116995/  |  
------------------------------------------- 
10.2967/jnumed.118.222893  |   Radiomics is a rapidly evolving field of research concerned with the extraction of quantitative metrics-the so-called radiomic features-within medical images. Radiomic features capture tissue and lesion characteristics such as heterogeneity and shape and may, alone or in combination with demographic, histologic, genomic, or proteomic data, be used for clinical problem solving. The goal of this continuing education article is to provide an introduction to the field, covering the basic radiomics workflow: feature calculation and selection, dimensionality reduction, and data processing. Potential clinical applications in nuclear medicine that include PET radiomics-based prediction of treatment response and survival will be discussed. Current limitations of radiomics, such as sensitivity to acquisition parameter variations, and common pitfalls will also be covered. 
  |  http://jnm.snmjournals.org/cgi/pmidlookup?view=long&pmid=32060219  |  
------------------------------------------- 
10.1002/sctm.19-0389  |   Regenerative Medicine Manufacturing Society (RMMS) is the first and only professional society dedicated toward advancing manufacturing solutions for the field of regenerative medicine. RMMS' vision is to provide greater patient access to regenerative medicine therapies through innovative manufacturing solutions. Our mission is to identify unmet needs and gaps in regenerative medicine manufacturing and catalyze the generation of new ideas and solutions by working with private and public stakeholders. We aim to accomplish our mission through outreach and education programs and securing grants for public-private collaborations in regenerative medicine manufacturing. This perspective article will cover four impact areas that the society's leadership team has identified as critical: (a) cell manufacturing and scale-up/out, respectively, for allogeneic and autologous cell therapies, (b) standards for regenerative medicine, (c) 3D bioprinting, and (d) artificial intelligence-enabled automation. In addition to covering these areas and ways in which the society intends to advance the field in a collaborative nature, we will also discuss education and training. Education and training is an area that is critical for communicating the current challenges, developing solutions to accelerate the commercialization of the latest technological advances, and growing the work force in the rapidly expanding sector of regenerative medicine. 
  |  https://doi.org/10.1002/sctm.19-0389  |  
------------------------------------------- 
10.1007/s12551-020-00685-6  |   Hardware and software advancements along with the accumulation of large amounts of data in recent years have together spurred a remarkable growth in the application of neural networks to various scientific fields. Machine learning based on neural networks with multiple (hidden) layers is becoming an extremely powerful approach for analyzing data. With the accumulation of large amounts of protein data such as structural and functional assay data, the effects of such approaches within the field of protein informatics are increasing. Here, we introduce our recent studies based on applications of neural networks for protein structure and function prediction and dynamic analysis involving: (i) inter-residue contact prediction based on a multiple sequence alignment (MSA) of amino acid sequences, (ii) prediction of protein-compound interaction using assay data, and (iii) detection of protein allostery from trajectories of molecular dynamic (MD) simulation. 
  |  https://dx.doi.org/10.1007/s12551-020-00685-6  |  
------------------------------------------- 
10.1021/acs.jmedchem.9b02044  |   Artificial intelligence offers promising solutions for property prediction, compound design, and retrosynthetic planning, which are expected to significantly accelerate the search for pharmacologically relevant molecules. Here, we investigate aspects of artificial intelligence based de novo design pertaining to its integration into real-life workflows. First, different chemical spaces were used as training sets for reinforcement learning (RL) in combination with different reward functions. With the trained neuronal networks different biologically active molecules could be regenerated. Excluding molecules with substructures such as five-membered rings from training spaces nevertheless produced results containing these moieties. Furthermore, different scoring functions in RL were investigated and produced different design ensembles. In summary, some of these design proposals are close in chemical space to the query, thus supporting lead optimization, while 3D-shape or QSAR (quantitative structure-activity relationship) models produced significantly different proposals by sampling a broader region of the chemical space, thus supporting lead generation. Therefore, RL provides a good framework to tailored design approaches for different discovery phases. 
  |  https://dx.doi.org/10.1021/acs.jmedchem.9b02044  |  
------------------------------------------- 
10.2147/CMAR.S244932  |    Objective:  The disease complexity of metastatic non-small-cell lung cancer (mNSCLC) makes it difficult for physicians to make clinical decisions efficiently and accurately. The Watson for Oncology (WFO) system of artificial intelligence might help physicians by providing fast and precise treatment regimens. This study measured the concordance of the medical treatment regimens of the WFO system and actual clinical regimens, with the aim of determining the suitability of WFO recommendations for Chinese patients with mNSCLC. 
  Methods:  Retrospective data of mNSCLC patients were input to the WFO, which generated a treatment regimen (WFO regimen). The actual regimen was made by physicians in a medical team for patients (medical-team regimen). The factors influencing the consistency of the two treatment options were analyzed by univariate and multivariate analyses. 
  Results:  The concordance rate was 85.16% between the WFO and medical-team regimens for mNSCLC patients. Logistic regression showed that the concordance differed significantly for various pathological types and gene mutations in two treatment regimens. Patients with adenocarcinoma had a lower rate of "recommended" regimen than those with squamous cell carcinoma. There was a statistically significant difference in EGFR-mutant patients for "not recommended" regimens with inconsistency rate of 18.75%. In conclusion, the WFO regimen has 85.16% consistency rate with medical-team regimen in our treatment center. The different pathological type and different gene mutation markedly influenced the agreement rate of the two treatment regimens. 
  Conclusion:  WFO recommendations have high applicability to mNSCLC patients in our hospital. This study demonstrates that the valuable WFO system may assist the doctors better to determine the accurate and effective treatment regimens for mNSCLC patients in the Chinese medical setting. 
  |  https://dx.doi.org/10.2147/CMAR.S244932  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32214852/  |  
------------------------------------------- 
10.21037/qims.2020.02.20  |    Background:  Bone age can reflect the true growth and development status of a child; thus, it plays a critical role in evaluating growth and endocrine disorders. This study established and validated an optimized Tanner-Whitehouse 3 artificial intelligence (TW3-AI) bone age assessment (BAA) system based on a convolutional neural network (CNN). 
  Methods:  A data set of 9,059 clinical radiographs of the left hand was obtained from the picture archives and communication systems (PACS) between January 2012 and December 2016. Among these, 8,005/9,059 (88%) samples were treated as the training set for model implementation, 804/9,059 (9%) samples as the validation set for parameters optimization, and the remaining 250/9,059 (3%) samples were used to verify the accuracy and reliability of the model compared to that of 4 experienced endocrinologists and 2 experienced radiologists. The overall variation of TW3-metacarpophalangeal, radius, ulna and short bones (RUS) and TW3-Carpal bone score, as well as each bone (13 RUS + 7 Carpal) between reviewers and the AI, were compared by Bland-Altman (BA) chart and Kappa test, respectively. Furthermore, the time consumption between the model and reviewers was also compared. 
  Results:  The performance of TW3-AI model was highly consistent with the reviewers' overall estimation, and the root mean square (RMS) was 0.50 years. The accuracy of the BAA of the TW3-AI model was better than the estimate of the reviewers. Further analysis revealed that human interpretations of the male capitate, hamate, the first distal and fifth middle phalanx and female capitate, the trapezoid, and the third and fifth middle phalanx, were most inconsistent. The average image processing time was 1.5±0.2 s in the TW3-AI model, which was significantly shorter than manual interpretation. 
  Conclusions:  The diagnostic performance of CNN-based TW3 BAA was accurate and timesaving, and possesses better stability compared to diagnostics made by experienced experts. 
  |  https://doi.org/10.21037/qims.2020.02.20  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32269926/  |  
------------------------------------------- 
10.1371/journal.pone.0231192  |   Artificial intelligence (AI) assisted human brain research is a dynamic interdisciplinary field with great interest, rich literature, and huge diversity. The diversity in research topics and technologies keeps increasing along with the tremendous growth in application scope of AI-assisted human brain research. A comprehensive understanding of this field is necessary to assess research efficacy, (re)allocate research resources, and conduct collaborations. This paper combines the structural topic modeling (STM) with the bibliometric analysis to automatically identify prominent research topics from the large-scale, unstructured text of AI-assisted human brain research publications in the past decade. Analyses on topical trends, correlations, and clusters reveal distinct developmental trends of these topics, promising research orientations, and diverse topical distributions in influential countries/regions and research institutes. These findings help better understand scientific and technological AI-assisted human brain research, provide insightful guidance for resource (re)allocation, and promote effective international collaborations. 
  |  http://dx.plos.org/10.1371/journal.pone.0231192  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32251489/  |  
------------------------------------------- 
10.1097/01.APO.0000656980.41190.bf  |   Artificial intelligence (AI) has been studied in ophthalmology since availability of digital information in ophthalmic care. The significant turning point was availability of commercial digital color fundus photography in the late 1990s, which caused digital screening for diabetic retinopathy (DR) to take off. Automated Retinal Disease Assessment software was then developed using machine learning to detect abnormal lesions in fundus to screen DR. The use of this version of AI had not been generalized because the specificity at 45% was not high enough, although the sensitivity reached 90%. The recent breakthrough in machine learning is the invent of deep learning, which accelerates its performance to be on par with experts. The first 2 breakthrough studies on deep learning for screening DR were conducted in Asia. The first represented collaboration of datasets between Asia and the United States for algorithms development, whereas the second represented algorithms developed in Asia but validated in different populations across the world. Both found accuracy for detecting referable DR of &gt;95%. Diversity and variety are unique strengths of Asia for AI studies. There are many more studies of AI ongoing in Asia not only as prospective deployments in DR but in glaucoma, age-related macular degeneration, cataract, and systemic disease, such as Alzheimer's disease. Some Asian countries have laid out plans for digital health care system using AI as one of the puzzle pieces for solving blindness. More studies on AI and digital health are expected to come from Asia in this new decade. 
  |  http://dx.doi.org/10.1097/01.APO.0000656980.41190.bf  |  
------------------------------------------- 
10.2196/14971  |    Background:  Since its inception, artificial intelligence has aimed to use computers to help make clinical diagnoses. Evidence-based medical reasoning is important for patient care. Inferring clinical diagnoses is a crucial step during the patient encounter. Previous works mainly used expert systems or machine learning-based methods to predict the International Classification of Diseases - Clinical Modification codes based on electronic health records. We report an alternative approach: inference of clinical diagnoses from patients' reported symptoms and physicians' clinical observations. 
  Objective:  We aimed to report a natural language processing system for generating medical assessments based on patient information described in the electronic health record (EHR) notes. 
  Methods:  We processed EHR notes into the Subjective, Objective, Assessment, and Plan sections. We trained a neural network model for medical assessment generation (N2MAG). Our N2MAG is an innovative deep neural model that uses the Subjective and Objective sections of an EHR note to automatically generate an "expert-like" assessment of the patient. N2MAG can be trained in an end-to-end fashion and does not require feature engineering and external knowledge resources. 
  Results:  We evaluated N2MAG and the baseline models both quantitatively and qualitatively. Evaluated by both the Recall-Oriented Understudy for Gisting Evaluation metrics and domain experts, our results show that N2MAG outperformed the existing state-of-the-art baseline models. 
  Conclusions:  N2MAG could generate a medical assessment from the Subject and Objective section descriptions in EHR notes. Future work will assess its potential for providing clinical decision support. 
  |  https://medinform.jmir.org/2020/1/e14971/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31939742/  |  
------------------------------------------- 
10.1007/s00261-020-02508-4  |    Purpose:  Deep learning reconstruction (DLR) introduces deep convolutional neural networks into the reconstruction flow. We examined the clinical applicability of drip-infusion cholangiography (DIC) acquired on an ultra-high-resolution CT (U-HRCT) scanner reconstructed with DLR in comparison to hybrid and model-based iterative reconstruction (hybrid-IR, MBIR). 
  Methods:  This retrospective, single-institution study included 30 patients seen between January 2018 and November 2019. A radiologist recorded the standard deviation of attenuation in the paraspinal muscle as the image noise and calculated the contrast-to-noise ratio (CNR) in the common bile duct. The overall visual image quality of the bile duct on thick-slab maximum intensity projections was assessed by two other radiologists and graded on a 5-point confidence scale ranging from 1 (not delineated) to 5 (clearly delineated). The difference among hybrid-IR, MBIR, and DLR images was compared. 
  Results:  The image noise was significantly lower on DLR than hybrid-IR and MBIR images and the CNR and the overall visual image quality of the bile duct were significantly higher on DLR than on hybrid-IR and MBIR images (all: p &lt; 0.001). 
  Conclusion:  DLR resulted in significant quantitative and qualitative improvement of DIC acquired with U-HRCT. 
  |  https://dx.doi.org/10.1007/s00261-020-02508-4  |  
------------------------------------------- 
10.3389/fpls.2020.00290  |   Agricultural productivity is subject to various stressors, including abiotic and biotic threats, many of which are exacerbated by a changing climate, thereby affecting long-term sustainability. The productivity of tree crops such as almond orchards, is particularly complex. To understand and mitigate these threats requires a collection of multi-layer large data sets, and advanced analytics is also critical to integrate these highly heterogeneous datasets to generate insights about the key constraints on the yields at tree and field scales. Here we used a machine learning approach to investigate the determinants of almond yield variation in California's almond orchards, based on a unique 10-year dataset of field measurements of light interception and almond yield along with meteorological data. We found that overall the maximum almond yield was highly dependent on light interception, e.g., with each one percent increase in light interception resulting in an increase of 57.9 lbs/acre in the potential yield. Light interception was highest for mature sites with higher long term mean spring incoming solar radiation (SRAD), and lowest for younger orchards when March maximum temperature was lower than 19°C. However, at any given level of light interception, actual yield often falls significantly below full yield potential, driven mostly by tree age, temperature profiles in June and winter, summer mean daily maximum vapor pressure deficit (VPD<sub>max</sub>), and SRAD. Utilizing a full random forest model, 82% (±1%) of yield variation could be explained when using a sixfold cross validation, with a RMSE of 480 ± 9 lbs/acre. When excluding light interception from the predictors, overall orchard characteristics (such as age, location, and tree density) and inclusive meteorological variables could still explain 78% of yield variation. The model analysis also showed that warmer winter conditions often limited mature orchards from reaching maximum yield potential and summer VPD<sub>max</sub> beyond 40 hPa significantly limited the yield. Our findings through the machine learning approach improved our understanding of the complex interaction between climate, canopy light interception, and almond nut production, and demonstrated a relatively robust predictability of almond yield. This will ultimately benefit data-driven climate adaptation and orchard nutrient management approaches. 
  |  https://doi.org/10.3389/fpls.2020.00290  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231679/  |  
------------------------------------------- 
10.1002/jum.15270  |    Objectives:  We sought to create a deep learning (DL) algorithm to identify vessels, bones, nerves, and tendons on transverse upper extremity (UE) ultrasound (US) images to enable providers new to US-guided peripheral vascular access to identify anatomy. 
  Methods:  We used publicly available DL architecture (YOLOv3) and deidentified transverse US videos of the UE for algorithm development. Vessels, bones, tendons, and nerves were labeled with bounding boxes. A total of 203,966 images were generated from videos, with corresponding label box coordinates in a YOLOv3 format. Training accuracy, losses, and learning curves were tracked. As a final real-world test, 50 randomly selected images from unrelated UE US videos were used to test the DL algorithm. Four different versions of the YOLOv3 algorithm were tested with varied amounts of training and sensitivity settings. The same 50 images were labeled by 2 blinded point-of-care ultrasound (POCUS) experts. The area under the curve (AUC) was calculated for the DL algorithm and POCUS expert performance. 
  Results:  The algorithm outperformed POCUS experts in detection of all structures in the UE, with an AUC of 0.78 versus 0.69 and 0.71, respectively. When considering vessels, only one of the POCUS experts attained an AUC of 0.85, just ahead of the DL algorithm, with an AUC of 0.83. 
  Conclusions:  Our DL algorithm proved accurate at identifying 4 common structures on cross-sectional US imaging of the UE, which would allow novice POCUS providers to more confidently and accurately target vessels for cannulation, avoiding other structures. Overall, the algorithm outperformed 2 blinded POCUS experts. 
  |  https://doi.org/10.1002/jum.15270  |  
------------------------------------------- 
10.1016/j.neulet.2020.134804  |   Because depression has high prevalence and cause enduring disability, it is important to predict onset of depression among community dwelling adults. In this study, we aimed to build a machine learning-based predictive model for future onset of depression. We used nationwide survey data to construct training and hold-out test set. The class imbalance was dealt with the Synthetic Minority Over-sampling Technique. A tree-based ensemble method, random forest, was used to build a predictive model. Depression was defined by 9 or more on the Center for Epidemiologic Studies - Depression Scale 11 items version. Hyperparameters were tuned throughout the 10-fold cross-validation. A total of 6,588 (6,067 of non-depression and 521 of depression) participants were included in the study. The area under receiver operating characteristics curve was 0.870. The overall accuracy, sensitivity, and specificity were 0.862, 0.730, and 0.866, respectively. Satisfactions for leisure, familial relationship, general, social relationship, and familial income had importance in building predictive model for the onset of future depression. Our study demonstrated that predicting future onset of depression by using survey data could be possible. This predictive model is expected to be used for early identification of individuals at risk for depression and secure time to intervention. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0304-3940(20)30074-4  |  
------------------------------------------- 
10.1016/j.jmgm.2020.107535  |   Amyloid-β (Aβ) aggregation is recognized to be a key toxic factor in the pathogenesis of Alzheimer disease, which is the most common progressive neurodegenerative disorder. In vitro experiments have elucidated that Aβ aggregation depends on several factors, such as pH, temperature and peptide concentration. Despite the research effort in this field, the fundamental mechanism responsible for the disease progression is still unclear. Recent research has proposed the application of electric fields as a non-invasive therapeutic option leading to the disruption of amyloid fibrils. In this regard, a molecular level understanding of the interactions governing the destabilization mechanism represents an important research advancement. Understanding the electric field effects on proteins, provides a more in-depth comprehension of the relationship between protein conformation and electrostatic dipole moment. The present study focuses on investigating the effect of static Electric Field (EF) on the conformational dynamics of Aβ fibrils by all-atom Molecular Dynamics (MD) simulations. The outcome of this work provides novel insight into this research field, demonstrating how the Aβ assembly may be destabilized by the applied EF. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1093-3263(19)30635-7  |  
------------------------------------------- 
10.1136/bmjdrc-2019-000892  |    Introduction:  The aim of this study is to evaluate the performance of the offline smart phone-based Medios artificial intelligence (AI) algorithm in the diagnosis of diabetic retinopathy (DR) using non-mydriatic (NM) retinal images. 
  Methods:  This cross-sectional study prospectively enrolled 922 individuals with diabetes mellitus. NM retinal images (disc and macula centered) from each eye were captured using the Remidio NM fundus-on-phone (FOP) camera. The images were run offline and the diagnosis of the AI was recorded (DR present or absent). The diagnosis of the AI was compared with the image diagnosis of five retina specialists (majority diagnosis considered as ground truth). 
  Results:  Analysis included images from 900 individuals (252 had DR). For any DR, the sensitivity and specificity of the AI algorithm was found to be 83.3% (95% CI 80.9% to 85.7%) and 95.5% (95% CI 94.1% to 96.8%). The sensitivity and specificity of the AI algorithm in detecting referable DR (RDR) was 93% (95% CI 91.3% to 94.7%) and 92.5% (95% CI 90.8% to 94.2%). 
  Conclusion:  The Medios AI has a high sensitivity and specificity in the detection of RDR using NM retinal images. 
  |  https://drc.bmj.com/cgi/pmidlookup?view=long&pmid=32049632  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32049632/  |  
------------------------------------------- 
10.1016/j.neunet.2019.12.025  |   In this paper, tracking synchronization for coupled reaction-diffusion neural networks with parameter mismatches is investigated. For such a networked control system, only local neighbor information is used to compensate the mismatch characteristic termed as parameter mismatch, uncertainty or external disturbance. Different from the general boundedness hypothesis, the parameter mismatches are permitted to be unbounded. For the known parameter mismatches, parameter-dependent controller and parameter-independent adaptive controller are respectively designed. While for fully unknown network parameters and parameter mismatches, a distributed adaptive controller is proposed. By means of partial differential equation theories and differential inequality techniques, the tracking synchronization errors driven by these nonlinear controllers are proved to be uniformly ultimately bounded and exponentially convergent to some adjustable bounded domains. Finally, three numerical examples are given to test the effectiveness of the proposed controllers. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30425-3  |  
------------------------------------------- 
10.1007/s10620-020-06156-y  |   In line with the current trajectory of healthcare reform, significant emphasis has been placed on improving the utilization of data collected during a clinical encounter. Although the structured fields of electronic health records have provided a convenient foundation on which to begin such efforts, it was well understood that a substantial portion of relevant information is confined in the free-text narratives documenting care. Unfortunately, extracting meaningful information from such narratives is a non-trivial task, traditionally requiring significant manual effort. Today, computational approaches from a field known as Natural Language Processing (NLP) are poised to make a transformational impact in the analysis and utilization of these documents across healthcare practice and research, particularly in procedure-heavy sub-disciplines such as gastroenterology (GI). As such, this manuscript provides a clinically focused review of NLP systems in GI practice. It begins with a detailed synopsis around the state of NLP techniques, presenting state-of-the-art methods and typical use cases in both clinical settings and across other domains. Next, it will present a robust literature review around current applications of NLP within four prominent areas of gastroenterology including endoscopy, inflammatory bowel disease, pancreaticobiliary, and liver diseases. Finally, it concludes with a discussion of open problems and future opportunities of this technology in the field of gastroenterology and health care as a whole. 
  |  https://doi.org/10.1007/s10620-020-06156-y  |  
------------------------------------------- 
10.1016/j.ijpsycho.2020.02.015  |   Prior research has shown neurophysiological measures of learning yield large effect sizes, suggesting that these measures have high potential in providing insight into learning. Yet, most literature on learning and neurophysiological measures focused on a single outcome measure, neglecting the interplay between different types of measures. Additionally, it is not yet clear which measures change robustly in a way specific to the learning process. The current study assessed implicit visuomotor sequence learning through multiple neurophysiological outcome measures. In two experiments participants were presented with an arm-movement version of the Serial Reaction Time Task with blocks in which targets were selected in a repeating sequence and blocks in which targets were selected randomly. While participants were executing this task, measures of EEG, skin conductance, heart rate (variability) and respiration, in addition to measures of behavioral performance, were collected. Although behavioral performance was sensitive to sequence learning, as demonstrated by faster responses in sequence than in random blocks, neurophysiology was not sensitive to sequence learning. However, in both experiments, skin conductance level and parietal EEG alpha and gamma power were sensitive to task induction and changed during sequence blocks in the direction of a pre-task baseline and were related to behavioral performance. In general, models including only EEG parietal gamma power were just as powerful in explaining behavioral measures during learning as models including a combination of neurophysiological outcome measures. The findings of the current study demonstrate that neurophysiology is not sensitive to implicit sequence learning specifically, but that general learning effects on a visuomotor learning task are reflected in measures of neurophysiology. Additionally, the findings highlight that a combination of neurophysiological outcome measures is not necessarily better in explaining task learning than a single measure. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0167-8760(20)30054-4  |  
------------------------------------------- 
10.1007/s11030-020-10074-6  |   An elastic network model (ENM) represents a molecule as a matrix of pairwise atomic interactions. Rich in coded information, ENMs are hereby proposed as a novel tool for the prediction of the activity of series of molecules, with widely different chemical structures, but a common biological activity. The new approach is developed and tested using a set of 183 inhibitors of serine/threonine-protein kinase enzyme (Plk3) which is an enzyme implicated in the regulation of cell cycle and tumorigenesis. The elastic network (EN) predictive model is found to exhibit high accuracy and speed compared to descriptor-based machine-trained modeling. EN modeling appears to be a highly promising new tool for the high demands of industrial applications such as drug and material design. 
  |  https://doi.org/10.1007/s11030-020-10074-6  |  
------------------------------------------- 
10.1002/hep.31207  |   Standardized and robust risk stratification systems for patients with hepatocellular carcinoma (HCC) are required to improve therapeutic strategies and investigate the benefits of adjuvant systemic therapies after curative resection/ablation. In this study, we used two deep-learning algorithms based on whole-slide digitized histological slides (WSI) to build models for predicting the survival of patients with HCC treated by surgical resection. Two independent series were investigated: a discovery set (Henri Mondor Hospital, n=194) used to develop our algorithms and an independent validation set (TCGA, n=328). WSIs were first divided into small squares ("tiles") and features were extracted with a pretrained convolutional neural network (preprocessing step). The first deep-learning based algorithm ("SCHMOWDER") uses an attention mechanism on tumoral areas annotated by a pathologist while the second ("CHOWDER") does not require human expertise. In the discovery set, c-indexes for survival prediction of SCHMOWDER and CHOWDER reached 0.78 and 0.75, respectively. Both models outperformed a composite score incorporating all baseline variables associated with survival. The prognostic value of the models was further validated in the TCGA dataset, and, as observed in the discovery series, both models had a higher discriminatory power than a score combining all baseline variables associated with survival. Pathological review showed that the tumoral areas most predictive of poor survival were characterized by vascular spaces, the macrotrabecular architectural pattern and a lack of immune infiltration. CONCLUSION: This study shows that artificial intelligence can help refine the prediction of HCC prognosis. It highlights the importance of pathologist/machine interactions for the construction of deep-learning algorithms that benefit from expert knowledge and allow a biological understanding of their output. 
  |  https://doi.org/10.1002/hep.31207  |  
------------------------------------------- 
10.1097/CM9.0000000000000634  |    |  http://dx.doi.org/10.1097/CM9.0000000000000634  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31934937/  |  
------------------------------------------- 
10.5664/jcsm.8388  |   Polysomnography remains the cornerstone of objective testing in sleep medicine and results in massive amounts of electrophysiological data, which is well-suited for analysis with artificial intelligence (AI)-based tools. Combined with other sources of health data, AI is expected to provide new insights to inform the clinical care of sleep disorders and advance our understanding of the integral role sleep plays in human health. Additionally, AI has the potential to streamline day-to-day operations and therefore optimize direct patient care by the sleep disorders team. However, clinicians, scientists, and other stakeholders must develop best practices to integrate this rapidly evolving technology into our daily work while maintaining the highest degree of quality and transparency in health care and research. Ultimately, when harnessed appropriately in conjunction with human expertise, AI will improve the practice of sleep medicine and further sleep science for the health and well-being of our patients. 
  |  https://doi.org/10.5664/jcsm.8388  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32065113/  |  
------------------------------------------- 
10.1002/jor.24617  |   Identifying the design of a failed implant is a key step in the preoperative planning of revision total joint arthroplasty. Manual identification of the implant design from radiographic images is time-consuming and prone to error. Failure to identify the implant design preoperatively can lead to increased operating room time, more complex surgery, increased blood loss, increased bone loss, increased recovery time, and overall increased healthcare costs. In this study, we present a novel, fully automatic and interpretable approach to identify the design of total hip replacement (THR) implants from plain radiographs using deep convolutional neural network (CNN). CNN achieved 100% accuracy in the identification of three commonly used THR implant designs. Such CNN can be used to automatically identify the design of a failed THR implant preoperatively in just a few seconds, saving time and improving the identification accuracy. This can potentially improve patient outcomes, free practitioners' time, and reduce healthcare costs. 
  |  https://doi.org/10.1002/jor.24617  |  
------------------------------------------- 
10.1186/s13023-020-01374-z  |    Introduction:  Rare diseases affect approximately 350 million people worldwide. Delayed diagnosis is frequent due to lack of knowledge of most clinicians and a small number of expert centers. Consequently, computerized diagnosis support systems have been developed to address these issues, with many relying on rare disease expertise and taking advantage of the increasing volume of generated and accessible health-related data. Our objective is to perform a review of all initiatives aiming to support the diagnosis of rare diseases. 
  Methods:  A scoping review was conducted based on methods proposed by Arksey and O'Malley. A charting form for relevant study analysis was developed and used to categorize data. 
  Results:  Sixty-eight studies were retained at the end of the charting process. Diagnosis targets varied from 1 rare disease to all rare diseases. Material used for diagnosis support consisted mostly of phenotype concepts, images or fluids. Fifty-seven percent of the studies used expert knowledge. Two-thirds of the studies relied on machine learning algorithms, and one-third used simple similarities. Manual algorithms were encountered as well. Most of the studies presented satisfying performance of evaluation by comparison with references or with external validation. Fourteen studies provided online tools, most of which aimed to support the diagnosis of all rare diseases by considering queries based on phenotype concepts. 
  Conclusion:  Numerous solutions relying on different materials and use of various methodologies are emerging with satisfying preliminary results. However, the variability of approaches and evaluation processes complicates the comparison of results. Efforts should be made to adequately validate these tools and guarantee reproducibility and explicability. 
  |  https://ojrd.biomedcentral.com/articles/10.1186/s13023-020-01374-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32299466/  |  
------------------------------------------- 
10.1002/jmri.27078  |   Artificial intelligence (AI) shows tremendous promise in the field of medical imaging, with recent breakthroughs applying deep-learning models for data acquisition, classification problems, segmentation, image synthesis, and image reconstruction. With an eye towards clinical applications, we summarize the active field of deep-learning-based MR image reconstruction. We review the basic concepts of how deep-learning algorithms aid in the transformation of raw k-space data to image data, and specifically examine accelerated imaging and artifact suppression. Recent efforts in these areas show that deep-learning-based algorithms can match and, in some cases, eclipse conventional reconstruction methods in terms of image quality and computational efficiency across a host of clinical imaging applications, including musculoskeletal, abdominal, cardiac, and brain imaging. This article is an introductory overview aimed at clinical radiologists with no experience in deep-learning-based MR image reconstruction and should enable them to understand the basic concepts and current clinical applications of this rapidly growing area of research across multiple organ systems. 
  |  https://doi.org/10.1002/jmri.27078  |  
------------------------------------------- 
10.1080/17460441.2020.1696307  |   <b>Introduction</b>: Even though there have been substantial advances in our understanding of biological systems, research in drug discovery is only just now beginning to utilize this type of information. The single-target paradigm, which exemplifies the reductionist approach, remains a mainstay of drug research today. A deeper view of the complexity involved in drug discovery is necessary to advance on this field.<b>Areas covered</b>: This perspective provides a summary of research areas where cheminformatics has played a key role in drug discovery, including of the available resources as well as a personal perspective of the challenges still faced in the field.<b>Expert opinion</b>: Although great strides have been made in the handling and analysis of biological and pharmacological data, more must be done to link the data to biological pathways. This is crucial if one is to understand how drugs modify disease phenotypes, although this will involve a shift from the single drug/single target paradigm that remains a mainstay of drug research. Moreover, such a shift would require an increased awareness of the role of physiology in the mechanism of drug action, which will require the introduction of new mathematical, computer, and biological methods for chemoinformaticians to be trained in. 
  |  http://www.tandfonline.com/doi/full/10.1080/17460441.2020.1696307  |  
------------------------------------------- 
10.1042/EBC20190047  |   Liquid chromatography-tandem mass spectrometry (LC-MS/MS) provides a high sensitivity, high specificity multiplexed method for concurrent detection of adducts formed by protein glycation, oxidation and nitration, also called AGEomics. Combined with stable isotopic dilution analysis, it provides for robust quantitation of protein glycation, oxidation and nitration adduct analytes. It is the reference method for such measurements. LC-MS/MS has been used to measure glycated, oxidized and nitrated amino acids - also called glycation, oxidation and nitration free adducts, with a concurrent quantitation of the amino acid metabolome in physiological fluids. Similar adduct residues in proteins may be quantitated with prior exhaustive enzymatic hydrolysis. It has also been applied to quantitation of other post-translation modifications, such as citrullination and formation of Nε-(γ-glutamyl)lysine crosslink by transglutaminases. Application to cellular and extracellular proteins gives estimates of the steady-state levels of protein modification by glycation, oxidation and nitration, and measurement of the accumulation of glycation, oxidation and nitration adducts in cell culture medium and urinary excretion gives an indication of flux of adduct formation. Measurement of glycation, oxidation and nitration free adducts in plasma and urine provides for estimates of renal clearance of free adducts. Diagnostic potential in clinical studies has been enhanced by the combination of estimates of multiple adducts in optimized diagnostic algorithms by machine learning. Recent applications have been in early-stage detection of metabolic, vascular and renal disease, and arthritis, metabolic control and risk of developing vascular complication in diabetes, and a blood test for autism. 
  |  https://portlandpress.com/essaysbiochem/article-lookup/doi/10.1042/EBC20190047  |  
------------------------------------------- 
10.1080/14740338.2020.1711882  |   <b>Objectives</b>: This study aimed to compare the risk of fractures, acute myocardial infarction, atrial fibrillation, and ventricular arrhythmia among Danish citizens aged ≥ 65 which were new users of promethazine or domperidone, triazolam, loratadine, and betahistine. Secondly, the study aimed to perform a risk stratification to identify the most relevant predictors for the study outcomes.<b>Methods</b>: The study period was 01/01/2015 to 31/12/2016. The data sources were the Danish registers. Each patient was followed for 90 days. A logistic regression model was used to compute the unadjusted and adjusted odds ratios (OR), and a conditional inference tree was used to identify the most relevant predictors for the study outcomes.<b>Results</b>: Promethazine had a higher risk of hospitalization for atrial fibrillation than loratadine and betahistine (OR 1.58; 95% CI 1.07-2.63 and OR 3.22; 95% CI 1.69-7.14, respectively). For fractures, acute myocardial infarction, and ventricular arrhythmia hospitalizations, no statistically significant differences were found among drugs under investigation. The medical history of cardiac arrhythmia (OR 4.14; 95% CI 2.94-5.78, p &lt; 0.0001) was the most relevant predictor for atrial fibrillation hospitalizations.<b>Conclusion</b>: This study found an increased risk of atrial fibrillation hospitalization among promethazine users, and the risk was higher among patients with prior cardiac arrhythmia. 
  |  http://www.tandfonline.com/doi/full/10.1080/14740338.2020.1711882  |  
------------------------------------------- 
10.4132/jptm.2019.12.31  |   Digital pathology (DP) is no longer an unfamiliar term for pathologists, but it is still difficult for many pathologists to understand the engineering and mathematics concepts involved in DP. Computer-aided pathology (CAP) aids pathologists in diagnosis. However, some consider CAP a threat to the existence of pathologists and are skeptical of its clinical utility. Implementation of DP is very burdensome for pathologists because technical factors, impact on workflow, and information technology infrastructure must be considered. In this paper, various terms related to DP and computer-aided pathologic diagnosis are defined, current applications of DP are discussed, and various issues related to implementation of DP are outlined. The development of computer-aided pathologic diagnostic tools and their limitations are also discussed. 
  |  https://dx.doi.org/10.4132/jptm.2019.12.31  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32045965/  |  
------------------------------------------- 
10.1016/j.artmed.2020.101805  |   Breast cancer is the most prevalent invasive type of cancer among women. The mortality rate of the disease can be reduced considerably through timely prognosis and felicitous treatment planning, by utilizing the computer aided detection and diagnosis techniques. With the advent of whole slide image (WSI) scanners for digitizing the histopathological tissue samples, there is a drastic increase in the availability of digital histopathological images. However, these samples are often unlabeled and hence they need labeling to be done through manual annotations by domain experts and experienced pathologists. But this annotation process required for acquiring high quality large labeled training set for nuclear atypia scoring is a tedious, expensive and time consuming job. Active learning techniques have achieved widespread acceptance in reducing this human effort in annotating the data samples. In this paper, we explore the possibilities of active learning on nuclear pleomorphism scoring over a non-Euclidean framework, the Riemannian manifold. Active learning technique adopted for the cancer grading is in the batch-mode framework, that adaptively identifies the apt batch size along with the batch of instances to be queried, following a submodular optimization framework. Samples for annotation are selected considering the diversity and redundancy between the pair of samples, based on the kernelized Riemannian distance measures such as log-Euclidean metrics and the two Bregman divergences - Stein and Jeffrey divergences. Results of the adaptive Batch Mode Active Learning on the Riemannian metric show a superior performance when compared with the state-of-the-art techniques for breast cancer nuclear pleomorphism scoring, as it makes use of the information from the unlabeled samples. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0933-3657(19)30696-7  |  
------------------------------------------- 
10.3348/kjr.2019.0752  |   Radiomics and deep learning have recently gained attention in the imaging assessment of various liver diseases. Recent research has demonstrated the potential utility of radiomics and deep learning in staging liver fibroses, detecting portal hypertension, characterizing focal hepatic lesions, prognosticating malignant hepatic tumors, and segmenting the liver and liver tumors. In this review, we outline the basic technical aspects of radiomics and deep learning and summarize recent investigations of the application of these techniques in liver disease. 
  |  https://www.kjronline.org/DOIx.php?id=10.3348/kjr.2019.0752  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32193887/  |  
------------------------------------------- 
10.3390/s20051500  |   This article presents the novel Python, C# and JavaScript libraries of Node Primitives (NEP), a high-level, open, distributed, and component-based framework designed to enable easy development of cross-platform software architectures. NEP is built on top of low-level, high-performance and robust sockets libraries (ZeroMQ and Nanomsg) and robot middlewares (ROS 1 and ROS 2). This enables platform-independent development of Human-Robot Interaction (HRI) software architectures. We show minimal code examples for enabling <i>Publish/Subscribe</i> communication between Internet of Things (IoT) and Robotics modules. Two user cases performed outside laboratories are briefly described in order to prove the technological feasibility of NEP for developing real-world applications. The first user case briefly shows the potential of using NEP for enabling the creation of End-User Development (EUD) interfaces for IoT-aided Human-Robot Interaction. The second user case briefly describes a software architecture integrating state-of-art sensory devices, deep learning perceptual modules, and a ROS -based humanoid robot to enable IoT-aided HRI in a public space. Finally, a comparative study showed better latency results of NEP over a popular state-of-art tool (ROS using rosbridge) for connecting different nodes executed in local-host and local area network (LAN). 
  |  http://www.mdpi.com/resolver?pii=s20051500  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32182906/  |  
------------------------------------------- 
10.3389/fnut.2020.00044  |   Nutrition plays a vital role in health and the recovery process. Deficiencies in macronutrients and micronutrients can impact the development and progression of various disorders. However, malnutrition screening tools and their utility in the clinical setting remain largely understudied. In this study, we summarize the importance of nutritional adequacy and its association with neurological, cardiovascular, and immune-related disorders. We also examine general and specific malnutrition assessment tools utilized in healthcare settings. Since the implementation of the screening process in 2016, malnutrition data from hospitalized patients in the Geisinger Health System is presented and discussed as a case study. Clinical data from five Geisinger hospitals shows that ~10% of all admitted patients are acknowledged for having some form of nutritional deficiency, from which about 60-80% of the patients are targeted for a more comprehensive assessment. Finally, we conclude that with a reflection on how technological advances, specifically machine learning-based algorithms, can be integrated into electronic health records to provide decision support system to care providers in the identification and management of patients at higher risk of malnutrition. 
  |  https://doi.org/10.3389/fnut.2020.00044  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32351968/  |  
------------------------------------------- 
10.1007/s00420-020-01525-6  |    Purpose:  There is increasing interest in the use of heart rate variability (HRV) as an objective measurement of mental stress in the surgical setting. To identify areas of improvement, the aim of our study was to review current use of HRV measurements in the surgical setting, evaluate the different methods used for the analysis of HRV, and to assess whether HRV is being measured correctly. 
  Methods:  A systematic review was performed according to the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). 17 studies regarding HRV as a measurement of mental stress in the surgical setting were included and analysed. 
  Results:  24% of the studies performed long-term measurements (24 h and longer) to assess the long-term effects of and recovery from mental stress. In 24% of the studies, artefact correction took place. 
  Conclusions:  HRV showed to be a good objective assessment method of stress induced in the workplace environment: it was able to pinpoint stressors during operations, determine which operating techniques induced most stress for surgeons, and indicate differences in stress levels between performing and assisting surgery. For future research, this review recommends using singular guidelines to standardize research, and performing artefact correction. This will improve further evaluation of the long-term effects of mental stress and its recovery. 
  |  https://dx.doi.org/10.1007/s00420-020-01525-6  |  
------------------------------------------- 
10.1007/s00256-020-03429-5  |   Greulich and Pyle (GP) is one of the most common methods to determine bone age from hand radiographs. In recent years, new methods were developed to increase the efficiency in bone age analysis like the shorthand bone age (SBA) and automated artificial intelligence algorithms. 
  Objective:  The aim of this study is to evaluate the accuracy and reliability of these two methods and examine if the reduction in analysis time compromises their efficacy. 
  Methods:  Two hundred thirteen males and 213 females had their bone age determined by two separate raters using the SBA and GP methods. Three weeks later, the two raters repeated the analysis of the radiographs. The raters timed themselves using an online stopwatch. De-identified radiographs were securely uploaded to an automated algorithm developed by a group of radiologists in Toronto. The gold standard was determined to be the radiology report attached to each radiograph, written by experienced radiologists using GP. 
  Results:  Intraclass correlation between each method and the gold standard fell within the range of 0.8-0.9, highlighting significant agreement. Most of the comparisons showed a statistically significant difference between the new methods and the gold standard; however, it may not be clinically significant as it ranges between 0.25 and 0.5 years. A bone age is considered clinically abnormal if it falls outside 2 standard deviations of the chronological age; standard deviations are calculated and provided in GP atlas. 
  Conclusion:  The shorthand bone age method and the automated algorithm produced values that are in agreement with the gold standard while reducing analysis time. 
  |  https://dx.doi.org/10.1007/s00256-020-03429-5  |  
------------------------------------------- 
10.1007/s10278-019-00271-7  |   While radiologists regularly issue follow-up recommendations, our preliminary research has shown that anywhere from 35 to 50% of patients who receive follow-up recommendations for findings of possible cancer on abdominopelvic imaging do not return for follow-up. As such, they remain at risk for adverse outcomes related to missed or delayed cancer diagnosis. In this study, we develop an algorithm to automatically detect free text radiology reports that have a follow-up recommendation using natural language processing (NLP) techniques and machine learning models. The data set used in this study consists of 6000 free text reports from the author's institution. NLP techniques are used to engineer 1500 features, which include the most informative unigrams, bigrams, and trigrams in the training corpus after performing tokenization and Porter stemming. On this data set, we train naive Bayes, decision tree, and maximum entropy models. The decision tree model, with an F1 score of 0.458 and accuracy of 0.862, outperforms both the naive Bayes (F1 score of 0.381) and maximum entropy (F1 score of 0.387) models. The models were analyzed to determine predictive features, with term frequency of n-grams such as "renal neoplasm" and "evalu with enhanc" being most predictive of a follow-up recommendation. Key to maximizing performance was feature engineering that extracts predictive information and appropriate selection of machine learning algorithms based on the feature set. 
  |  https://doi.org/10.1007/s10278-019-00271-7  |  
------------------------------------------- 
10.2214/AJR.19.22074  |   <b>OBJECTIVE.</b> This study evaluated the utility of a deep learning method for determining whether a small (≤ 4 cm) solid renal mass was benign or malignant on multiphase contrast-enhanced CT. <b>MATERIALS AND METHODS.</b> This retrospective study included 1807 image sets from 168 pathologically diagnosed small (≤ 4 cm) solid renal masses with four CT phases (unenhanced, corticomedullary, nephrogenic, and excretory) in 159 patients between 2012 and 2016. Masses were classified as malignant (<i>n</i> = 136) or benign (<i>n</i> = 32). The dataset was randomly divided into five subsets: four were used for augmentation and supervised training (48,832 images), and one was used for testing (281 images). The Inception-v3 architecture convolutional neural network (CNN) model was used. The AUC for malignancy and accuracy at optimal cutoff values of output data were evaluated in six different CNN models. Multivariate logistic regression analysis was also performed. <b>RESULTS.</b> Malignant and benign lesions showed no significant difference of size. The AUC value of corticomedullary phase was higher than that of other phases (corticomedullary vs excretory, <i>p</i> = 0.022). The highest accuracy (88%) was achieved in corticomedullary phase images. Multivariate analysis revealed that the CNN model of corticomedullary phase was a significant predictor for malignancy compared with other CNN models, age, sex, and lesion size. <b>CONCLUSION.</b> A deep learning method with a CNN allowed acceptable differentiation of small (≤ 4 cm) solid renal masses in dynamic CT images, especially in the corticomedullary image model. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.22074  |  
------------------------------------------- 
10.1002/lary.28508  |    Objectives/hypothesis:  Machine learning (ML) is a type of artificial intelligence wherein a computer learns patterns and associations between variables to correctly predict outcomes. The objectives of this study were to 1) use a ML platform to identify factors important in predicting surgical complications in patients undergoing head and neck free tissue transfer, and 2) compare ML outputs to traditionally employed logistic regression models. 
  Study design:  Retrospective cohort study. 
  Methods:  Using a dataset of 364 consecutive patients who underwent head and neck microvascular free tissue transfer at a single institution, 14 clinicopathologic characteristics were analyzed using a supervised ML algorithm of ensemble decision trees to predict surgical complications. The relative importance values of each variable in the ML analysis were then compared to logistic regression models. 
  Results:  There were 166 surgical complications, which included bleeding or hematoma in 30 patients (8.2%), fistulae in 25 patients (6.9%), and infection or dehiscence in 52 patients (14.4%). There were 59 take-backs (16.2%), and six total (1.6%) and five partial (1.4%) flap failures. ML models were able to correctly classify outcomes with an accuracy of 65% to 75%. Factors that were identified in ML analyses as most important for predicting complications included institutional experience, flap ischemia time, age, and smoking pack-years. In contrast, the significant factors most frequently identified in traditional logistic regression analyses were patient age (P = .03), flap type (P = .03), and primary site of reconstruction (P = .06). 
  Conclusions:  In this single-institution dataset, ML algorithms identified factors for predicting complications after free tissue transfer that were distinct from traditional regression models. 
  Level of evidence:  2c Laryngoscope, 2020. 
  |  https://doi.org/10.1002/lary.28508  |  
------------------------------------------- 
10.1016/j.acra.2020.01.014  |    Rationale and objectives:  To assess if vessel suppression (VS) improves nodule detection rate, interreader agreement, and reduces reading time in oncologic chest computed tomography (CT). 
  Material and methods:  One-hundred consecutive oncologic patients (65 male; median age 60y) who underwent contrast-enhanced chest CT were retrospectively included. For all exams, additional VS series (ClearRead CT, Riverrain Technologies, Miamisburg) were reconstructed. Two groups of three radiologists each with matched experience were defined. Each group evaluated the SD-CT as well as VS-CT. Each reader marked the presence, size, and position of pulmonary nodules and documented reading time. In addition, for the VS-CT the presence of false positive nodules had to be stated. Cohen's Kappa (k) was used to calculate the interreader-agreement between groups. Reading time was compared using paired t test. 
  Results:  Nodule detection rate was significantly higher in VS-CT compared to the SD-CT (+21%; p &lt;0.001). Interreader-agreement was higher in the VS-CT (k = 0.431, moderate agreement) compared to SD-CT (k = 0.209, fair agreement). Almost all VS-CT series had false positive findings (97-99 out of 100). Average reading time was significantly shorter in the VS-CT compared to the SD-CT (154 ± 134vs. 194 ± 126; 21%, p&lt;0.001). 
  Conclusions:  Vessel suppression increases nodule detection rate, improves interreader agreement, and reduces reading time in chest CT of oncologic patients. Due to false positive results a consensus reading with the SD-CT is essential. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30041-6  |  
------------------------------------------- 
10.1016/j.jacr.2019.12.015  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(19)31478-4  |  
------------------------------------------- 
10.2147/OPTH.S235751  |    Purpose:  To assess the performance of deep learning algorithms for different tasks in retinal fundus images: (1) detection of retinal fundus images versus optical coherence tomography (OCT) or other images, (2) evaluation of good quality retinal fundus images, (3) distinction between right eye (OD) and left eye (OS) retinal fundus images,(4) detection of age-related macular degeneration (AMD) and (5) detection of referable glaucomatous optic neuropathy (GON). 
  Patients and methods:  Five algorithms were designed. Retrospective study from a database of 306,302 images, Optretina's tagged dataset. Three different ophthalmologists, all retinal specialists, classified all images. The dataset was split per patient in a training (80%) and testing (20%) splits. Three different CNN architectures were employed, two of which were custom designed to minimize the number of parameters with minimal impact on its accuracy. Main outcome measure was area under the curve (AUC) with accuracy, sensitivity and specificity. 
  Results:  Determination of retinal fundus image had AUC of 0.979 with an accuracy of 96% (sensitivity 97.7%, specificity 92.4%). Determination of good quality retinal fundus image had AUC of 0.947, accuracy 91.8% (sensitivity 96.9%, specificity 81.8%). Algorithm for OD/OS had AUC 0.989, accuracy 97.4%. AMD had AUC of 0.936, accuracy 86.3% (sensitivity 90.2% specificity 82.5%), GON had AUC of 0.863, accuracy 80.2% (sensitivity 76.8%, specificity 83.8%). 
  Conclusion:  Deep learning algorithms can differentiate a retinal fundus image from other images. Algorithms can evaluate the quality of an image, discriminate between right or left eye and detect the presence of AMD and GON with a high level of accuracy, sensitivity and specificity. 
  |  https://dx.doi.org/10.2147/OPTH.S235751  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32103888/  |  
------------------------------------------- 
10.1200/JCO.20.00402  |    |  http://ascopubs.org/doi/full/10.1200/JCO.20.00402?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/s20030633  |   Computational intelligence is a very active and fruitful research of artificial intelligence with a broad spectrum of applications. Remote sensing data has been a salient field of application of computational intelligence algorithms, both for the exploitation of the data and for the research/development of new data analysis tools. In this editorial paper we provide the setting of the special issue "Computational Intelligence in Remote Sensing" and an overview of the published papers. The 11 accepted and published papers cover a wide spectrum of applications and computational tools that we try to summarize and put in perspective in this editorial paper. 
  |  http://www.mdpi.com/resolver?pii=s20030633  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31979240/  |  
------------------------------------------- 
10.2196/16235  |    Background:  Previous research suggests that artificial agents may be a promising source of social support for humans. However, the bulk of this research has been conducted in the context of social support interventions that specifically address stressful situations or health improvements. Little research has examined social support received from artificial agents in everyday contexts. 
  Objective:  Considering that social support manifests in not only crises but also everyday situations and that everyday social support forms the basis of support received during more stressful events, we aimed to investigate the types of everyday social support that can be received from artificial agents. 
  Methods:  In Study 1, we examined publicly available user reviews (N=1854) of Replika, a popular companion chatbot. In Study 2, a sample (n=66) of Replika users provided detailed open-ended responses regarding their experiences of using Replika. We conducted thematic analysis on both datasets to gain insight into the kind of everyday social support that users receive through interactions with Replika. 
  Results:  Replika provides some level of companionship that can help curtail loneliness, provide a "safe space" in which users can discuss any topic without the fear of judgment or retaliation, increase positive affect through uplifting and nurturing messages, and provide helpful information/advice when normal sources of informational support are not available. 
  Conclusions:  Artificial agents may be a promising source of everyday social support, particularly companionship, emotional, informational, and appraisal support, but not as tangible support. Future studies are needed to determine who might benefit from these types of everyday social support the most and why. These results could potentially be used to help address global health issues or other crises early on in everyday situations before they potentially manifest into larger issues. 
  |  https://www.jmir.org/2020/3/e16235/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32141837/  |  
------------------------------------------- 
10.1093/bioinformatics/btaa274  |    Summary:  Recently, novel machine-learning algorithms have shown potential for predicting undiscovered links in biomedical knowledge networks. However, dedicated benchmarks for measuring algorithmic progress have not yet emerged. With OpenBioLink, we introduce a large-scale, high-quality and highly challenging biomedical link prediction benchmark to transparently and reproducibly evaluate such algorithms. Furthermore, we present preliminary baseline evaluation results. 
  Availability and implementation:  Source code and data are openly available at https://github.com/OpenBioLink/OpenBioLink. 
  Supplementary information:  Supplementary data are available at Bioinformatics online. 
  |  https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btaa274  |  
------------------------------------------- 
10.1093/jamia/ocz211  |    Objectives:  Current machine learning models aiming to predict sepsis from electronic health records (EHR) do not account 20 for the heterogeneity of the condition despite its emerging importance in prognosis and treatment. This work demonstrates the added value of stratifying the types of organ dysfunction observed in patients who develop sepsis in the intensive care unit (ICU) in improving the ability to recognize patients at risk of sepsis from their EHR data. 
  Materials and methods:  Using an ICU dataset of 13 728 records, we identify clinically significant sepsis subpopulations with distinct organ dysfunction patterns. We perform classification experiments with random forest, gradient boost trees, and support vector machines, using the identified subpopulations to distinguish patients who develop sepsis in the ICU from those who do not. 
  Results:  The classification results show that features selected using sepsis subpopulations as background knowledge yield a superior performance in distinguishing septic from non-septic patients regardless of the classification model used. The improved performance is especially pronounced in specificity, which is a current bottleneck in sepsis prediction machine learning models. 
  Conclusion:  Our findings can steer machine learning efforts toward more personalized models for complex conditions including sepsis. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz211  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31951005/  |  
------------------------------------------- 
10.1002/med.21658  |   Discovery and development of biopeptides are time-consuming, laborious, and dependent on various factors. Data-driven computational methods, especially machine learning (ML) approach, can rapidly and efficiently predict the utility of therapeutic peptides. ML methods offer an array of tools that can accelerate and enhance decision making and discovery for well-defined queries with ample and sophisticated data quality. Various ML approaches, such as support vector machines, random forest, extremely randomized tree, and more recently deep learning methods, are useful in peptide-based drug discovery. These approaches leverage the peptide data sets, created via high-throughput sequencing and computational methods, and enable the prediction of functional peptides with increased levels of accuracy. The use of ML approaches in the development of peptide-based therapeutics is relatively recent; however, these techniques are already revolutionizing protein research by unraveling their novel therapeutic peptide functions. In this review, we discuss several ML-based state-of-the-art peptide-prediction tools and compare these methods in terms of their algorithms, feature encodings, prediction scores, evaluation methodologies, and software utilities. We also assessed the prediction performance of these methods using well-constructed independent data sets. In addition, we discuss the common pitfalls and challenges of using ML approaches for peptide therapeutics. Overall, we show that using ML models in peptide research can streamline the development of targeted peptide therapies. 
  |  https://doi.org/10.1002/med.21658  |  
------------------------------------------- 
10.1371/journal.pone.0228078  |   Leaf color is an important agronomic trait in flowering plants, including orchids. However, factors underlying leaf phenotypes in plants remain largely unclear. A mutant displaying yellow leaves was obtained by the γ-ray-based mutagenesis of a Cymbidium orchid and characterized using RNA sequencing. A total of 144,918 unigenes obtained from over 25 million reads were assigned to 22 metabolic pathways in the Kyoto Encyclopedia of Genes and Genomes database. In addition, gene ontology was used to classify the predicted functions of transcripts into 73 functional groups. The RNA sequencing analysis identified 2,267 differentially expressed genes between wild-type and mutant Cymbidium sp. Genes involved in the chlorophyll biosynthesis and degradation, as well as ion transport, were identified and assayed for their expression levels in wild-type and mutant plants using quantitative real-time profiling. No critical expression changes were detected in genes involved in chlorophyll biosynthesis. In contrast, seven genes involved in ion transport, including two metal ion transporters, were down-regulated, and chlorophyllase 2, associated with chlorophyll degradation, was up-regulated. Together, these results suggest that alterations in chlorophyll metabolism and/or ion transport might contribute to leaf color in Cymbidium orchids. 
  |  http://dx.plos.org/10.1371/journal.pone.0228078  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31995594/  |  
------------------------------------------- 
10.1097/CCM.0000000000004144  |    |  https://dx.doi.org/10.1097/CCM.0000000000004144  |  
------------------------------------------- 
10.3171/2019.10.JNS191400  |    Objective:  Cushing's disease (CD) involves brain impairments caused by excessive cortisol. Whether these impairments are reversible in remitted CD after surgery has long been controversial due to a lack of high-quality longitudinal studies. In this study the authors aimed to assess the reversibility of whole-brain changes in remitted CD after transsphenoidal surgery (TSS), and its correlations with clinical and hormonal parameters, in the largest longitudinal study cohort to date for CD patient brain analysis. 
  Methods:  Fifty patients with pathologically diagnosed CD and 36 matched healthy controls (HCs) were enrolled in a tertiary comprehensive hospital and national pituitary disease registry center in China. 3-T MRI studies were analyzed using an artificial intelligence-assisted web-based autosegmentation tool to quantify 3D brain volumes. Clinical parameters as well as levels of serum cortisol, adrenocorticotrophic hormone (ACTH), and 24-hour urinary free cortisol were collected for the correlation analysis. All CD patients underwent TSS and 46 patients achieved remission. All clinical, hormonal, and MRI parameters were reevaluated at the 3-month follow-up after surgery. 
  Results:  Widespread brain volume loss was observed in active CD patients compared with HCs, including total gray matter (p = 0.003, with false discovery rate [FDR] correction) and the frontal, parietal, occipital, and temporal lobes; insula; cingulate lobe; and enlargement of lateral and third ventricles (p &lt; 0.05, corrected with FDR). All affected brain regions improved significantly after TSS (p &lt; 0.05, corrected with FDR). In patients with remitted CD, total gray matter and most brain regions (except the frontal and temporal lobes) showed full recovery of volume, with volumes that did not differ from those of HCs (p &gt; 0.05, corrected with FDR). ACTH and serum cortisol changes were negatively correlated with brain volume changes during recovery (p &lt; 0.05). 
  Conclusions:  This study demonstrates the rapid reversal of total gray matter loss in remitted CD. The combination of full recovery areas and partial recovery areas after TSS is consistent with the incomplete recovery of memory and cognitive function observed in CD patients in clinical practice. Correlation analyses suggest that ACTH and serum cortisol levels are reliable serum biomarkers of brain recovery for clinical use after surgery. 
  |  https://thejns.org/doi/10.3171/2019.10.JNS191400  |  
------------------------------------------- 
10.2471/BLT.19.237503  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284656/  |  
------------------------------------------- 
10.1002/ase.1936  |    |  https://doi.org/10.1002/ase.1936  |  
------------------------------------------- 
10.1016/j.athoracsur.2020.02.074  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0003-4975(20)30509-9  |  
------------------------------------------- 
10.1245/s10434-019-08182-1  |    |  https://dx.doi.org/10.1245/s10434-019-08182-1  |  
------------------------------------------- 
10.1017/ice.2020.103  |    |  https://www.cambridge.org/core/product/identifier/S0899823X20001038/type/journal_article  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32238219/  |  
------------------------------------------- 
10.1016/j.eururo.2019.12.006  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0302-2838(19)30945-5  |  
------------------------------------------- 
10.1007/s12032-020-01358-w  |   MRI-guided vacuum-assisted breast biopsy (VABB) is used for suspicious breast cancer (BC) lesions which are detectable only with MRI: because the high sensitivity but limited specificity of breast MRI it is a fundamental tool in breast imaging divisions. We analyse our experience of MRI-guided VABB and critically discuss the potentialities of diffusion-weighted imaging (DWI) and artificial intelligence (AI) in this matter. We retrospectively analysed a population of consecutive women underwent VABB at our tertiary referral BC centre from 01/2011 to 01/2019. Reference standard was histological diagnosis or at least 1-year negative follow-up. McNemar, Mann-Whitney and χ<sup>2</sup> tests at 95% level of significance were used as statistical exams. 217 women (mean age = 52, 18-72 years) underwent MRI-guided VABB; 11 were excluded and 208 MRI-guided VABB lesions were performed: 34/208 invasive carcinomas, 32/208 DCIS, 8/208 LCIS, 3/208 high-risk lesions and 131/208 benign lesions were reported. Accuracy of MRI-guided VABB was 97%. The predictive features for malignancy were mass with irregular shape (OR 8.4; 95% CI 0.59-31.6), size of the lesion (OR 4.4; 95% CI 1.69-9.7) and mass with irregular/spiculated margins (OR 5.4; 95% CI 6.8-31.1). Six-month follow-up showed 4 false-negative cases (1.9%). Invasive BC showed a statistically significant higher hyperintense signal at DWI compared to benign lesions (p = 0.03). No major complications occurred. MR-guided VABB showed high accuracy. Benign-concordant lesions should be followed up with breast MRI in 6-12 months due to the risk of false-negative results. DWI and AI applications showed potential benefit as support tools for radiologists. 
  |  https://dx.doi.org/10.1007/s12032-020-01358-w  |  
------------------------------------------- 
10.3348/kjr.2019.0821  |   Chest X-ray radiography and computed tomography, the two mainstay modalities in thoracic radiology, are under active investigation with deep learning technology, which has shown promising performance in various tasks, including detection, classification, segmentation, and image synthesis, outperforming conventional methods and suggesting its potential for clinical implementation. However, the implementation of deep learning in daily clinical practice is in its infancy and facing several challenges, such as its limited ability to explain the output results, uncertain benefits regarding patient outcomes, and incomplete integration in daily workflow. In this review article, we will introduce the potential clinical applications of deep learning technology in thoracic radiology and discuss several challenges for its implementation in daily clinical practice. 
  |  https://www.kjronline.org/DOIx.php?id=10.3348/kjr.2019.0821  |  
------------------------------------------- 
10.1038/s41598-020-62939-3  |   Multi-agent coordination is prevalent in many real-world applications. However, such coordination is challenging due to its combinatorial nature. An important observation in this regard is that agents in the real world often only directly affect a limited set of neighbouring agents. Leveraging such loose couplings among agents is key to making coordination in multi-agent systems feasible. In this work, we focus on learning to coordinate. Specifically, we consider the multi-agent multi-armed bandit framework, in which fully cooperative loosely-coupled agents must learn to coordinate their decisions to optimize a common objective. We propose multi-agent Thompson sampling (MATS), a new Bayesian exploration-exploitation algorithm that leverages loose couplings. We provide a regret bound that is sublinear in time and low-order polynomial in the highest number of actions of a single agent for sparse coordination graphs. Additionally, we empirically show that MATS outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks, and a novel benchmark with Poisson distributions. An example of a loosely-coupled multi-agent system is a wind farm. Coordination within the wind farm is necessary to maximize power production. As upstream wind turbines only affect nearby downstream turbines, we can use MATS to efficiently learn the optimal control mechanism for the farm. To demonstrate the benefits of our method toward applications we apply MATS to a realistic wind farm control task. In this task, wind turbines must coordinate their alignments with respect to the incoming wind vector in order to optimize power production. Our results show that MATS improves significantly upon state-of-the-art coordination methods in terms of performance, demonstrating the value of using MATS in practical applications with sparse neighbourhood structures. 
  |  http://dx.doi.org/10.1038/s41598-020-62939-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32317732/  |  
------------------------------------------- 
10.1001/jamaophthalmol.2020.0515  |    |  https://jamanetwork.com/journals/jamaophthalmology/fullarticle/10.1001/jamaophthalmol.2020.0515  |  
------------------------------------------- 
10.1093/humrep/deaa013  |    Study question:  Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? 
  Summary answer:  We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. 
  What is known already:  Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. 
  Study design, size, duration:  These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. 
  Participants/materials, setting, methods:  The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists' predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. 
  Main results and the role of chance:  The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was &gt;63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists' accuracy (P = 0.047, n = 2, Student's t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student's t test). 
  Limitations, reasons for caution:  The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. 
  Wider implications of the findings:  These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists' traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. 
  Study funding/competing interest(s):  Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). 'In kind' support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer. 
  |  https://academic.oup.com/humrep/article-lookup/doi/10.1093/humrep/deaa013  |  
------------------------------------------- 
10.1002/jmri.27054  |    |  https://doi.org/10.1002/jmri.27054  |  
------------------------------------------- 
10.1186/s13054-020-2785-y  |   This article is one of ten reviews selected from the Annual Update in Intensive Care and Emergency Medicine 2020. Other selected articles can be found online at https://www.biomedcentral.com/collections/annualupdate2020. Further information about the Annual Update in Intensive Care and Emergency Medicine is available from http://www.springer.com/series/8901. 
  |  https://ccforum.biomedcentral.com/articles/10.1186/s13054-020-2785-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32204716/  |  
------------------------------------------- 
10.1097/BRS.0000000000003233  |    Study design:  Retrospective administrative claims database analysis. 
  Objective:  Identify distinct presurgery health care resource utilization (HCRU) patterns among posterior lumbar spinal fusion patients and quantify their association with postsurgery costs. 
  Summary of background data:  Presurgical HCRU may be predictive of postsurgical economic outcomes and help health care providers to identify patients who may benefit from innovation in care pathways and/or surgical approach. 
  Methods:  Privately insured patients who received one- to two-level posterior lumbar spinal fusion between 2007 and 2016 were identified from a claims database. Agglomerative hierarchical clustering (HC), an unsupervised machine learning technique, was used to cluster patients by presurgery HCRU across 90 resource categories. A generalized linear model was used to compare 2-year postoperative costs across clusters controlling for age, levels fused, spinal diagnosis, posterolateral/interbody approach, and Elixhauser Comorbidity Index. 
  Results:  Among 18,770 patients, 56.1% were female, mean age was 51.3, 79.4% had one-level fusion, and 89.6% had inpatient surgery. Three patient clusters were identified: Clust1 (n = 13,987 [74.5%]), Clust2 (n = 4270 [22.7%]), Clust3 (n = 513 [2.7%]). The largest between-cluster differences were found in mean days supplied for antidepressants (Clust1: 97.1 days, Clust2: 175.2 days, Clust3: 287.1 days), opioids (Clust1: 76.7 days, Clust2: 166.9 days, Clust3: 129.7 days), and anticonvulsants (Clust1: 35.1 days, Clust2: 67.8 days, Clust3: 98.7 days). For mean medical visits, the largest between-cluster differences were for behavioral health (Clust1: 0.14, Clust2: 0.88, Clust3: 16.3) and nonthoracolumbar office visits (Clust1: 7.8, Clust2: 13.4, Clust3: 13.8). Mean (95% confidence interval) adjusted 2-year all-cause postoperative costs were lower for Clust1 ($34,048 [$33,265-$34,84]) versus both Clust2 ($52,505 [$50,306-$54,800]) and Clust3 ($48,452 [$43,007-$54,790]), P &lt; 0.0001. 
  Conclusion:  Distinct presurgery HCRU clusters were characterized by greater utilization of antidepressants, opioids, and behavioral health services and these clusters were associated with significantly higher 2-year postsurgical costs. 
  Level of evidence:  3. 
  |  http://dx.doi.org/10.1097/BRS.0000000000003233  |  
------------------------------------------- 
10.1128/CMR.00057-19  |   SUMMARYClinical microbiology is experiencing revolutionary advances in the deployment of molecular, genome sequencing-based, and mass spectrometry-driven detection, identification, and characterization assays. Laboratory automation and the linkage of information systems for big(ger) data management, including artificial intelligence (AI) approaches, also are being introduced. The initial optimism associated with these developments has now entered a more reality-driven phase of reflection on the significant challenges, complexities, and health care benefits posed by these innovations. With this in mind, the ongoing process of clinical laboratory consolidation, covering large geographical regions, represents an opportunity for the efficient and cost-effective introduction of new laboratory technologies and improvements in translational research and development. This will further define and generate the mandatory infrastructure used in validation and implementation of newer high-throughput diagnostic approaches. Effective, structured access to large numbers of well-documented biobanked biological materials from networked laboratories will release countless opportunities for clinical and scientific infectious disease research and will generate positive health care impacts. We describe why consolidation of clinical microbiology laboratories will generate quality benefits for many, if not most, aspects of the services separate institutions already provided individually. We also define the important role of innovative and large-scale diagnostic platforms. Such platforms lend themselves particularly well to computational (AI)-driven genomics and bioinformatics applications. These and other diagnostic innovations will allow for better infectious disease detection, surveillance, and prevention with novel translational research and optimized (diagnostic) product and service development opportunities as key results. 
  |  http://cmr.asm.org/cgi/pmidlookup?view=long&pmid=32102900  |  
------------------------------------------- 
10.21037/gs.2019.12.23  |    Background:  In recent years well-recognized scientific societies introduced guidelines for ultrasound (US) malignancy risk stratification of thyroid nodules. These guidelines categorize the risk of malignancy in relation to a combination of several US features. Based on these US image lexicons an US-based computer-aided diagnosis (CAD) systems were developed. Nevertheless, their clinical utility has not been evaluated in any study of surgeon-performed office US of the thyroid. Hence, the aim of this pilot study was to validate s-Detect<sup>TM</sup> mode in semi-automated US classification of thyroid lesions during surgeon-performed office US. 
  Methods:  This is a prospective study of 50 patients who underwent surgeon-performed thyroid US (basic US skills without CAD <i>vs.</i> with CAD <i>vs.</i> expert US skills without CAD) in the out-patient office as part of the preoperative workup. The real-time CAD system software using artificial intelligence (S-Detect<sup>TM</sup> for Thyroid; Samsung Medison Co.) was integrated into the RS85 US system. Primary outcome was CAD system added-value to the surgeon-performed office US evaluation. Secondary outcomes were: diagnostic accuracy of CAD system, intra and interobserver variability in the US assessment of thyroid nodules. Surgical pathology report was used to validate the pre-surgical diagnosis. 
  Results:  CAD system added-value to thyroid assessment by a surgeon with basic US skills was equal to 6% (overall accuracy of 82% for evaluation with CAD <i>vs.</i> 76% for evaluation without CAD system; P&lt;0.001), and final diagnosis was different than predicted by US assessment in 3 patients (1 more true-positive and 2 more true-negative results). However, CAD system was inferior to thyroid assessment by a surgeon with expert US skills in 6 patients who had false-positive results (P&lt;0.001). 
  Conclusions:  The sensitivity and negative predictive value of CAD system for US classification of thyroid lesions were similar as surgeon with expert US skills whereas specificity and positive predictive value were significantly inferior but markedly better than judgement of a surgeon with basic US skills alone. 
  |  https://doi.org/10.21037/gs.2019.12.23  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32175248/  |  
------------------------------------------- 
10.1016/j.acra.2019.09.014  |    Rationale and objectives:  Our primary aim was to improve radiology reports by increasing concordance of target lesion measurements with oncology records using radiology preprocessors (RP). Faster notification of incidental actionable findings to referring clinicians and clinical radiologist exam interpretation time savings with RPs quantifying tumor burden were also assessed. 
  Materials and methods:  In this prospective quality improvement initiative, RPs annotated lesions before radiologist interpretation of CT exams. Clinical radiologists then hyperlinked approved measurements into interactive reports during interpretations. RPs evaluated concordance with our tumor measurement radiologist, the determinant of tumor burden. Actionable finding detection and notification times were also deduced. Clinical radiologist interpretation times were calculated from established average CT chest, abdomen, and pelvis interpretation times. 
  Results:  RPs assessed 1287 body CT exams with 812 follow-up CT chest, abdomen, and pelvis studies; 95 (11.7%) of which had 241 verified target lesions. There was improved concordance (67.8% vs. 22.5%) of target lesion measurements. RPs detected 93.1% incidental actionable findings with faster clinician notification by a median time of 1 hour (range: 15 minutes-16 hours). Radiologist exam interpretation times decreased by 37%. 
  Conclusions:  This workflow resulted in three-fold improved target lesion measurement concordance with oncology records, earlier detection and faster notification of incidental actionable findings to referring clinicians, and decreased exam interpretation times for clinical radiologists. These findings demonstrate potential roles for automation (such as AI) to improve report value, worklist prioritization, and patient care. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30447-7  |  
------------------------------------------- 
10.1007/s00330-020-06783-z  |    Objective:  To enhance the positive predictive value (PPV) of chest digital tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics features. 
  Method:  The investigation was carried out within the SOS clinical trial (<a href="http://clinicaltrials.gov/show/NCT03645018" title="See in ClinicalTrials.gov">NCT03645018</a>) for lung cancer screening with DTS. Lung nodules were identified by visual analysis and then classified using the diameter and the radiological aspect of the nodule following lung-RADS. Haralick texture features were extracted from the segmented nodules. Both semantic variables and radiomics features were used to build a predictive model using logistic regression on a subset of variables selected with backward feature selection and using two machine learning: a Random Forest and a neural network with the whole subset of variables. The methods were applied to a train set and validated on a test set where diagnostic accuracy metrics were calculated. 
  Results:  Binary visual analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS classification increased the PPV (0.19) but with an unacceptable low sensitivity (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a low sensitivity (0.30). Neural network demonstrated to be the best predictor with a high PPV (0.95) and a high sensitivity (0.90). 
  Conclusions:  The neural network demonstrated the best PPV. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS. 
  Key points:  • We investigated several approaches to enhance the positive predictive value of chest digital tomosynthesis in the lung cancer detection. • Neural network demonstrated to be the best predictor with a nearly perfect PPV. • Neural network could help radiologists to reduce the number of false positive in DTS. 
  |  https://dx.doi.org/10.1007/s00330-020-06783-z  |  
------------------------------------------- 
10.1002/acm2.12856  |    Purpose:  The purpose of this study was to address the dosimetric accuracy of synthetic computed tomography (sCT) images of patients with brain tumor generated using a modified generative adversarial network (GAN) method, for their use in magnetic resonance imaging (MRI)-only treatment planning for proton therapy. 
  Methods:  Dose volume histogram (DVH) analysis was performed on CT and sCT images of patients with brain tumor for plans generated for intensity-modulated proton therapy (IMPT). All plans were robustly optimized using a commercially available treatment planning system (RayStation, from RaySearch Laboratories) and standard robust parameters reported in the literature. The IMPT plan was then used to compute the dose on CT and sCT images for dosimetric comparison, using RayStation analytical (pencil beam) dose algorithm. We used a second, independent Monte Carlo dose calculation engine to recompute the dose on both CT and sCT images to ensure a proper analysis of the dosimetric accuracy of the sCT images. 
  Results:  The results extracted from RayStation showed excellent agreement for most DVH metrics computed on the CT and sCT for the nominal case, with a mean absolute difference below 0.5% (0.3 Gy) of the prescription dose for the clinical target volume (CTV) and below 2% (1.2 Gy) for the organs at risk (OARs) considered. This demonstrates a high dosimetric accuracy for the generated sCT images, especially in the target volume. The metrics obtained from the Monte Carlo doses mostly agreed with the values extracted from RayStation for the nominal and worst-case scenarios (mean difference below 3%). 
  Conclusions:  This work demonstrated the feasibility of using sCT generated with a GAN-based deep learning method for MRI-only treatment planning of patients with brain tumor in intensity-modulated proton therapy. 
  |  https://doi.org/10.1002/acm2.12856  |  
------------------------------------------- 
10.1093/jamia/ocz229  |    Objective:  Implementation of machine learning (ML) may be limited by patients' right to "meaningful information about the logic involved" when ML influences healthcare decisions. Given the complexity of healthcare decisions, it is likely that ML outputs will need to be understood and trusted by physicians, and then explained to patients. We therefore investigated the association between physician understanding of ML outputs, their ability to explain these to patients, and their willingness to trust the ML outputs, using various ML explainability methods. 
  Materials and methods:  We designed a survey for physicians with a diagnostic dilemma that could be resolved by an ML risk calculator. Physicians were asked to rate their understanding, explainability, and trust in response to 3 different ML outputs. One ML output had no explanation of its logic (the control) and 2 ML outputs used different model-agnostic explainability methods. The relationships among understanding, explainability, and trust were assessed using Cochran-Mantel-Haenszel tests of association. 
  Results:  The survey was sent to 1315 physicians, and 170 (13%) provided completed surveys. There were significant associations between physician understanding and explainability (P &lt; .001), between physician understanding and trust (P &lt; .001), and between explainability and trust (P &lt; .001). ML outputs that used model-agnostic explainability methods were preferred by 88% of physicians when compared with the control condition; however, no particular ML explainability method had a greater influence on intended physician behavior. 
  Conclusions:  Physician understanding, explainability, and trust in ML risk calculators are related. Physicians preferred ML outputs accompanied by model-agnostic explanations but the explainability method did not alter intended physician behavior. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz229  |  
------------------------------------------- 
10.1007/s00426-020-01317-0  |   While artificial agents (AA) such as Artificial Intelligence are being extensively developed, a popular belief that AA will someday surpass human intelligence is growing. The present research examined whether this common belief translates into negative psychological and behavioral consequences when individuals assess that an AA performs better than them on cognitive and intellectual tasks. In two studies, participants were led to believe that an AA performed better or less well than them on a cognitive inhibition task (Study 1) and on an intelligence task (Study 2). Results indicated that being outperformed by an AA increased subsequent participants' performance as long as they did not experience psychological discomfort towards the AA and self-threat. Psychological implications in terms of motivation and potential threat as well as the prerequisite for the future interactions of humans with AAs are further discussed. 
  |  https://dx.doi.org/10.1007/s00426-020-01317-0  |  
------------------------------------------- 
10.2217/pgs-2019-0134  |   Pharmacogenomics (PGx) is one of the core elements of personalized medicine. PGx information reduces the likelihood of adverse drug reactions and optimizes therapeutic efficacy. St Catherine Specialty Hospital in Zagreb/Zabok, Croatia has implemented a personalized patient approach using the RightMed<sup>®</sup> Comprehensive PGx panel of 25 pharmacogenes plus Facor V Leiden, Factor II and MTHFR genes, which is interpreted by a special counseling team to offer the best quality of care. With the advent of significant technological advances comes another challenge: how can we harness the data to inform clinically actionable measures and how can we use it to develop better predictive risk models? We propose to apply the principles artificial intelligence to develop a medication optimization platform to prevent, manage and treat different diseases. 
  |  http://www.futuremedicine.com/doi/full/10.2217/pgs-2019-0134?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1002/lary.28695  |    Objectives/hypothesis:  To determine if an automated vestibular schwannoma (VS) segmentation model has comparable performance to using the greatest linear dimension to detect growth. 
  Study design:  Case-control Study. 
  Methods:  Patients were selected from an internal database who had an initial gadolinium-enhanced T1-weighted magnetic resonance imaging scan and a follow-up scan captured at least 5 months later. Two observers manually segmented the VS to compute volumes, and one observer's segmentations were used to train a convolutional neural network model to automatically segment the VS and determine the volume. The results of automatic segmentation were compared to the observer whose measurements were not used in model development to measure agreement. We then examined the sensitivity, specificity, and area under the receiver-operating characteristic curve (AUC) to compare automated volumetric growth detection versus using the greatest linear dimension. Growth detection determined by the external observer's measurements served as the gold standard. 
  Results:  A total of 65 patients and 130 scans were studied. The automated method of segmentation demonstrated excellent agreement with the observer whose measurements were not used for model development for the initial scan (interclass correlational coefficient [ICC] = 0.995; 95% confidence interval [CI]: 0.991-0.997) and follow-up scan (ICC = 0.960; 95% CI: 0.935-0.975). The automated method of segmentation demonstrated increased sensitivity (72.2% vs. 63.9%), specificity (79.3% vs. 69.0%), and AUC (0.822 vs. 0.701) compared to using the greatest linear dimension for growth detection. 
  Conclusions:  In detecting VS growth, a convolutional neural network model outperformed using the greatest linear dimension, demonstrating a potential application of artificial intelligence methods to VS surveillance. 
  Level of evidence:  4 Laryngoscope, 2020. 
  |  https://doi.org/10.1002/lary.28695  |  
------------------------------------------- 
10.1093/database/baaa010  |   Precision medicine is one of the recent and powerful developments in medical care, which has the potential to improve the traditional symptom-driven practice of medicine, allowing earlier interventions using advanced diagnostics and tailoring better and economically personalized treatments. Identifying the best pathway to personalized and population medicine involves the ability to analyze comprehensive patient information together with broader aspects to monitor and distinguish between sick and relatively healthy people, which will lead to a better understanding of biological indicators that can signal shifts in health. While the complexities of disease at the individual level have made it difficult to utilize healthcare information in clinical decision-making, some of the existing constraints have been greatly minimized by technological advancements. To implement effective precision medicine with enhanced ability to positively impact patient outcomes and provide real-time decision support, it is important to harness the power of electronic health records by integrating disparate data sources and discovering patient-specific patterns of disease progression. Useful analytic tools, technologies, databases, and approaches are required to augment networking and interoperability of clinical, laboratory and public health systems, as well as addressing ethical and social issues related to the privacy and protection of healthcare data with effective balance. Developing multifunctional machine learning platforms for clinical data extraction, aggregation, management and analysis can support clinicians by efficiently stratifying subjects to understand specific scenarios and optimize decision-making. Implementation of artificial intelligence in healthcare is a compelling vision that has the potential in leading to the significant improvements for achieving the goals of providing real-time, better personalized and population medicine at lower costs. In this study, we focused on analyzing and discussing various published artificial intelligence and machine learning solutions, approaches and perspectives, aiming to advance academic solutions in paving the way for a new data-centric era of discovery in healthcare. 
  |  https://academic.oup.com/database/article-lookup/doi/10.1093/database/baaa010  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32185396/  |  
------------------------------------------- 
10.1080/02640414.2019.1708036  |   The aim of the current study was to objectively identify position-specific key performance indicators in professional football that predict out-field players league status. The sample consisted of 966 out-field players who completed the full 90 minutes in a match during the 2008/09 or 2009/10 season in the Football League Championship. Players were assigned to one of three categories (0, 1 and 2) based on where they completed most of their match time in the following season, and then split based on five playing positions. 340 performance, biographical and esteem variables were analysed using a Stepwise Artificial Neural Network approach. The models correctly predicted between 72.7% and 100% of test cases (Mean prediction of models = 85.9%), the test error ranged from 1.0% to 9.8% (Mean test error of models = 6.3%). Variables related to passing, shooting, regaining possession and international appearances were key factors in the predictive models. This is highly significant as objective position-specific predictors of players league status have not previously been published. The method could be used to aid the identification and comparison of transfer targets as part of the due diligence process in professional football. 
  |  None  |  
------------------------------------------- 
10.1371/journal.pgen.1008577  |   Circadian systems provide a fitness advantage to organisms by allowing them to adapt to daily changes of environmental cues, such as light/dark cycles. The molecular mechanism underlying the circadian clock has been well characterized. However, how internal circadian clocks are entrained with regular daily light/dark cycles remains unclear. By collecting and analyzing indirect calorimetry (IC) data from more than 2000 wild-type mice available from the International Mouse Phenotyping Consortium (IMPC), we show that the onset time and peak phase of activity and food intake rhythms are reliable parameters for screening defects of circadian misalignment. We developed a machine learning algorithm to quantify these two parameters in our misalignment screen (SyncScreener) with existing datasets and used it to screen 750 mutant mouse lines from five IMPC phenotyping centres. Mutants of five genes (Slc7a11, Rhbdl1, Spop, Ctc1 and Oxtr) were found to be associated with altered patterns of activity or food intake. By further studying the Slc7a11tm1a/tm1a mice, we confirmed its advanced activity phase phenotype in response to a simulated jetlag and skeleton photoperiod stimuli. Disruption of Slc7a11 affected the intercellular communication in the suprachiasmatic nucleus, suggesting a defect in synchronization of clock neurons. Our study has established a systematic phenotype analysis approach that can be used to uncover the mechanism of circadian entrainment in mice. 
  |  http://dx.plos.org/10.1371/journal.pgen.1008577  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31929527/  |  
------------------------------------------- 
10.1186/s12888-020-02535-x  |    Background:  This study aimed to determine conditional dependence relationships of variables that contribute to psychological vulnerability associated with suicide risk. A Bayesian network (BN) was developed and applied to establish conditional dependence relationships among variables for each individual subject studied. These conditional dependencies represented the different states that patients could experience in relation to suicidal behavior (SB). The clinical sample included 650 mental health patients with mood and anxiety symptomatology. 
  Results:  Mainly indicated that variables within the Bayesian network are part of each patient's state of psychological vulnerability and have the potential to impact such states and that these variables coexist and are relatively stable over time. These results have enabled us to offer a tool to detect states of psychological vulnerability associated with suicide risk. 
  Conclusion:  If we accept that suicidal behaviors (vulnerability, ideation, and suicidal attempts) exist in constant change and are unstable, we can investigate what individuals experience at specific moments to become better able to intervene in a timely manner to prevent such behaviors. Future testing of the tool developed in this study is needed, not only in specialized mental health environments but also in other environments with high rates of mental illness, such as primary healthcare facilities and educational institutions. 
  |  https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-020-02535-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32228548/  |  
------------------------------------------- 
10.3389/fcvm.2020.00017  |   Cardiac magnetic resonance (CMR) imaging is an important tool for the non-invasive assessment of cardiovascular disease. However, CMR suffers from long acquisition times due to the need of obtaining images with high temporal and spatial resolution, different contrasts, and/or whole-heart coverage. In addition, both cardiac and respiratory-induced motion of the heart during the acquisition need to be accounted for, further increasing the scan time. Several undersampling reconstruction techniques have been proposed during the last decades to speed up CMR acquisition. These techniques rely on acquiring less data than needed and estimating the non-acquired data exploiting some sort of prior information. Parallel imaging and compressed sensing undersampling reconstruction techniques have revolutionized the field, enabling 2- to 3-fold scan time accelerations to become standard in clinical practice. Recent scientific advances in CMR reconstruction hinge on the thriving field of artificial intelligence. Machine learning reconstruction approaches have been recently proposed to learn the non-linear optimization process employed in CMR reconstruction. Unlike analytical methods for which the reconstruction problem is explicitly defined into the optimization process, machine learning techniques make use of large data sets to learn the key reconstruction parameters and priors. In particular, deep learning techniques promise to use deep neural networks (DNN) to learn the reconstruction process from existing datasets in advance, providing a fast and efficient reconstruction that can be applied to all newly acquired data. However, before machine learning and DNN can realize their full potentials and enter widespread clinical routine for CMR image reconstruction, there are several technical hurdles that need to be addressed. In this article, we provide an overview of the recent developments in the area of artificial intelligence for CMR image reconstruction. The underlying assumptions of established techniques such as compressed sensing and low-rank reconstruction are briefly summarized, while a greater focus is given to recent advances in dictionary learning and deep learning based CMR reconstruction. In particular, approaches that exploit neural networks as implicit or explicit priors are discussed for 2D dynamic cardiac imaging and 3D whole-heart CMR imaging. Current limitations, challenges, and potential future directions of these techniques are also discussed. 
  |  https://doi.org/10.3389/fcvm.2020.00017  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32158767/  |  
------------------------------------------- 
10.1089/dna.2019.5272  |   Head and neck squamous cell carcinoma (HNSCC) is the sixth malignancy, which is characterized by poor prognosis or high mortality because of the lack of predicting markers. Aberrant cancer pseudogenes have been found predictive for prognosis. We aim to identify a pseudogene-based prognosis signature for HNSCC by machine learning. RNA-seq data were downloaded from The Cancer Genome Atlas, and 700 differentially-expressed pseudogenes were identified. The survival-related pseudogenes were screened through COX-regression analysis, which includes univariate regression, least absolute shrinkage and selection operator regression, and multivariate regression, and a five-pseudogene signature was constructed. The value of prediction for the signature was validated in multiple subgroups in terms of survival. Gene set enrichment analysis (GSEA) and coexpression analysis were used to determine the underlying biological functions. Seven hundred dysregulated pseudogenes were identified, and the five-pseudogene signature can distinguish the low-risk and high-risk patients for both training and testing sets and predicted prognosis with high sensitivity and specificity. Furthermore, the signature was applicable to patients of different genders, ages, stages, and grades. Coexpression analysis revealed that the five-pseudogene is associated with immune system. GSEA showed cancer-related biological process and pathways the five-pseudogene involved in. The five-pseudogene signature is not only a novel marker for prognosis but also a promising signature for monitoring therapeutic schedule. Therefore, our findings may have potential clinical significance. 
  |  https://www.liebertpub.com/doi/full/10.1089/dna.2019.5272?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41581-019-0241-5  |    |  http://dx.doi.org/10.1038/s41581-019-0241-5  |  
------------------------------------------- 
10.1038/s41467-019-13869-w  |   The molecular mechanisms underlying the response to exercise and inactivity are not fully understood. We propose an innovative approach to profile the skeletal muscle transcriptome to exercise and inactivity using 66 published datasets. Data collected from human studies of aerobic and resistance exercise, including acute and chronic exercise training, were integrated using meta-analysis methods (www.metamex.eu). Here we use gene ontology and pathway analyses to reveal selective pathways activated by inactivity, aerobic versus resistance and acute versus chronic exercise training. We identify NR4A3 as one of the most exercise- and inactivity-responsive genes, and establish a role for this nuclear receptor in mediating the metabolic responses to exercise-like stimuli in vitro. The meta-analysis (MetaMEx) also highlights the differential response to exercise in individuals with metabolic impairments. MetaMEx provides the most extensive dataset of skeletal muscle transcriptional responses to different modes of exercise and an online interface to readily interrogate the database. 
  |  http://dx.doi.org/10.1038/s41467-019-13869-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31980607/  |  
------------------------------------------- 
10.1097/MD.0000000000019628  |    Background:  Prostate cancer (PCa) is one of the leading causes of cancer-related death. In the present research, we adopted a comprehensive bioinformatics method to identify some biomarkers associated with the tumor progression and prognosis of PCa. 
  Methods:  Differentially expressed genes (DEGs) analysis and weighted gene co-expression network analysis (WGCNA) were applied for exploring gene modules correlative with tumor progression and prognosis of PCa. Clinically Significant Modules were distinguished, and Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis were used to Annotation, Visualization and Integrated Discovery (DAVID). Protein-protein interaction (PPI) networks were used in selecting potential hub genes. RNA-Seq data and clinical materials of prostate cancer from The Cancer Genome Atlas (TCGA) database were used for the identification and validation of hub genes. The significance of these genes was confirmed via survival analysis and immunohistochemistry. 
  Results:  2688 DEGs were filtered. Weighted gene co-expression network was constructed, and DEGs were divided into 6 modules. Two modules were selected as hub modules which were highly associated with the tumor grades. Functional enrichment analysis was performed on genes in hub modules. Thirteen hub genes in these hub modules were identified through PPT networks. Based on TCGA data, 4 of them (CCNB1, TTK, CNN1, and ACTG2) were correlated with prognosis. The protein levels of CCNB1, TTK, and ACTG2 had a degree of differences between tumor tissues and normal tissues. 
  Conclusion:  Four hub genes were identified as candidate biomarkers and potential therapeutic targets for further studies of exploring molecular mechanisms and individual therapy on PCa. 
  |  http://dx.doi.org/10.1097/MD.0000000000019628  |  
------------------------------------------- 
10.1016/S0140-6736(19)32998-8  |    Background:  Improved markers of prognosis are needed to stratify patients with early-stage colorectal cancer to refine selection of adjuvant therapy. The aim of the present study was to develop a biomarker of patient outcome after primary colorectal cancer resection by directly analysing scanned conventional haematoxylin and eosin stained sections using deep learning. 
  Methods:  More than 12 000 000 image tiles from patients with a distinctly good or poor disease outcome from four cohorts were used to train a total of ten convolutional neural networks, purpose-built for classifying supersized heterogeneous images. A prognostic biomarker integrating the ten networks was determined using patients with a non-distinct outcome. The marker was tested on 920 patients with slides prepared in the UK, and then independently validated according to a predefined protocol in 1122 patients treated with single-agent capecitabine using slides prepared in Norway. All cohorts included only patients with resectable tumours, and a formalin-fixed, paraffin-embedded tumour tissue block available for analysis. The primary outcome was cancer-specific survival. 
  Findings:  828 patients from four cohorts had a distinct outcome and were used as a training cohort to obtain clear ground truth. 1645 patients had a non-distinct outcome and were used for tuning. The biomarker provided a hazard ratio for poor versus good prognosis of 3·84 (95% CI 2·72-5·43; p&lt;0·0001) in the primary analysis of the validation cohort, and 3·04 (2·07-4·47; p&lt;0·0001) after adjusting for established prognostic markers significant in univariable analyses of the same cohort, which were pN stage, pT stage, lymphatic invasion, and venous vascular invasion. 
  Interpretation:  A clinically useful prognostic marker was developed using deep learning allied to digital scanning of conventional haematoxylin and eosin stained tumour tissue sections. The assay has been extensively evaluated in large, independent patient populations, correlates with and outperforms established molecular and morphological prognostic markers, and gives consistent results across tumour and nodal stage. The biomarker stratified stage II and III patients into sufficiently distinct prognostic groups that potentially could be used to guide selection of adjuvant treatment by avoiding therapy in very low risk groups and identifying patients who would benefit from more intensive treatment regimes. 
  Funding:  The Research Council of Norway. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)32998-8  |  
------------------------------------------- 
10.1007/s00120-019-01105-8  |    Background:  Localized renal cell carcinoma is increasingly relevant in daily urological practice due to earlier diagnosis and higher life expectancy. 
  Objectives:  To analyze and compare current treatment evidence for localized renal cell carcinoma regarding new aspects of nephron-sparing surgery, the different surgical approaches and focal therapy. 
  Methods:  A systematic search was performed to identify relevant publications from 2018 and 2019. 
  Results:  Prospective randomized trials comparing nephrectomy with partial nephrectomy, the three different surgical approaches with each other, and focal therapy with surgery are still lacking. Positive effects on survival by partial nephrectomy could be demonstrated, even though partial nephrectomy has a higher morbidity than radical nephrectomy. Older patients (&gt;75 years) with advanced localized renal cell carcinoma did not appear to benefit from partial nephrectomy so far, but minimally invasive surgical approaches are underrepresented in such studies. Minimally invasive partial nephrectomy is superior to the open approach, and robot-assisted partial nephrectomy has better results than laparoscopy. Focal therapy of kidney tumors is technically safe and feasible, but relevant comparisons with partial nephrectomy are still lacking. 
  Conclusions:  Partial nephrectomy is still the gold standard treatment for localized renal cell carcinoma, it should be preferably performed by a robot-assisted approach. Focal therapy can serve as an alternative in highly selected cases. 
  |  https://dx.doi.org/10.1007/s00120-019-01105-8  |  
------------------------------------------- 
10.1016/j.jacr.2020.01.007  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(20)30031-4  |  
------------------------------------------- 
10.1089/pop.2020.0030  |    |  https://www.liebertpub.com/doi/full/10.1089/pop.2020.0030?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/S1470-2045(19)30721-1  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1470-2045(19)30721-1  |  
------------------------------------------- 
10.1001/jama.2020.5035  |    |  https://jamanetwork.com/journals/jama/fullarticle/10.1001/jama.2020.5035  |  
------------------------------------------- 
10.2471/BLT.19.237230  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284657/  |  
------------------------------------------- 
10.1093/neuros/nyz452  |    |  https://academic.oup.com/neurosurgery/article-lookup/doi/10.1093/neuros/nyz452  |  
------------------------------------------- 
10.1016/j.fertnstert.2019.12.001  |   "Once a new technology rolls over you, if you're not part of the steamroller, you're part of the road." -Stewart Brand. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0015-0282(19)32602-0  |  
------------------------------------------- 
10.1093/bib/bbaa021  |   Modern machine learning techniques (such as deep learning) offer immense opportunities in the field of human biological aging research. Aging is a complex process, experienced by all living organisms. While traditional machine learning and data mining approaches are still popular in aging research, they typically need feature engineering or feature extraction for robust performance. Explicit feature engineering represents a major challenge, as it requires significant domain knowledge. The latest advances in deep learning provide a paradigm shift in eliciting meaningful knowledge from complex data without performing explicit feature engineering. In this article, we review the recent literature on applying deep learning in biological age estimation. We consider the current data modalities that have been used to study aging and the deep learning architectures that have been applied. We identify four broad classes of measures to quantify the performance of algorithms for biological age estimation and based on these evaluate the current approaches. The paper concludes with a brief discussion on possible future directions in biological aging research using deep learning. This study has significant potentials for improving our understanding of the health status of individuals, for instance, based on their physical activities, blood samples and body shapes. Thus, the results of the study could have implications in different health care settings, from palliative care to public health. 
  |  None  |  
------------------------------------------- 
10.3791/59649  |   Mild cognitive impairment (MCI) is the first sign of dementia among elderly populations and its early detection is crucial in our aging societies. Common MCI tests are time-consuming such that indiscriminate massive screening would not be cost-effective. Here, we describe a protocol that uses machine learning techniques to rapidly select candidates for further screening via a question-based MCI test. This minimizes the number of resources required for screening because only patients who are potentially MCI positive are tested further. This methodology was applied in an initial MCI research study that formed the starting point for the design of a selective screening decision tree. The initial study collected many demographic and lifestyle variables as well as details about patient medications. The Short Portable Mental Status Questionnaire (SPMSQ) and the Mini-Mental State Examination (MMSE) were used to detect possible cases of MCI. Finally, we used this method to design an efficient process for classifying individuals at risk of MCI. This work also provides insights into lifestyle-related factors associated with MCI that could be leveraged in the prevention and early detection of MCI among elderly populations. 
  |  https://doi.org/10.3791/59649  |  
------------------------------------------- 
10.1016/j.isatra.2020.01.016  |   In this work a fuzzy reinforcement learning (RL) based intelligent classifier for power transformer incipient faults is proposed. Fault classifiers proposed till date have low identification accuracy and do not identify all types of transformer faults. Herein, an attempt has been made to design an adaptive, intelligent transformer fault classifier that progressively learns to identify faults on-line with high accuracy for all fault types. In the proposed approach, dissolved gas analysis (DGA) data of oil samples collected from real power transformers (and from credible sources) has been used, which serves as input to a fuzzy RL based classifier. Typically, classification accuracy is heavily dependent on the number of input variables chosen. This has been resolved by using the J48 algorithm to select 8 most appropriate input variables from the 24 variables obtained using DGA. Proposed fuzzy RL approach achieves a fault identification accuracy of 99.7%, which is significantly higher than other contemporary soft computing based identifiers. Experimental results and comparison with other state-of-the-art approaches, highlights superiority and efficacy of the proposed fuzzy RL technique for transformer fault classification. 
  |  None  |  
------------------------------------------- 
10.1007/s00103-019-03080-z  |   Digital epidemiology is a new and rapidly growing field. The technological revolution we have been witnessing during the last decade, the global rise of the Internet, the emergence of social media and social networks that connect individuals worldwide for information exchange and social interactions, and the almost complete social penetration of mobile devices such as smartphones provide access to data on individual behavior with unprecedented resolution and precision. In digital epidemiology, this type of high-resolution behavioral data is analyzed to advance our understanding of, for example, infectious disease dynamics and improve our abilities to forecast epidemic outbreaks and related phenomena.This article provides an overview on the topic. Different aspects of digital epidemiology are alluded to. Based on examples, I will explain how epidemiological data is integrated on new comprehensive and interactive websites, how the analysis of interactions and activities on social media platforms can yield answers to epidemiological questions, and finally how individual-based data collected by smartphones or wearable sensors in natural experiments can be used to reconstruct contact and physical proximity networks the knowledge of which substantially improves the predictive power of computational models for transmissible infectious diseases.The challenges posed in terms of privacy protection and data security will be discussed. Concepts and solutions will be explained that may help to improve public health by leveraging the new data while at the same time protecting the individual's data sovereignty and personal dignity. 
  |  https://dx.doi.org/10.1007/s00103-019-03080-z  |  
------------------------------------------- 
10.1007/s41030-020-00110-z  |   Machine learning (ML) is a discipline of computer science in which statistical methods are applied to data in order to classify, predict, or optimize, based on previously observed data. Pulmonary and critical care medicine have seen a surge in the application of this methodology, potentially delivering improvements in our ability to diagnose, treat, and better understand a multitude of disease states. Here we review the literature and provide a detailed overview of the recent advances in ML as applied to these areas of medicine. In addition, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of this non-traditional methodology for clinical purposes. 
  |  None  |  
------------------------------------------- 
10.1097/RTI.0000000000000485  |   The field of artificial intelligence (AI) is currently experiencing a period of extensive growth in a wide variety of fields, medicine not being the exception. The base of AI is mathematics and computer science, and the current fame of AI in industry and research stands on 3 pillars: big data, high performance computing infrastructure, and algorithms. In the current digital era, increased storage capabilities and data collection systems, lead to a massive influx of data for AI algorithm. The size and quality of data are 2 major factors influencing performance of AI applications. However, it is highly dependent on the type of task at hand and algorithm chosen to perform this task. AI may potentially automate several tedious tasks in radiology, particularly in cardiothoracic imaging, by pre-readings for the detection of abnormalities, accurate quantifications, for example, oncologic volume lesion tracking and cardiac volume and image optimization. Although AI-based applications offer great opportunity to improve radiology workflow, several challenges need to be addressed starting from image standardization, sophisticated algorithm development, and large-scale evaluation. Integration of AI into the clinical workflow also needs to address legal barriers related to security and protection of patient-sensitive data and liability before AI will reach its full potential in cardiothoracic imaging. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000485  |  
------------------------------------------- 
10.1016/j.injury.2020.01.035  |    Hypothesis:  We aimed (1) to discover the prevalence of vascularized bone grafting in the treatment of scaphoid nonunion and (2) to compare healing using vascularized bone grafting versus standard non-vascularized techniques. Secondarily, we sought to compare resource utilization between procedures and identify factors that may be associated with nonunion repair failure. We hypothesized that, despite being less common, vascularized bone grafts have greater success than non-vascularized bone grafting surgeries. 
  Methods:  We performed a large population analysis using the Truven MarketScan databases to identify patients from 2009 to 2017 with a diagnosis of a scaphoid nonunion undergoing repair surgery with and without the insertion of a pedicled or free vascularized bone graft. We defined any subsequent scaphoid or wrist surgery within 12 months after surgery as surgery failure. We compared success rates and post-operative resource utilization using Chi-squared tests. 
  Results:  Of 4177 eligible patients, 358 underwent nonunion repair with vascularized bone graft and 3819 patients received non-vascularized bone grafting. The failure rate requiring revision surgery was 5.0% in vascularized repair, versus 6.1% for non-vascularized surgery. Age and comorbidities did not affect bone graft type. Areas with higher median household incomes had more vascularized repairs. Vascularized bone graft patients received significantly more therapy and imaging after surgery. 
  Conclusions:  Most scaphoid nonunion repairs are performed without vascularize bone grafting. Typical nonunions may not justify the increased time and technical demands of vascularized bone grafting, and traditional repair should remain first line treatment for scaphoid nonunions without additional risk factors. Further studies to elucidate which fractures benefit most from vascularized grafting are needed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0020-1383(20)30063-2  |  
------------------------------------------- 
10.1007/s11154-020-09540-1  |   Cushing's syndrome (CS) provides a unique model for assessing the neurotoxic effect of chronic hypercortisolism on human brains. With the ongoing development of different computer-assisted tools, four research stages emerged, each with its own pearls and pitfalls. This review summarizes current knowledge and describes the dynamic changes of views on the brain changes of CS, especially in the current era of the rapid development of artificial intelligence and big data. The adverse effects of GC on brain are proven to be on structural, functional and cellular levels at the same time. 
  |  https://doi.org/10.1007/s11154-020-09540-1  |  
------------------------------------------- 
10.1007/s10557-019-06922-9  |    Purpose:  Although impaired glucose tolerance (IGT) promotes cardiovascular events, our Alpha-glucosidase-inhibitor Blocks Cardiac Events in Patients with Myocardial Infarction and Impaired Glucose Tolerance (ABC) study showed that alpha-glucosidase inhibitors do not prevent cardiovascular events in patients with myocardial infarction (MI) and IGT. The aim of the present study was to identify potential clinical factors for cardiovascular events in patients with MI and IGT. 
  Methods:  Using the limitless-arity multiple testing procedure, an artificial intelligence (AI)-based data mining method, we analyzed 385,391 combinations of fewer than four clinical parameters. 
  Results:  We identified 380 combinations predicting the occurrence of (1) all-cause hospitalization, (2) hospitalization due to worsening of heart failure (HF), (3) hospitalization due to non-fatal MI, and (4) hospitalization due to percutaneous coronary intervention (PCI) and coronary artery bypass grafting (CABG) for stable angina among 385,391 combinations in 853 patients. Among these, either plasma BNP levels ≥ 200 pg/dl or diuretic use exclusively predicted (1) all-cause hospitalization, (2) hospitalization due to worsening of HF, and (3) hospitalization due to a non-fatal MI, with plasma BNP levels ≥ 200 pg/dl being the sole predictor of hospitalization due to PCI and CABG. Importantly, each finding was verified by independently drawn Kaplan-Meier curves, revealing the unexpected role of plasma BNP levels in the progression of coronary stenosis determined as the necessity of PCI and CABG for stable angina. 
  Conclusions:  In patients with MI and IGT, high plasma BNP levels predicted the occurrence of coronary stenosis, recurrent MI, and worsening of HF, whereas diuretic use did not predict the progression of coronary stenosis but non-fatal MI and worsening of HF. 
  |  https://doi.org/10.1007/s10557-019-06922-9  |  
------------------------------------------- 
10.1021/acs.jmedchem.9b02130  |   Artificial intelligence (AI) is becoming established in drug discovery. For example, many in the industry are applying machine learning approaches to target discovery or to optimize compound synthesis. While our organization is certainly applying these sorts of approaches, we propose an additional approach: using AI to augment human intelligence. We have been working on a series of recommendation systems that take advantage of our existing laboratory processes, both wet and computational, in order to provide inspiration to our chemists, suggest next steps in their work, and automate existing workflows. We will describe five such systems in various stages of deployment within the Novartis Institutes for BioMedical Research. While each of these systems addresses different stages of the discovery pipeline, all of them share three common features: a trigger that initiates the recommendation, an analysis that leverages our existing systems with AI, and the delivery of a recommendation. The goal of all of these systems is to inspire and accelerate the drug discovery process. 
  |  https://dx.doi.org/10.1021/acs.jmedchem.9b02130  |  
------------------------------------------- 
10.1016/j.ejso.2020.04.010  |   Advanced instrumentation whether robotic or non-robotic- hasn't itself made for better surgery as all critical measures of operative success depend still on intraoperative surgeon judgement and decision-making. Computer assisted surgery, or digital surgery, refers to the combination of technology with real-time data during an operation and is often assumed to need new hardware platforms to become a reality. However, methods to support personalised surgical endeavour exist now and can be deployed today within standard laparoscopic paradigms. Here we describe in detail the rationale for the deployment of such assistance for surgical step-advancement in our current practice evolution from traditional proximal colon cancer resection to complete mesocolic excision focussing on personalised 3d anatomical display, intraoperative, quantificative fluorescence assessment of intracorporeal anastomoses and postoperative digital feedback to enable reflection and identify areas of technical improvement. 
  |  None  |  
------------------------------------------- 
10.1007/s00104-019-01066-w  |    Background:  As in other areas, military surgery is being transformed by developments in artificial intelligence, robotics and digitalization. Although the prospect of operating with a robot-assisted surgery system in the country of deployment while the responsible surgeon is in Germany is still a long way off, the training of military surgeons and the treatment of injured soldiers on deployment would nowadays be unimaginable without the digitalization of surgery in the armed forces. The structure of the medical environment in German clinics places restrictions on training that is close to operational reality. In the daily routine it is not possible to carry out the necessary numbers of deployment-relevant emergency surgical procedures under the expected conditions. Such procedures thus require the use of appropriate simulators or simulated scenarios that are as close to reality as possible. Although military surgeons are qualified in at least two specialist areas, the availability of telemedicine on deployment is helping to noticeably improve the treatment of injured soldiers. Telemedical consultation with colleagues in Germany makes it possible, for example, to reach joint decisions across different branches and disciplines. 
  Conclusion:  Until now it has not been possible to substitute the attending surgeon in the country of deployment with robot-assisted surgery systems or even robots for carrying out life-saving and stabilizing procedures; however, in order to provide surgeons with the necessary tools to successfully operate in situations where there is a shortage of personnel and materials in an inhospitable environment, use is made of the means that are currently available in the German medical services and constant efforts are made to explore the future possibilities of digital simulation. This article shows the reader the current status of digitalization in surgical training and deployments in the German armed forces. 
  |  https://dx.doi.org/10.1007/s00104-019-01066-w  |  
------------------------------------------- 
10.2471/BLT.20.253823  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284640/  |  
------------------------------------------- 
10.1093/jmicro/dfz116  |   Single-molecule imaging analysis has been applied to study the dynamics and kinetics of molecular behaviors and interactions in living cells. In spite of its high potential as a technique to investigate the molecular mechanisms of cellular phenomena, single-molecule imaging analysis has not been extended to a large scale of molecules in cells due to the low measurement throughput as well as required expertise. To overcome these problems, we have automated the imaging processes by using computer operations, robotics and artificial intelligence (AI). AI is an ideal substitute for expertise to obtain high-quality images for quantitative analysis. Our automated in-cell single-molecule imaging system, AiSIS, could analyze 1600 cells in 1 day, which corresponds to ∼ 100-fold higher efficiency than manual analysis. The large-scale analysis revealed cell-to-cell heterogeneity in the molecular behavior, which had not been recognized in previous studies. An analysis of the receptor behavior and downstream signaling was accomplished within a significantly reduced time frame and revealed the detailed activation scheme of signal transduction, advancing cell biology research. Furthermore, by combining the high-throughput analysis with our previous finding that a receptor changes its behavioral dynamics depending on the presence of a ligand/agonist or inhibitor/antagonist, we show that AiSIS is applicable to comprehensive pharmacological analysis such as drug screening. This AI-aided automation has wide applications for single-molecule analysis. 
  |  https://academic.oup.com/jmicro/article-lookup/doi/10.1093/jmicro/dfz116  |  
------------------------------------------- 
10.1007/s00104-019-01103-8  |    Background:  When using digitalization and artificial intelligence (AI), large amounts of data (big data) are produced, which can be processed by computers and used in the field of microvascular-reconstructive craniomaxillofacial surgery (CMFS). 
  Objective:  The aim of this article is to summarize current applications of digitalized medicine and AI in microvascular reconstructive CMFS. 
  Material and methods:  Review of frequent applications of digital medicine for microvascular CMFS reconstruction, focusing on digital planning, navigation, robotics and potential applications with AI. 
  Results:  The broadest utilization of medical digitalization is in the virtual planning of microvascular transplants, individualized implants and template-guided reconstruction. Navigation is commonly used for ablative tumor surgery but less frequently in reconstructions. Robotics are mainly employed in the transoral approach for tumor surgery of the hypopharynx, whereas the use of AI is still limited even if possible applications would be automated virtual planning and monitoring systems. 
  Conclusion:  The use of digitalized methods and AI are adjuncts to microvascular reconstruction. Automatization approaches and simplification of technologies will provide such applications to a broader clientele in the future; however, in CMFS, robotic-assisted resections and automated flap monitoring are not yet the standard of care. 
  |  https://dx.doi.org/10.1007/s00104-019-01103-8  |  
------------------------------------------- 
10.1097/MAO.0000000000002566  |    Objective:  Cochlear implant (CI) technology and techniques have advanced over the years. There has not been the same degree of change in programming and there remains a lack of standardization techniques. The purpose of this study is to compare performance in cochlear implant subjects using experienced clinician (EC) standard programming methods versus an Artificial Intelligence, FOX based algorithm for programming. 
  Study design:  Prospective, nonrandomized, multicenter study using within-subject experimental design SETTING:: Tertiary referral centers. 
  Patients:  Fifty-five adult patients with ≥ 3 months experience with a Nucleus 5, 6, Kanso, or 7 series sound processor. 
  Intervention:  Therapeutic Main Outcome Measures: CNC words and AzBio sentences in noise (+10 dB SNR) tests were administered in a soundproof booth followed by a direct connect psychoacoustic battery using the EC program. Tests were repeated 1 month later using the optimized FOX program. Subjective measures of patient satisfaction were also measured. 
  Results:  Performance for the EC program was compared to the FOX program for both measures. Group mean results revealed equivalent performance (Kruskal-Wallis ANOVA p = 0.934) with both programming methods. While some patients had better performance with the FOX method and some performed more poorly, the majority had equivalent performance and preferred the FOX system. 
  Conclusion:  The study demonstrated that on average, FOX outcomes are equivalent to those using traditional programming techniques. In addition, the FOX programming method can effect standardization across centers and increase access for many individuals who could benefit. 
  |  http://dx.doi.org/10.1097/MAO.0000000000002566  |  
------------------------------------------- 
10.1016/j.gaitpost.2020.01.021  |    Background:  Due to the high susceptivity of the walking pattern to be affected by several disorders, accurate analysis methods are necessary. Given the complexity and relevance of such assessment, the utilization of methods to facilitate it plays a significant role, provided that they do not compromise the outcomes. 
  Research questions:  This paper aimed at identifying the standards for the application of adaptive predictive systems to gait analysis, given the extensive research on this field. Furthermore, we also intended to check whether such methods can effectively support clinicians in determining the number of physiotherapy sessions necessary to recover gait-related dysfunctions. 
  Methods:  Through a screening process of scientific databases, we considered studies encompassed from 1968 to April 2019. Within these 50 years, we found 24 papers that met our inclusion criteria. They were analyzed according to their data acquisition and processing methods via ad hoc questionnaires. Additionally, we examined quantitatively the adaptive approaches. 
  Results:  Concerning data acquisition, the included papers presented a mean score of 6.1 SD 1.0, most of them applying optoelectronic systems, and the ground reaction force (GRF) was the most used parameter. The AI quality assessment showed an above-average rate of 7.8 SD 1.0, and artificial neural networks (ANN) being the paradigm most frequently utilized. Our systematic review identified only one study that addressed therapeutics including a predictive method. 
  Significance:  While much progress has been identified to predict assessment aspects, there is little effort to assist healthcare professionals in establishing the rehabilitation duration and prognostics. Therefore, future studies should focus on accomplishing the production of applications of predictive methods to therapeutics and prognosis, not lingering extremely on the analysis of gait features. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0966-6362(20)30050-3  |  
------------------------------------------- 
10.3390/antiox9030210  |   We combined machine learning and plant in vitro culture methodologies as a novel approach for unraveling the phytochemical potential of unexploited medicinal plants<i>.</i> In order to induce phenolic compound biosynthesis, the in vitro culture of three different species of <i>Bryophyllum</i> under nutritional stress was established. To optimize phenolic extraction, four solvents with different MeOH proportions were used, and total phenolic content (TPC), flavonoid content (FC) and radical-scavenging activity (RSA) were determined. All results were subjected to data modeling with the application of artificial neural networks to provide insight into the significant factors that influence such multifactorial processes. Our findings suggest that aerial parts accumulate a higher proportion of phenolic compounds and flavonoids in comparison to roots. TPC was increased under ammonium concentrations below 15 mM, and their extraction was maximum when using solvents with intermediate methanol proportions (55-85%). The same behavior was reported for RSA, and, conversely, FC was independent of culture media composition, and their extraction was enhanced using solvents with high methanol proportions (&gt;85%). These findings confer a wide perspective about the relationship between abiotic stress and secondary metabolism and could serve as the starting point for the optimization of bioactive compound production at a biotechnological scale. 
  |  http://www.mdpi.com/resolver?pii=antiox9030210  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32143282/  |  
------------------------------------------- 
10.1007/s00464-020-07548-x  |    Background:  The occurrence of bile duct injury (BDI) during laparoscopic cholecystectomy (LC) is an important medical issue. Expert surgeons prevent intraoperative BDI by identifying four landmarks. The present study aimed to develop a system that outlines these landmarks on endoscopic images in real time. 
  Methods:  An intraoperative landmark indication system was constructed using YOLOv3, which is an algorithm for object detection based on deep learning. The training datasets comprised approximately 2000 endoscopic images of the region of Calot's triangle in the gallbladder neck obtained from 76 videos of LC. The YOLOv3 learning model with the training datasets was applied to 23 videos of LC that were not used in training, to evaluate the estimation accuracy of the system to identify four landmarks: the cystic duct, common bile duct, lower edge of the left medial liver segment, and Rouviere's sulcus. Additionally, we constructed a prototype and used it in a verification experiment in an operation for a patient with cholelithiasis. 
  Results:  The YOLOv3 learning model was quantitatively and subjectively evaluated in this study. The average precision values for each landmark were as follows: common bile duct: 0.320, cystic duct: 0.074, lower edge of the left medial liver segment: 0.314, and Rouviere's sulcus: 0.101. The two expert surgeons involved in the annotation confirmed consensus regarding valid indications for each landmark in 22 of the 23 LC videos. In the verification experiment, the use of the intraoperative landmark indication system made the surgical team more aware of the landmarks. 
  Conclusions:  Intraoperative landmark indication successfully identified four landmarks during LC, which may help to reduce the incidence of BDI, and thus, increase the safety of LC. The novel system proposed in the present study may prevent BDI during LC in clinical practice. 
  |  https://doi.org/10.1007/s00464-020-07548-x  |  
------------------------------------------- 
10.1002/cpt.1795  |   As the field of artificial intelligence and machine learning (AI/ML) for drug discovery is rapidly advancing, we address the question "What is the impact of recent AI/ML trends in the area of Clinical Pharmacology?" We address difficulties and AI/ML developments for target identification, their use in generative chemistry for small molecule drug discovery, and the potential role of AI/ML in clinical trial outcome evaluation. We briefly discuss current trends in the use of AI/ML in health care and the impact of AI/ML context of the daily practice of clinical pharmacologists. 
  |  https://doi.org/10.1002/cpt.1795  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31957003/  |  
------------------------------------------- 
10.1177/0022034520915714  |   The term "artificial intelligence" (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which "learns" intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute ("explainable AI"). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce. 
  |  https://journals.sagepub.com/doi/10.1177/0022034520915714?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  
------------------------------------------- 
10.1016/j.jid.2020.02.026  |   Artificial intelligence (AI) is becoming increasingly important in dermatology, with studies reporting accuracy matching or exceeding dermatologists for the diagnosis of skin lesions from clinical and dermoscopic images. However, real-world clinical validation is currently lacking. We review dermatological applications of deep learning, the leading AI technology for image analysis, and discuss its current capabilities, potential failure modes, and challenges surrounding performance assessment and interpretability. We address three primary applications: (1) teledermatology, including triage for referral to dermatologists, (2) augmenting clinical assessment during face-to-face visits, and (3) dermatopathology. We discuss equity and ethical issues related to future clinical adoption and recommend specific standardization of metrics for reporting model performance. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-202X(20)31201-X  |  
------------------------------------------- 
10.1097/RTI.0000000000000497  |    |  http://dx.doi.org/10.1097/RTI.0000000000000497  |  
------------------------------------------- 
10.1016/j.annemergmed.2019.12.024  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0196-0644(19)31465-9  |  
------------------------------------------- 
10.1111/tops.12492  |   We describe some recent trends in research on lying from a multidisciplinary perspective, including logic, philosophy, linguistics, psychology, cognitive science, behavioral economics, and artificial intelligence. Furthermore, we outline the seven contributions to this special issue of topiCS. 
  |  https://doi.org/10.1111/tops.12492  |  
------------------------------------------- 
10.1007/s11739-019-02265-3  |   Length of stay (LOS) and discharge destination predictions are key parts of the discharge planning process for general medical hospital inpatients. It is possible that machine learning, using natural language processing, may be able to assist with accurate LOS and discharge destination prediction for this patient group. Emergency department triage and doctor notes were retrospectively collected on consecutive general medical and acute medical unit admissions to a single tertiary hospital from a 2-month period in 2019. These data were used to assess the feasibility of predicting LOS and discharge destination using natural language processing and a variety of machine learning models. 313 patients were included in the study. The artificial neural network achieved the highest accuracy on the primary outcome of predicting whether a patient would remain in hospital for &gt; 2 days (accuracy 0.82, area under the received operator curve 0.75, sensitivity 0.47 and specificity 0.97). When predicting LOS as an exact number of days, the artificial neural network achieved a mean absolute error of 2.9 and a mean squared error of 16.8 on the test set. For the prediction of home as a discharge destination (vs any non-home alternative), all models performed similarly with an accuracy of approximately 0.74. This study supports the feasibility of using natural language processing to predict general medical inpatient LOS and discharge destination. Further research is indicated with larger, more detailed, datasets from multiple centres to optimise and examine the accuracy that may be achieved with such predictions. 
  |  https://dx.doi.org/10.1007/s11739-019-02265-3  |  
------------------------------------------- 
10.1111/bjd.18955  |    |  https://doi.org/10.1111/bjd.18955  |  
------------------------------------------- 
10.1016/j.ejvs.2020.01.019  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1078-5884(20)30065-4  |  
------------------------------------------- 
10.1016/j.telpol.2020.101976  |   The rush to understand new socio-economic contexts created by the wide adoption of AI is justified by its far-ranging consequences, spanning almost every walk of life. Yet, the public sector's predicament is a tragic double bind: its obligations to protect citizens from potential algorithmic harms are at odds with the temptation to increase its own efficiency - or in other words - to govern algorithms, while governing <i>by</i> algorithms. Whether such dual role is even possible, has been a matter of debate, the challenge stemming from algorithms' intrinsic properties, that make them distinct from other digital solutions, long embraced by the governments, create externalities that rule-based programming lacks. As the pressures to deploy automated decision making systems in the public sector become prevalent, this paper aims to examine how the use of AI in the public sector in relation to existing data governance regimes and national regulatory practices can be <i>intensifying</i> existing power asymmetries. To this end, investigating the legal and policy instruments associated with the use of AI for strenghtening the immigration process control system in Canada; "optimising" the employment services" in Poland, and personalising the digital service experience in Finland, the paper advocates for the need of a common framework to evaluate the potential impact of the use of AI in the public sector. In this regard, it discusses the specific effects of automated decision support systems on public services and the growing expectations for governments to play a more prevalent role in the digital society and to ensure that the potential of technology is harnessed, while negative effects are controlled and possibly avoided. This is of particular importance in light of the current COVID-19 emergency crisis where AI and the underpinning regulatory framework of data ecosystems, have become crucial policy issues as more and more innovations are based on large scale data collections from digital devices, and the real-time accessibility of information and services, contact and relationships between institutions and citizens could strengthen - or undermine - trust in governance systems and democracy. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32313360/  |  
------------------------------------------- 
10.1016/j.xphs.2020.01.020  |   The process of assembling regulatory documents for submission to multiple global health agencies can present a repetitive cycle of authoring, editing, and data verification, which increases in complexity as changes are made for approved products, particularly from a chemistry, manufacturing, and controls (CMC) perspective. Currently, pharmaceutical companies rely on a workflow that involves manual CMC change management across documents. Similarly, when regulators review submissions, they provide feedback and insight into regulatory decision making in a narrative format. As accelerated review pathways are increasingly used and pressure mounts to bring products to market quickly, innovative solutions for assembling, distributing, and reviewing regulatory information are being considered. Structured content management (SCM) solutions, in which data are collated into centrally organized content blocks for use across different documents, may aid in the efficient processing of data and create opportunities for automation and machine learning in its interpretation. The US Food and Drug Administration (FDA) has recently created initiatives that encourage application of SCM for CMC data, though many challenges could impede their success and efficiency. The goal is for industry and health authorities to collaborate in the development of SCM for CMC applications, to potentially streamline compilation of quality data in regulatory submissions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-3549(20)30057-5  |  
------------------------------------------- 
10.1088/1361-6560/ab6f51  |   As one of the most popular approaches in artificial intelligence, deep learning (DL) has attracted a lot of attention in the medical physics field over the past few years. The goals of this topical review article are twofold. First, we will provide an overview of the method to medical physics researchers interested in DL to help them start the endeavor. Second, we will give in-depth discussions on the DL technology to make researchers aware of its potential challenges and possible solutions. As such, we divide the article into two major parts. The first part introduces general concepts and principles of DL and summarizes major research resources, such as computational tools and databases. The second part discusses challenges faced by DL, present available methods to mitigate some of these challenges, as well as our recommendations. 
  |  https://doi.org/10.1088/1361-6560/ab6f51  |  
------------------------------------------- 
10.1093/jamia/ocz205  |    Objective:  Clinical interventions and death in the intensive care unit (ICU) depend on complex patterns in patients' longitudinal data. We aim to anticipate these events earlier and more consistently so that staff can consider preemptive action. 
  Materials and methods:  We use a temporal convolutional network to encode longitudinal data and a feedforward neural network to encode demographic data from 4713 ICU admissions in 2014-2018. For each hour of each admission, we predict events in the subsequent 1-6 hours. We compare performance with other models including a recurrent neural network. 
  Results:  Our model performed similarly to the recurrent neural network for some events and outperformed it for others. This performance increase was more evident in a sensitivity analysis where the prediction timeframe was varied. Average positive predictive value (95% CI) was 0.786 (0.781-0.790) and 0.738 (0.732-0.743) for up- and down-titrating FiO2, 0.574 (0.519-0.625) for extubation, 0.139 (0.117-0.162) for intubation, 0.533 (0.492-0.572) for starting noradrenaline, 0.441 (0.433-0.448) for fluid challenge, and 0.315 (0.282-0.352) for death. 
  Discussion:  Events were better predicted where their important determinants were captured in structured electronic health data, and where they occurred in homogeneous circumstances. We produce partial dependence plots that show our model learns clinically-plausible associations between its inputs and predictions. 
  Conclusion:  Temporal convolutional networks improve prediction of clinical events when used to represent longitudinal ICU data. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz205  |  
------------------------------------------- 
10.1016/j.kint.2019.11.037  |   With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0085-2538(20)30012-0  |  
------------------------------------------- 
10.1016/j.hrthm.2020.02.015  |    Background:  Increasing utilization of long-term outpatient ambulatory electrocardiographic (ECG) monitoring continues to drive the need for improved ECG interpretation algorithms. 
  Objective:  The purpose of this study was to describe the BeatLogic® platform for ECG interpretation and to validate the platform using electrophysiologist-adjudicated real-world data and publicly available validation data. 
  Methods:  Deep learning models were trained to perform beat and rhythm detection/classification using ECGs collected with the Preventice BodyGuardian® Heart monitor. Training annotations were created by certified ECG technicians, and validation annotations were adjudicated by a team of board-certified electrophysiologists. Deep learning model classification results were used to generate contiguous annotation results, and performance was assessed in accordance with the EC57 standard. 
  Results:  On the real-world validation dataset, BeatLogic beat detection sensitivity and positive predictive value were 99.84% and 99.78%, respectively. Ventricular ectopic beat classification sensitivity and positive predictive value were 89.4% and 97.8%, respectively. Episode and duration F<sub>1</sub> scores (range 0-100) exceeded 70 for all 14 rhythms (including noise) that were evaluated. F<sub>1</sub> scores for 11 rhythms exceeded 80, 7 exceeded 90, and 5 including atrial fibrillation/flutter, ventricular tachycardia, ventricular bigeminy, ventricular trigeminy, and third-degree heart block exceeded 95. 
  Conclusion:  The BeatLogic platform represents the next stage of advancement for algorithmic ECG interpretation. This comprehensive platform performs beat detection, beat classification, and rhythm detection/classification with greatly improved performance over the current state of the art, with comparable or improved performance over previously published algorithms that can accomplish only 1 of these 3 tasks. 
  |  None  |  
------------------------------------------- 
10.14309/ajg.0000000000000565  |    Objectives:  Exposure to ionizing radiation remains a hazard for patients and healthcare providers. We evaluated the utility of an artificial intelligence (AI)-enabled fluoroscopy system to minimize radiation exposure during image-guided endoscopic procedures. 
  Methods:  We conducted a prospective study of 100 consecutive patients who underwent fluoroscopy-guided endoscopic procedures. Patients underwent interventions using either conventional or AI-equipped fluoroscopy system that uses ultrafast collimation to limit radiation exposure to the region of interest. The main outcome measure was to compare radiation exposure with patients, which was measured by dose area product. Secondary outcome was radiation scatter to endoscopy personnel measured using dosimeter. 
  Results:  Of 100 patients who underwent procedures using traditional (n = 50) or AI-enabled (n = 50) fluoroscopy systems, there was no significant difference in demographics, body mass index, procedural type, and procedural or fluoroscopy time between the conventional and the AI-enabled fluoroscopy systems. Radiation exposure to patients was lower (median dose area product 2,178 vs 5,708 mGym, P = 0.001) and scatter effect to endoscopy personnel was less (total deep dose equivalent 0.28 vs 0.69 mSv; difference of 59.4%) for AI-enabled fluoroscopy as compared to conventional system. On multivariate linear regression analysis, after adjusting for patient characteristics, procedural/fluoroscopy duration, and type of fluoroscopy system, only AI-equipped fluoroscopy system (coefficient 3,331.9 [95% confidence interval: 1,926.8-4,737.1, P &lt; 0.001) and fluoroscopy duration (coefficient 813.2 [95% confidence interval: 640.5-985.9], P &lt; 0.001) were associated with radiation exposure. 
  Discussion:  The AI-enabled fluoroscopy system significantly reduces radiation exposure to patients and scatter effect to endoscopy personnel (see Graphical abstract, Supplementary Digital Content, http://links.lww.com/AJG/B461). 
  |  http://Insights.ovid.com/pubmed?pmid=32195731  |  
------------------------------------------- 
10.1093/jamia/ocz152  |    Objective:  Consumers increasingly turn to the internet in search of health-related information; and they want their questions answered with short and precise passages, rather than needing to analyze lists of relevant documents returned by search engines and reading each document to find an answer. We aim to answer consumer health questions with information from reliable sources. 
  Materials and methods:  We combine knowledge-based, traditional machine and deep learning approaches to understand consumers' questions and select the best answers from consumer-oriented sources. We evaluate the end-to-end system and its components on simple questions generated in a pilot development of MedlinePlus Alexa skill, as well as the short and long real-life questions submitted to the National Library of Medicine by consumers. 
  Results:  Our system achieves 78.7% mean average precision and 87.9% mean reciprocal rank on simple Alexa questions, and 44.5% mean average precision and 51.6% mean reciprocal rank on real-life questions submitted by National Library of Medicine consumers. 
  Discussion:  The ensemble of deep learning, domain knowledge, and traditional approaches recognizes question type and focus well in the simple questions, but it leaves room for improvement on the real-life consumers' questions. Information retrieval approaches alone are sufficient for finding answers to simple Alexa questions. Answering real-life questions, however, benefits from a combination of information retrieval and inference approaches. 
  Conclusion:  A pilot practical implementation of research needed to help consumers find reliable answers to their health-related questions demonstrates that for most questions the reliable answers exist and can be found automatically with acceptable accuracy. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz152  |  
------------------------------------------- 
10.3390/s20051456  |   The transition of the energy system into a more efficient state requires innovative ideas to finance new schemes and engage people into adjusting their behavioural patterns concerning consumption. Effective energy management combined with Information and Communication Technologies (ICTs) open new opportunities for local and regional authorities, but also for energy suppliers, utilities and other obligated parties, or even energy cooperatives, to implement mechanisms that allow people to become more efficient either by producing and trading energy or by reducing their energy consumption. In this paper, a novel framework is proposed connecting energy savings with a digital energy currency. This framework builds reward schemes where the energy end-users could benefit financially from saving energy, by receiving coins according to their real consumption compared to the predicted consumption if no actions were to take place. A pilot appraisal of such a scheme is presented for the case of Bahrain, so as to simulate the behaviour of the proposed framework in order for it to become a viable choice for intelligent energy management in future action plans. 
  |  http://www.mdpi.com/resolver?pii=s20051456  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32155853/  |  
------------------------------------------- 
10.3389/fmicb.2020.00393  |   Food and human health are inextricably linked. As such, revolutionary impacts on health have been derived from advances in the production and distribution of food relating to food safety and fortification with micronutrients. During the past two decades, it has become apparent that the human microbiome has the potential to modulate health, including in ways that may be related to diet and the composition of specific foods. Despite the excitement and potential surrounding this area, the complexity of the gut microbiome, the chemical composition of food, and their interplay <i>in situ</i> remains a daunting task to fully understand. However, recent advances in high-throughput sequencing, metabolomics profiling, compositional analysis of food, and the emergence of electronic health records provide new sources of data that can contribute to addressing this challenge. Computational science will play an essential role in this effort as it will provide the foundation to integrate these data layers and derive insights capable of revealing and understanding the complex interactions between diet, gut microbiome, and health. Here, we review the current knowledge on diet-health-gut microbiota, relevant data sources, bioinformatics tools, machine learning capabilities, as well as the intellectual property and legislative regulatory landscape. We provide guidance on employing machine learning and data analytics, identify gaps in current methods, and describe new scenarios to be unlocked in the next few years in the context of current knowledge. 
  |  https://doi.org/10.3389/fmicb.2020.00393  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32318028/  |  
------------------------------------------- 
10.1002/lary.28669  |    Objectives:  Contemporary clinical assessment of vocal fold adduction and abduction is qualitative and subjective. Herein is described a novel computer vision tool for automated quantitative tracking of vocal fold motion from videolaryngoscopy. The potential of this software as a diagnostic aid in unilateral vocal fold paralysis is demonstrated. 
  Study design:  Case-control. 
  Methods:  A deep-learning algorithm was trained for vocal fold localization from videoendoscopy for automated frame-wise estimation of glottic opening angles. Algorithm accuracy was compared against manual expert markings. Maximum glottic opening angles between adults with normal movements (N = 20) and those with unilateral vocal fold paralysis (N = 20) were characterized. 
  Results:  Algorithm angle estimations demonstrated a correlation coefficient of 0.97 (P &lt; .001) and mean absolute difference of 3.72° (standard deviation [SD], 3.49°) in comparison to manual expert markings. In comparison to those with normal movements, patients with unilateral vocal fold paralysis demonstrated significantly lower maximal glottic opening angles (mean 68.75° ± 11.82° vs. 49.44° ± 10.42°; difference, 19.31°; 95% confidence interval [CI] [12.17°-26.44°]; P &lt; .001). Maximum opening angle less than 58.65° predicted unilateral vocal fold paralysis with a sensitivity of 0.85 and specificity of 0.85, with an area under the receiver operating characteristic curve of 0.888 (95% CI [0.784-0.991]; P &lt; .001). 
  Conclusion:  A user-friendly software tool for automated quantification of vocal fold movements from previously recorded videolaryngoscopy examinations is presented, termed automated glottic action tracking by artificial intelligence (AGATI). This tool may prove useful for diagnosis and outcomes tracking of vocal fold movement disorders. 
  Level of evidence:  IV Laryngoscope, 2020. 
  |  https://doi.org/10.1002/lary.28669  |  
------------------------------------------- 
10.2196/17550  |    Background:  Machine-learning or deep-learning algorithms for clinical diagnosis are inherently dependent on the availability of large-scale clinical datasets. Lack of such datasets and inherent problems such as overfitting often necessitate the development of innovative solutions. Probabilistic modeling closely mimics the rationale behind clinical diagnosis and represents a unique solution. 
  Objective:  The aim of this study was to develop and validate a probabilistic model for differential diagnosis in different medical domains. 
  Methods:  Numerical values of symptom-disease associations were utilized to mathematically represent medical domain knowledge. These values served as the core engine for the probabilistic model. For the given set of symptoms, the model was utilized to produce a ranked list of differential diagnoses, which was compared to the differential diagnosis constructed by a physician in a consult. Practicing medical specialists were integral in the development and validation of this model. Clinical vignettes (patient case studies) were utilized to compare the accuracy of doctors and the model against the assumed gold standard. The accuracy analysis was carried out over the following metrics: top 3 accuracy, precision, and recall. 
  Results:  The model demonstrated a statistically significant improvement (P=.002) in diagnostic accuracy (85%) as compared to the doctors' performance (67%). This advantage was retained across all three categories of clinical vignettes: 100% vs 82% (P&lt;.001) for highly specific disease presentation, 83% vs 65% for moderately specific disease presentation (P=.005), and 72% vs 49% (P&lt;.001) for nonspecific disease presentation. The model performed slightly better than the doctors' average in precision (62% vs 60%, P=.43) but there was no improvement with respect to recall (53% vs 56%, P=.27). However, neither difference was statistically significant. 
  Conclusions:  The present study demonstrates a drastic improvement over previously reported results that can be attributed to the development of a stable probabilistic framework utilizing symptom-disease associations to mathematically represent medical domain knowledge. The current iteration relies on static, manually curated values for calculating the degree of association. Shifting to real-world data-derived values represents the next step in model development. 
  |  https://www.jmir.org/2020/4/e17550/  |  
------------------------------------------- 
10.3390/s20051420  |   With the continuing advancements in technologies (such as machine to machine, wireless telecommunications, artificial intelligence, and big data analysis), the Internet of Things (IoT) aims to connect everything for information sharing and intelligent decision-making. Swarm intelligence (SI) provides the possibility of SI behavior through collaboration in individuals that have limited or no intelligence. Its potential parallelism and distribution characteristics can be used to realize global optimization and solve nonlinear complex problems. This paper reviews representative SI algorithms and summarizes their applications in the IoT. The main focus consists in the analysis of SI-enabled applications to wireless sensor network (WSN) and discussion of related research problems in the WSN. Also, we concluded SI-based applications in other IoT fields, such as SI in UAV-aided wireless network. Finally, possible research prospects and future trends are drawn. 
  |  http://www.mdpi.com/resolver?pii=s20051420  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32150912/  |  
------------------------------------------- 
10.1093/sleep/zsaa045  |    Study objectives:  Multisensor wearable consumer devices allowing collection of multiple data sources, such as heart rate and motion, for the evaluation of sleep in the home environment, are increasingly ubiquitous. However, the validity of such devices for sleep assessment has not been directly compared to alternatives such as wrist actigraphy or PSG. 
  Methods:  Eight participants each completed four nights in a sleep laboratory, equipped with polysomnography (PSG) and several wearable devices. RPSGT-scored PSG served as ground truth for sleep-wake state. Wearable devices providing sleep-wake classification data were compared to PSG at both an epoch-by-epoch and night level. Data from multisensor wearables (Apple Watch and Oura Ring) were compared to data available from ECG and a tri-axial wrist actigraph to evaluate quality and utility of heart rate and motion data. Machine learning methods were used to train and test sleep-wake classifiers, using data from consumer wearables. Quality of classifications derived from devices were compared. 
  Results:  For epoch-by-epoch sleep-wake performance, research devices ranged in d' between 1.771 and 1.874, with sensitivity between 0.912 and 0.982, and specificity between 0.366 and 0.647. Data from multisensor wearables were strongly correlated at an epoch-by-epoch level with reference data sources. Classifiers developed from the multisensor wearable data ranged in d' between 1.827 and 2.347, with sensitivity between 0.883 and 0.977, and specificity between 0.407 and 0.821. 
  Conclusions:  Data from multisensor consumer wearables is strongly correlated with reference devices at the epoch level and can be used to develop epoch-by-epoch models of sleep-wake rivaling existing research devices. 
  |  https://academic.oup.com/sleep/article-lookup/doi/10.1093/sleep/zsaa045  |  
------------------------------------------- 
10.7717/peerj.8854  |    Objective:  Bone age (BA) is a crucial indicator for revealing the growth and development of children. This study tested the performance of a fully automated artificial intelligence (AI) system for BA assessment of Chinese children with abnormal growth and development. 
  Materials and methods:  A fully automated AI system based on the Greulich and Pyle (GP) method was developed for Chinese children by using 8,000 BA radiographs from five medical centers nationwide in China. Then, a total of 745 cases (360 boys and 385 girls) with abnormal growth and development from another tertiary medical center of north China were consecutively collected between January and October 2018 to test the system. The reference standard was defined as the result interpreted by two experienced reviewers (a radiologist with 10 years and an endocrinologist with 15 years of experience in BA reading) through consensus using the GP atlas. BA accuracy within 1 year, root mean square error (RMSE), mean absolute difference (MAD), and 95% limits of agreement according to the Bland-Altman plot were statistically calculated. 
  Results:  For Chinese pediatric patients with abnormal growth and development, the accuracy of this new automated AI system within 1 year was 84.60% as compared to the reference standard, with the highest percentage of 89.45% in the 12- to 18-year group. The RMSE, MAD, and 95% limits of agreement of the AI system were 0.76 years, 0.58 years, and -1.547 to 1.428, respectively, according to the Bland-Altman plot. The largest difference between the AI and experts' BA result was noted for patients of short stature with bone deformities, severe osteomalacia, or different rates of maturation of the carpals and phalanges. 
  Conclusions:  The developed automated AI system could achieve comparable BA results to experienced reviewers for Chinese children with abnormal growth and development. 
  |  https://doi.org/10.7717/peerj.8854  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274267/  |  
------------------------------------------- 
10.3233/THC-191718  |    Background:  Applied research on artificial intelligence, mainly in deep learning, is widely performed. If medical images can be evaluated using artificial intelligence, this could substantially improve examination efficiency. 
  Objective:  We investigated an evaluation system for medical images with different noise characteristics using a deep convolutional neural network. 
  Methods:  Simulated computed tomography images are the targets of the system. We used an AlexNet trained with natural images for the deep convolutional neural network and a support vector machine for classification. Synthetic computed tomography images with circular and rectangular signal bodies at different levels of contrast and added Gaussian noise were used for training and testing. 
  Results:  Two transfer learning methods were tested: classification by a re-trained support vector machine using the AlexNet features, and a method that fine-tuned the deep convolutional neural network. Using the first method, all the test image noise levels could be classified correctly. The fine-tuning method achieved an accuracy rate of 92.6%. 
  Conclusions:  An image quality evaluation method using artificial intelligence will be useful for clinical images and different image quality indices in the future. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/THC-191718  |  
------------------------------------------- 
10.3174/ajnr.A6468  |   Intracranial aneurysms with subarachnoid hemorrhage lead to high morbidity and mortality. It is of critical importance to detect aneurysms, identify risk factors of rupture, and predict treatment response of aneurysms to guide clinical interventions. Artificial intelligence has received worldwide attention for its impressive performance in image-based tasks. Artificial intelligence serves as an adjunct to physicians in a series of clinical settings, which substantially improves diagnostic accuracy while reducing physicians' workload. Computer-assisted diagnosis systems of aneurysms based on MRA and CTA using deep learning have been evaluated, and excellent performances have been reported. Artificial intelligence has also been used in automated morphologic calculation, rupture risk stratification, and outcomes prediction with the implementation of machine learning methods, which have exhibited incremental value. This review summarizes current advances of artificial intelligence in the management of aneurysms, including detection and prediction. The challenges and future directions of clinical implementations of artificial intelligence are briefly discussed. 
  |  http://www.ajnr.org/cgi/pmidlookup?view=long&pmid=32165361  |  
------------------------------------------- 
10.1038/d41586-020-01181-3  |    |  https://doi.org/10.1038/d41586-020-01181-3  |  
------------------------------------------- 
10.1371/journal.pone.0231166  |   State-of-the-art machine learning (ML) artificial intelligence methods are increasingly leveraged in clinical predictive modeling to provide clinical decision support systems to physicians. Modern ML approaches such as artificial neural networks (ANNs) and tree boosting often perform better than more traditional methods like logistic regression. On the other hand, these modern methods yield a limited understanding of the resulting predictions. However, in the medical domain, understanding of applied models is essential, in particular, when informing clinical decision support. Thus, in recent years, interpretability methods for modern ML methods have emerged to potentially allow explainable predictions paired with high performance. To our knowledge, we present in this work the first explainability comparison of two modern ML methods, tree boosting and multilayer perceptrons (MLPs), to traditional logistic regression methods using a stroke outcome prediction paradigm. Here, we used clinical features to predict a dichotomized 90 days post-stroke modified Rankin Scale (mRS) score. For interpretability, we evaluated clinical features' importance with regard to predictions using deep Taylor decomposition for MLP, Shapley values for tree boosting and model coefficients for logistic regression. With regard to performance as measured by Area under the Curve (AUC) values on the test dataset, all models performed comparably: Logistic regression AUCs were 0.83, 0.83, 0.81 for three different regularization schemes; tree boosting AUC was 0.81; MLP AUC was 0.83. Importantly, the interpretability analysis demonstrated consistent results across models by rating age and stroke severity consecutively amongst the most important predictive features. For less important features, some differences were observed between the methods. Our analysis suggests that modern machine learning methods can provide explainability which is compatible with domain knowledge interpretation and traditional method rankings. Future work should focus on replication of these findings in other datasets and further testing of different explainability methods. 
  |  http://dx.plos.org/10.1371/journal.pone.0231166  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32251471/  |  
------------------------------------------- 
10.2196/16377  |   Recent advances in the collection and processing of health data from multiple sources at scale-known as big data-have become appealing across public health domains. However, present discussions often do not thoroughly consider the implications of big data or health informatics in the context of continuing health disparities. The 2 key objectives of this paper were as follows: first, it introduced 2 main problems of health big data in the context of health disparities-data absenteeism (lack of representation from underprivileged groups) and data chauvinism (faith in the size of data without considerations for quality and contexts). Second, this paper suggested that health organizations should strive to go beyond the current fad and seek to understand and coordinate efforts across the surrounding societal-, organizational-, individual-, and data-level contexts in a realistic manner to leverage big data to address health disparities. 
  |  https://www.jmir.org/2020/1/e16377/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31909724/  |  
------------------------------------------- 
10.1371/journal.pone.0228068  |   Hepatitis E is an enteric disease highly prevalent in the developing countries. The basis for high mortality among pregnant hepatitis E patients remains unclear. Importantly, a large proportion of infected pregnant women present with subclinical infection as well. In order to understand the possible mechanisms influencing clinical presentation of hepatitis E in pregnant women, we explored a system biology approach. For this, PBMCs from various categories were subjected to RNAseq analysis. These included non-pregnant (NPR, acute and convalescent phases) and pregnant (PR, 2nd and 3rd trimesters, acute phase and subclinical HEV infections) patients and corresponding healthy controls. The current study deals with immune response genes. In contrast to exclusive up-regulation of nonspecific, early immune response transcripts in the NPR patients, the PR patients exhibited broader and heightened expression of genes associated with innate as well as adaptive T and B cell responses. The study identified for the first time (1) inverse relationship of immunoglobulin (Ig) genes overexpression and (2) association of differential expression of S100 series genes with disease presentation. The data suggests possible involvement of TLR4 and NOD1 in pregnant patients and alpha defensins in all patient categories suggesting a role in protection. Induction of IFNγ gene was not detected during the acute phase irrespective of pregnancy. Association of response to vitamin D, transcripts related to NK/NKT and regulatory T cells during subclinical infection are noteworthy. The data obtained here could be correlated with several studies reported earlier in hepatitis E patients suggesting utility of PBMCs as an alternate specimen. The extensive, informative data provided here for the first time should form basis for future studies that will help in understanding pathogenesis of fulminant hepatitis E. 
  |  http://dx.plos.org/10.1371/journal.pone.0228068  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012176/  |  
------------------------------------------- 
10.1093/jamia/ocaa004  |    Objective:  Reliable longitudinal risk prediction for hospitalized patients is needed to provide quality care. Our goal is to develop a generalizable model capable of leveraging clinical notes to predict healthcare-associated diseases 24-96 hours in advance. 
  Methods:  We developed a reCurrent Additive Network for Temporal RIsk Prediction (CANTRIP) to predict the risk of hospital acquired (occurring ≥ 48 hours after admission) acute kidney injury, pressure injury, or anemia ≥ 24 hours before it is implicated by the patient's chart, labs, or notes. We rely on the MIMIC III critical care database and extract distinct positive and negative cohorts for each disease. We retrospectively determine the date-of-event using structured and unstructured criteria and use it as a form of indirect supervision to train and evaluate CANTRIP to predict disease risk using clinical notes. 
  Results:  Our experiments indicate that CANTRIP, operating on text alone, obtains 74%-87% area under the curve and 77%-85% Specificity. Baseline shallow models showed lower performance on all metrics, while bidirectional long short-term memory obtained the highest Sensitivity at the cost of significantly lower Specificity and Precision. 
  Discussion:  Proper model architecture allows clinical text to be successfully harnessed to predict nosocomial disease, outperforming shallow models and obtaining similar performance to disease-specific models reported in the literature. 
  Conclusion:  Clinical text on its own can provide a competitive alternative to traditional structured features (eg, lab values, vital signs). CANTRIP is able to generalize across nosocomial diseases without disease-specific feature extraction and is available at https://github.com/h4ste/cantrip. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocaa004  |  
------------------------------------------- 
10.1016/j.ygeno.2020.01.017  |   DNA replication is a fundamental task that plays a crucial role in the propagation of all living things on earth. Hence, the accurate identification of its origin could be the key to giving an insightful understanding of the regulatory mechanism of gene expression. Indeed, with the robust development of computational techniques and the abundant biological sequencing data, it has become possible for scientists to identify the origin of replication accurately and promptly. This growing concern has drawn a lot of attention among experts in this field. However, to gain better outcomes, more work is required. Therefore, this study is designed to explore the combination of state-of-the-art features and extreme gradient boosting learning system in classifying DNA sequences. Our hybrid approach is able to identify the origin of DNA replication with achieved sensitivity of 85.19%, specificity of 93.83%, accuracy of 89.51%, and MCC of 0.7931. Evidence is presented to show that our proposed method is superior to the state-of-the-art methods on the same benchmark dataset. Moreover, the research results represent a further step towards developing the prediction models for DNA replication in particular and DNA sequences in general. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0888-7543(19)30777-3  |  
------------------------------------------- 
10.1016/j.jneumeth.2019.108567  |    Background:  Feature selection is a crucial step in the machine learning methods that are currently used to assist with decoding brain states from fMRI data. This step can be based on either feature discrimination or feature reliability, but there is no clear evidence indicating which method is more suitable for fMRI data. 
  Methods:  We used ANOVA and Kendall's concordance coefficient as proxies for the two kinds of feature selection criteria. The performances of both methods were compared using different subject and feature numbers. The study included 987 subjects from the Human Connectome Project (HCP). 
  Results:  Classification performance suggested that features based on discrimination were more capable of distinguishing between various brain states for any number of subjects or extracted features. In addition, reliability-based features were always more stable than other features, and these properties (discernment and stability) of features, to some degree, related to the number of subjects and features. Furthermore, when the number of extracted features increased, the feature distributions also gradually extended from occipital lobe to more association regions of the brain. 
  Conclusion:  The results from this study provide empirical guides for feature selection for the prediction of individual brain states. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0165-0270(19)30424-8  |  
------------------------------------------- 
10.1097/SLA.0000000000003887  |    Objective:  To identify what strategies supervisors use to entrust autonomy during surgical procedures and to clarify the consequences of each strategy for a resident's level of autonomy. 
  Background:  Entrusting autonomy is at the core of teaching and learning surgical procedures. The better the level of autonomy matches the learning needs of residents, the steeper their learning curves. However, entrusting too much autonomy endangers patient outcome, while entrusting too little autonomy results in expertise gaps at the end of training. Understanding how supervisors regulate autonomy during surgical procedures is essential to improve intraoperative learning without compromising patient outcome. 
  Methods:  In an observational study, all the verbal and nonverbal interactions of 6 different supervisors and residents were captured by cameras. Using the iterative inductive process of conversational analysis, each supervisor initiative to guide the resident was identified, categorized, and analyzed to determine how supervisors affect autonomy of residents. 
  Results:  In the end, all the 475 behaviors of supervisors to regulate autonomy in this study could be classified into 4 categories and nine strategies: I) Evaluate the progress of the procedure: inspection (1), request for information (2), and expressing their expert opinion (3); II) Influence decision-making: explore (4), suggest (5), or declare the next decision (6); III) Influence the manual ongoing action: adjust (7), or stop the resident's manual activity (8); IV) take over (9). 
  Conclusions:  This study provides new insights into how supervisors regulate autonomy in the operating room. This insight is useful toward analyzing whether supervisors meet learning needs of residents as effectively as possible. 
  |  http://Insights.ovid.com/pubmed?pmid=32224741  |  
------------------------------------------- 
10.1002/jmri.27105  |   Glioblastoma is the most common and most malignant primary brain tumor. Despite aggressive multimodal treatment, its prognosis remains poor. Even with continuous developments in MRI, which has provided us with newer insights into the diagnosis and understanding of tumor biology, response assessment in the posttherapy setting remains challenging. We believe that the integration of additional information from advanced neuroimaging techniques can further improve the diagnostic accuracy of conventional MRI. In this article, we review the utility of advanced neuroimaging techniques such as diffusion-weighted imaging, diffusion tensor imaging, perfusion-weighted imaging, proton magnetic resonance spectroscopy, and chemical exchange saturation transfer in characterizing and evaluating treatment response in patients with glioblastoma. We will also discuss the existing challenges and limitations of using these techniques in clinical settings and possible solutions to avoiding pitfalls in study design, data acquisition, and analysis for future studies. LEVEL OF EVIDENCE: 2 TECHNICAL EFFICACY STAGE: 3. 
  |  None  |  
------------------------------------------- 
10.1007/s11948-020-00175-8  |   This article presents the first thematic review of the literature on the ethical issues concerning digital well-being. The term 'digital well-being' is used to refer to the impact of digital technologies on what it means to live a life that is good for a human being. The review explores the existing literature on the ethics of digital well-being, with the goal of mapping the current debate and identifying open questions for future research. The review identifies major issues related to several key social domains: healthcare, education, governance and social development, and media and entertainment. It also highlights three broader themes: positive computing, personalised human-computer interaction, and autonomy and self-determination. The review argues that three themes will be central to ongoing discussions and research by showing how they can be used to identify open questions related to the ethics of digital well-being. 
  |  https://dx.doi.org/10.1007/s11948-020-00175-8  |  
------------------------------------------- 
10.3390/ijerph17072461  |   A sensor is a device used to gather information registered by some biological, physical or chemical change, and then convert the information into a measurable signal. The first biosensor prototype was conceived more than a century ago, in 1906, but a properly defined biosensor was only developed later in 1956. Some of them have reached the commercial stage and are routinely used in environmental and agricultural applications, and especially, in clinical laboratory and industrial analysis, mostly because it is an economical, simple and efficient instrument for the in situ detection of the bioavailability of a broad range of environmental pollutants. We propose a narrative review, that found 32 papers and aims to discuss the possible uses of biosensors, focusing on their use in the area of occupational safety and health (OSH). 
  |  http://www.mdpi.com/resolver?pii=ijerph17072461  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32260295/  |  
------------------------------------------- 
10.3389/fpsyg.2020.00220  |   Advances in the use of neuroimaging in combination with A.I., and specifically the use of machine learning techniques, have led to the development of brain-reading technologies which, in the nearby future, could have many applications, such as lie detection, neuromarketing or brain-computer interfaces. Some of these could, in principle, also be used in forensic psychiatry. The application of these methods in forensic psychiatry could, for instance, be helpful to increase the accuracy of risk assessment and to identify possible interventions. This technique could be referred to as 'A.I. neuroprediction,' and involves identifying potential neurocognitive markers for the prediction of recidivism. However, the future implications of this technique and the role of neuroscience and A.I. in violence risk assessment remain to be established. In this paper, we review and analyze the literature concerning the use of brain-reading A.I. for neuroprediction of violence and rearrest to identify possibilities and challenges in the future use of these techniques in the fields of forensic psychiatry and criminal justice, considering legal implications and ethical issues. The analysis suggests that additional research is required on A.I. neuroprediction techniques, and there is still a great need to understand how they can be implemented in risk assessment in the field of forensic psychiatry. Besides the alluring potential of A.I. neuroprediction, we argue that its use in criminal justice and forensic psychiatry should be subjected to thorough harms/benefits analyses not only when these technologies will be fully available, but also while they are being researched and developed. 
  |  https://doi.org/10.3389/fpsyg.2020.00220  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32256422/  |  
------------------------------------------- 
10.1007/s00464-019-07240-9  |    Background:  Content-based image retrieval (CBIR) is an application of machine learning used to retrieve images by similarity on the basis of features. Our objective was to develop a CBIR system that could identify images containing the same polyp ('polyp fingerprint'). 
  Methods:  A machine learning technique called Bag of Words was used to describe each endoscopic image containing a polyp in a unique way. The system was tested with 243 white light images belonging to 99 different polyps (for each polyp there were at least two images representing it in two different temporal moments). Images were acquired in routine colonoscopies at Hospital Clínic using high-definition Olympus endoscopes. The method provided for each image the closest match within the dataset. 
  Results:  The system matched another image of the same polyp in 221/243 cases (91%). No differences were observed in the number of correct matches according to Paris classification (protruded: 90.7% vs. non-protruded: 91.3%) and size (&lt; 10 mm: 91.6% vs. &gt; 10 mm: 90%). 
  Conclusions:  A CBIR system can match accurately two images containing the same polyp, which could be a helpful aid for polyp image recognition. 
  |  https://doi.org/10.1007/s00464-019-07240-9  |  
------------------------------------------- 
10.3390/s20061753  |   The main purpose of the study was to develop a high accuracy system able to diagnose skin lesions using deep learning-based methods. We propose a new decision system based on multiple classifiers like neural networks and feature-based methods. Each classifier (method) gives the final decision system a certain weight, depending on the calculated accuracy, helping the system make a better decision. First, we created a neural network (NN) that can differentiate melanoma from benign nevus. The NN architecture is analyzed by evaluating it during the training process. Some biostatistic parameters, such as accuracy, specificity, sensitivity, and Dice coefficient are calculated. Then, we developed three other methods based on convolutional neural networks (CNNs). The CNNs were pre-trained using large ImageNet and Places365 databases. GoogleNet, ResNet-101, and NasNet-Large, were used in the enumeration order. CNN architectures were fine-tuned in order to distinguish the different types of skin lesions using transfer learning. The accuracies of the classifications were determined. The last proposed method uses the classical method of image object detection, more precisely, the one in which some features are extracted from the images, followed by the classification step. In this case, the classification was done by using a support vector machine. Just as in the first method, the sensitivity, specificity, Dice similarity coefficient and accuracy are determined. A comparison of the obtained results from all the methods is then done. As mentioned above, the novelty of this paper is the integration of these methods in a global fusion-based decision system that uses the results obtained by each individual method to establish the fusion weights. The results obtained by carrying out the experiments on two different free databases shows that the proposed system offers higher accuracy results. 
  |  http://www.mdpi.com/resolver?pii=s20061753  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245258/  |  
------------------------------------------- 
10.1007/s00134-020-05947-1  |    |  https://dx.doi.org/10.1007/s00134-020-05947-1  |  
------------------------------------------- 
10.3390/microorganisms8040549  |   Highly dimensional data generated from bacterial whole-genome sequencing is providing an unprecedented scale of information that requires an appropriate statistical analysis framework to infer biological function from populations of genomes. The application of genome-wide association study (GWAS) methods is an appropriate framework for bacterial population genome analysis that yields a list of candidate genes associated with a phenotype, but it provides an unranked measure of importance. Here, we validated a novel framework to define infection mechanism using the combination of GWAS, machine learning, and bacterial population genomics that ranked allelic variants that accurately identified disease. This approach parsed a dataset of 1.2 million single nucleotide polymorphisms (SNPs) and indels that resulted in an importance ranked list of associated alleles of <i>porA</i> in <i>Campylobacter</i> <i>jejuni</i> using spatiotemporal analysis over 30 years. We validated this approach using previously proven laboratory experimental alleles from an in vivo guinea pig abortion model. This framework, termed µPathML, defined intestinal and extraintestinal groups that have differential allelic <i>porA</i> variants that cause abortion. Divergent variants containing indels that defeated automated annotation were rescued using biological context and knowledge that resulted in defining rare, divergent variants that were maintained in the population over two continents and 30 years. This study defines the capability of machine learning coupled with GWAS and population genomics to simultaneously identify and rank alleles to define their role in infectious disease mechanisms. 
  |  http://www.mdpi.com/resolver?pii=microorganisms8040549  |  
------------------------------------------- 
10.1016/j.avsg.2020.04.022  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0890-5096(20)30341-1  |  
------------------------------------------- 
10.1109/TCYB.2020.2979803  |   In multiagent reinforcement learning (MARL), it is crucial for each agent to model the relation with its neighbors. Existing approaches usually resort to concatenate the features of multiple neighbors, fixing the size and the identity of the inputs. But these settings are inflexible and unscalable. In this article, we propose an attentive relational encoder (ARE), which is a novel scalable feedforward neural module, to attentionally aggregate an arbitrary-sized neighboring feature set for state representation in the decentralized MARL. The ARE actively selects the relevant information from the neighboring agents and is permutation invariant, computationally efficient, and flexible to interactive multiagent systems. Our method consistently outperforms the latest competing decentralized MARL methods in several multiagent tasks. In particular, it shows strong cooperative performance in challenging StarCraft micromanagement tasks and achieves over a 96% winning rate against the most difficult noncheating built-in artificial intelligence bots. 
  |  https://dx.doi.org/10.1109/TCYB.2020.2979803  |  
------------------------------------------- 
10.1111/den.13649  |    |  https://doi.org/10.1111/den.13649  |  
------------------------------------------- 
10.1080/21507740.2020.1740355  |   This article examines the ethical and policy implications of using voice computing and artificial intelligence to screen for mental health conditions in low income and minority populations. Mental health is unequally distributed among these groups, which is further exacerbated by increased barriers to psychiatric care. Advancements in voice computing and artificial intelligence promise increased screening and more sensitive diagnostic assessments. Machine learning algorithms have the capacity to identify vocal features that can screen those with depression. However, in order to screen for mental health pathology, computer algorithms must first be able to account for the fundamental differences in vocal characteristics between low income minorities and those who are not. While researchers have envisioned this technology as a beneficent tool, this technology could be repurposed to scale up discrimination or exploitation. Studies on the use of big data and predictive analytics demonstrate that low income minority populations already face significant discrimination. This article urges researchers developing AI tools for vulnerable populations to consider the full ethical, legal, and social impact of their work. Without a national, coherent framework of legal regulations and ethical guidelines to protect vulnerable populations, it will be difficult to limit AI applications to solely beneficial uses. Without such protections, vulnerable populations will rightfully be wary of participating in such studies which also will negatively impact the robustness of such tools. Thus, for research involving AI tools like voice computing, it is in the research community's interest to demand more guidance and regulatory oversight from the federal government. 
  |  None  |  
------------------------------------------- 
10.1016/j.jacr.2019.07.004  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(19)30824-5  |  
------------------------------------------- 
10.1007/s13167-020-00199-x  |    Background:  Cellulite is a common physiological condition of dermis, epidermis, and subcutaneous tissues experienced by 85 to 98% of the post-pubertal females in developed countries. Infrared (IR) thermography combined with artificial intelligence (AI)-based automated image processing can detect both early and advanced cellulite stages and open up the possibility of reliable diagnosis. Although the cellulite lesions may have various levels of severity, the quality of life of every woman, both in the physical and emotional sphere, is always an individual concern and therefore requires patient-oriented approach. 
  Objectives:  The purpose of this work was to elaborate an objective, fast, and cost-effective method for automatic identification of different stages of cellulite based on IR imaging that may be used for prescreening and personalization of the therapy. 
  Materials and methods:  In this study, we use custom-developed image preprocessing algorithms to automatically select cellulite regions and combine a total of 9 feature extraction methods with 9 different classification algorithms to determine the efficacy of cellulite stage recognition based on thermographic images taken from 212 female volunteers aged between 19 and 22. 
  Results:  A combination of histogram of oriented gradients (HOG) and artificial neural network (ANN) enables determination of all stages of cellulite with an average accuracy higher than 80%. For primary stages of cellulite, the average accuracy achieved was more than 90%. 
  Conclusions:  The implementation of computer-aided, automatic identification of cellulite severity using infrared imaging is feasible for reliable diagnosis. Such a combination can be used for early diagnosis, as well as monitoring of cellulite progress or therapeutic outcomes in an objective way. IR thermography coupled to AI sets the vision towards their use as an effective tool for complex assessment of cellulite pathogenesis and stratification, which are critical in the implementation of IR thermographic imaging in predictive, preventive, and personalized medicine (PPPM). 
  |  https://dx.doi.org/10.1007/s13167-020-00199-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32140183/  |  
------------------------------------------- 
10.1016/j.jhqr.2019.07.012  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2603-6479(20)30005-1  |  
------------------------------------------- 
10.1088/1361-6579/ab8770  |    Objective:  Heart abnormality detection using heart sound signals (phonocardiogram) has been an active research area for the last few decades. In this paper, automatic heart sound classification using segmented and unsegmented phonocardiogram (PCG) signals is presented. 
  Approach:  In this paper, we performed (1) an in-depth analysis of various time and frequency domain features, followed by experimental determination of effective feature subsets for improved classification performance; (2) both segmented and unsegmented phonocardiogram signals are studied and important results concerning the respective feature subsets and their classification performances are reported; (3) different classification algorithms including support vector machine, kth nearest neighbor, decision tree, ensemble classifier, artificial neural network and long short-term memory (LSTM) networks are employed to evaluate the performance of the proposed feature subsets and their comparison with other established features and methods is presented. 
  Main results:  It is observed that although LSTM performed better on Mel Frequency Cepstral Coefficients (MFCC) features extracted from unsegmented PCG data with AUC score of 91.39%, however, MFCC features did not show consistent performance with other classifiers (as the second highest AUC score is 62.08% with decision tree classifier). In contrast, in case of time-frequency features from segmented data, performance of all the classifiers was appreciable with an over 70% of AUC scores. In particular, the conventional machine learning techniques showed consistency in achieving over 80% of AUC scores. 
  Significance:  The results of this study highlight the importance of time and frequency domain features thus it is concluded to employ both time and frequency features of segmented phonocardiogram signals to achieve improved classification. 
  |  https://doi.org/10.1088/1361-6579/ab8770  |  
------------------------------------------- 
10.1007/s00104-019-01091-9  |   Artificial intelligence (AI) is a very relevant topic for the medicine of the future. This article focuses on the field of AI in the context of orthopedics and trauma surgery. The main focus is on the potentials of AI in the analysis of symptoms, radiological images, clinical data sets, use in hospitals and operating theaters as well as for training and education. For the orthopedics and trauma surgery of the future AI is much more than pure fiction; however, there is still a long way to go before the potential of an optimized and individualized patient care can be utilized. Interdisciplinary and international approaches, including personnel, economic, legal and ethical aspects will play a decisive role in this respect. 
  |  https://dx.doi.org/10.1007/s00104-019-01091-9  |  
------------------------------------------- 
10.1097/MPG.0000000000002507  |   Artificial intelligence (AI), a discipline encompassed by data science, has seen recent rapid growth in its application to healthcare and beyond, and is now an integral part of daily life. Uses of AI in gastroenterology include the automated detection of disease and differentiation of pathology subtypes and disease severity. Although a majority of AI research in gastroenterology focuses on adult applications, there are a number of pediatric pathologies that could benefit from more research. As new and improved diagnostic tools become available and more information is retrieved from them, AI could provide physicians a method to distill enormous amounts of data into enhanced decision-making and cost saving for children with digestive disorders. This review provides a broad overview of AI and examples of its possible applications in pediatric gastroenterology. 
  |  http://dx.doi.org/10.1097/MPG.0000000000002507  |  
------------------------------------------- 
10.1080/03602532.2020.1726944  |   Historically, failure rates in drug development are high; increased sophistication and investment throughout the process has shifted the reasons for attrition, but the overall success rates have remained stubbornly and consistently low. Only 8% of new entities entering clinical testing gain regulatory approval, indicating that significant obstacles still exist for efficient therapeutic development. The continued high failure rate can be partially attributed to the inability to link drug exposure with the magnitude of observed safety and efficacy-related pharmacodynamic (PD) responses; frequently, this is a result of nonclinical models exhibiting poor prediction of human outcomes across a wide range of disease conditions, resulting in faulty evaluation of drug toxicology and efficacy. However, the increasing quality and standardization of experimental methods in preclinical stages of testing has created valuable data sets within companies that can be leveraged to further improve the efficiency and accuracy of preclinical prediction for both pharmacokinetics (PK) and PD. Models of Quantitative structure-activity relationships (QSAR), physiologically based pharmacokinetics (PBPK), and PK/PD relationships have also improved efficiency. Founded on a core understanding of biochemistry and physiological interactions of xenobiotics, these <i>in silico</i> methods have the potential to increase the probability of compound success in clinical trials. Integration of traditional computational methods with machine-learning approaches and existing internal pharma databases stands to make a fundamental impact on the speed and accuracy of predictions during the process of drug development and approval. 
  |  http://www.tandfonline.com/doi/full/10.1080/03602532.2020.1726944  |  
------------------------------------------- 
10.1109/TNSRE.2020.2966784  |   Recent studies have shown that balance performance assessment based on artificial intelligence (AI) is feasible. However, balance control is very complex and requires different subsystems to participate, which have not been evaluated individually yet. Furthermore, these studies only classified individual's balance performance across limited grades. Therefore, in this study we attempted to implement AI to precisely evaluate different types of balance control subsystems (BCSes). First, a total of 224 commonly used and newly developed features were extracted from the center of pressure (CoP) data for each participant, respectively. Then, regressors were employed in order to map these features to the evaluation scores given by physical therapists, which include the total score in Mini-Balance-Evaluation-Systems-Tests (Mini-BESTest) and its sub-scores on BCSes, namely anticipatory postural adjustments (APA), reactive postural control (RPC), sensory orientation (SO), and dynamic gait (DG). Their scoring ranges should be 0-28, 0-6, 0-6, 0-6, and 0-10, respectively. The results show that their minimum mean absolute errors from AI estimation reach up to 2.658, 0.827, 0.970, 0.642, and 0.98, respectively. In sum, our study is a preliminary study for assessing BCSes based on AI, which shows its possibility to be used in the clinics in the future. 
  |  https://dx.doi.org/10.1109/TNSRE.2020.2966784  |  
------------------------------------------- 
10.3389/fcvm.2020.00025  |   Deep learning has become the most widely used approach for cardiac image segmentation in recent years. In this paper, we provide a review of over 100 cardiac image segmentation papers using deep learning, which covers common imaging modalities including magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound and major anatomical structures of interest (ventricles, atria, and vessels). In addition, a summary of publicly available cardiac image datasets and code repositories are included to provide a base for encouraging reproducible research. Finally, we discuss the challenges and limitations with current deep learning-based approaches (scarcity of labels, model generalizability across different domains, interpretability) and suggest potential directions for future research. 
  |  https://doi.org/10.3389/fcvm.2020.00025  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32195270/  |  
------------------------------------------- 
10.1002/hep.31103  |   Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology. 
  |  https://doi.org/10.1002/hep.31103  |  
------------------------------------------- 
10.3389/fchem.2020.00203  |   The ultra-high-field magnetic resonance imaging (MRI) nowadays has been receiving enormous attention in both biomaterial research and clinical diagnosis. MRI contrast agents are generally comprising of T<sub>1</sub>-weighted and T<sub>2</sub>-weighted contrast agent types, where T<sub>1</sub>-weighted contrast agents show positive contrast enhancement with brighter images by decreasing the proton's longitudinal relaxation times and T<sub>2</sub>-weighted contrast agents show negative contrast enhancement with darker images by decreasing the proton's transverse relaxation times. To meet the incredible demand of MRI, ultra-high-field T<sub>2</sub> MRI is gradually attracting the attention of research and medical needs owing to its high resolution and high accuracy for detection. It is anticipated that high field MRI contrast agents can achieve high performance in MRI imaging, where parameters of chemical composition, molecular structure and size of varied contrast agents show contrasted influence in each specific diagnostic test. This review firstly presents the recent advances of nanoparticle contrast agents for MRI. Moreover, multimodal molecular imaging with MRI for better monitoring is discussed during biological process. To fasten the process of developing better contrast agents, deep learning of artificial intelligent (AI) can be well-integrated into optimizing the crucial parameters of nanoparticle contrast agents and achieving high resolution MRI prior to the clinical applications. Finally, prospects and challenges are summarized. 
  |  https://doi.org/10.3389/fchem.2020.00203  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32266217/  |  
------------------------------------------- 
10.1259/bjr.20190812  |   In this review, we describe the technical aspects of artificial intelligence (AI) in cardiac imaging, starting with radiomics, basic algorithms of deep learning and application tasks of algorithms, until recently the availability of the public database. Subsequently, we conducted a systematic literature search for recently published clinically relevant studies on AI in cardiac imaging. As a result, 24 and 14 studies using CT and MRI, respectively, were included and summarized. From these studies, it can be concluded that AI is widely applied in cardiac applications in the clinic, including coronary calcium scoring, coronary CT angiography, fractional flow reserve CT, plaque analysis, left ventricular myocardium analysis, diagnosis of myocardial infarction, prognosis of coronary artery disease, assessment of cardiac function, and diagnosis and prognosis of cardiomyopathy. These advancements show that AI has a promising prospect in cardiac imaging. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190812?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s00104-020-01132-8  |   Less than 10 years ago a breakthrough was made in the world of computer science and artificial intelligence (AI) with the application of deep neural networks, which initially found little attention in medicine. In 2017 the first high-ranking publications on the medical application of AI were published. The potential of AI became known to many both in clinical medicine as well as in clinical and biomedical research. At the end of 2019 a phase of upheaval is occurring: first concepts for regulatory procedures have appeared, a large number of start-ups but also established companies are endeavoring to introduce AI-based medical devices into the market. This article discusses the basic principles for understanding AI-based medical devices as well as an overview of current AI-based solutions specific to cardiac surgery. 
  |  https://dx.doi.org/10.1007/s00104-020-01132-8  |  
------------------------------------------- 
10.1093/toxsci/kfaa053  |   Cerebral chemical stability, which underlies normal human thinking, learning, and behavior, is strictly regulated by brain barrier systems that separate the blood circulation from brain extracellular fluids. The blood-brain barrier (BBB), whose structural basis is capillary endothelial cells sealed by tight junctions, disconnects the blood from cerebral interstitial fluid. The BBB is central to the neurovascular unit which includes pericytes and astrocyte end-feet and allows regional coupling of blood supply with neuronal activity. The epithelial cells of choroid plexuses are also sealed by tight junctions, but separate blood from the ventricular cerebrospinal fluid (CSF), constituting the blood-CSF barrier. Much more than physical barriers, these cellular monolayers transport materials and produce endogenous proteins for the brain, eliminate cerebral metabolites, and regulate neuro-immune interactions. Understanding brain barrier properties is currently rapidly advancing, thanks to powerful technology innovations that facilitate in-depth barrier functional studies. 
  |  https://academic.oup.com/toxsci/article-lookup/doi/10.1093/toxsci/kfaa053  |  
------------------------------------------- 
10.1002/chem.202000246  |   Since the concept of Deep Learning (DL) was formally proposed in 2006, it had a major impact on academic research and industry. Nowadays, DL provides an unprecedented way to analyze and process data with demonstrated great results in computer vision, medical imaging, natural language processing, etc. In this Minireview, we summarize applications of DL in Nuclear Magnetic Resonance (NMR) spectroscopy and outline a perspective for DL as entirely new approaches that are likely to transform NMR spectroscopy into a much more efficient and powerful technique in chemistry and life science. 
  |  https://doi.org/10.1002/chem.202000246  |  
------------------------------------------- 
10.1007/s00117-020-00646-w  |    Clinical issue:  Hybrid imaging enables the precise visualization of cellular metabolism by combining anatomical and metabolic information. Advances in artificial intelligence (AI) offer new methods for processing and evaluating this data. 
  Methodological innovations:  This review summarizes current developments and applications of AI methods in hybrid imaging. Applications in image processing as well as methods for disease-related evaluation are presented and discussed. 
  Materials and methods:  This article is based on a selective literature search with the search engines PubMed and arXiv. 
  Assessment:  Currently, there are only a few AI applications using hybrid imaging data and no applications are established in clinical routine yet. Although the first promising approaches are emerging, they still need to be evaluated prospectively. In the future, AI applications will support radiologists and nuclear medicine radiologists in diagnosis and therapy. 
  |  https://dx.doi.org/10.1007/s00117-020-00646-w  |  
------------------------------------------- 
10.1016/j.radi.2020.03.007  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1078-8174(20)30037-7  |  
------------------------------------------- 
10.1080/17460441.2020.1745183  |   <b>Introduction</b>: Deep discriminative and generative neural-network models are becoming an integral part of the modern approach to ligand-based novel drug discovery. The variety of different architectures of neural networks, the methods of their training, and the procedures of generating new molecules require expert knowledge to choose the most suitable approach.<b>Areas covered</b>: Three different approaches to deep learning use in ligand-based drug discovery are considered: virtual screening, neural generative models, and mutation-based structure generation. Several architectures of neural networks for building either discriminative or generative models are considered in this paper, including deep multilayer neural networks, different kinds of convolutional neural networks, recurrent neural networks, and several types of autoencoders. Several kinds of learning frameworks are also considered, including adversarial learning and reinforcement learning. Different types of representations for generating molecules, including SMILES, graphs, and several alternative string representations are also considered.<b>Expert opinion</b>: Two kinds of problem should be solved in order to make the models built using deep neural networks, especially generative models, a valuable option in ligand-based drug discovery: the issue of interpretability and explainability of deep-learning models and the issue of synthetic accessibility of novel compounds designed by deep-learning algorithms. 
  |  http://www.tandfonline.com/doi/full/10.1080/17460441.2020.1745183  |  
------------------------------------------- 
10.4103/jpi.jpi_64_19  |   The introduction of digital pathology is changing the practice of diagnostic anatomic pathology. Digital pathology offers numerous advantages over using a physical slide on a physical microscope, including more discriminative tools to render a more precise diagnostic report. The development of these tools is being facilitated by public challenges related to specific diagnostic tasks within anatomic pathology. To date, 24 public challenges related to pathology tasks have been published. This article discusses these public challenges and briefly reviews the underlying characteristics of public challenges and why they are helpful to the development of digital tools. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32318315/  |  
------------------------------------------- 
10.1002/path.5388  |   Tissue diagnostics is the world of pathologists, and it is increasingly becoming digitalised to leverage the enormous potential of personalised medicine and of stratifying patients, enabling the administration of modern therapies. Therefore, the daily task for pathologists is changing drastically and will become increasingly demanding in order to take advantage of the development of modern computer technologies. The role of pathologist has rapidly evolved from exclusively describing the morphology and phenomenology of a disease, to becoming a gatekeeper for novel and most effective treatment options. This is possible based on the retrieval and management of a wide range of complex information from tissue or a group of cells and associated meta-data. Intelligent and self-learning software solutions can support and guide pathologists to score clinically relevant decisions based on the accurate and robust quantification of multiple target molecules or surrogate biomarker as companion or complimentary diagnostics along with relevant spatial relationships and contextual information from digital H&amp;E and multiplexed images. With the availability of multiplex staining techniques on a single slide, high-resolution image analysis tools, and high-end computer hardware, machine and deep learning solutions now offer diagnostic rulesets and algorithms that still require clinical validation in well-designed studies. Before entering the clinical practice, the 'human factor' pathologist needs to develop trust in the output coming from the 'digital black box of computational pathology', including image analysis solutions and artificial intelligence algorithms to support critical clinical decisions which otherwise would not be available. © 2020 Pathological Society of Great Britain and Ireland. Published by John Wiley &amp; Sons, Ltd. 
  |  https://doi.org/10.1002/path.5388  |  
------------------------------------------- 
10.1097/CCM.0000000000004236  |    Objectives:  As the performance of a conventional track and trigger system in a rapid response system has been unsatisfactory, we developed and implemented an artificial intelligence for predicting in-hospital cardiac arrest, denoted the deep learning-based early warning system. The purpose of this study was to compare the performance of an artificial intelligence-based early warning system with that of conventional methods in a real hospital situation. 
  Design:  Retrospective cohort study. 
  Setting:  This study was conducted at a hospital in which deep learning-based early warning system was implemented. 
  Patients:  We reviewed the records of adult patients who were admitted to the general ward of our hospital from April 2018 to March 2019. 
  Interventions:  The study population included 8,039 adult patients. A total 83 events of deterioration occurred during the study period. The outcome was events of deterioration, defined as cardiac arrest and unexpected ICU admission. We defined a true alarm as an alarm occurring within 0.5-24 hours before a deteriorating event. 
  Measurements and main results:  We used the area under the receiver operating characteristic curve, area under the precision-recall curve, number needed to examine, and mean alarm count per day as comparative measures. The deep learning-based early warning system (area under the receiver operating characteristic curve, 0.865; area under the precision-recall curve, 0.066) outperformed the modified early warning score (area under the receiver operating characteristic curve, 0.682; area under the precision-recall curve, 0.010) and reduced the number needed to examine and mean alarm count per day by 69.2% and 59.6%, respectively. At the same specificity, deep learning-based early warning system had up to 257% higher sensitivity than conventional methods. 
  Conclusions:  The developed artificial intelligence based on deep-learning, deep learning-based early warning system, accurately predicted deterioration of patients in a general ward and outperformed conventional methods. This study showed the potential and effectiveness of artificial intelligence in an rapid response system, which can be applied together with electronic health records. This will be a useful method to identify patients with deterioration and help with precise decision-making in daily practice. 
  |  https://dx.doi.org/10.1097/CCM.0000000000004236  |  
------------------------------------------- 
10.1097/IIO.0000000000000298  |    |  https://dx.doi.org/10.1097/IIO.0000000000000298  |  
------------------------------------------- 
10.1097/MNH.0000000000000598  |    Purpose of review:  Successful integration of artificial intelligence into extant clinical workflows is contingent upon a number of factors including clinician comprehension and interpretation of computer vision. This article discusses how image analysis and machine learning have enabled comprehensive characterization of kidney morphology for development of automated diagnostic and prognostic renal pathology applications. 
  Recent findings:  The primordial digital pathology informatics work employed classical image analysis and machine learning to prognosticate renal disease. Although this classical approach demonstrated tremendous potential, subsequent advancements in hardware technology rendered artificial neural networks '(ANNs) the method of choice for machine vision in computational pathology'. Offering rapid and reproducible detection, characterization and classification of kidney morphology, ANNs have facilitated the development of diagnostic and prognostic applications. In addition, modern machine learning with ANNs has revealed novel biomarkers in kidney disease, demonstrating the potential for machine vision to elucidate novel pathologic mechanisms beyond extant clinical knowledge. 
  Summary:  Despite the revolutionary developments potentiated by modern machine learning, several challenges remain, including data quality control and curation, image annotation and ontology, integration of multimodal data and interpretation of machine vision or 'opening the black box'. Resolution of these challenges will not only revolutionize diagnostic pathology but also pave the way for precision medicine and integration of artificial intelligence in the process of care. 
  |  http://dx.doi.org/10.1097/MNH.0000000000000598  |  
------------------------------------------- 
10.1016/j.cmi.2020.03.012  |    Background:  Microbiologists are valued for their time-honed skills in image analysis, including identification of pathogens and inflammatory context in Gram stains, ova and parasite preparations, blood smears and histopathologic slides. They also must classify colony growth on a variety of agar plates for triage and assessment. Recent advances in image analysis, in particular application of artificial intelligence (AI), have the potential to automate these processes and support more timely and accurate diagnoses. 
  Objectives:  To review current AI-based image analysis as applied to clinical microbiology; and to discuss future trends in the field. 
  Sources:  Material sourced for this review included peer-reviewed literature annotated in the PubMed or Google Scholar databases and preprint articles from bioRxiv. Articles describing use of AI for analysis of images used in infectious disease diagnostics were reviewed. 
  Content:  We describe application of machine learning towards analysis of different types of microbiologic image data. Specifically, we outline progress in smear and plate interpretation as well as the potential for AI diagnostic applications in the clinical microbiology laboratory. 
  Implications:  Combined with automation, we predict that AI algorithms will be used in the future to prescreen and preclassify image data, thereby increasing productivity and enabling more accurate diagnoses through collaboration between the AI and the microbiologist. Once developed, image-based AI analysis is inexpensive and amenable to local and remote diagnostic use. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1198-743X(20)30155-5  |  
------------------------------------------- 
10.1093/ajcp/aqaa001  |    Objectives:  To assess and improve the assistive role of a deep, densely connected convolutional neural network (CNN) to hematopathologists in differentiating histologic images of Burkitt lymphoma (BL) from diffuse large B-cell lymphoma (DLBCL). 
  Methods:  A total of 10,818 images from BL (n = 34) and DLBCL (n = 36) cases were used to either train or apply different CNNs. Networks differed by number of training images and pixels of images, absence of color, pixel and staining augmentation, and depth of the network, among other parameters. 
  Results:  Cases classified correctly were 17 of 18 (94%), nine with 100% of images correct by the best performing network showing a receiver operating characteristic curve analysis area under the curve 0.92 for both DLBCL and BL. The best performing CNN used all available training images, two random subcrops per image of 448 × 448 pixels, random H&amp;E staining image augmentation, random horizontal flipping of images, random alteration of contrast, reduction on validation error plateau of 15 epochs, block size of six, batch size of 32, and depth of 22. Other networks and decreasing training images had poorer performance. 
  Conclusions:  CNNs are promising augmented human intelligence tools for differentiating a subset of BL and DLBCL cases. 
  |  https://academic.oup.com/ajcp/article-lookup/doi/10.1093/ajcp/aqaa001  |  
------------------------------------------- 
10.1007/s10877-020-00474-2  |   Intensive care unit (ICU) patients develop stress induced insulin resistance causing hyperglycemia, large glucose variability and hypoglycemia. These glucose metrics have all been associated with increased rates of morbidity and mortality. The only way to achieve safe glucose control at a lower glucose range (e.g., 4.4-6.6 mmol/L) will be through use of an autonomous closed loop glucose control system (artificial pancreas). Our goal with the present study was to assess the safety and performance of an artificial pancreas system, composed of the EIRUS (Maquet Critical Care AB) continuous glucose monitor (CGM) and novel artificial intelligence-based glucose control software, in a swine model using unannounced hypo- and hyperglycemia challenges. Fourteen piglets (6 control, 8 treated) underwent sequential unannounced hypoglycemic and hyperglycemic challenges with 3 IU of NovoRapid and a glucose infusion at 17 mg/kg/min over the course of 5 h. In the Control animals an experienced ICU physician used every 30-min blood glucose values to maintain control to a range of 4.4-9 mmol/L. In the Treated group the artificial pancreas system attempted to maintain blood glucose control to a range of 4.4-6.6 mmol/L. Five of six Control animals and none of eight Treated animals experienced severe hypoglycemia (&lt; 2.22 mmol/L). The area under the curve 3.5 mmol/L was 28.9 (21.1-54.2) for Control and 4.8 (3.1-5.2) for the Treated animals. The total percent time within tight glucose control range, 4.4-6.6 mmol/L, was 32.8% (32.4-47.1) for Controls and 55.4% (52.9-59.4) for Treated (p &lt; 0.034). Data are median and quartiles. The artificial pancreas system abolished severe hypoglycemia and outperformed the experienced ICU physician in avoiding clinically significant hypoglycemic excursions. 
  |  https://doi.org/10.1007/s10877-020-00474-2  |  
------------------------------------------- 
10.1208/s12249-020-01660-w  |   Low solubility of active pharmaceutical compounds (APIs) remains an important challenge in dosage form development process. In the manuscript, empirical models were developed and analyzed in order to predict dissolution of bicalutamide (BCL) from solid dispersion with various carriers. BCL was chosen as an example of a poor water-soluble API. Two separate datasets were created: one from literature data and another based on in-house experimental data. Computational experiments were conducted using artificial intelligence tools based on machine learning (AI/ML) with a plethora of techniques including artificial neural networks, decision trees, rule-based systems, and evolutionary computations. The latter resulting in classical mathematical equations provided models characterized by the lowest prediction error. In-house data turned out to be more homogeneous, as well as formulations were more extensively characterized than literature-based data. Thus, in-house data resulted in better models than literature-based data set. Among the other covariates, the best model uses for prediction of BCL dissolution profile the transmittance from IR spectrum at 1260 cm<sup>-1</sup> wavenumber. Ab initio modeling-based in silico simulations were conducted to reveal potential BCL-excipients interaction. All crucial variables were selected automatically by AI/ML tools and resulted in reasonably simple and yet predictive models suitable for application in Quality by Design (QbD) approaches. Presented data-driven model development using AI/ML could be useful in various problems in the field of pharmaceutical technology, resulting in both predictive and investigational tools revealing new knowledge. 
  |  https://dx.doi.org/10.1208/s12249-020-01660-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32236750/  |  
------------------------------------------- 
10.1016/j.arbres.2019.12.037  |    |  http://www.archbronconeumol.org/en/linksolver/ft/pii/S0300-2896(20)30033-8  |  
------------------------------------------- 
10.1021/acs.analchem.9b04946  |   Raman spectroscopy is a nondestructive, label-free, highly specific approach that provides the chemical information on materials. Thus, it is suitable to be used as an effective analytical tool to characterize biological samples. Here we introduce a novel method that uses artificial intelligence to analyze biological Raman spectra and identify the microbes at a single-cell level. The combination of a framework of convolutional neural network (ConvNet) and Raman spectroscopy allows the extraction of the Raman spectral features of a single microbial cell and then categorizes cells according to their spectral features. As the proof of concept, we measured Raman spectra of 14 microbial species at a single-cell level and constructed an optimal ConvNet model using the Raman data. The average accuracy of classification by ConvNet is 95.64 ± 5.46%. Meanwhile, we introduced an occlusion-based Raman spectra feature extraction to visualize the weights of Raman features for distinguishing different species. 
  |  https://dx.doi.org/10.1021/acs.analchem.9b04946  |  
------------------------------------------- 
10.1097/MD.0000000000019239  |   Despite the availability of a series of tests, detection of chronic traumatic osteomyelitis is still exhausting in clinical practice. We hypothesized that machine learning based on computed-tomography (CT) images would provide better diagnostic performance for extremity traumatic chronic osteomyelitis than the serological biomarker alone. A retrospective study was carried out to collect medical data from patients with extremity traumatic osteomyelitis according to the criteria of musculoskeletal infection society. In each patient, serum levels of C-reactive protein (CRP), erythrocyte sedimentation rate (ESR), and D-dimer were measured and CT scan of the extremity was conducted 7 days after admission preoperatively. A deep residual network (ResNet) machine learning model was established for recognition of bone lesion on the CT image. A total of 28,718 CT images from 163 adult patients were included. Then, we randomly extracted 80% of all CT images from each patient for training, 10% for validation, and 10% for testing. Our results showed that machine learning (83.4%) outperformed CRP (53.2%), ESR (68.8%), and D-dimer (68.1%) separately in accuracy. Meanwhile, machine learning (88.0%) demonstrated highest sensitivity when compared with CRP (50.6%), ESR (73.0%), and D-dimer (51.7%). Considering the specificity, machine learning (77.0%) is better than CRP (59.4%) and ESR (62.2%), but not D-dimer (83.8%). Our findings indicated that machine learning based on CT images is an effective and promising avenue for detection of chronic traumatic osteomyelitis in the extremity. 
  |  http://dx.doi.org/10.1097/MD.0000000000019239  |  
------------------------------------------- 
10.12659/MSM.920754  |   BACKGROUND Rupture of intracranial aneurysms (IA) is associated with high rates of mortality around the world. Use of intestinal probiotics can regulate the pathophysiology of aneurysms, but the details of the mechanism involved have been unclear. MATERIAL AND METHODS The GEO2R analysis website was used to detect the DEGs between IAs, AAAs, samples after supplementation with probiotics, and normal samples. The online tool DAVID provides functional classification and annotation analyses of associated genes, including GO and KEGG pathway. PPI of these DEGs was analyzed based on the STRING database, followed by analysis using Cytoscape software. RESULTS We found 170 intersecting DEGs (contained in GSE75240 and more than 2 of the 4 aneurysms datasets), 5 intersecting DEGs (contained in all datasets) and 1 intersecting DEG (contained in GSE75240 and all IAs datasets). GO analysis results suggested that the DEGs primarily participate in signal transduction, cell adhesion, immune response, response to drug, extracellular matrix organization, cell-cell signaling, and inflammatory response in the BP terms, and the KEGG pathways are mainly enriched in focal adhesion, cytokine-cytokine receptor interaction, ECM-receptor interaction, amoebiasis, chemokine signaling pathway, proteoglycans, and PI3K-Akt signaling pathway in cancer pathways. Through PPI network analysis, we confirmed 2 candidates for further study: CAV1 and MYH11. These downregulated DEGs are associated with the formation of aneurysms, and the change of these DEGs is the opposite in probiotics-treated animals. CONCLUSIONS Our study suggests that MYH11 and CAV1 are potential target genes for prevention of aneurysms. Further experiments are needed to verify these findings. 
  |  https://www.medscimonit.com/download/index/idArt/920754  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32141441/  |  
------------------------------------------- 
10.1055/s-0039-3400511  |    |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400511  |  
------------------------------------------- 
10.1097/CORR.0000000000001189  |    |  http://Insights.ovid.com/pubmed?pmid=32101887  |  
------------------------------------------- 
10.21873/anticanres.13949  |    Background/aim:  To investigate whether a radiomic machine learning (ML) approach employing texture-analysis (TA) features extracted from primary tumor lesions (PTLs) is able to predict tumor grade (TG) and nodal status (NS) in patients with oropharyngeal (OP) and oral cavity (OC) squamous-cell carcinoma (SCC). 
  Patients and methods:  Contrast-enhanced CT images of 40 patients with OP and OC SCC were post-processed to extract TA features from PTLs. A feature selection method and different ML algorithms were applied to find the most accurate subset of features to predict TG and NS. 
  Results:  For the prediction of TG, the best accuracy (92.9%) was achieved by Naïve Bayes (NB), bagging of NB and K Nearest Neighbor (KNN). For the prediction of NS, J48, NB, bagging of NB and boosting of J48 overcame the accuracy of 90%. 
  Conclusion:  A radiomic ML approach applied to PTLs is able to predict TG and NS in patients with OC and OP SCC. 
  |  http://ar.iiarjournals.org/cgi/pmidlookup?view=long&pmid=31892576  |  
------------------------------------------- 
10.1093/nar/gkz1097  |   Guanine-rich nucleic acids can fold into the non-B DNA or RNA structures called G-quadruplexes (G4). Recent methodological developments have allowed the characterization of specific G-quadruplex structures in vitro as well as in vivo, and at a much higher throughput, in silico, which has greatly expanded our understanding of G4-associated functions. Typically, the consensus motif G3+N1-7G3+N1-7G3+N1-7G3+ has been used to identify potential G-quadruplexes from primary sequence. Since, various algorithms have been developed to predict the potential formation of quadruplexes directly from DNA or RNA sequences and the number of studies reporting genome-wide G4 exploration across species has rapidly increased. More recently, new methodologies have also appeared, proposing other estimates which consider non-canonical sequences and/or structure propensity and stability. The present review aims at providing an updated overview of the current open-source G-quadruplex prediction algorithms and straightforward examples of their implementation. 
  |  https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkz1097  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31754698/  |  
------------------------------------------- 
10.1038/s42003-020-0846-z  |   Efficient action control is indispensable for goal-directed behaviour. Different theories have stressed the importance of either attention or response selection sub-processes for action control. Yet, it is unclear to what extent these processes can be identified in the dynamics of neurophysiological (EEG) processes at the single-trial level and be used to predict the presence of conflicts in a given moment. Applying deep learning, which was blind to cognitive theory, on single-trial EEG data allowed to predict the presence of conflict in ~95% of subjects ~33% above chance level. Neurophysiological features related to attentional and motor response selection processes in the occipital cortex and the superior frontal gyrus contributed most to prediction accuracy. Importantly, deep learning was able to identify predictive neurophysiological processes in single-trial neural dynamics. Hence, mathematical (artificial intelligence) approaches may be used to foster the validation and development of links between cognitive theory and neurophysiology of human behavior. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32152375/  |  
------------------------------------------- 
10.1002/cpt.1777  |   The availability of multidimensional data together with the development of modern techniques for data analysis represent an exceptional opportunity for clinical pharmacology. Data science-defined in this special issue as the novel approaches to the collection, aggregation, and analysis of data-can significantly contribute to characterize drug-response variability at the individual level, thus enabling clinical pharmacology to become a critical contributor to personalized healthcare through precision dosing. We propose a minireview of methodologies for achieving precision dosing with a focus on an artificial intelligence technique called reinforcement learning, which is currently used for individualizing dosing regimen in patients with life-threatening diseases. We highlight the interplay of such techniques with conventional pharmacokinetic/pharmacodynamic approaches and discuss applicability in drug research and early development. 
  |  https://doi.org/10.1002/cpt.1777  |  
------------------------------------------- 
10.1111/den.13653  |    Objectives:  The prognosis for pharyngeal cancer is relatively poor. It is usually diagnosed in an advanced stage. Although the recent development of narrow-band imaging (NBI) and increased awareness among endoscopists have enabled detection of superficial pharyngeal cancer, these techniques are still not prevalent worldwide. Nevertheless, artificial intelligence (AI)-based deep learning has led to significant advancements in various medical fields. Here, we demonstrate the diagnostic ability of AI-based detection of pharyngeal cancer from endoscopic images in esophagogastroduodenoscopy. 
  Methods:  We retrospectively collected 5403 training images of pharyngeal cancer from 202 superficial cancers and 45 advanced cancers from the Cancer Institute Hospital, Tokyo, Japan. Using these images, we developed an AI-based diagnostic system with convolutional neural networks. We prepared 1912 validation images from 35 patients with 40 pharyngeal cancers and 40 patients without pharyngeal cancer to evaluate our system. 
  Results:  Our AI-based diagnostic system correctly detected all pharyngeal cancer lesions (40/40) in the patients with cancer, including three small lesions smaller than 10 mm. For each image, the AI-based system correctly detected pharyngeal cancers in images obtained via NBI with a sensitivity of 85.6%, much higher sensitivity than that for images obtained via white light imaging (70.1%). The novel diagnostic system took only 28 s to analyze 1912 validation images. 
  Conclusions:  The novel AI-based diagnostic system detected pharyngeal cancer with high sensitivity. It could facilitate early detection, thereby leading to better prognosis and quality of life for patients with pharyngeal cancers in the near future. 
  |  https://doi.org/10.1111/den.13653  |  
------------------------------------------- 
10.1016/j.jacc.2019.12.030  |    Background:  Hypertrophic cardiomyopathy (HCM) is an uncommon but important cause of sudden cardiac death. 
  Objectives:  This study sought to develop an artificial intelligence approach for the detection of HCM based on 12-lead electrocardiography (ECG). 
  Methods:  A convolutional neural network (CNN) was trained and validated using digital 12-lead ECG from 2,448 patients with a verified HCM diagnosis and 51,153 non-HCM age- and sex-matched control subjects. The ability of the CNN to detect HCM was then tested on a different dataset of 612 HCM and 12,788 control subjects. 
  Results:  In the combined datasets, mean age was 54.8 ± 15.9 years for the HCM group and 57.5 ± 15.5 years for the control group. After training and validation, the area under the curve (AUC) of the CNN in the validation dataset was 0.95 (95% confidence interval [CI]: 0.94 to 0.97) at the optimal probability threshold of 11% for having HCM. When applying this probability threshold to the testing dataset, the CNN's AUC was 0.96 (95% CI: 0.95 to 0.96) with sensitivity 87% and specificity 90%. In subgroup analyses, the AUC was 0.95 (95% CI: 0.94 to 0.97) among patients with left ventricular hypertrophy by ECG criteria and 0.95 (95% CI: 0.90 to 1.00) among patients with a normal ECG. The model performed particularly well in younger patients (sensitivity 95%, specificity 92%). In patients with HCM with and without sarcomeric mutations, the model-derived median probabilities for having HCM were 97% and 96%, respectively. 
  Conclusions:  ECG-based detection of HCM by an artificial intelligence algorithm can be achieved with high diagnostic performance, particularly in younger patients. This model requires further refinement and external validation, but it may hold promise for HCM screening. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0735-1097(20)30003-6  |  
------------------------------------------- 
10.2196/16606  |    Background:  Mapping out the research landscape around a project is often time consuming and difficult. 
  Objective:  This study evaluates a commercial artificial intelligence (AI) search engine (IRIS.AI) for its applicability in an automated literature search on a specific medical topic. 
  Methods:  To evaluate the AI search engine in a standardized manner, the concept of a science hackathon was applied. Three groups of researchers were tasked with performing a literature search on a clearly defined scientific project. All participants had a high level of expertise for this specific field of research. Two groups were given access to the AI search engine IRIS.AI. All groups were given the same amount of time for their search and were instructed to document their results. Search results were summarized and ranked according to a predetermined scoring system. 
  Results:  The final scoring awarded 49 and 39 points out of 60 to AI groups 1 and 2, respectively, and the control group received 46 points. A total of 20 scientific studies with high relevance were identified, and 5 highly relevant studies ("spot on") were reported by each group. 
  Conclusions:  AI technology is a promising approach to facilitate literature searches and the management of medical libraries. In this study, however, the application of AI technology lead to a more focused literature search without a significant improvement in the number of results. 
  |  https://www.i-jmr.org/2020/1/e16606/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32224481/  |  
------------------------------------------- 
10.1097/CCM.0000000000004236  |    Objectives:  As the performance of a conventional track and trigger system in a rapid response system has been unsatisfactory, we developed and implemented an artificial intelligence for predicting in-hospital cardiac arrest, denoted the deep learning-based early warning system. The purpose of this study was to compare the performance of an artificial intelligence-based early warning system with that of conventional methods in a real hospital situation. 
  Design:  Retrospective cohort study. 
  Setting:  This study was conducted at a hospital in which deep learning-based early warning system was implemented. 
  Patients:  We reviewed the records of adult patients who were admitted to the general ward of our hospital from April 2018 to March 2019. 
  Interventions:  The study population included 8,039 adult patients. A total 83 events of deterioration occurred during the study period. The outcome was events of deterioration, defined as cardiac arrest and unexpected ICU admission. We defined a true alarm as an alarm occurring within 0.5-24 hours before a deteriorating event. 
  Measurements and main results:  We used the area under the receiver operating characteristic curve, area under the precision-recall curve, number needed to examine, and mean alarm count per day as comparative measures. The deep learning-based early warning system (area under the receiver operating characteristic curve, 0.865; area under the precision-recall curve, 0.066) outperformed the modified early warning score (area under the receiver operating characteristic curve, 0.682; area under the precision-recall curve, 0.010) and reduced the number needed to examine and mean alarm count per day by 69.2% and 59.6%, respectively. At the same specificity, deep learning-based early warning system had up to 257% higher sensitivity than conventional methods. 
  Conclusions:  The developed artificial intelligence based on deep-learning, deep learning-based early warning system, accurately predicted deterioration of patients in a general ward and outperformed conventional methods. This study showed the potential and effectiveness of artificial intelligence in an rapid response system, which can be applied together with electronic health records. This will be a useful method to identify patients with deterioration and help with precise decision-making in daily practice. 
  |  https://dx.doi.org/10.1097/CCM.0000000000004236  |  
------------------------------------------- 
10.1177/1932296820906212  |    Purpose:  The purpose of this study is to compare the diagnostic performance of an autonomous artificial intelligence (AI) system for the diagnosis of referable diabetic retinopathy (RDR) to manual grading by Spanish ophthalmologists. 
  Methods:  Subjects with type 1 and 2 diabetes participated in a diabetic retinopathy (DR) screening program in 2011 to 2012 in Valencia (Spain), and two images per eye were collected according to their standard protocol. Mydriatic drops were used in all patients. Retinal images-one disc and one fovea centered-were obtained under the Medical Research Ethics Committee approval and de-identified. Exams were graded by the autonomous AI system (IDx-DR, Coralville, Iowa, United States), and manually by masked ophthalmologists using adjudication. The outputs of the AI system and manual adjudicated grading were compared using sensitivity and specificity for diagnosis of both RDR and vision-threatening diabetic retinopathy (VTDR). 
  Results:  A total of 2680 subjects were included in the study. According to manual grading, prevalence of RDR was 111/2680 (4.14%) and of VTDR was 69/2680 (2.57%). Against manual grading, the AI system had a 100% (95% confidence interval [CI]: 97%-100%) sensitivity and 81.82% (95% CI: 80%-83%) specificity for RDR, and a 100% (95% CI: 95%-100%) sensitivity and 94.64% (95% CI: 94%-95%) specificity for VTDR. 
  Conclusion:  Compared to manual grading by ophthalmologists, the autonomous diagnostic AI system had high sensitivity (100%) and specificity (82%) for diagnosing RDR and macular edema in people with diabetes in a screening program. Because of its immediate, point of care diagnosis, autonomous diagnostic AI has the potential to increase the accessibility of RDR screening in primary care settings. 
  |  http://journals.sagepub.com/doi/full/10.1177/1932296820906212?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1097/RLI.0000000000000666  |   Radiological images have been assessed qualitatively in most clinical settings by the expert eyes of radiologists and other clinicians. On the other hand, quantification of radiological images has the potential to detect early disease that may be difficult to detect with human eyes, complement or replace biopsy, and provide clear differentiation of disease stage. Further, objective assessment by quantification is a prerequisite of personalized/precision medicine. This review article aims to summarize and discuss how the variability of quantitative values derived from radiological images are induced by a number of factors and how these variabilities are mitigated and standardization of the quantitative values are achieved. We discuss the variabilities of specific biomarkers derived from magnetic resonance imaging and computed tomography, and focus on diffusion-weighted imaging, relaxometry, lung density evaluation, and computer-aided computed tomography volumetry. We also review the sources of variability and current efforts of standardization of the rapidly evolving techniques, which include radiomics and artificial intelligence. 
  |  http://dx.doi.org/10.1097/RLI.0000000000000666  |  
------------------------------------------- 
10.1016/j.jbiosc.2020.01.006  |   The cell growth and ethanol production from hydrolysates of various types were estimated from the volatile composition of lignocellulosic biomass by deep neural network (DNN) and the significant compositions estimated by asymmetric autoencoder-decoder (AAE). A six-layer DNN achieved good accuracy with learning and validation losses-0.033 and 0.507, respectively-and estimated overall time courses of yeast growth and ethanol fermentation. The AAE decoded the volatile compositions and represented the features of significant inhibitors via nonlinear dimensionality reduction, which was partly different from those using partial least squares regression reported previously. It revealed the significant features of hydrolysates for bioethanol production, which are lost in conventional approaches. The approach using DNN and AAE is, therefore, useful for bioethanol fermentation and other bioproductions using raw materials. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1389-1723(19)30781-9  |  
------------------------------------------- 
10.1016/j.neunet.2020.01.019  |   Hybrid artificial intelligence deals with the construction of intelligent systems by relying on both human knowledge and historical data records. In this paper, we approach this problem from a neural perspective, particularly when modeling and simulating dynamic systems. Firstly, we propose a Fuzzy Cognitive Map architecture in which experts are requested to define the interaction among the input neurons. As a second contribution, we introduce a fast and deterministic learning rule to compute the weights among input and output neurons. This parameterless learning method is based on the Moore-Penrose inverse and it can be performed in a single step. In addition, we discuss a model to determine the relevance of weights, which allows us to better understand the system. Last but not least, we introduce two calibration methods to adjust the model after the removal of potentially superfluous weights. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30021-6  |  
------------------------------------------- 
10.7150/ijbs.39387  |   In the past 20 years, the concept of surgery has undergone profound changes. Surgical practice has shifted from emphasizing the complete elimination of lesions to achieving optimal rehabilitation in patients. Collaborative optimization of surgery consists of three core elements, removal of lesions, organ protection and injury close monitoring, and controlled surgical intervention. As a result, the traditional surgical paradigm has quietly transformed into a modern precision surgical paradigm. In this review, we summarized the latest breakthroughs and applications of precision medicine in liver surgery. In addition, we also outlined the progresses that have been made in precision liver surgery, the opportunities and challenges that may encountered in the future. 
  |  http://www.ijbs.com/v16p0365.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32015674/  |  
------------------------------------------- 
10.1055/s-0039-3400265  |   Magnetic resonance imaging (MRI) is a leading image modality for the assessment of musculoskeletal (MSK) injuries and disorders. A significant drawback, however, is the lengthy data acquisition. This issue has motivated the development of methods to improve the speed of MRI. The field of artificial intelligence (AI) for accelerated MRI, although in its infancy, has seen tremendous progress over the past 3 years. Promising approaches include deep learning methods for reconstructing undersampled MRI data and generating high-resolution from low-resolution data. Preliminary studies show the promise of the variational network, a state-of-the-art technique, to generalize to many different anatomical regions and achieve comparable diagnostic accuracy as conventional methods. This article discusses the state-of-the-art methods, considerations for clinical applicability, followed by future perspectives for the field. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400265  |  
------------------------------------------- 
10.1055/s-0039-3401041  |   Artificial intelligence (AI) has made stunning progress in the last decade, made possible largely due to the advances in training deep neural networks with large data sets. Many of these solutions, initially developed for natural images, speech, or text, are now becoming successful in medical imaging. In this article we briefly summarize in an accessible way the current state of the field of AI. Furthermore, we highlight the most promising approaches and describe the current challenges that will need to be solved to enable broad deployment of AI in clinical practice. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3401041  |  
------------------------------------------- 
10.1016/j.dld.2019.12.146  |    Background:  The sensitivity of endoscopy in diagnosing chronic atrophic gastritis is only 42%, and multipoint biopsy, despite being more accurate, is not always available. 
  Aims:  This study aimed to construct a convolutional neural network to improve the diagnostic rate of chronic atrophic gastritis. 
  Methods:  We collected 5470 images of the gastric antrums of 1699 patients and labeled them with their pathological findings. Of these, 3042 images depicted atrophic gastritis and 2428 did not. We designed and trained a convolutional neural network-chronic atrophic gastritis model to diagnose atrophic gastritis accurately, verified by five-fold cross-validation. Moreover, the diagnoses of the deep learning model were compared with those of three experts. 
  Results:  The diagnostic accuracy, sensitivity, and specificity of the convolutional neural network-chronic atrophic gastritis model in diagnosing atrophic gastritis were 0.942, 0.945, and 0.940, respectively, which were higher than those of the experts. The detection rates of mild, moderate, and severe atrophic gastritis were 93%, 95%, and 99%, respectively. 
  Conclusion:  Chronic atrophic gastritis could be diagnosed by gastroscopic images using the convolutional neural network-chronic atrophic gastritis model. This may greatly reduce the burden on endoscopy physicians, simplify diagnostic routines, and reduce costs for doctors and patients. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1590-8658(20)30003-7  |  
------------------------------------------- 
10.1055/s-0039-3400270  |   Artificial intelligence (AI) is an emerging technology that brings a wide array of new tools to the field of radiology. AI will certainly have an impact on the day-to-day work of radiologists in the coming decades, thus training programs must prepare radiology residents adequately for their future careers. Radiology training programs should aim to give residents an understanding of the fundamentals and types of AI in radiology, the broad areas AI can be applied in radiology, how to assess AI applications in radiology, and resources available to build their knowledge in IA applications in radiology. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400270  |  
------------------------------------------- 
10.1007/s00104-020-01143-5  |   New digital technologies will also gain in importance in vascular surgery. There is a wide field of potential applications. Simulation-based training of endovascular procedures can lead to improvement in procedure-specific parameters and reduce fluoroscopy and procedural times. The use of intraoperative image-guided navigation and robotics also enables a reduction of the radiation dose. Artificial intelligence can be used for risk stratification and individualization of treatment approaches. Health apps can be used to improve the follow-up care of patients. 
  |  https://dx.doi.org/10.1007/s00104-020-01143-5  |  
------------------------------------------- 
10.3389/fpsyg.2020.00355  |   Physical and emotional intimacy between humans and robots may become commonplace over the next decades, as technology improves at a rapid rate. This development provides new questions pertaining to how people perceive robots designed for different kinds of intimacy, both as companions and potentially as competitors. We performed a randomized experiment where participants read of either a robot that could only perform sexual acts, or only engage in non-sexual platonic love relationships. The results of the current study show that females have less positive views of robots, and especially of sex robots, compared to men. Contrary to the expectation rooted in evolutionary psychology, females expected to feel more jealousy if their partner got a sex robot, rather than a platonic love robot. The results further suggests that people project their own feelings about robots onto their partner, erroneously expecting their partner to react as they would to the thought of ones' partner having a robot. 
  |  https://doi.org/10.3389/fpsyg.2020.00355  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231613/  |  
------------------------------------------- 
10.1016/j.breast.2019.12.015  |    Objectives:  Artificial intelligence (AI) is poised to transform breast cancer care. However, most scientists, engineers, and clinicians are not prepared to contribute to the AI revolution in healthcare. In this paper, we describe our experiences teaching a new undergraduate course for American students that aims to prepare the next generation for cross-cultural designthinking, which we believe is crucial for AI to achieve its full potential in breast cancer care. 
  Materials and methods:  The key course activities are planning, conducting, and interpreting interviews of healthcare professionals from both Portugal and the United States. Since the course is offered as a short-term faculty-led study abroad program in Portugal, students are able to explore the impact of culture on healthcare delivery and the design of healthcare technologies. 
  Results:  The learning assessments demonstrated student growth in several areas pertinent for future development of AI for breast cancer care. With respect to understanding breast cancer care, prior to taking this course, most students had underestimated the impact of cancer and its treatment on women's quality of life and most were unaware of the importance of multidisciplinary care teams. Regarding AI in medicine, students became more mindful of data privacy issues and the need to consider the effect of AI on healthcare professionals. 
  Conclusion:  This course illustrates the potential benefits for AI in medicine of introducing future scientists, engineers, and clinicians to cross cultural design-thinking early in their educational experiences. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9776(19)31222-6  |  
------------------------------------------- 
10.1242/jcs.245050  |   Measuring the physical size of a cell is valuable in understanding cell growth control. Current single-cell volume measurement methods for mammalian cells are labor intensive, inflexible and can cause cell damage. We introduce CTRL: Cell Topography Reconstruction Learner, a label-free technique incorporating the deep learning algorithm and the fluorescence exclusion method for reconstructing cell topography and estimating mammalian cell volume from differential interference contrast (DIC) microscopy images alone. The method achieves quantitative accuracy, requires minimal sample preparation, and applies to a wide range of biological and experimental conditions. The method can be used to track single-cell volume dynamics over arbitrarily long time periods. For HT1080 fibrosarcoma cells, we observe that the cell size at division is positively correlated with the cell size at birth (sizer), and there is a noticeable reduction in cell size fluctuations at 25% completion of the cell cycle in HT1080 fibrosarcoma cells. 
  |  http://jcs.biologists.org/cgi/pmidlookup?view=long&pmid=32094267  |  
------------------------------------------- 
10.2196/16866  |    Background:  Positive economic impact is a key decision factor in making the case for or against investing in an artificial intelligence (AI) solution in the health care industry. It is most relevant for the care provider and insurer as well as for the pharmaceutical and medical technology sectors. Although the broad economic impact of digital health solutions in general has been assessed many times in literature and the benefit for patients and society has also been analyzed, the specific economic impact of AI in health care has been addressed only sporadically. 
  Objective:  This study aimed to systematically review and summarize the cost-effectiveness studies dedicated to AI in health care and to assess whether they meet the established quality criteria. 
  Methods:  In a first step, the quality criteria for economic impact studies were defined based on the established and adapted criteria schemes for cost impact assessments. In a second step, a systematic literature review based on qualitative and quantitative inclusion and exclusion criteria was conducted to identify relevant publications for an in-depth analysis of the economic impact assessment. In a final step, the quality of the identified economic impact studies was evaluated based on the defined quality criteria for cost-effectiveness studies. 
  Results:  Very few publications have thoroughly addressed the economic impact assessment, and the economic assessment quality of the reviewed publications on AI shows severe methodological deficits. Only 6 out of 66 publications could be included in the second step of the analysis based on the inclusion criteria. Out of these 6 studies, none comprised a methodologically complete cost impact analysis. There are two areas for improvement in future studies. First, the initial investment and operational costs for the AI infrastructure and service need to be included. Second, alternatives to achieve similar impact must be evaluated to provide a comprehensive comparison. 
  Conclusions:  This systematic literature analysis proved that the existing impact assessments show methodological deficits and that upcoming evaluations require more comprehensive economic analyses to enable economic decisions for or against implementing AI technology in health care. 
  |  https://www.jmir.org/2020/2/e16866/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32130134/  |  
------------------------------------------- 
10.3390/s20030779  |   An automatic "museum audio guide" is presented as a new type of audio guide for museums. The device consists of a headset equipped with a camera that captures exhibit pictures and the eyes of things computer vision device (EoT). The EoT board is capable of recognizing artworks using features from accelerated segment test (FAST) keypoints and a random forest classifier, and is able to be used for an entire day without the need to recharge the batteries. In addition, an application logic has been implemented, which allows for a special highly-efficient behavior upon recognition of the painting. Two different use case scenarios have been implemented. The main testing was performed with a piloting phase in a real world museum. Results show that the system keeps its promises regarding its main benefit, which is simplicity of use and the user's preference of the proposed system over traditional audioguides. 
  |  http://www.mdpi.com/resolver?pii=s20030779  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023954/  |  
------------------------------------------- 
10.1186/s41747-019-0143-0  |   Radiomics, artificial intelligence, and deep learning figure amongst recent buzzwords in current medical imaging research and technological development. Analysis of medical big data in assessment and follow-up of personalised treatments has also become a major research topic in the area of precision medicine. In this review, current research trends in radiomics are analysed, from handcrafted radiomics feature extraction and statistical analysis to deep learning. Radiomics algorithms now include genomics and immunomics data to improve patient stratification and prediction of treatment response. Several applications have already shown conclusive results demonstrating the potential of including other "omics" data to existing imaging features. We also discuss further challenges of data harmonisation and management infrastructure to shed a light on the much-needed integration of radiomics and all other "omics" into clinical workflows. In particular, we point to the emerging paradigm shift in the implementation of big data infrastructures to facilitate databanks growth, data extraction and the development of expert software tools. Secured access, sharing, and integration of all health data, called "holomics", will accelerate the revolution of personalised medicine and oncology as well as expand the role of imaging specialists. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32034573/  |  
------------------------------------------- 
10.1053/j.jvca.2020.01.024  |    Objectives:  Echocardiographic assessment of right ventricular (RV) function is based largely on visual estimation of tricuspid annulus and motion of the free wall. Regional strain analysis has provided an objective measure of myocardial performance assessment, but is limited in use by vendor-specific software. The study was designed to investigate statistical correlation between RV region-specific strain and echocardiographic parameters of RV function using a vendor-neutral RV-specific strain assessment program. 
  Design:  This is a retrospective study. 
  Setting:  Tertiary hospital. 
  Participants:  One hundred seven patients undergoing coronary artery bypass graft, valve repair or replacement, or a combination of procedures. 
  Intervention:  None. 
  Measurements and main results:  One hundred seven patients underwent comprehensive echocardiographic of RV function intraoperatively. Off-line analysis of global, longitudinal, and septal strain was performed using a vendor-neutral software. The 2 values were compared statistically. All pairs demonstrated strong statistical significance; the strongest relationships were between (1) RV fractional area change (FAC) (%)-RV longitudinal strain (r<sup>2</sup> = 0.83, p &lt; 0.001), and (2) tricuspid annular plane systolic excursion (mm)-lateral S' velocity (cm/s) (r<sup>2</sup> = 0.80, p &lt; 0.001). The weakest correlations were (1) RV FAC (%)-lateral S' velocity (cm/s) (r<sup>2</sup> = 0.37, p &lt; 0.001), and (2) lateral S' velocity (cm/s)-RV longitudinal strain (r<sup>2</sup> = 0.40, p &lt; 0.001). 
  Conclusion:  RV function can be assessed objectively by strain analyses across different platforms using the artificial intelligence-based vendor-neutral strain analysis software. There is a statistically significant correlation between strain values and conventional 2-dimensional echocardiographic parameters of RV function. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1053-0770(20)30085-9  |  
------------------------------------------- 
10.2174/1381612826666200422091753  |    Background:  The diagnosis and prognosis of pathological conditions, such as age-related macular degeneration (AMD) and cancer still need improvement. AMD is primarily caused due to the dysfunction of retinal pigment epithelium (RPE), whereas endothelial cells (ECs) play one of the major roles in angiogenesis; an important process which occurs in malignant progression of cancer. Several reports suggested about the augmented release of nano-vesicles under pathological conditions, including from RPE as well as cancer-associated ECs, which take part in various biological process including intercellular communication in disease progression. Importantly, these nano-vesicles are around 30-1000 nm and carry fingerprint of their initiating parent cells (IPCs). Therefore, these nano-vesicles could be utilized as the diagnostic tool for AMD and cancer, respectively. However, the analysis of nano-vesicles for biomarker study is confounded by their extensive heterogeneous nature. 
  Methods:  To confront this challenge, we utilized artificial intelligence (AI) based machine learning (ML) algorithms such as support vector machine (SVM) and decision tree model on the dataset of nano-vesicles from RPE and ECs cell lines with low dimensionality. 
  Results:  Overall, Gaussian SVM demonstrated highest prediction accuracy of the IPCs of nanovesicles, among all the chosen SVM classifiers. Additionally, the bagged tree showed highest prediction among the chosen decision tree-based classifiers. 
  Conclusion:  Therefore, overall bagged tree showed the best performance for the prediction of IPCs of nano-vesicles, suggesting the applicability of AI based prediction approach in diagnosis and prognosis of pathological conditions, including non-invasive liquid biopsy via various biofluids-derived nano-vesicles. 
  |  http://www.eurekaselect.com/181176/article  |  
------------------------------------------- 
10.3390/jcm9020572  |   A key issue in the field of kidney transplants is the analysis of transplant recipients' survival. By means of the information obtained from transplant patients, it is possible to analyse in which cases a transplant has a higher likelihood of success and the factors on which it will depend. In general, these analyses have been conducted by applying traditional statistical techniques, as the amount and variety of data available about kidney transplant processes were limited. However, two main changes have taken place in this field in the last decade. Firstly, the digitalisation of medical information through the use of electronic health records (EHRs), which store patients' medical histories electronically. This facilitates automatic information processing through specialised software. Secondly, medical Big Data has provided access to vast amounts of data on medical processes. The information currently available on kidney transplants is huge and varied by comparison to that initially available for this kind of study. This new context has led to the use of other non-traditional techniques more suitable to conduct survival analyses in these new conditions. Specifically, this paper provides a review of the main machine learning methods and tools that are being used to conduct kidney transplant patient and graft survival analyses. 
  |  http://www.mdpi.com/resolver?pii=jcm9020572  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093027/  |  
------------------------------------------- 
10.1007/s13311-020-00846-1  |   The critical care environment drives huge volumes of data, and clinicians are tasked with quickly processing this data and responding to it urgently. The neurocritical care environment increasingly involves EEG, multimodal intracranial monitoring, and complex imaging which preclude comprehensive human synthesis, and requires new concepts to integrate data into clinical care. By definition, Big Data is data that cannot be handled using traditional infrastructures and is characterized by the volume, variety, velocity, and variability of the data being produced. Big Data in the neurocritical care unit requires rethinking of data storage infrastructures and the development of tools and analytics to drive advancements in the field. Preprocessing, feature extraction, statistical inference, and analytic tools are required in order to achieve the primary goals of Big Data for clinical use: description, prediction, and prescription. Barriers to its use at bedside include a lack of infrastructure development within the healthcare industry, lack of standardization of data inputs, and ultimately existential and scientific concerns about the outputs that result from the use of tools such as artificial intelligence. However, as implied by the fundamental theorem of biomedical informatics, physicians remain central to the development and utility of Big Data to improve patient care. 
  |  https://dx.doi.org/10.1007/s13311-020-00846-1  |  
------------------------------------------- 
10.1128/JCM.00511-20  |   Artificial intelligence (AI) is increasingly becoming an important component of clinical microbiology informatics. Researchers, microbiologists, laboratorians, and diagnosticians are interested in AI-based testing because these solutions have the potential to improve a test's turnaround time, quality, and cost. Mathison's study used computer vision AI (https://doi.org/10.1128/JCM.02053-19), but additional opportunities for AI applications exist within the clinical microbiology laboratory. Large data sets within clinical microbiology that are amenable to the development of AI diagnostics include genomic information from isolated bacteria, metagenomic microbial findings from primary specimens, mass spectra captured from cultured bacterial isolates, and large digital images, which is the medium that Mathison chose to use. AI in general and computer vision in specific are emerging tools that clinical microbiologists need to study, develop, and implement in order to improve clinical microbiology. 
  |  http://jcm.asm.org/cgi/pmidlookup?view=long&pmid=32295889  |  
------------------------------------------- 
10.1111/apha.13479  |    Aim:  How can we convert biomarkers into reliable, validated laboratory tests? GFR estimators exist for more than a century. The first utilitarian biomarkers were endogenously produced urea and creatinine. Clinicians then developed simple tests to determine whether or not renal tubular function was maintained. Are there faster and better tests that reflect decreased renal function and increased acute kidney injury (AKI) risk? 
  Methods:  We inspect earlier, and recently propagated biomarkers. Cystatin C reflects GFR and is not confounded by muscle mass. Direct GFR and plasma volume can now be measured acutely within 3 h. Better yet would be tests that give information before GFR decreases and prior to urea, creatinine, and cystatin C increases. Prospective tests identifying those persons likely to develop AKI would be helpful. Even more utilitarian would be a test that also suggests a therapeutic avenue. 
  Results:  A number of highly provocative biomarkers have recently been proposed. Moreover, the application of big data from huge electronic medical records promise new directions in identifying and dealing with AKI. 
  Conclusions:  Pipedreams are in the pipeline; the novel findings require immediate testing, verification, and perhaps application. Future research promises to make such dreams come true. 
  |  https://doi.org/10.1111/apha.13479  |  
------------------------------------------- 
10.1007/s00415-020-09790-8  |    Background:  To differentiate dementia with Lewy bodies (DLB) from Alzheimer disease (AD) using a single imaging modality is challenging, because of their common hypometabolic findings. Scaled subprofile modeling/principal component analysis (SSM/PCA), an unsupervised artificial intelligence, has the potential to offer an alternative to image analysis. 
  Objective:  We aimed to produce spatial metabolic profiles to discriminate DLB from AD and to identify the characteristics of the profiles. 
  Methods:  Fifty individuals each with DLB, AD, and normal cognition (NL) underwent <sup>18</sup>F-FDG-PET and MRI. The spatial metabolic profile to differentiate DLB from AD (DLB-AD discrimination profile) was determined using SSM/PCA with tenfold cross validation. For comparison, we also produced disease-related profiles that can discriminate AD and DLB from NL (AD- and DLB-related profiles, respectively). 
  Results:  The DLB-AD discrimination profile significantly differentiated DLB from AD with comparable accuracy to that of discriminating DLB and AD from NL. The AD- and DLB-related profiles comprised metabolic imaging features typical of each pathology. In contrast, the DLB-AD discrimination profile emphasized preservation in the posterior cingulate cortex (cingulate island sign) and medial temporal lobe, and occipital hypometabolism. Common hypometabolic findings between DLB and AD were less noticeable in the profile. The DLB-related profile significantly correlated with cognitive function and three core features of DLB, whereas the DLB-AD discrimination profile did not. 
  Conclusions:  Spatial metabolic profile that could discriminate DLB from AD emphasized different imaging features and eliminated common findings between DLB and AD. Neither cognitive function nor core features were associated with the profile. 
  |  https://dx.doi.org/10.1007/s00415-020-09790-8  |  
------------------------------------------- 
10.1136/rmdopen-2019-001063  |   After decades of basic research with many setbacks, artificial intelligence (AI) has recently obtained significant breakthroughs, enabling computer programs to outperform human interpretation of medical images in very specific areas. After this shock wave that probably exceeds the impact of the first AI victory of defeating the world chess champion in 1997, some reflection may be appropriate on the consequences for clinical imaging in rheumatology. In this narrative review, a short explanation is given about the various AI techniques, including 'deep learning', and how these have been applied to rheumatological imaging, focussing on rheumatoid arthritis and systemic sclerosis as examples. By discussing the principle limitations of AI and deep learning, this review aims to give insight into possible future perspectives of AI applications in rheumatology. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31958283/  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105316  |   Prostate cancer represents today the most typical example of a pathology whose diagnosis requires multiparametric imaging, a strategy where multiple imaging techniques are combined to reach an acceptable diagnostic performance. However, the reviewing, weighing and coupling of multiple images not only places additional burden on the radiologist, it also complicates the reviewing process. Prostate cancer imaging has therefore been an important target for the development of computer-aided diagnostic (CAD) tools. In this survey, we discuss the advances in CAD for prostate cancer over the last decades with special attention to the deep-learning techniques that have been designed in the last few years. Moreover, we elaborate and compare the methods employed to deliver the CAD output to the operator for further medical decision making. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31044-2  |  
------------------------------------------- 
10.1007/s00104-020-01131-9  |    Background:  Artificial intelligence (AI) in neurosurgery is becoming increasingly more important as the technology advances. This development can be measured by the increase of publications on AI in neurosurgery over the last years. 
  Objective:  This article provides insights into the current possibilities of using AI in neurosurgery. 
  Material and methods:  A review of the literature was carried out with a focus on exemplary work on the use of AI in neurosurgery. 
  Results:  The current neurosurgical publications on the use of AI show the diversity of the topic in this field. The main areas of application are diagnostics, outcome and treatment models. 
  Conclusion:  The various areas of application of AI in the field of neurosurgery with a refined preoperative diagnostics and outcome predictions will significantly influence the future of neurosurgery. Neurosurgeons will continue to make the decisions on the indications for surgery but an optimized statement on diagnosis, treatment options and on the risk of surgery will be made by neurosurgeons with the help of AI in the future. 
  |  https://dx.doi.org/10.1007/s00104-020-01131-9  |  
------------------------------------------- 
10.1007/s00464-019-07330-8  |    Background:  In laparoscopy, the digital camera offers surgeons the opportunity to receive support from image-guided surgery systems. Such systems require image understanding, the ability for a computer to understand what the laparoscope sees. Image understanding has recently progressed owing to the emergence of artificial intelligence and especially deep learning techniques. However, the state of the art of deep learning in gynaecology only offers image-based detection, reporting the presence or absence of an anatomical structure, without finding its location. A solution to the localisation problem is given by the concept of semantic segmentation, giving the detection and pixel-level location of a structure in an image. The state-of-the-art results in semantic segmentation are achieved by deep learning, whose usage requires a massive amount of annotated data. We propose the first dataset dedicated to this task and the first evaluation of deep learning-based semantic segmentation in gynaecology. 
  Methods:  We used the deep learning method called Mask R-CNN. Our dataset has 461 laparoscopic images manually annotated with three classes: uterus, ovaries and surgical tools. We split our dataset in 361 images to train Mask R-CNN and 100 images to evaluate its performance. 
  Results:  The segmentation accuracy is reported in terms of percentage of overlap between the segmented regions from Mask R-CNN and the manually annotated ones. The accuracy is 84.5%, 29.6% and 54.5% for uterus, ovaries and surgical tools, respectively. An automatic detection of these structures was then inferred from the semantic segmentation results which led to state-of-the-art detection performance, except for the ovaries. Specifically, the detection accuracy is 97%, 24% and 86% for uterus, ovaries and surgical tools, respectively. 
  Conclusion:  Our preliminary results are very promising, given the relatively small size of our initial dataset. The creation of an international surgical database seems essential. 
  |  https://doi.org/10.1007/s00464-019-07330-8  |  
------------------------------------------- 
10.1016/j.heliyon.2020.e03669  |   The inputs to the outputs of nonlinear systems can be modeled using machine and deep learning approaches, among which artificial neural networks (ANNs) are a promising option. However, noisy signals affect ANN modeling negatively; hence, it is important to investigate these signals prior to the modeling. Herein, two customized and simple approaches, visual inspection and absolute correlation, are proposed to examine the relationship between the inputs and outputs of a nonlinear system. The system under consideration uses biosignals from surface electromyography as inputs and human finger joint angles as outputs, acquired from eight intact participants performing movements and grasping tasks in dynamic conditions. Furthermore, the results of these approaches are tested using the standard mutual information measure. Hence, the system dimensionality is reduced, and the ANN learning (convergence) is accelerated, where the most informative inputs are selected for the next phase. Subsequently, four ANN types, i.e., feedforward, cascade-forward, radial basis function, and generalized regression ANNs, are used to perform the modeling. Finally, the performance of the ANNs is compared with findings from the signal analysis. Results indicate a high level of consistency among all the aforementioned signal pre-analysis techniques from one side, and they also indicate that these techniques match the ANN performances from the other side. As an example, for a certain movement set, the ANN models resulted in the rotation estimation accuracy of the joints in the following descending order: carpometacarpal, metacarpophalangeal, proximal interphalangeal, and distal interphalangeal. This information has been indicated in the signal pre-analysis step. Therefore, this step is crucial in input-output variable selections prior to machine-/deep-learning-based modeling approaches. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2405-8440(20)30514-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274431/  |  
------------------------------------------- 
10.1109/TNNLS.2020.2965567  |   From the medical field to agriculture, from energy to transportation, every industry is going through a revolution by embracing artificial intelligence (AI); nevertheless, AI is still in its infancy. Inspired by the evolution of the human brain, this article demonstrates a novel method and framework to synthesize an artificial brain with cognitive abilities by taking advantage of the same process responsible for the growth of the biological brain called ``neuroembryogenesis.'' This framework shares some of the key behavioral aspects of the biological brain, such as spiking neurons, neuroplasticity, neuronal pruning, and excitatory and inhibitory interactions between neurons, together making it capable of learning and memorizing. One of the highlights of the proposed design is its potential to incrementally improve itself over generations based on system performance, using genetic algorithms. A proof of concept at the end of this article demonstrates how a simplified implementation of the human visual cortex using the proposed framework is capable of character recognition. Our framework is open source, and the code is shared with the scientific community at www.feagi.org. 
  |  https://dx.doi.org/10.1109/TNNLS.2020.2965567  |  
------------------------------------------- 
10.1097/RCT.0000000000000965  |    Objective:  To evaluate image quality and radiation dose exposure of low-kV setting and low-volume contrast medium (CM) computed tomography angiography (CTA) protocol for transcatheter aortic valve implantation (TAVI) planning in comparison with standard CTA protocol. 
  Methods:  Sixty-patients were examined with 256-row MDCT for TAVI planning: 32 patients (study group) were evaluated using 80-kV electrocardiogram-gated protocol with 60 mL of CM and IMR reconstruction; 28 patients underwent a standard electrocardiogram-gated CTA study (100 kV; 80 mL of CM; iDose4 reconstruction). Subjective and objective image quality was evaluated in each patient at different aortic levels. Finally, we collected radiation dose exposure data (CT dose index and dose-length product) of both groups. 
  Results:  In study protocol, significant higher mean attenuation values were achieved in all measurements compared with the standard protocol. There were no significant differences in the subjective image quality evaluation in both groups. Mean dose-length product of study group was 56% lower than in the control one (P &lt; 0.0001). 
  Conclusion:  Low-kV and low-CM volume CTA, combined with IMR, allows to correctly performing TAVI planning with high-quality images and significant radiation dose reduction compared with standard CTA protocol. 
  |  http://dx.doi.org/10.1097/RCT.0000000000000965  |  
------------------------------------------- 
10.1097/ACO.0000000000000845  |    Purpose of review:  Acute care technologies, including novel monitoring devices, big data, increased computing capabilities, machine-learning algorithms and automation, are converging. This enables the application of augmented intelligence for improved outcome predictions, clinical decision-making, and offers unprecedented opportunities to improve patient outcomes, reduce costs, and improve clinician workflow. This article briefly explores recent work in the areas of automation, artificial intelligence and outcome prediction models in pediatric anesthesia and pediatric critical care. 
  Recent findings:  Recent years have yielded little published research into pediatric physiological closed loop control (a type of automation) beyond studies focused on glycemic control for type 1 diabetes. However, there has been a greater range of research in augmented decision-making, leveraging artificial intelligence and machine-learning techniques, in particular, for pediatric ICU outcome prediction. 
  Summary:  Most studies focusing on artificial intelligence demonstrate good performance on prediction or classification, whether they use traditional statistical tools or novel machine-learning approaches. Yet the challenges of implementation, user acceptance, ethics and regulation cannot be underestimated. Areas in which there is easy access to routinely labeled data and robust outcomes, such as those collected through national networks and quality improvement programs, are likely to be at the forefront of the adoption of these advances. 
  |  http://dx.doi.org/10.1097/ACO.0000000000000845  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105315  |    Background and objective:  The interrupted time-series (ITS) concept is performed using linear regression to evaluate the impact of policy changes in public health at a specific time. Objectives of this study were to verify, with an artificial intelligence-based nonlinear approach, if the estimation of ITS data could be facilitated, in addition to providing a computationally explicit equation. 
  Methods:  Dataset were from a study of Hawley et al. (2018) in which they evaluated the impact of UK National Institute for Health and Care Excellence (NICE) approval of tumor necrosis factor inhibitor therapies on the incidence of total hip (THR) and knee (TKR) replacement in rheumatoid arthritis patients. We used the newly developed Generalized Structure Group Method of Data Handling (GS-GMDH) model, a nonlinear method, for the prediction of THR and TKR incidence in the abovementioned population. 
  Results:  In contrast to linear regression, the GS-GMDH yields for both THR and TKR prediction values that almost fitted with the measured ones. These models demonstrated a low mean absolute relative error (0.10 and 0.09 respectively) and high correlation coefficient values (0.98 and 0.78). The GS-GMDH model for THR demonstrated 6.4/1000 person years (PYs) at the mid-point of the linear regression line post-NICE, whereas at the same point linear regression is 4.12/1000 PYs, a difference of around 35%. Similarly for the TKR, the linear regression to the datasets post-NICE was 9.05/1000 PYs, which is lower by about 27% than the GS-GMDH values of 12.47/1000 PYs. Importantly, with the GS-GMDH models, there is no need to identify the change point and intervention lag time as they simulate ITS continually throughout modelling. 
  Conclusions:  The results demonstrate that in the medical field, when looking at the estimation of the impact of a new drug using ITS, a nonlinear GS-GMDH method could be used as a better alternative to regression-based methods data processing. In addition to yielding more accurate predictions and requiring less time-consuming experimental measurements, this nonlinear method addresses, for the first time, one of the most challenging tasks in ITS modelling, i.e. avoiding the need to identify the change point and intervention lag time. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)31677-3  |  
------------------------------------------- 
10.3390/jcm9030749  |   Breast cancer is the leading cause of mortality in women. Early diagnosis of breast cancer can reduce the mortality rate. In the diagnosis, the mitotic cell count is an important biomarker for predicting the aggressiveness, prognosis, and grade of breast cancer. In general, pathologists manually examine histopathology images under high-resolution microscopes for the detection of mitotic cells. However, because of the minute differences between the mitotic and normal cells, this process is tiresome, time-consuming, and subjective. To overcome these challenges, artificial-intelligence-based (AI-based) techniques have been developed which automatically detect mitotic cells in the histopathology images. Such AI techniques accelerate the diagnosis and can be used as a second-opinion system for a medical doctor. Previously, conventional image-processing techniques were used for the detection of mitotic cells, which have low accuracy and high computational cost. Therefore, a number of deep-learning techniques that demonstrate outstanding performance and low computational cost were recently developed; however, they still require improvement in terms of accuracy and reliability. Therefore, we present a multistage mitotic-cell-detection method based on Faster region convolutional neural network (Faster R-CNN) and deep CNNs. Two open datasets (international conference on pattern recognition (ICPR) 2012 and ICPR 2014 (MITOS-ATYPIA-14)) of breast cancer histopathology were used in our experiments. The experimental results showed that our method achieves the state-of-the-art results of 0.876 precision, 0.841 recall, and 0.858 F1-measure for the ICPR 2012 dataset, and 0.848 precision, 0.583 recall, and 0.691 F1-measure for the ICPR 2014 dataset, which were higher than those obtained using previous methods. Moreover, we tested the generalization capability of our technique by testing on the tumor proliferation assessment challenge 2016 (TUPAC16) dataset and found that our technique also performs well in a cross-dataset experiment which proved the generalization capability of our proposed technique. 
  |  http://www.mdpi.com/resolver?pii=jcm9030749  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32164298/  |  
------------------------------------------- 
10.1016/j.compbiomed.2020.103659  |   The left ventricular ejection fraction is of significant importance for the early identification and diagnosis of cardiac disease. However, estimation of the left ventricular ejection fraction with consistently reliable and high accuracy remains a great challenge, owing to the high variability of cardiac structures and the complexity of the temporal dynamics in the cardiac magnetic resonance imaging sequences. The popular methods of left ventricular ejection fraction estimation rely on the left ventricular volume. Thus, strong prior knowledge is often necessary, impeding the ease of use of the existing methods as clinical tools. In this study, we propose a cardiac cycle feature learning architecture for achieving an accurate and reliable estimation of the left ventricular ejection fraction. The proposed method constructs a cardiac cycle extraction module that generates and analyzes an optical flow to obtain the cardiac cycle of all images, a motion feature fusion and extraction module for temporal modeling of the cardiac sequences, and a fully connected regression module for achieving a direct estimation. Experiments on 2900 left ventricle segments of 145 subjects from short-axis magnetic resonance imaging sequences of multiple lengths prove that our proposed method achieves reliable performance (correlation coefficient: 0.946; mean absolute error 2.67; standard deviation: 3.23). As compared with the current state-of-the-art method, our proposed method improves the performance by approximately 3% insofar as the mean absolute error. As the first solution for estimating the left ventricular ejection fraction directly, our proposed method demonstrates great potential for future clinical applications. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0010-4825(20)30053-6  |  
------------------------------------------- 
10.1186/s41747-019-0139-9  |    Background:  Bone age (BA) assessment performed by artificial intelligence (AI) is of growing interest due to improved accuracy, precision and time efficiency in daily routine. The aim of this study was to investigate the accuracy and efficiency of a novel AI software version for automated BA assessment in comparison to the Greulich-Pyle method. 
  Methods:  Radiographs of 514 patients were analysed in this retrospective study. Total BA was assessed independently by three blinded radiologists applying the GP method and by the AI software. Overall and gender-specific BA assessment results, as well as reading times of both approaches, were compared, while the reference BA was defined by two blinded experienced paediatric radiologists in consensus by application of the Greulich-Pyle method. 
  Results:  Mean absolute deviation (MAD) and root mean square deviation (RSMD) were significantly lower between AI-derived BA and reference BA (MAD 0.34 years, RSMD 0.38 years) than between reader-calculated BA and reference BA (MAD 0.79 years, RSMD 0.89 years; p &lt; 0.001). The correlation between AI-derived BA and reference BA (r = 0.99) was significantly higher than between reader-calculated BA and reference BA (r = 0.90; p &lt; 0.001). No statistical difference was found in reader agreement and correlation analyses regarding gender (p = 0.241). Mean reading times were reduced by 87% using the AI system. 
  Conclusions:  A novel AI software enabled highly accurate automated BA assessment. It may improve efficiency in clinical routine by reducing reading times without compromising the accuracy compared with the Greulich-Pyle method. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31993795/  |  
------------------------------------------- 
10.1016/j.ejrad.2020.108918  |    Purpose:  To evaluate the performance of an artificial intelligence (AI) based software solution tested on liver volumetric analyses and to compare the results to the manual contour segmentation. 
  Materials and methods:  We retrospectively obtained 462 multiphasic CT datasets with six series for each patient: three different contrast phases and two slice thickness reconstructions (1.5/5 mm), totaling 2772 series. AI-based liver volumes were determined using multi-scale deep-reinforcement learning for 3D body markers detection and 3D structure segmentation. The algorithm was trained for liver volumetry on approximately 5000 datasets. We computed the absolute error of each automatically- and manually-derived volume relative to the mean manual volume. The mean processing time/dataset and method was recorded. Variations of liver volumes were compared using univariate generalized linear model analyses. A subgroup of 60 datasets was manually segmented by three radiologists, with a further subgroup of 20 segmented three times by each, to compare the automatically-derived results with the ground-truth. 
  Results:  The mean absolute error of the automatically-derived measurement was 44.3 mL (representing 2.37 % of the averaged liver volumes). The liver volume was neither dependent on the contrast phase (p = 0.697), nor on the slice thickness (p = 0.446). The mean processing time/dataset with the algorithm was 9.94 s (sec) compared to manual segmentation with 219.34 s. We found an excellent agreement between both approaches with an ICC value of 0.996. 
  Conclusion:  The results of our study demonstrate that AI-powered fully automated liver volumetric analyses can be done with excellent accuracy, reproducibility, robustness, speed and agreement with the manual segmentation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0720-048X(20)30107-8  |  
------------------------------------------- 
10.1007/s10067-020-04969-w  |   Big data analytics and processing through artificial intelligence (AI) are increasingly being used in the health sector. This includes both clinical and research settings, and newly in specialties like rheumatology. It is, however, important to consider how these new methodologies are used, and particularly the sensitivities associated with personal information. Based on current applications in rheumatology, this article provides a narrative review of the bioethical perspectives of big data. It presents examples of databases, data analytic methods, and AI in this specialty to address four main ethical issues: privacy and confidentiality, informed consent, the impact on the medical profession, and justice. The use of big data and AI processing in healthcare has great potential to improve the quality of clinical care, including through better diagnosis, treatment, and prognosis. They may also increase patient and societal participation and engagement in healthcare and research. Developing these methodologies and using the information generated from them in line with ethical standards could positively affect the design of global health policies and introduce a new phase in the democratization of health.Key Points• Current applications of big data, data analytics, and AI in rheumatology-including registries, machine learning algorithms, and consumer-facing platforms-raise issues in four main bioethical areas: privacy and confidentiality, informed consent, the impact on the medical profession, and justice.• Bioethical concerns about rheumatology registries require careful consideration of privacy provisions, set within the context of local, national, and regional law.• Machine learning and big data aid diagnosis, treatment, and prognosis, but the final decision about the use of information from algorithms should be left to rheumatology specialists to maintain the promise of fiduciary obligations in the physician-patient relationship.• International collaboration in big data projects and increased patient engagement could be ways to counteract health inequalities in the practice of rheumatology, even on a global scale. 
  |  https://dx.doi.org/10.1007/s10067-020-04969-w  |  
------------------------------------------- 
10.1021/acsami.9b22251  |   Fingerprint formed through lifted papillary ridges is considered the best reference for personal identification. However, the currently available latent fingerprint (LFP) images often suffer from poor resolution, have a low degree of information, and require multifarious steps for identification. Herein, an individual Cloud-based fingerprint operation platform has been designed and fabricated to achieve high-definition LFPs analysis by using CsPbBr<sub>3</sub> perovskite nanocrystals (NCs) as eikonogen. Moreover, since CsPbBr<sub>3</sub> NCs have a special response to some fingerprint-associated amino acids, the proposed platform can be further used to detect metabolites on LFPs. Consequently, in virtue of Cloud computing and artificial intelligence (AI), this study has demonstrated a champion platform to realize the whole LFP identification analysis. In a double-blind simulative crime game, the enhanced LFP images can be easily obtained and used to lock the suspect accurately within one second on a smartphone, which can help investigators track the criminal clue and handle cases efficiently. 
  |  https://dx.doi.org/10.1021/acsami.9b22251  |  
------------------------------------------- 
10.1111/bju.14985  |    Objectives:  To develop and evaluate the feasibility of an objective method using artificial intelligence (AI) and image processing in a semi-automated fashion for tumour-to-cortex peak early-phase enhancement ratio (PEER) in order to differentiate CD117(+) oncocytoma from the chromophobe subtype of renal cell carcinoma (ChRCC) using convolutional neural networks (CNNs) on computed tomography imaging. 
  Methods:  The CNN was trained and validated to identify the kidney + tumour areas in images from 192 patients. The tumour type was differentiated through automated measurement of PEER after manual segmentation of tumours. The performance of this diagnostic model was compared with that of manual expert identification and tumour pathology with regard to accuracy, sensitivity and specificity, along with the root-mean-square error (RMSE), for the remaining 20 patients with CD117(+) oncocytoma or ChRCC. 
  Results:  The mean ± sd Dice similarity score for segmentation was 0.66 ± 0.14 for the CNN model to identify the kidney + tumour areas. PEER evaluation achieved accuracy of 95% in tumour type classification (100% sensitivity and 89% specificity) compared with the final pathology results (RMSE of 0.15 for PEER ratio). 
  Conclusions:  We have shown that deep learning could help to produce reliable discrimination of CD117(+) benign oncocytoma and malignant ChRCC through PEER measurements obtained by computer vision. 
  |  https://doi.org/10.1111/bju.14985  |  
------------------------------------------- 
10.3389/fbioe.2020.00181  |   The design of markerless systems to reconstruct human motion in a timely, unobtrusive and externally valid manner is still an open challenge. Artificial intelligence algorithms based on automatic landmarks identification on video images opened to a new approach, potentially e-viable with low-cost hardware. OpenPose is a library that t using a two-branch convolutional neural network allows for the recognition of skeletons in the scene. Although OpenPose-based solutions are spreading, their metrological performances relative to video setup are still largely unexplored. This paper aimed at validating a two-cameras OpenPose-based markerless system for gait analysis, considering its accuracy relative to three factors: cameras' relative distance, gait direction and video resolution. Two volunteers performed a walking test within a gait analysis laboratory. A marker-based optical motion capture system was taken as a reference. Procedures involved: calibration of the stereoscopic system; acquisition of video recordings, simultaneously with the reference marker-based system; video processing within OpenPose to extract the subject's skeleton; videos synchronization; triangulation of the skeletons in the two videos to obtain the 3D coordinates of the joints. Two set of parameters were considered for the accuracy assessment: errors in trajectory reconstruction and error in selected gait space-temporal parameters (step length, swing and stance time). The lowest error in trajectories (~20 mm) was obtained with cameras 1.8 m apart, highest resolution and straight gait, and the highest (~60 mm) with the 1.0 m, low resolution and diagonal gait configuration. The OpenPose-based system tended to underestimate step length of about 1.5 cm, while no systematic biases were found for swing/stance time. Step length significantly changed according to gait direction (<i>p</i> = 0.008), camera distance (<i>p</i> = 0.020), and resolution (<i>p</i> &lt; 0.001). Among stance and swing times, the lowest errors (0.02 and 0.05 s for stance and swing, respectively) were obtained with the 1 m, highest resolution and straight gait configuration. These findings confirm the feasibility of tracking kinematics and gait parameters of a single subject in a 3D space using two low-cost webcams and the OpenPose engine. In particular, the maximization of cameras distance and video resolution enabled to achieve the highest metrological performances. 
  |  https://doi.org/10.3389/fbioe.2020.00181  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32195243/  |  
------------------------------------------- 
10.3390/nano10040645  |   Nanoindentation was utilized as a non-destructive technique to identify Portland Cement hydration phases. Artificial Intelligence (AI) and semi-supervised Machine Learning (ML) were used for knowledge gain on the effect of carbon nanotubes to nanomechanics in novel cement formulations. Data labelling is performed with unsupervised ML with k-means clustering. Supervised ML classification is used in order to predict the hydration products composition and 97.6% accuracy was achieved. Analysis included multiple nanoindentation raw data variables, and required less time to execute than conventional single component probability density analysis (PDA). Also, PDA was less informative than ML regarding information exchange and re-usability of input in design predictions. In principle, ML is the appropriate science for predictive modeling, such as cement phase identification and facilitates the acquisition of precise results. This study introduces unbiased structure-property relations with ML to monitor cement durability based on cement phases nanomechanics compared to PDA, which offers a solution based on local optima of a multidimensional space solution. Evaluation of nanomaterials inclusion in composite reinforcement using semi-supervised ML was proved feasible. This methodology is expected to contribute to design informatics due to the high prediction metrics, which holds promise for the transfer learning potential of these models for studying other novel cement formulations. 
  |  http://www.mdpi.com/resolver?pii=nano10040645  |  
------------------------------------------- 
10.1038/s41746-020-0262-2  |   Artificial intelligence (AI) and Machine learning (ML) systems in medicine are poised to significantly improve health care, for example, by offering earlier diagnoses of diseases or recommending optimally individualized treatment plans. However, the emergence of AI/ML in medicine also creates challenges, which regulators must pay attention to. Which medical AI/ML-based products should be reviewed by regulators? What evidence should be required to permit marketing for AI/ML-based software as a medical device (SaMD)? How can we ensure the safety and effectiveness of AI/ML-based SaMD that may change over time as they are applied to new data? The U.S. Food and Drug Administration (FDA), for example, has recently proposed a discussion paper to address some of these issues. But it misses an important point: we argue that regulators like the FDA need to widen their scope from evaluating medical AI/ML-based products to assessing systems. This shift in perspective-from a product view to a system view-is central to maximizing the safety and efficacy of AI/ML in health care, but it also poses significant challenges for agencies like the FDA who are used to regulating products, not systems. We offer several suggestions for regulators to make this challenging but important transition. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32285013/  |  
------------------------------------------- 
10.1002/lio2.362  |    Objectives:  To compare two statistical models, namely logistic regression and artificial neural network (ANN), in prediction of vestibular schwannoma (VS) recurrence. 
  Methods:  Seven hundred eighty-nine patients with VS diagnosis completed an online survey. Potential predictors for recurrence were derived from univariate analysis by reaching the cut off <i>P</i> value of .05. Those nine potential predictors were years since treatment, surgeon's specialty, resection amount, and having incomplete eye closure, dry eye, double vision, facial pain, seizure, and voice/swallowing problem as a complication following treatment. Multivariate binary logistic regression model was compared with a four-layer 9-5-10-1 feedforward backpropagation ANN for prediction of recurrence. 
  Results:  The overall recurrence rate was 14.5%. Significant predictors of recurrence in the regression model were years since treatment and resection amount (both <i>P</i> &lt; .001). The regression model did not show an acceptable performance (area under the curve [AUC] = 0.64; <i>P</i> = .27). The regression model's sensitivity and specificity were 44% and 69%, respectively and correctly classified 56% of cases. The ANN showed a superior performance compared to the regression model (AUC = 0.79; <i>P</i> = .001) with higher sensitivity (61%) and specificity (81%), and correctly classified 70% of cases. 
  Conclusion:  The constructed ANN model was superior to logistic regression in predicting patient-answered VS recurrence in an anonymous survey with higher sensitivity and specificity. Since artificial intelligence tools such as neural networks can have higher predictive abilities compared to logistic regression models, continuous investigation into their utility as complementary clinical tools in predicting certain surgical outcomes is warranted. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32337359/  |  
------------------------------------------- 
10.3390/ijerph17061987  |   The digital transformation in dental medicine, based on electronic health data information, is recognized as one of the major game-changers of the 21st century to tackle present and upcoming challenges in dental and oral healthcare. This opinion letter focuses on the estimated top five trends and innovations of this new digital era, with potential to decisively influence the direction of dental research: (1) rapid prototyping (RP), (2) augmented and virtual reality (AR/VR), (3) artificial intelligence (AI) and machine learning (ML), (4) personalized (dental) medicine, and (5) tele-healthcare. Digital dentistry requires managing expectations pragmatically and ensuring transparency for all stakeholders: patients, healthcare providers, university and research institutions, the medtech industry, insurance, public media, and state policy. It should not be claimed or implied that digital smart data technologies will replace humans providing dental expertise and the capacity for patient empathy. The dental team that controls digital applications remains the key and will continue to play the central role in treating patients. In this context, the latest trend word is created: augmented intelligence, e.g., the meaningful combination of digital applications paired with human qualities and abilities in order to achieve improved dental and oral healthcare, ensuring quality of life. 
  |  http://www.mdpi.com/resolver?pii=ijerph17061987  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32197311/  |  
------------------------------------------- 
10.1016/j.jvs.2019.12.026  |    Objective:  Abdominal aortic aneurysm (AAA) is a life-threatening disease, and the only curative treatment relies on open or endovascular repair. The decision to treat relies on the evaluation of the risk of AAA growth and rupture, which can be difficult to assess in practice. Artificial intelligence (AI) has revealed new insights into the management of cardiovascular diseases, but its application in AAA has so far been poorly described. The aim of this review was to summarize the current knowledge on the potential applications of AI in patients with AAA. 
  Methods:  A comprehensive literature review was performed. The MEDLINE database was searched according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The search strategy used a combination of keywords and included studies using AI in patients with AAA published between May 2019 and January 2000. Two authors independently screened titles and abstracts and performed data extraction. The search of published literature identified 34 studies with distinct methodologies, aims, and study designs. 
  Results:  AI was used in patients with AAA to improve image segmentation and for quantitative analysis and characterization of AAA morphology, geometry, and fluid dynamics. AI allowed computation of large data sets to identify patterns that may be predictive of AAA growth and rupture. Several predictive and prognostic programs were also developed to assess patients' postoperative outcomes, including mortality and complications after endovascular aneurysm repair. 
  Conclusions:  AI represents a useful tool in the interpretation and analysis of AAA imaging by enabling automatic quantitative measurements and morphologic characterization. It could be used to help surgeons in preoperative planning. AI-driven data management may lead to the development of computational programs for the prediction of AAA evolution and risk of rupture as well as postoperative outcomes. AI could also be used to better evaluate the indications and types of surgical treatment and to plan the postoperative follow-up. AI represents an attractive tool for decision-making and may facilitate development of personalized therapeutic approaches for patients with AAA. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0741-5214(19)32911-8  |  
------------------------------------------- 
10.1016/j.ijmedinf.2020.104094  |    Introduction:  Research has shown that frailty, a geriatric syndrome associated with an increased risk of negative outcomes for older people, is highly prevalent among residents of residential aged care facilities (also called long term care facilities or nursing homes). However, progress on effective identification of frailty within residential care remains at an early stage, necessitating the development of new methods for accurate and efficient screening. 
  Objectives:  We aimed to determine the effectiveness of artificial intelligence (AI) algorithms in accurately identifying frailty among residents aged 75 years and over in comparison with a calculated electronic Frailty Index (eFI) based on a routinely-collected residential aged care administrative data set drawn from 10 residential care facilities located in Queensland, Australia. A secondary objective included the identification of best-performing candidate algorithms. 
  Methods:  We designed a frailty prediction system based on the eFI identification of frailty, allocating 84.5 % and 15.5 % of the data to training and test data sets respectively. We compared the performance of 18 specific scenarios to predict frailty against eFI based on unique combinations of three ML algorithms (support vector machines [SVM], decision trees [DT] and K-nearest neighbours [KNN]) and six cases (6, 10, 11, 14, 39 and 70 input variables). We calculated accuracy, percentage positive and negative agreement, sensitivity, specificity, Cohen's kappa and Prevalence- and Bias- Adjusted Kappa (PABAK), table frequencies and positive and negative predictive values. 
  Results:  Of 592 eligible resident records, 500 were allocated to the training set and 92 to the test set. Three scenarios (10, 11 and 70 input variables), all based on SVM algorithm, returned overall accuracy above 75 %. 
  Conclusions:  There is some potential for AI techniques to contribute towards better frailty identification within residential care. However, potential benefits will need to be weighed against administrative burden, data quality concerns and presence of potential bias. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1386-5056(19)31364-4  |  
------------------------------------------- 
10.1007/s00104-019-01052-2  |    Background:  Artificial intelligence (AI) has long been established in various parts of everyday life due to the instrumentalization of machines and robotics in industry, autonomous vehicles and the rapid development of computer-based systems. 
  Objective:  Demonstration of current developments and perspectives of AI in plastic surgery. 
  Material and methods:  Evaluation of statistics, press releases and original articles from journals and discussion of reviews. 
  Results:  In the healthcare system and in plastic surgery AI is particularly useful in the context of data analysis from digital patient files and big data from central registers. The use of 3D imaging systems provides objective feedback on surgical results in terms of volume and aesthetics. Intelligent robots assist plastic surgeons in microsurgical anastomoses of increasingly smaller vessels and the implementation of AI in the field of prosthetics enables patients to regain hand function following amputation injuries. 
  Conclusion:  For the benefit of the patients, it is the responsibility of experimental surgery to explore the opportunities, risks and limitations of applications with AI. 
  |  https://dx.doi.org/10.1007/s00104-019-01052-2  |  
------------------------------------------- 
10.1016/j.wneu.2020.03.187  |    Background:  Advancement and evolution of current virtual reality (VR) surgical simulation technologies is integral to improve the available armamentarium of surgical skill education. This is especially important in high-risk surgical specialties. Such fields including neurosurgery are beginning to explore the utilization of virtual reality simulation in the assessment and training of psychomotor skills. An important issue facing the available VR simulation technologies is the lack of complexity of scenarios which fail to replicate the visual and haptic realities of complex neurosurgical procedures. Therefore, there is a need to create more realistic and complex scenarios with the appropriate visual and haptic realities to maximize the potential of virtual reality technology METHODS: We outline a roadmap for creating complex virtual reality neurosurgical simulation scenarios using a step-wise description of our team's subpial tumor resection project as a model. 
  Results:  The creation of complex neurosurgical simulations involves integrating multiple modules into a scenario building roadmap. The components of each module are described outlining the important stages in the process of complex VR simulation creation. 
  Conclusion:  We have outlined a roadmap of step-wise approach for the creation of complex VR simulated neurosurgical procedures. This roadmap may also serve as a guide to aid the development of other virtual reality scenarios in a variety of surgical fields. The generation of new VR complex simulated neurosurgical procedures, by surgeons for surgeons, with the help of computer scientists and engineers may improve the assessment and training of residents and ultimately improve patient care. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1878-8750(20)30658-6  |  
------------------------------------------- 
10.1007/s00192-020-04243-2  |    |  https://dx.doi.org/10.1007/s00192-020-04243-2  |  
------------------------------------------- 
10.1016/j.amjmed.2019.12.016  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(20)30026-7  |  
------------------------------------------- 
10.1371/journal.pone.0229862  |   The current Edisonian approach to discovery requires up to two decades of fundamental and applied research for materials technologies to reach the market. Such a slow and capital-intensive turnaround calls for disruptive strategies to expedite innovation. Self-driving laboratories have the potential to provide the means to revolutionize experimentation by empowering automation with artificial intelligence to enable autonomous discovery. However, the lack of adequate software solutions significantly impedes the development of self-driving laboratories. In this paper, we make progress towards addressing this challenge, and we propose and develop an implementation of ChemOS; a portable, modular and versatile software package which supplies the structured layers necessary for the deployment and operation of self-driving laboratories. ChemOS facilitates the integration of automated equipment, and it enables remote control of automated laboratories. ChemOS can operate at various degrees of autonomy; from fully unsupervised experimentation to actively including inputs and feedbacks from researchers into the experimentation loop. The flexibility of ChemOS provides a broad range of functionality as demonstrated on five applications, which were executed on different automated equipment, highlighting various aspects of the software package. 
  |  http://dx.plos.org/10.1371/journal.pone.0229862  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32298284/  |  
------------------------------------------- 
10.1053/j.gastro.2020.03.038  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5085(20)30389-9  |  
------------------------------------------- 
10.1177/2192568219899944  |    |  https://journals.sagepub.com/doi/10.1177/2192568219899944?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32206509/  |  
------------------------------------------- 
10.1109/RBME.2020.2987975  |   The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19. 
  |  https://dx.doi.org/10.1109/RBME.2020.2987975  |  
------------------------------------------- 
10.14309/ajg.0000000000000429  |    Objectives:  Reliable in situ diagnosis of diminutive (≤5 mm) colorectal polyps could allow for "resect and discard" and "diagnose and leave" strategies, resulting in $1 billion cost savings per year in the United States alone. Current methodologies have failed to consistently meet the Preservation and Incorporation of Valuable endoscopic Innovations (PIVIs) initiative thresholds. Convolutional neural networks (CNNs) have the potential to predict polyp pathology and achieve PIVI thresholds in real time. 
  Methods:  We developed a CNN-based optical pathology (OP) model using Tensorflow and pretrained on ImageNet, capable of operating at 77 frames per second. A total of 6,223 images of unique colorectal polyps of known pathology, location, size, and light source (white light or narrow band imaging [NBI]) underwent 5-fold cross-training (80%) and validation (20%). Separate fresh validation was performed on 634 polyp images. Surveillance intervals were calculated, comparing OP with true pathology. 
  Results:  In the original validation set, the negative predictive value for adenomas was 97% among diminutive rectum/rectosigmoid polyps. Results were independent of use of NBI or white light. Surveillance interval concordance comparing OP and true pathology was 93%. In the fresh validation set, the negative predictive value was 97% among diminutive polyps in the rectum and rectosigmoid and surveillance concordance was 94%. 
  Discussion:  This study demonstrates the feasibility of in situ diagnosis of colorectal polyps using CNN. Our model exceeds PIVI thresholds for both "resect and discard" and "diagnose and leave" strategies independent of NBI use. Point-of-care adenoma detection rate and surveillance recommendations are potential added benefits. 
  |  http://Insights.ovid.com/pubmed?pmid=31651444  |  
------------------------------------------- 
10.1016/j.clinph.2020.02.032  |    Objective:  To validate an artificial intelligence-based computer algorithm for detection of epileptiform EEG discharges (EDs) and subsequent identification of patients with epilepsy. 
  Methods:  We developed an algorithm for automatic detection of EDs, based on a novel deep learning method that requires a low amount of labeled EEG data for training. Detected EDs are automatically grouped into clusters, consisting of the same type of EDs, for rapid visual inspection. We validated the algorithm on an independent dataset of 100 patients with sharp transients in their EEG recordings (54 with epilepsy and 46 with non-epileptic paroxysmal events). The diagnostic gold standard was derived from the video-EEG recordings of the patients' habitual events. 
  Results:  The algorithm had a sensitivity of 89% for identifying EEGs with EDs recorded from patients with epilepsy, a specificity of 70%, and an overall accuracy of 80%. 
  Conclusions:  Automated detection of EDs using an artificial intelligence-based computer algorithm had a high sensitivity. Human (expert) supervision is still necessary for confirming the clusters of detected EDs and for describing clinical correlations. Further studies on different patient populations will be needed to confirm our results. 
  Significance:  The automated algorithm we describe here is a useful tool, assisting neurophysiologist in rapid assessment of EEG recordings. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1388-2457(20)30116-4  |  
------------------------------------------- 
10.1016/j.earlhumdev.2020.105018  |   Machine intelligence, whether it constitutes Strong artificial intelligence (AI) or Weak AI, may have varying degrees of independence. Both Strong and Weak AIs are often depicted as being programmed with safeguards which prevent harm to humanity, informed by Asimov's programs called the Laws of Robotics. This paper will review these programs through a reading of instances of machine intelligence in Star Trek, and will attempt to show that these "ethical subroutines" may well be vital to our continued existence, irrespective of whether the machine intelligences constitute Strong or Weak AI. In effect, this paper will analyse the machine analogues of conscience in Star Trek, and will do so through an analysis of the android Data and the Emergency Medical Hologram. We will argue that AI should be treated with caution, lest we create powerful intelligences that may not only ignore us but also find us threatening. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-3782(20)30149-3  |  
------------------------------------------- 
10.1371/journal.pone.0227730  |    Background:  Chronic obstructive pulmonary disease (COPD) is associated with poor quality of life, hospitalization and mortality. COPD phenotype includes using pulmonary function tests to determine airflow obstruction from the forced expiratory volume in one second (FEV1):forced vital capacity. FEV1 is a commonly used value for severity but is difficult to identify in structured electronic health record (EHR) data. 
  Data source and methods:  Using the Microsoft SQL Server's full-text search feature and string functions supporting regular-expression-like operations, we developed an automated tool to extract FEV1 values from progress notes to improve ascertainment of FEV1 in EHR in the Veterans Aging Cohort Study (VACS). 
  Results:  The automated tool increased quantifiable FEV1 values from 12,425 to 16,274 (24% increase in numeric FEV1). Using chart review as the reference, positive predictive value of the tool was 99% (95% Confidence interval: 98.2-100.0%) for identifying quantifiable FEV1 values and a recall value of 100%, yielding an F-measure of 0.99. The tool correctly identified FEV1 measurements in 95% of cases. 
  Conclusion:  A SQL-based full text search of clinical notes for quantifiable FEV1 is efficient and improves the number of values available in VA data. Future work will examine how these methods can improve phenotyping of patients with COPD in the VA. 
  |  http://dx.plos.org/10.1371/journal.pone.0227730  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31945115/  |  
------------------------------------------- 
10.2174/1573399815666190531100012  |    Background:  One of the greatest challenges in the field of medicine is the increasing burden of chronic diseases, such as diabetes. Diabetes may cause several complications, such as kidney failure which is followed by hemodialysis and an increasing risk of cardiovascular diseases. 
  Objective:  The purpose of this research was to develop a clinical decision support system for assessing the risk of cardiovascular diseases in diabetic patients undergoing hemodialysis by using a fuzzy logic approach. 
  Methods:  This study was conducted in 2018. Initially, the views of physicians on the importance of assessment parameters were determined by using a questionnaire. The face and content validity of the questionnaire was approved by the experts in the field of medicine. The reliability of the questionnaire was calculated by using the test-retest method (r = 0.89). This system was designed and implemented by using MATLAB software. Then, it was evaluated by using the medical records of diabetic patients undergoing hemodialysis (n=208). 
  Results:  According to the physicians' point of view, the most important parameters for assessing the risk of cardiovascular diseases were glomerular filtration, duration of diabetes, age, blood pressure, type of diabetes, body mass index, smoking, and C reactive protein. The system was designed and the evaluation results showed that the values of sensitivity, accuracy, and validity were 85%, 92% and 90%, respectively. The K-value was 0.62. 
  Conclusion:  The results of the system were largely similar to the patients' records and showed that the designed system can be used to help physicians to assess the risk of cardiovascular diseases and to improve the quality of care services for diabetic patients undergoing hemodialysis. By predicting the risk of the disease and classifying patients in different risk groups, it is possible to provide them with better care plans. 
  |  http://www.eurekaselect.com/172361/article  |  
------------------------------------------- 
10.1371/journal.pone.0227928  |   Breast cancer is one of the commonest cancers among Algerian females. Compared to Western populations, the median age of diagnosis of breast cancer is much lower in Algeria. The objective of this study is to explore the expression of several miRNAs reported to be deregulated in breast cancer. The miRNAs miR-21, miR-125b, miR-100, miR-425-5p, miR-200c, miR-183 and miR-182 were studied on tumor and normal adjacent Algerian breast tissues using quantitative reverse transcription real time PCR, and the results were analyzed according to clinical characteristics. Compared to the normal adjacent tissues, miR-21, miR-183, miR-182, miR-425-5p and miR-200c were found to be upregulated while miR-100 and miR-125b were insignificantly deregulated. A positive correlation was noted among miR-183, miR-182 and miR-200c and among miR-425-5p, miR-183, miR-200c and miR-21. Further global miRNA microarray profiling studies can aid in finding ethnic specific miRNA biomarkers in the Algerian breast cancer population. 
  |  http://dx.plos.org/10.1371/journal.pone.0227928  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32040529/  |  
------------------------------------------- 
10.1016/j.neuron.2020.01.041  |   Highly portable, cloud-enabled neuroimaging technologies will fundamentally change neuroimaging research. Instead of participants traveling to the scanner, the scanner will now come to them. Field-based brain imaging research, including populations underrepresented in neuroscience research to date, will enlarge and diversify databases and pave the way for clinical and direct-to-consumer (DTC) applications. Yet these technological developments urgently require analysis of their ethical, legal, and social implications (ELSI). No consensus ethical frameworks for mobile neuroimaging exist, and existing policies for traditional MRI research are inadequate. Based on literature review and ethics analysis of neurotechnology development efforts, Shen et al. identify seven foundational, yet unresolved, ELSI issues posed by portable neuroimaging: (1) informed consent; (2) privacy; (3) capacity to accurately communicate neuroimaging results to remote participants; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data. The article proposes a path forward to address these urgent issues. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0896-6273(20)30068-4  |  
------------------------------------------- 
10.3348/kjr.2020.0146  |   The epidemic of 2019 novel coronavirus, later named as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is still gradually spreading worldwide. The nucleic acid test or genetic sequencing serves as the gold standard method for confirmation of infection, yet several recent studies have reported false-negative results of real-time reverse-transcriptase polymerase chain reaction (rRT-PCR). Here, we report two representative false-negative cases and discuss the supplementary role of clinical data with rRT-PCR, including laboratory examination results and computed tomography features. Coinfection with SARS-COV-2 and other viruses has been discussed as well. 
  |  https://www.kjronline.org/DOIx.php?id=10.3348/kjr.2020.0146  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32174053/  |  
------------------------------------------- 
10.3390/s20030785  |   Although access control based on human face recognition has become popular in consumer applications, it still has several implementation issues before it can realize a stand-alone access control system. Owing to a lack of computational resources, lightweight and computationally efficient face recognition algorithms are required. The conventional access control systems require significant active cooperation from the users despite its non-aggressive nature. The lighting/illumination change is one of the most difficult and challenging problems for human-face-recognition-based access control applications. This paper presents the design and implementation of a user-friendly, stand-alone access control system based on human face recognition at a distance. The local binary pattern (LBP)-AdaBoost framework was employed for face and eyes detection, which is fast and invariant to illumination changes. It can detect faces and eyes of varied sizes at a distance. For fast face recognition with a high accuracy, the Gabor-LBP histogram framework was modified by substituting the Gabor wavelet with Gaussian derivative filters, which reduced the facial feature size by 40% of the Gabor-LBP-based facial features, and was robust to significant illumination changes and complicated backgrounds. The experiments on benchmark datasets produced face recognition accuracies of 97.27% on an E-face dataset and 99.06% on an XM2VTS dataset, respectively. The system achieved a 91.5% true acceptance rate with a 0.28% false acceptance rate and averaged a 5.26 frames/sec processing speed on a newly collected face image and video dataset in an indoor office environment. 
  |  http://www.mdpi.com/resolver?pii=s20030785  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023973/  |  
------------------------------------------- 
10.1259/bjr.20190948  |   Historically, medical imaging has been a qualitative or semi-quantitative modality. It is difficult to quantify what can be seen in an image, and to turn it into valuable predictive outcomes. As a result of advances in both computational hardware and machine learning algorithms, computers are making great strides in obtaining quantitative information from imaging and correlating it with outcomes. Radiomics, in its two forms "handcrafted and deep," is an emerging field that translates medical images into quantitative data to yield biological information and enable radiologic phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Handcrafted radiomics is a multistage process in which features based on shape, pixel intensities, and texture are extracted from radiographs. Within this review, we describe the steps: starting with quantitative imaging data, how it can be extracted, how to correlate it with clinical and biological outcomes, resulting in models that can be used to make predictions, such as survival, or for detection and classification used in diagnostics. The application of deep learning, the second arm of radiomics, and its place in the radiomics workflow is discussed, along with its advantages and disadvantages. To better illustrate the technologies being used, we provide real-world clinical applications of radiomics in oncology, showcasing research on the applications of radiomics, as well as covering its limitations and its future direction. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190948?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1097/CCO.0000000000000604  |    |  http://dx.doi.org/10.1097/CCO.0000000000000604  |  
------------------------------------------- 
10.1016/j.ejca.2020.02.050  |    Background:  New technologies and techniques in radiation oncology and imaging offer opportunities to enhance the benefit of loco-regional treatments, expand treatment to new patient populations such as those with oligometastatic disease and decrease normal tissue toxicity. Furthermore, novel agents have become available which may be combined with radiation therapy, and identification of radiation-related biomarkers can be studied to refine treatment prescriptions. Finally, the use of artificial intelligence (AI) capabilities may also improve treatment quality assurance or the ease with which radiation dosing is prescribed. All of these potential advances present both opportunities and challenges for academic clinical researchers. 
  Methods:  Recently, the European Organisation for Research and Treatment of Cancer addressed these topics in a meeting of multiple stakeholders from Europe and North America. The following five themes radiobiology-based biomarkers, new technologies - particularly proton beam therapy, combination systemic and radiation therapy, management of oligometastatic disease and AI opportunities in radiation oncology were discussed in a State of Science format to define key controversies, unanswered questions and propose clinical trial priorities for development. 
  Conclusions:  Priorities for clinical trials implementing new science and technologies have been defined. Solutions to integrate the multidimensional complexity of data have been explored. New types of platforms and partnerships can support innovative approaches for clinical research in radiation oncology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0959-8049(20)30124-6  |  
------------------------------------------- 
10.4045/tidsskr.19.0779  |    |  http://tidsskriftet.no/article/19-0779  |  
------------------------------------------- 
10.1002/cpt.1792  |    |  https://doi.org/10.1002/cpt.1792  |  
------------------------------------------- 
10.3389/fncom.2020.00016  |   Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand. 
  |  https://doi.org/10.3389/fncom.2020.00016  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32194389/  |  
------------------------------------------- 
10.1007/s00259-019-04610-2  |   The publisher regret to inform the readers that the original version of this article contained errors in the text and published inadvertently. 
  |  https://dx.doi.org/10.1007/s00259-019-04610-2  |  
------------------------------------------- 
10.1161/CIRCGEN.119.002870  |    |  http://www.ahajournals.org/doi/full/10.1161/CIRCGEN.119.002870?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s00347-020-01063-z  |    Background:  Procedures with artificial intelligence (AI), such as deep neural networks, show promising results in automatic analysis of ophthalmological imaging data. 
  Objective:  This article discusses to what extent the application of AI algorithms can contribute to quality assurance in the field of ophthalmology. 
  Methods:  Relevant aspects from the literature are discussed. 
  Findings:  Systems based on artificial deep neural networks achieve remarkable results in the diagnostics of eye diseases, such as diabetic retinopathy and are very helpful, for example by segmenting optical coherence tomographic (OCT) images and detecting lesion components with high fidelity. To train these algorithms large data sets are required. The quality and availability of such data sets determine the continuous improvement of the algorithms. The comparison between the AI algorithms and physicians for image interpretation has also enabled insights into the diagnostic concordance between physicians. Current challenges include the development of methods for modelling decision uncertainty and improved interpretability of automated diagnostic decisions. 
  Conclusion:  Systems based on AI can support decision making for physicians and thereby contribute to a more efficient quality assurance. 
  |  https://dx.doi.org/10.1007/s00347-020-01063-z  |  
------------------------------------------- 
10.2174/157340561601200106142451  |    |  None  |  
------------------------------------------- 
10.1002/lt.25772  |    Background:  The demand for liver transplantation far outstrips the supply of deceased donor organs, and so listing and allocation decisions aim to maximise utility. Most existing methods for predicting transplant outcomes utilise basic methods such as regression modelling - newer artificial intelligence techniques have the potential to improve predictive accuracy. 
  Aims:  To systematically review studies predicting graft outcomes following deceased liver transplantation using Artificial Intelligence (AI) techniques and comparing these to linear regression and standard predictive modelling (donor risk index, DRI; Model for end-stage liver disease, MELD; survival outcome following liver transplantation, SOFT). 
  Methods:  A systematic review was performed. PubMed, Cochrane, MEDLINE, Science Direct, Springer Link, Elsevier, and reference lists were analysed for appropriate inclusion. 
  Results:  A total of 52 papers were reviewed for inclusion. Of these papers, 9 met the inclusion criteria, reporting outcomes from 18,771 liver transplants. Artificial neural networks (ANN) were the most commonly utilised methodology, being reported in 7 studies. Only two studies directly compared Machine Learning (ML) techniques to liver scoring modalities (i.e. DRI, SOFT, BAR). Both of these studies showed better prediction of individual organ survival with the optimal ANN model reporting AUC ROC 0.82 compared with BAR: 0.62 and SOFT: 0.57; and the other ANN model showing an AUC ROC: 0.84 compared to DRI: 0.68 and SOFT: 0.64. 
  Conclusion:  AI techniques can provide high accuracy in predicting graft survival based on donors and recipient variables. When compared to standard techniques, AI methods are dynamic - able to be trained and validated within every population. However, the high accuracy of AI may come at a cost of losing explainability (to patients and clinicians) on how the technology works. 
  |  https://doi.org/10.1002/lt.25772  |  
------------------------------------------- 
10.1016/j.jacr.2019.12.025  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(20)30002-8  |  
------------------------------------------- 
10.1515/dx-2019-0016  |   Since the 2015 publication of the National Academy of Medicine's (NAM) Improving Diagnosis in Health Care (Improving Diagnosis in Health Care. In: Balogh EP, Miller BT, Ball JR, editors. Improving Diagnosis in Health Care. Washington (DC): National Academies Press, 2015.), literature in diagnostic safety has grown rapidly. This update was presented at the annual international meeting of the Society to Improve Diagnosis in Medicine (SIDM). We focused our literature search on articles published between 2016 and 2018 using keywords in Pubmed and the Agency for Healthcare Research and Quality (AHRQ)'s Patient Safety Network's running bibliography of diagnostic error literature (Diagnostic Errors Patient Safety Network: Agency for Healthcare Research and Quality; Available from: https://psnet.ahrq.gov/search?topic=Diagnostic-Errors&amp;f_topicIDs=407). Three key topics emerged from our review of recent abstracts in diagnostic safety. First, definitions of diagnostic error and related concepts are evolving since the NAM's report. Second, medical educators are grappling with new approaches to teaching clinical reasoning and diagnosis. Finally, the potential of artificial intelligence (AI) to advance diagnostic excellence is coming to fruition. Here we present contemporary debates around these three topics in a pro/con format. 
  |  https://www.degruyter.com/doi/10.1515/dx-2019-0016  |  
------------------------------------------- 
10.1017/S0022215120000717  |    Objective:  To explore the feasibility of constructing a proof-of-concept artificial intelligence algorithm to detect tympanic membrane perforations, for future application in under-resourced rural settings. 
  Methods:  A retrospective review was conducted of otoscopic images analysed using transfer learning with Google's Inception-V3 convolutional neural network architecture. The 'gold standard' 'ground truth' was defined by otolaryngologists. Perforation size was categorised as less than one-third (small), one-third to two-thirds (medium), or more than two-thirds (large) of the total tympanic membrane diameter. 
  Results:  A total of 233 tympanic membrane images were used (183 for training, 50 for testing). The algorithm correctly identified intact and perforated tympanic membranes (overall accuracy = 76.0 per cent, 95 per cent confidence interval = 62.1-86.0 per cent); the area under the curve was 0.867 (95 per cent confidence interval = 0.771-0.963). 
  Conclusion:  A proof-of-concept image-classification artificial intelligence algorithm can be used to detect tympanic membrane perforations and, with further development, may prove to be a valuable tool for ear disease screening. Future endeavours are warranted to develop a point-of-care tool for healthcare workers in areas distant from otolaryngology. 
  |  https://www.cambridge.org/core/product/identifier/S0022215120000717/type/journal_article  |  
------------------------------------------- 
10.1016/j.cmi.2020.02.003  |    Background:  Machine learning (ML) is increasingly being used in many areas of health care. Its use in infection management is catching up as identified in a recent review in this journal. We present here a complementary review to this work. 
  Objectives:  To support clinicians and researchers in navigating through the methodological aspects of ML approaches in the field of infection management. 
  Sources:  A Medline search was performed with the keywords artificial intelligence, machine learning, infection∗, and infectious disease∗ for the years 2014-2019. Studies using routinely available electronic hospital record data from an inpatient setting with a focus on bacterial and fungal infections were included. 
  Content:  Fifty-two studies were included and divided into six groups based on their focus. These studies covered detection/prediction of sepsis (n = 19), hospital-acquired infections (n = 11), surgical site infections and other postoperative infections (n = 11), microbiological test results (n = 4), infections in general (n = 2), musculoskeletal infections (n = 2), and other topics (urinary tract infections, deep fungal infections, antimicrobial prescriptions; n = 1 each). In total, 35 different ML techniques were used. Logistic regression was applied in 18 studies followed by random forest, support vector machines, and artificial neural networks in 18, 12, and seven studies, respectively. Overall, the studies were very heterogeneous in their approach and their reporting. Detailed information on data handling and software code was often missing. Validation on new datasets and/or in other institutions was rarely done. Clinical studies on the impact of ML in infection management were lacking. 
  Implications:  Promising approaches for ML use in infectious diseases were identified. But building trust in these new technologies will require improved reporting. Explainability and interpretability of the models used were rarely addressed and should be further explored. Independent model validation and clinical studies evaluating the added value of ML approaches are needed. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1198-743X(20)30082-3  |  
------------------------------------------- 
10.1016/j.gie.2019.12.049  |    Background and aims:  The visual detection of early esophageal neoplasia (high-grade dysplasia and T1 cancer) in Barrett's esophagus (BE) with white-light and virtual chromoendoscopy still remains challenging. The aim of this study was to assess whether a convolutional neural artificial intelligence network can aid in the recognition of early esophageal neoplasia in BE. 
  Methods:  Nine hundred sixteen images from 65 patients of histology-proven early esophageal neoplasia in BE containing high-grade dysplasia or T1 cancer were collected. The area of neoplasia was masked using image annotation software. Nine hundred nineteen control images were collected of BE without high-grade dysplasia. A convolutional neural network (CNN) algorithm was pretrained on ImageNet and then fine-tuned with the goal of providing the correct binary classification of "dysplastic" or "nondysplastic." We developed an object detection algorithm that drew localization boxes around regions classified as dysplasia. 
  Results:  The CNN analyzed 458 test images (225 dysplasia and 233 nondysplasia) and correctly detected early neoplasia with sensitivity of 96.4%, specificity of 94.2%, and accuracy of 95.4%. With regard to the object detection algorithm for all images in the validation set, the system was able to achieve a mean average precision of .7533 at an intersection over union of .3 CONCLUSIONS: In this pilot study, our artificial intelligence model was able to detect early esophageal neoplasia in BE images with high accuracy. In addition, the object detection algorithm was able to draw a localization box around the areas of dysplasia with high precision and at a speed that allows for real-time implementation. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)30026-2  |  
------------------------------------------- 
10.1016/j.comtox.2019.100114  |   As the basis for managing the risks of chemical exposure, the Chemical Risk Assessment (CRA) process can impact a substantial part of the economy, the health of hundreds of millions of people, and the condition of the environment. However, the number of properly assessed chemicals falls short of societal needs due to a lack of experts for evaluation, interference of third party interests, and the sheer volume of potentially relevant information on the chemicals from disparate sources. In order to explore ways in which computational methods may help overcome this discrepancy between the number of chemical risk assessments required on the one hand and the number and adequateness of assessments actually being conducted on the other, the European Commission's Joint Research Centre organised a workshop on Artificial Intelligence for Chemical Risk Assessment (AI4CRA). The workshop identified a number of areas where Artificial Intelligence could potentially increase the number and quality of regulatory risk management decisions based on CRA, involving process simulation, supporting evaluation, identifying problems, facilitating collaboration, finding experts, evidence gathering, systematic review, knowledge discovery, and building cognitive models. Although these are interconnected, they are organised and discussed under two main themes: scientific-technical process and social aspects and the decision making process. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32140631/  |  
------------------------------------------- 
10.1016/j.heliyon.2020.e03395  |   Rotating machines are critical equipment in many processes, and failures in their operation can have serious implications. Consequently, fault detection in rotating machines has been widely investigated. Conventional detection systems include two blocks: feature extraction and classification. These systems are based on manually engineered features (ball pass frequencies, RMS value, kurtosis, crest factor, etc.) and therefore require a high level of human expertise (it is a human who designs and selects the most appropriate set of features to perform the classification). Instead, we propose a system for condition monitoring and fault detection in rotating machines based on a 1-D deep convolutional neural network (1D DCNN), which merges the tasks of feature extraction and classification into a single learning body. The proposed system has been designed for use on a rotating machine with seven possible operating states and it proves to be able to determine the operating condition of the machine almost as accurately as conventional feature-engineered classifiers, but without the need for prior knowledge of the machine. The proposed system has also reported good classification on a bearing fault dataset from another machine, thus demonstrating its capability to monitor the condition of different machines. Finally, the analysis of the features learned by the deep model has revealed valuable and previously unknown machine information, such as the rotational speed of the machine or the number of balls in the bearings. In this way, our results illustrate not only the good performance of CNNs, but also their versatility and the valuable information they could provide about the monitored machine. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2405-8440(20)30240-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32090183/  |  
------------------------------------------- 
10.1128/mSystems.00445-19  |   <i>Toxoplasma gondii</i>, one of the world's most common parasites, can infect all types of warm-blooded animals, including one-third of the world's human population. Most current routine diagnostic methods are costly, time-consuming, and labor-intensive. Although <i>T. gondii</i> can be directly observed under the microscope in tissue or spinal fluid samples, this form of identification is difficult and requires well-trained professionals. Nevertheless, the traditional identification of parasites under the microscope is still performed by a large number of laboratories. Novel, efficient, and reliable methods of <i>T. gondii</i> identification are therefore needed, particularly in developing countries. To this end, we developed a novel transfer learning-based microscopic image recognition method for <i>T. gondii</i> identification. This approach employs the fuzzy cycle generative adversarial network (FCGAN) with transfer learning utilizing knowledge gained by parasitologists that <i>Toxoplasma</i> is banana or crescent shaped. Our approach aims to build connections between microscopic and macroscopic associated objects by embedding the fuzzy C-means cluster algorithm into the cycle generative adversarial network (Cycle GAN). Our approach achieves 93.1% and 94.0% detection accuracy for ×400 and ×1,000 <i>Toxoplasma</i> microscopic images, respectively. We showed the high accuracy and effectiveness of our approach in newly collected unlabeled <i>Toxoplasma</i> microscopic images, compared to other currently available deep learning methods. This novel method for <i>Toxoplasma</i> microscopic image recognition will open a new window for developing cost-effective and scalable deep learning-based diagnostic solutions, potentially enabling broader clinical access in developing countries.<b>IMPORTANCE</b> <i>Toxoplasma gondii</i>, one of the world's most common parasites, can infect all types of warm-blooded animals, including one-third of the world's human population. Artificial intelligence (AI) could provide accurate and rapid diagnosis in fighting <i>Toxoplasma</i> So far, none of the previously reported deep learning methods have attempted to explore the advantages of transfer learning for <i>Toxoplasma</i> detection. The knowledge from parasitologists is that the <i>Toxoplasma</i> parasite is generally banana or crescent shaped. Based on this, we built connections between microscopic and macroscopic associated objects by embedding the fuzzy C-means cluster algorithm into the cycle generative adversarial network (Cycle GAN). Our approach achieves high accuracy and effectiveness in ×400 and ×1,000 <i>Toxoplasma</i> microscopic images. 
  |  https://doi.org/10.1128/mSystems.00445-19  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31992631/  |  
------------------------------------------- 
10.1515/dx-2020-0022  |   Background The number of websites providing laboratory test information is increasing fast, although the accuracy of reported resources is sometimes questionable. The aim of this study was to assess the quality of online retrievable information by Google Search engine. Methods Considering urinalysis, cholesterol and prostate-specific antigen (PSA) as keywords, the Google Search engine was queried. Using Google Trends, users' search trends (interest over time) were evaluated in a 5-year period. The first three or 10 retrieved hits were analysed in blind by two reviewers and classified according to the type of owner or publisher and for the quality of the reported Web content. Results The interest over time constantly increased for all the three considered tests. Most of the Web content owners were editorial and/or publishing groups (mean percentage 35.5% and 30.0% for the first three and 10 hits, respectively). Public and health agencies and scientific societies are less represented. Among the first three and 10 hits, cited sources were found to vary from 26.0% to 46.7% of Web page results, whilst for cholesterol, 60% of the retrieved Web contents reported only authors' signatures. Conclusions Our findings confirm those obtained in other studies in the literature, demonstrating that online Web searches can lead patients to inadequately written or reviewed health information. 
  |  https://www.degruyter.com/doi/10.1515/dx-2020-0022  |  
------------------------------------------- 
10.1016/j.gastrohep.2019.11.004  |   Computer-aided diagnosis (CAD) is a tool with great potential to help endoscopists in the tasks of detecting and histologically classifying colorectal polyps. In recent years, different technologies have been described and their potential utility has been increasingly evidenced, which has generated great expectations among scientific societies. However, most of these works are retrospective and use images of different quality and characteristics which are analysed off line. This review aims to familiarise gastroenterologists with computational methods and the particularities of endoscopic imaging, which have an impact on image processing analysis. Finally, the publicly available image databases, needed to compare and confirm the results obtained with different methods, are presented. 
  |  http://www.elsevier.es/en/linksolver/ft/pii/S0210-5705(20)30014-5  |  
------------------------------------------- 
10.1088/1361-6560/ab82e8  |   Deep convolutional neural network (DCNN), now popularly called artificial intelligence (AI), has shown the potential to improve over previous computer-assisted tools in medical imaging developed in the past decades. A DCNN has millions of free parameters that need to be trained, but the training sample set is limited in size for most medical imaging tasks so that transfer learning is typically used. Automatic data mining may be an efficient way to enlarge the collected data set but the data can be noisy such as incorrect labels or even a wrong type of images. In this work we studied the generalization error of DCNN with transfer learning in medical imaging for the task of classifying malignant and benign masses on mammograms. With a finite available data set, we simulated a training set containing corrupted data or noisy labels. The balance between learning and memorization of the DCNN was manipulated by varying the proportion of corrupted data in the training set. The generalization error of DCNN was analyzed by the area under the receiver operating characteristic curve for the training and test sets and the weight changes after transfer learning. The study demonstrates that the transfer learning strategy of DCNN for such tasks needs to be designed properly, taking into consideration the constraints of the available training set having limited size and quality for the classification task at hand, to minimize memorization and improve generalizability. 
  |  https://doi.org/10.1088/1361-6560/ab82e8  |  
------------------------------------------- 
10.30773/pi.2019.0270  |    Objective:  Suicidal ideation (SI) precedes actual suicidal event. Thus, it is important for the prevention of suicide to screen the individuals with SI. This study aimed to identify the factors associated with SI and to build prediction models in Korean adults using machine learning methods. 
  Methods:  The 2010-2013 dataset of the Korea National Health and Nutritional Examination Survey was used as the training dataset (n=16,437), and the subset collected in 2015 was used as the testing dataset (n=3,788). Various machine learning algorithms were applied and compared to the conventional logistic regression (LR)-based model. 
  Results:  Common risk factors for SI included stress awareness, experience of continuous depressive mood, EQ-5D score, depressive disorder, household income, educational status, alcohol abuse, and unmet medical service needs. The prediction performances of the machine learning models, as measured by the area under receiver-operating curve, ranged from 0.794 to 0.877, some of which were better than that of the conventional LR model (0.867). The Bayesian network, LogitBoost with LR, and ANN models outperformed the conventional LR model. 
  Conclusion:  A machine learning-based approach could provide better SI prediction performance compared to a conventional LR-based model. These may help primary care physicians to identify patients at risk of SI and will facilitate the early prevention of suicide. 
  |  http://psychiatryinvestigation.org/journal/view.php?doi=10.30773/pi.2019.0270  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32213803/  |  
------------------------------------------- 
10.1007/s00066-020-01615-x  |   Current research, especially in oncology, increasingly focuses on the integration of quantitative, multiparametric and functional imaging data. In this fast-growing field of research, radiomics may allow for a more sophisticated analysis of imaging data, far beyond the qualitative evaluation of visible tissue changes. Through use of quantitative imaging data, more tailored and tumour-specific diagnostic work-up and individualized treatment concepts may be applied for oncologic patients in the future. This is of special importance in cross-sectional disciplines such as radiology and radiation oncology, with already high and still further increasing use of imaging data in daily clinical practice. Liver targets are generally treated with stereotactic body radiotherapy (SBRT), allowing for local dose escalation while preserving surrounding normal tissue. With the introduction of online target surveillance with implanted markers, 3D-ultrasound on conventional linacs and hybrid magnetic resonance imaging (MRI)-linear accelerators, individualized adaptive radiotherapy is heading towards realization. The use of big data such as radiomics and the integration of artificial intelligence techniques have the potential to further improve image-based treatment planning and structured follow-up, with outcome/toxicity prediction and immediate detection of (oligo)progression. The scope of current research in this innovative field is to identify and critically discuss possible application forms of radiomics, which is why this review tries to summarize current knowledge about interdisciplinary integration of radiomics in oncologic patients, with a focus on investigations of radiotherapy in patients with liver cancer or oligometastases including multiparametric, quantitative data into (radio)-oncologic workflow from disease diagnosis, treatment planning, delivery and patient follow-up. 
  |  https://dx.doi.org/10.1007/s00066-020-01615-x  |  
------------------------------------------- 
10.1016/j.gerinurse.2020.03.011  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0197-4572(20)30081-1  |  
------------------------------------------- 
10.1177/1756284820910659  |    Background:  Recently the American Society for Gastrointestinal Endoscopy addressed the 'resect and discard' strategy, determining that accurate <i>in vivo</i> differentiation of colorectal polyps (CP) is necessary. Previous studies have suggested a promising application of artificial intelligence (AI), using deep learning in object recognition. Therefore, we aimed to construct an AI system that can accurately detect and classify CP using stored still images during colonoscopy. 
  Methods:  We used a deep convolutional neural network (CNN) architecture called Single Shot MultiBox Detector. We trained the CNN using 16,418 images from 4752 CPs and 4013 images of normal colorectums, and subsequently validated the performance of the trained CNN in 7077 colonoscopy images, including 1172 CP images from 309 various types of CP. Diagnostic speed and yields for the detection and classification of CP were evaluated as a measure of performance of the trained CNN. 
  Results:  The processing time of the CNN was 20 ms per frame. The trained CNN detected 1246 CP with a sensitivity of 92% and a positive predictive value (PPV) of 86%. The sensitivity and PPV were 90% and 83%, respectively, for the white light images, and 97% and 98% for the narrow band images. Among the correctly detected polyps, 83% of the CP were accurately classified through images. Furthermore, 97% of adenomas were precisely identified under the white light imaging. 
  Conclusions:  Our CNN showed promise in being able to detect and classify CP through endoscopic images, highlighting its high potential for future application as an AI-based CP diagnosis support system for colonoscopy. 
  |  https://journals.sagepub.com/doi/10.1177/1756284820910659?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231710/  |  
------------------------------------------- 
10.1016/j.ejrad.2020.108940  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0720-048X(20)30129-7  |  
------------------------------------------- 
10.3390/s20061649  |   Non-intrusive load monitoring (NILM) is a cost-effective approach that electrical appliances are identified from aggregated whole-field electrical signals, according to their extracted electrical characteristics, with no need to intrusively deploy smart power meters (power plugs) installed for individual monitored electrical appliances in a practical field of interest. This work addresses NILM by a parallel Genetic Algorithm (GA)-embodied Artificial Neural Network (ANN) for Demand-Side Management (DSM) in a smart home. An ANN's performance in terms of classification accuracy depends on its training algorithm. Additionally, training an ANN/deep NN learning from massive training samples is extremely computationally intensive. Therefore, in this work, a parallel GA has been conducted and used to integrate meta-heuristics (evolutionary computing) with an ANN (neurocomputing) considering its evolution in a parallel execution relating to load disaggregation in a Home Energy Management System (HEMS) deployed in a real residential field. The parallel GA that involves iterations to excessively cost its execution time for evolving an ANN learning model from massive training samples to NILM in the HEMS and works in a divide-and-conquer manner that can exploit massively parallel computing for evolving an ANN and, thus, reduce execution time drastically. This work confirms the feasibility and effectiveness of the parallel GA-embodied ANN applied to NILM in the HEMS for DSM. 
  |  http://www.mdpi.com/resolver?pii=s20061649  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32188065/  |  
------------------------------------------- 
10.3390/s20030612  |   Nowadays, vehicles have advanced driver-assistance systems which help to improve vehicle safety and save the lives of drivers, passengers and pedestrians. Identification of the road-surface type and condition in real time using a video image sensor, can increase the effectiveness of such systems significantly, especially when adapting it for braking and stability-related solutions. This paper contributes to the development of the new efficient engineering solution aimed at improving vehicle dynamics control via the anti-lock braking system (ABS) by estimating friction coefficient using video data. The experimental research on three different road surface types in dry and wet conditions has been carried out and braking performance was established with a car mathematical model (MM). Testing of a deep neural networks (DNN)-based road-surface and conditions classification algorithm revealed that this is the most promising approach for this task. The research has shown that the proposed solution increases the performance of ABS with a rule-based control strategy. 
  |  http://www.mdpi.com/resolver?pii=s20030612  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31979141/  |  
------------------------------------------- 
10.3390/s20030873  |   The exponential growth in population and their overall reliance on the usage of electrical and electronic devices have increased the demand for energy production. It needs precise energy management systems that can forecast the usage of the consumers for future policymaking. Embedded smart sensors attached to electricity meters and home appliances enable power suppliers to effectively analyze the energy usage to generate and distribute electricity into residential areas based on their level of energy consumption. Therefore, this paper proposes a clustering-based analysis of energy consumption to categorize the consumers' electricity usage into different levels. First, a deep autoencoder that transfers the low-dimensional energy consumption data to high-level representations was trained. Second, the high-level representations were fed into an adaptive self-organizing map (SOM) clustering algorithm. Afterward, the levels of electricity energy consumption were established by conducting the statistical analysis on the obtained clustered data. Finally, the results were visualized in graphs and calendar views, and the predicted levels of energy consumption were plotted over the city map, providing a compact overview to the providers for energy utilization analysis. 
  |  http://www.mdpi.com/resolver?pii=s20030873  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32041362/  |  
------------------------------------------- 
10.1159/000505800  |   The diagnosis of rare genetic diseases is one of the most difficult areas in medicine. Whole-exome sequencing (WES) technology makes it easier to diagnose these diseases. In addition, next-generation phenotyping can help to diagnose computer-based algorithms. Detailed dysmorphologic findings of 25 patients diagnosed by WES in our center were described. The success of this technology in diagnosing rare genetic diseases was investigated by scanning the photographs of 25 patients with Face2Gene application. The application listed possible preliminary diagnoses (30 disease suggestion). Of these, 12 (48%) cases were correctly matched. The most common disease group in the patients was neurological disease (96%). The most common mode of inheritance in the patients was autosomal recessive. The rate of consanguineous marriages was determined in 80% of the patients. Ten patients had microcephaly and 7 patients had corpus callosum anomaly. In our study, we found that the success of Face2Gene was lower than described in the literature. We think that the probable cause of this condition is that the cases are very rare, and there is not enough data about these diseases in the application. Therefore, it is recommended that applications should be used more frequently by pediatricians and clinical geneticists. The diagnosis of rare diseases still is quite difficult. Nowadays, WES is a successful method. However, applications such as Face2Gene help to make a clinical prediagnosis and create a larger database. 
  |  https://www.karger.com?DOI=10.1159/000505800  |  
------------------------------------------- 
10.1093/europace/euz349  |    |  https://academic.oup.com/europace/article-lookup/doi/10.1093/europace/euz349  |  
------------------------------------------- 
10.1177/2382120519889348  |   Discussions surrounding the future of artificial intelligenc (AI) in healthcare often cause consternation among healthcare professionals. These feelings may stem from a lack of formal education on AI and how to be a leader of AI implementation in medical systems. To address this, our academic medical center hosted an educational summit exploring how to become a leader of AI in healthcare. This article presents three lessons learned from hosting this summit, thus providing guidance for developing medical curriculum on the topic of AI in healthcare. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32064356/  |  
------------------------------------------- 
10.1097/RTI.0000000000000499  |   In this review article, the current and future impact of artificial intelligence (AI) technologies on diagnostic imaging is discussed, with a focus on cardio-thoracic applications. The processing of imaging data is described at 4 levels of increasing complexity and wider implications. At the examination level, AI aims at improving, simplifying, and standardizing image acquisition and processing. Systems for AI-driven automatic patient iso-centering before a computed tomography (CT) scan, patient-specific adaptation of image acquisition parameters, and creation of optimized and standardized visualizations, for example, automatic rib-unfolding, are discussed. At the reading and reporting levels, AI focuses on automatic detection and characterization of features and on automatic measurements in the images. A recently introduced AI system for chest CT imaging is presented that reports specific findings such as nodules, low-attenuation parenchyma, and coronary calcifications, including automatic measurements of, for example, aortic diameters. At the prediction and prescription levels, AI focuses on risk prediction and stratification, as opposed to merely detecting, measuring, and quantifying images. An AI-based approach for individualizing radiation dose in lung stereotactic body radiotherapy is discussed. The digital twin is presented as a concept of individualized computational modeling of human physiology, with AI-based CT-fractional flow reserve modeling as a first example. Finally, at the cohort and population analysis levels, the focus of AI shifts from clinical decision-making to operational decisions. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000499  |  
------------------------------------------- 
10.1371/journal.pcbi.1007792  |   Until date, several machine learning approaches have been proposed for the dynamic modeling of temporal omics data. Although they have yielded impressive results in terms of model accuracy and predictive ability, most of these applications are based on "Black-box" algorithms and more interpretable models have been claimed by the research community. The recent eXplainable Artificial Intelligence (XAI) revolution offers a solution for this issue, were rule-based approaches are highly suitable for explanatory purposes. The further integration of the data mining process along with functional-annotation and pathway analyses is an additional way towards more explanatory and biologically soundness models. In this paper, we present a novel rule-based XAI strategy (including pre-processing, knowledge-extraction and functional validation) for finding biologically relevant sequential patterns from longitudinal human gene expression data (GED). To illustrate the performance of our pipeline, we work on in vivo temporal GED collected within the course of a long-term dietary intervention in 57 subjects with obesity (GSE77962). As validation populations, we employ three independent datasets following the same experimental design. As a result, we validate primarily extracted gene patterns and prove the goodness of our strategy for the mining of biologically relevant gene-gene temporal relations. Our whole pipeline has been gathered under open-source software and could be easily extended to other human temporal GED applications. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007792  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32275707/  |  
------------------------------------------- 
10.7717/peerj.8764  |   Industrial pig farming is associated with negative technological pressure on the bodies of pigs. Leg weakness and lameness are the sources of significant economic loss in raising pigs. Therefore, it is important to identify the predictors of limb condition. This work presents assessments of the state of limbs using indicators of growth and meat characteristics of pigs based on machine learning algorithms. We have evaluated and compared the accuracy of prediction for nine ML classification algorithms (Random Forest, K-Nearest Neighbors, Artificial Neural Networks, C50Tree, Support Vector Machines, Naive Bayes, Generalized Linear Models, Boost, and Linear Discriminant Analysis) and have identified the Random Forest and K-Nearest Neighbors as the best-performing algorithms for predicting pig leg weakness using a small set of simple measurements that can be taken at an early stage of animal development. Measurements of Muscle Thickness, Back Fat amount, and Average Daily Gain were found to be significant predictors of the conformation of pig limbs. Our work demonstrates the utility and relative ease of using machine learning algorithms to assess the state of limbs in pigs based on growth rate and meat characteristics. 
  |  https://doi.org/10.7717/peerj.8764  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32231879/  |  
------------------------------------------- 
10.1080/13697137.2019.1682804  |    |  http://www.tandfonline.com/doi/full/10.1080/13697137.2019.1682804  |  
------------------------------------------- 
PMID:32296809  |    |  http://www.annals.edu.sg/pdf/special/COM19208.pdf  |  
------------------------------------------- 
10.22454/FamMed.2020.881454  |    |  https://journals.stfm.org/familymedicine/2020/january/liaw-2019-0311/  |  
------------------------------------------- 
10.1016/j.gie.2020.04.044  |    Background and aims:  Artificial intelligence (AI), specifically deep learning, offers the potential to enhance the field of gastrointestinal endoscopy in areas ranging from lesion detection and classification, to quality metrics and documentation. Progress in this field will be measured by whether AI implementation can lead to improved patient outcomes and more-efficient clinical workflow for GI endoscopists. The aims of this article are to report the findings of a multidisciplinary group of experts focusing on issues in artificial intelligence research and applications related to gastroenterology and endoscopy, to review the current status of the field, and to produce recommendations for investigators developing and studying new AI technologies for gastroenterology. 
  Methods:  A multidisciplinary meeting was held on September 28, 2019, bringing together academic, industry, and regulatory experts in diverse fields including gastroenterology, computer and imaging sciences, machine learning, and computer vision, Food and Drug Administration (FDA) and National Institutes of Health (NIH). Recent and ongoing studies in gastroenterology and current technology in AI were presented and discussed, key gaps in knowledge were identified, and recommendations were made for research that would have the highest impact in making advances and implementation in the field of AI to gastroenterology. 
  Results:  There was a consensus that AI will transform the field of gastroenterology, particularly endoscopy and image interpretation. Powered by advanced machine learning algorithms, the use of computer vision to endoscopy has the potential to result in better prediction and treatment outcomes for patients with gastroenterology disorders and cancer. Large libraries of endoscopic images, "EndoNet," will be important to facilitate development and application of AI systems. The regulatory environment for implementation of AI systems is evolving, but common outcomes such as colon polyp detection have been highlighted as potential clinical trial endpoints. Other threshold outcomes will be important, as well as clarity on iterative improvement of clinical systems. 
  Conclusions:  Gastroenterology is a prime candidate for early adoption of AI. AI is rapidly moving from an experimental phase to a clinical implementation phase in gastroenterology. It is anticipated that the implementation of AI in gastroenterology over the next decade will have a significant and positive impact on patient care and clinical workflows. Ongoing collaboration among gastroenterologists, industry experts, and regulatory agencies will be important to ensure that progress is rapid and clinically meaningful. However, there are several constraints and areas that will benefit from further exploration, including potential clinical applications, implementation, structure and governance, role of gastroenterologists, and potential impact of AI in gastroenterology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(20)34198-5  |  
------------------------------------------- 
10.1126/science.aaz3023  |    |  http://www.sciencemag.org/cgi/pmidlookup?view=long&pmid=32108102  |  
------------------------------------------- 
10.1073/pnas.2005329117  |    |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=32317384  |  
------------------------------------------- 
10.21037/atm.2019.10.99  |    |  https://doi.org/10.21037/atm.2019.10.99  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32175362/  |  
------------------------------------------- 
10.1097/CM9.0000000000000714  |    |  http://dx.doi.org/10.1097/CM9.0000000000000714  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32132365/  |  
------------------------------------------- 
10.1097/01.APO.0000656984.56467.2c  |    |  http://dx.doi.org/10.1097/01.APO.0000656984.56467.2c  |  
------------------------------------------- 
10.2471/BLT.19.237099  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284653/  |  
------------------------------------------- 
10.1016/j.ijtb.2020.02.002  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0019-5707(20)30014-7  |  
------------------------------------------- 
10.1016/j.neunet.2020.03.006  |   The global exponential stabilization and lag synchronization control of delayed inertial neural networks (INNs) are investigated. By constructing nonnegative function and employing inequality techniques, several new results about exponential stabilization and exponential lag synchronization are derived via adaptive control. And the theoretical outcomes are developed directly from the INNs themselves without variable substitution. In addition, the synchronization results are also applied to image encryption and decryption. Finally, an example is presented to illustrate the validity of the derived results. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(20)30083-6  |  
------------------------------------------- 
PMID:32219808  |    Objective:  We aimed to identify new biomarkers in Diffuse Large B-cell Lymphoma (DLBCL) using the deep learning technique. 
  Methods and results:  The multilayer perceptron (MLP) analysis was performed in the GSE10846 series, divided into discovery (n = 100) and validation (n = 414) sets. The top 25 gene-probes from a total of 54,614 were selected based on their normalized importance for outcome prediction (dead/alive). By Gene Set Enrichment Analysis (GSEA) the association to unfavorable prognosis was confirmed. In the validation set, by univariate Cox regression analysis, high expression of <i>ARHGAP19</i>, <i>MESD</i>, <i>WDCP</i>, <i>DIP2A</i>, <i>CACNA1B</i>, <i>TNFAIP8</i>, <i>POLR3H</i>, <i>ENO3</i>, <i>SERPINB8</i>, <i>SZRD1</i>, <i>KIF23</i> and <i>GGA3</i> associated to poor, and high <i>SFTPC</i>, <i>ZSCAN12</i>, <i>LPXN</i> and <i>METTL21A</i> to favorable outcome. A multivariate analysis confirmed <i>MESD</i>, <i>TNFAIP8</i> and <i>ENO3</i> as risk factors and <i>ZSCAN12</i> and <i>LPXN</i> as protective factors. Using a risk score formula, the 25 genes identified two groups of patients with different survival that was independent to the cell-of-origin molecular classification (5-year OS, low vs. high risk): 65% vs. 24%, respectively (Hazard Risk = 3.2, P &lt; 0.000001). Finally, correlation with known DLBCL markers showed that high expression of all <i>MYC</i>, <i>BCL2</i> and <i>ENO3</i> associated to the worst outcome. 
  Conclusion:  By artificial intelligence we identified a set of genes with prognostic relevance. 
  |  http://mj-med-u-tokai.com/pdf/450107.pdf  |  
------------------------------------------- 
10.1038/s41598-020-62368-2  |   Tuberculosis (TB), an infectious disease caused by Mycobacterium tuberculosis (M.tb), causes highest number of deaths globally for any bacterial disease necessitating novel diagnosis and treatment strategies. High-throughput sequencing methods generate a large amount of data which could be exploited in determining multi-drug resistant (MDR-TB) associated mutations. The present work is a computational framework that uses artificial intelligence (AI) based machine learning (ML) approaches for predicting resistance in the genes rpoB, inhA, katG, pncA, gyrA and gyrB for the drugs rifampicin, isoniazid, pyrazinamide and fluoroquinolones. The single nucleotide variations were represented by several sequence and structural features that indicate the influence of mutations on the target protein coded by each gene. We used ML algorithms - naïve bayes, k nearest neighbor, support vector machine, and artificial neural network, to build the prediction models. The classification models had an average accuracy of 85% across all examined genes and were evaluated on an external unseen dataset to demonstrate their application. Further, molecular docking and molecular dynamics simulations were performed for wild type and predicted resistance causing mutant protein and anti-TB drug complexes to study their impact on the conformation of proteins to confirm the observed phenotype. 
  |  http://dx.doi.org/10.1038/s41598-020-62368-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32218465/  |  
------------------------------------------- 
10.1088/1741-2552/ab8131  |   Primary brain tumors including gliomas continue to pose significant management challenges to clinicians. While the presentation, the pathology, and the clinical course of these lesions are variable, the initial investigations are usually similar. Patients who are suspected to have a brain tumor will be assessed with computed tomography (CT) and magnetic resonance imaging (MRI). The imaging findings are used by neurosurgeons to determine the feasibility of surgical resection and plan such an undertaking. Imaging studies are also an indispensable tool in tracking tumor progression or its response to treatment. As these imaging studies are non-invasive, relatively cheap and accessible to patients, there have been many efforts over the past two decades to increase the amount of clinically-relevant information that can be extracted from brain imaging. Most recently, artificial intelligence (AI) techniques have been employed to segment and characterize brain tumors, as well as to detect progression or treatment-response. However, the clinical utility of such endeavours remains limited due to challenges in data collection and annotation, model training, and the reliability of AI-generated information. We provide a review of recent advances in addressing the above challenges. First, to overcome the challenge of data paucity, different image imputation and synthesis techniques along with annotation collection efforts are summarized. Next, various training strategies are presented to meet multiple desiderata, such as model performance, generalization ability, data privacy protection, and learning with sparse annotations. Finally, standardized performance evaluation and model interpretability methods have been reviewed. We believe that these technical approaches will facilitate the development of a fully-functional AI tool in the clinical care of patients with gliomas. 
  |  https://doi.org/10.1088/1741-2552/ab8131  |  
------------------------------------------- 
10.1016/j.acra.2019.12.024  |    Rationale and objectives:  Generative adversarial networks (GANs) are deep learning models aimed at generating fake realistic looking images. These novel models made a great impact on the computer vision field. Our study aims to review the literature on GANs applications in radiology. 
  Materials and methods:  This systematic review followed the PRISMA guidelines. Electronic datasets were searched for studies describing applications of GANs in radiology. We included studies published up-to September 2019. 
  Results:  Data were extracted from 33 studies published between 2017 and 2019. Eighteen studies focused on CT images generation, ten on MRI, three on PET/MRI and PET/CT, one on ultrasound and one on X-ray. Applications in radiology included image reconstruction and denoising for dose and scan time reduction (fourteen studies), data augmentation (six studies), transfer between modalities (eight studies) and image segmentation (five studies). All studies reported that generated images improved the performance of the developed algorithms. 
  Conclusion:  GANs are increasingly studied for various radiology applications. They enable the creation of new data, which can be used to improve clinical care, education and research. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30021-0  |  
------------------------------------------- 
10.3390/ijerph17020463  |   According to literature, myopia has become the second most common eye disease in China, and the incidence of myopia is increasing year by year, and showing a trend of younger age. Previous researches have shown that the occurrence of myopia is mainly determined by poor eye habits, including reading and writing posture, eye length, and so on, and parents' heredity. In order to better prevent myopia in adolescents, this paper studies the influence of related factors on myopia incidence in adolescents based on machine learning method. A feature selection method based on both univariate correlation analysis and multivariate correlation analysis is used to better construct a feature sub-set for model training. A method based on GBRT is provided to help fill in missing items in the original data. The prediction model is built based on SVM model. Data transformation has been used to improve the prediction accuracy. Results show that our method could achieve reasonable performance and accuracy. 
  |  http://www.mdpi.com/resolver?pii=ijerph17020463  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31936770/  |  
------------------------------------------- 
10.1111/liv.14360  |   A healthy diet together with physical activity could induce weight loss and control the progression of non-alcoholic fatty liver disease (NAFLD). However, the composition of diet has not been clearly established. Macronutrients such as saturated fatty acids (SFA), trans-fats, simple sugars and animal proteins have a harmful effect on the liver. On the other hand, monounsaturated fats (MUFAs), polyunsaturated (PUFAs) omega-3-fats, plant-based proteins and dietary fibres are considered to be beneficial to the liver. The impact of specific micronutrients is less well-known. Nutrients are part of the food we eat. Food makes up our meals, which compose our dietary patterns. Non-alcoholic fatty liver disease patients usually follow Western diets which are rich in soda, frozen junk food, juice, red meat, lard, processed meats, whole fat dairy foods, fatty snack foods, take-away foods, cakes and biscuits and poor in cereals, whole grains, fruit, vegetables, extra virgin olive oil (EVOO) and fish. On the other hand, the Mediterranean diet (MD) is beneficial for NAFLD even when it is iso-caloric or there are no changes in body weight. A new approach, called 'nutritional geometry' considers the importance of integrating nutrition, animals and the environment. The goal of this approach is to combine nutrients and foods in a model to understand how food components interact to regulate the properties of diets affecting health and disease. The use of algorithms developed by artificial intelligence (AI) to create a personalized diet for patients can provide customized nutritional counselling to prevent and treat NAFLD. 
  |  https://doi.org/10.1111/liv.14360  |  
------------------------------------------- 
10.1002/mus.26895  |    Introduction:  Conventional processing of nerve for histomorphometry is resource-intensive, precluding use in intraoperative assessment of nerve quality during nerve transfer procedures. Stimulated Raman scattering (SRS) microscopy is a label-free technique that enables rapid and high-resolution histology. 
  Methods:  Segments of healthy murine sciatic nerve, healthy human obturator nerve, and human cross-facial nerve autografts were imaged on a custom SRS microscope. Myelinated axon quantification was performed through segmentation using a random forest machine learning algorithm in commercial software. 
  Results:  High contrast, high-resolution imaging of nerve morphology was obtained with SRS imaging. Automated myelinated axon quantification from cross-sections of healthy human nerve imaged using SRS was achieved. 
  Discussion:  Herein we demonstrate the use of a label-free technique for rapid imaging of murine and human peripheral nerve cryosections. We illustrate the potential of this technique to inform intraoperative decision making through rapid automated quantification of myelinated axons using a machine learning algorithm. This article is protected by copyright. All rights reserved. 
  |  https://doi.org/10.1002/mus.26895  |  
------------------------------------------- 
10.1093/toxsci/kfz214  |   Bisphenol F (BPF) is one of several Bisphenol A (BPA) substituents that is increasingly used in manufacturing industry leading to detectable human exposure. Whereas a large number of studies have been devoted to decipher BPA effects, much less is known about its substituents. To support decision making on BPF's safety, we have developed a new computational approach to rapidly explore the available data on its toxicological effects, combining text mining and integrative systems biology, and aiming at connecting BPF to adverse outcome pathways (AOPs). We first extracted from different databases BPF-protein associations that were expanded to protein complexes using protein-protein interaction datasets. Over-representation analysis of the protein complexes allowed to identify the most relevant biological pathways putatively targeted by BPF. Then, automatic screening of scientific abstracts from literature using the text mining tool, AOP-helpFinder, combined with data integration from various sources (AOP-wiki, CompTox, etc.) and manual curation allowed us to link BPF to AOP events. Finally, we combined all the information gathered through those analyses and built a comprehensive complex framework linking BPF to an AOP network including, as adverse outcomes, various types of cancers such as breast and thyroid malignancies. These results which integrate different types of data can support regulatory assessment of the BPA substituent, BPF, and trigger new epidemiological and experimental studies. 
  |  https://academic.oup.com/toxsci/article-lookup/doi/10.1093/toxsci/kfz214  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31596483/  |  
------------------------------------------- 
10.1177/2055207620911580  |    Objective:  To understand the therapeutic processes associated with the helpfulness of an online relational agent intervention, Manage Your Life Online (MYLO). 
  Methods:  Fifteen participants experiencing a mental health related problem used Manage Your Life Online for 2 weeks. At follow-up, the participants each identified two helpful and two unhelpful questions posed by Manage Your Life Online within a single intervention session. Qualitative interviews were conducted and analyzed using thematic and content analysis to gain insight into the process of therapy with Manage Your Life Online. 
  Results:  MYLO appeared acceptable to participants with a range of presenting problems. Questions enabling free expression, increased awareness, and new insights were key to a helpful intervention. The findings were consistent with the core processes of therapeutic change, according to Perceptual Control Theory, a unifying theory of psychological distress. Questions that elicited intense emotions, were repetitive, confusing, or inappropriate were identified as unhelpful and were associated with disengagement or loss of faith in Manage Your Life Online. 
  Conclusions:  The findings provide insight into the likely core therapy processes experienced as helpful or hindering and outlines further ways to optimize acceptability of Manage Your Life Online. 
  |  https://journals.sagepub.com/doi/10.1177/2055207620911580?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32206331/  |  
------------------------------------------- 
10.3758/s13414-019-01969-0  |   We report novel findings from experiments on the enumeration of canonical patterns under attentional load. While previous studies have shown that the process of enumerating randomized arrangements can be disrupted by attentional load, the effect of attentional load on canonical patterns has been unexplored. To investigate this case, we adapted a spatial dual-task paradigm previously used to study attentional disruption during the enumeration of randomized arrangements. We begin by replicating previous findings for randomized arrangements, with enumeration error increasing with cluster numerosity and attentional load. For dice patterns, enumeration error also increased under attentional load. However, contrary to findings from studies on single-task enumeration of dice patterns, we observed conflation of patterns with similar outlines. In subsequent experiments, we manipulated the spatial location of the enumeration task, placing the dot cluster in the center. With centrally located, canonical patterns that remained in the same location across trials, enumeration accuracy was more consistent with results from single-task studies. We hypothesize that participants may be using shape cues to inform guessing during enumeration tasks when unable to both localize and fully attend to target patterns. 
  |  https://dx.doi.org/10.3758/s13414-019-01969-0  |  
------------------------------------------- 
10.1016/S2468-1253(19)30407-8  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2468-1253(19)30407-8  |  
------------------------------------------- 
10.1016/j.jbi.2020.103407  |   The aim of eXplainable Artificial Intelligence (XAI) is to design intelligent systems that can explain their predictions or recommendations to humans. Such systems are particularly desirable for therapeutic decision support, because physicians need to understand rcommendations to have confidence in their application and to adapt them if required, e.g. in case of patient contraindication. We propose here an explainable and visual approach for decision support in antibiotic treatment, based on an ontology. There were three steps to our method. We first generated a tabular dataset from the ontology, containing features defined on various domains and n-ary features. A preference model was then learned from patient profiles, antibiotic features and expert recommendations found in clinical practice guidelines. This model made the implicit rationale of the expert explicit, including the way in which missing data was treated. We then visualized the preference model and its application to all antibiotics available on the market for a given clinical situation, using rainbow boxes, a recently developed technique for set visualization. The resulting preference model had an error rate of 3.5% on the learning data, and 5.2% on test data (10-fold validation). These findings suggest that our system can help physicians to prescribe antibiotics correctly, even for clinical situations not present in the guidelines (e.g. due to allergies or contraindications for the recommended treatment). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1532-0464(20)30035-6  |  
------------------------------------------- 
10.1002/lary.28708  |    Objectives/hypothesis:  Create an autonomous computational system to classify endoscopy findings. 
  Study design:  Computational analysis of vocal fold images at an academic, tertiary-care laryngology practice. 
  Methods:  A series of normal and abnormal vocal fold images were obtained from the image database of an academic tertiary care laryngology practice. The benign images included normals, nodules, papilloma, polyps, and webs. A separate set of carcinoma and leukoplakia images comprised a single malignant-premalignant class. All images were classified with their existing labels. Images were randomly withheld from each class for testing. The remaining images were used to train and validate a neural network for classifying vocal fold lesions. Two classifiers were developed. A multiclass system classified the five categories of benign lesions. A separate analysis was performed using a binary classifier trained to distinguish malignant-premalignant from benign lesions. 
  Results:  Precision ranged from 71.7% (polyps) to 89.7% (papilloma), and recall ranged from 70.0% (papilloma) to 88.0% (nodules) for the benign classifier. Overall accuracy for the benign classifier was 80.8%. The binary classifier correctly identified 92.0% of the malignant-premalignant lesions with an overall accuracy of 93.0%. 
  Conclusions:  Autonomous classification of endoscopic images with artificial intelligence technology is possible. Better network implementations and larger datasets will continue to improve classifier accuracy. A clinically useful optical cancer screening system may require a multimodality approach that incorporates nonvisual spectra. 
  Level of evidence:  NA Laryngoscope, 2020. 
  |  https://doi.org/10.1002/lary.28708  |  
------------------------------------------- 
10.1080/14779072.2020.1732208  |   <b>Introduction</b>: With the increase in the number of patients with cardiovascular diseases, better risk-prediction models for cardiovascular events are needed. Statistical-based risk-prediction models for cardiovascular events (CVEs) are available, but they lack the ability to predict individual-level risk. Machine learning (ML) methods are especially equipped to handle complex data and provide accurate risk-prediction models at the individual level.<b>Areas covered</b>: In this review, the authors summarize the literature comparing the performance of machine learning methods to that of traditional, statistical-based models in predicting CVEs. They provide a brief summary of ML methods and then discuss risk-prediction models for CVEs such as major adverse cardiovascular events, heart failure and arrhythmias.<b>Expert opinion</b>: Current evidence supports the superiority of ML methods over statistical-based models in predicting CVEs. Statistical models are applicable at the population level and are subject to overfitting, while ML methods can provide an individualized risk level for CVEs. Further prospective research on ML-guided treatments to prevent CVEs is needed. 
  |  http://www.tandfonline.com/doi/full/10.1080/14779072.2020.1732208  |  
------------------------------------------- 
10.1161/CIRCEP.119.007988  |    Background:  Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in health and health care. In this study, we aimed to (1) assess whether the performance of a deep learning algorithm designed to detect low left ventricular ejection fraction using the 12-lead ECG varies by race/ethnicity and to (2) determine whether its performance is determined by the derivation population or by racial variation in the ECG. 
  Methods:  We performed a retrospective cohort analysis that included 97 829 patients with paired ECGs and echocardiograms. We tested the model performance by race/ethnicity for convolutional neural network designed to identify patients with a left ventricular ejection fraction ≤35% from the 12-lead ECG. 
  Results:  The convolutional neural network that was previously derived in a homogeneous population (derivation cohort, n=44 959; 96.2% non-Hispanic white) demonstrated consistent performance to detect low left ventricular ejection fraction across a range of racial/ethnic subgroups in a separate testing cohort (n=52 870): non-Hispanic white (n=44 524; area under the curve [AUC], 0.931), Asian (n=557; AUC, 0.961), black/African American (n=651; AUC, 0.937), Hispanic/Latino (n=331; AUC, 0.937), and American Indian/Native Alaskan (n=223; AUC, 0.938). In secondary analyses, a separate neural network was able to discern racial subgroup category (black/African American [AUC, 0.84], and white, non-Hispanic [AUC, 0.76] in a 5-class classifier), and a network trained only in non-Hispanic whites from the original derivation cohort performed similarly well across a range of racial/ethnic subgroups in the testing cohort with an AUC of at least 0.930 in all racial/ethnic subgroups. 
  Conclusions:  Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine. 
  |  http://www.ahajournals.org/doi/full/10.1161/CIRCEP.119.007988?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1001/jamadermatol.2019.5013  |    |  https://jamanetwork.com/journals/jamadermatology/fullarticle/10.1001/jamadermatol.2019.5013  |  
------------------------------------------- 
10.1007/s00104-019-01090-w  |   Artificial intelligence procedures will find special fields of application also in general and visceral surgery. These will not only be limited to intraoperative surgical applications but also extend to perioperative processes, education and training as well as to future scientific developments. Major impulses are to be expected in decision support systems, cognitive collaborative interventional environments and in evidence-based knowledge acquisition models; however, the implementation into the daily practice not only requires profound insights into the field of informatics and computer science but also a comprehensive knowledge of the surgical domain. Accordingly, the future implementation of artificial intelligence in surgery requires a new culture of collaboration between surgeons and researchers/computer scientists. 
  |  https://dx.doi.org/10.1007/s00104-019-01090-w  |  
------------------------------------------- 
10.1186/s40662-020-00182-7  |    Background:  Effective screening is a desirable method for the early detection and successful treatment for diabetic retinopathy, and fundus photography is currently the dominant medium for retinal imaging due to its convenience and accessibility. Manual screening using fundus photographs has however involved considerable costs for patients, clinicians and national health systems, which has limited its application particularly in less-developed countries. The advent of artificial intelligence, and in particular deep learning techniques, has however raised the possibility of widespread automated screening. 
  Main text:  In this review, we first briefly survey major published advances in retinal analysis using artificial intelligence. We take care to separately describe standard multiple-field fundus photography, and the newer modalities of ultra-wide field photography and smartphone-based photography. Finally, we consider several machine learning concepts that have been particularly relevant to the domain and illustrate their usage with extant works. 
  Conclusions:  In the ophthalmology field, it was demonstrated that deep learning tools for diabetic retinopathy show clinically acceptable diagnostic performance when using colour retinal fundus images. Artificial intelligence models are among the most promising solutions to tackle the burden of diabetic retinopathy management in a comprehensive manner. However, future research is crucial to assess the potential clinical deployment, evaluate the cost-effectiveness of different DL systems in clinical practice and improve clinical acceptance. 
  |  https://eandv.biomedcentral.com/articles/10.1186/s40662-020-00182-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32313813/  |  
------------------------------------------- 
10.2471/BLT.19.234732  |   Technological advances in big data (large amounts of highly varied data from many different sources that may be processed rapidly), data sciences and artificial intelligence can improve health-system functions and promote personalized care and public good. However, these technologies will not replace the fundamental components of the health system, such as ethical leadership and governance, or avoid the need for a robust ethical and regulatory environment. In this paper, we discuss what a robust ethical and regulatory environment might look like for big data analytics in health insurance, and describe examples of safeguards and participatory mechanisms that should be established. First, a clear and effective data governance framework is critical. Legal standards need to be enacted and insurers should be encouraged and given incentives to adopt a human-centred approach in the design and use of big data analytics and artificial intelligence. Second, a clear and accountable process is necessary to explain what information can be used and how it can be used. Third, people whose data may be used should be empowered through their active involvement in determining how their personal data may be managed and governed. Fourth, insurers and governance bodies, including regulators and policy-makers, need to work together to ensure that the big data analytics based on artificial intelligence that are developed are transparent and accurate. Unless an enabling ethical environment is in place, the use of such analytics will likely contribute to the proliferation of unconnected data systems, worsen existing inequalities, and erode trustworthiness and trust. 
 Les progrès technologiques en matière de big data (un terme qui désigne de grandes quantités de données extrêmement variées, provenant de différentes sources et pouvant être traitées rapidement), de sciences de l'information et d'intelligence artificielle peuvent améliorer le fonctionnement du système de santé, mais aussi promouvoir des soins personnalisés et servir l'intérêt public. Néanmoins, ces technologies ne permettront pas de remplacer les composantes fondamentales du système de santé, comme le leadership éthique et la bonne gouvernance, ni d'éviter la nécessité de créer un environnement déontologique et réglementaire solide. Le présent document se penche sur la définition de cet environnement déontologique et réglementaire solide pour l'analyse des big data dans le domaine de l'assurance maladie, et fournit à titre d'exemple les mécanismes de protection et de participation qu'il convient d'instaurer. En premier lieu, imposer un cadre de gouvernance précis et efficace est essentiel au traitement des données. Des normes juridiques doivent être promulguées, tandis que les assureurs doivent être encouragés et incités à adopter une approche centrée sur l'humain, tant dans leur conception que dans leur utilisation de l'analyse des big data et de l'intelligence artificielle. Deuxièmement, il faut mettre en place un processus clair et responsable afin d'expliquer quels types d'informations sont susceptibles d'être employés et à quelles fins. Troisièmement, les personnes concernées doivent avoir la possibilité de déterminer de quelle manière leurs données personnelles sont gérées et régies, en étant activement impliquées dans ce processus. Et quatrièmement, les assureurs et les organes de gouvernance, dont les régulateurs et législateurs, doivent collaborer pour faire en sorte que l'analyse des big data basée sur l'intelligence artificielle soit correcte et transparente. À moins d'établir un environnement éthique, l'usage d'une telle analyse entraînera probablement la prolifération de systèmes de données non connectés, l'aggravation des inégalités actuelles ainsi qu'une perte de confiance et de fiabilité. 
 Los avances tecnológicos relativos a los macrodatos (es decir, grandes cantidades de datos muy variados de muchas fuentes diversas que pueden procesarse rápidamente), las ciencias de los datos y la inteligencia artificial pueden mejorar las funciones del sistema sanitario y promover la atención personalizada y el bien público. No obstante, estas tecnologías no sustituirán los componentes fundamentales del sistema sanitario, como el liderazgo ético y la gobernanza, ni evitarán la necesidad de un entorno ético y normativo sólido. En el presente documento se examina cómo podría ser un entorno ético y normativo sólido para el análisis de macrodatos en el ámbito de los seguros médicos, y se describen ejemplos de mecanismos de protección y participación que deberían establecerse. En primer lugar, es fundamental contar con un marco claro y eficaz de gestión de datos. Es necesario promulgar normas jurídicas y alentar e incentivar a las aseguradoras para que adopten un enfoque centrado en el ser humano en el diseño y la aplicación de análisis de macrodatos e inteligencia artificial. En segundo lugar, es necesario un proceso claro y responsable para explicar cómo y qué información se puede utilizar. En tercer lugar, se debe facultar a las personas cuyos datos puedan ser utilizados mediante su participación activa en la determinación de cómo se pueden gestionar y regular sus datos personales. En cuarto lugar, las aseguradoras y los órganos de gobierno, incluidos los reguladores y los responsables de formular políticas, deben colaborar para garantizar que los análisis de macrodatos basados en la inteligencia artificial que se elaboren sean transparentes y precisos. A menos que exista un entorno ético adecuado, el uso de esos análisis probablemente contribuirá a la proliferación de sistemas de datos sin conexión, empeorará las desigualdades existentes y reducirá la fiabilidad y la confianza. 
 يمكن للتطورات التكنولوجية في البيانات الضخمة (وهي الكميات الكبيرة من البيانات شديدة التنوع من العديد من المصادر المختلفة والتي يمكن معالجتها بسرعة)، وعلوم البيانات والذكاء الاصطناعي، تحسين وظائف النظام الصحي وتعزيز الرعاية الشخصية والصالح العام. ومع ذلك، إلا أن هذه التقنيات لن تحل محل المكونات الأساسية للنظام الصحي، مثل القيادة والحوكمة الأخلاقية، أو تجنب الحاجة إلى بيئة أخلاقية وتنظيمية قوية. سوف نناقش في هذه الورقة كيف تبدو البيئة الأخلاقية التنظيمية القوية بالنسبة لتحليلات البيانات الضخمة في التأمين الصحي، كما نصف أمثلة للضمانات والآليات المشتركة التي يجب إنشاؤها. أولاً، يعد وجود إطار حوكمة واضح وفعال للبيانات أمراً حيوياً. يجب تفعيل المعايير القانونية كما يجب تشجيع جهات التأمين ومنحها الحوافز لاعتماد نهج يركز على الإنسان في التصميم واستخدام تحليلات البيانات الضخمة والذكاء الاصطناعي. ثانياً، من الضروري وجود عملية واضحة وخاضعة للمساءلة تهدف إلى شرح المعلومات التي يمكن استخدامها وكيف يمكن استخدامها. ثالثاً، يجب دعم الأشخاص الذين يمكن استخدام بياناتهم، وذلك من خلال انخراطهم بشكل نشط في تحديد كيفية إدارة بياناتهم الشخصية والتحكم فيها. رابعاً، تحتج جهات التأمين وهيئات الحوكمة، بما في ذلك الجهات التنظيمية وواضعي السياسات، إلى العمل معًا لضمان أن تكون تحليلات البيانات الضخمة القائمة على الذكاء الاصطناعي، والتي تم تطويرها، تتميز بالشفافية والدقة. ما لم تكن هناك بيئة أخلاقية مناسبة، فإن استخدام هذه التحليلات من المحتمل أن يسهم في انتشار أنظمة للبيانات غير المتصلة، ويزيد من سوء الحالات القائمة لعدم المساواة، ويحد من الثقة والجدارة بالثقة. 
 大数据（即，可以快速处理大量不同来源的高度差异化数据）、数据科学和人工智能领域的技术进步可以改善医疗系统功能，促进个性化护理和公益服务。然而，这些技术不会取代医疗系统中的道德领导和治理等基本组成要素，也不会消除对稳健的道德和监管环境的需求。在本文中，我们讨论了医疗保险大数据分析中的稳健道德和监管环境可能是什么样子的，并且举例描述了应该建立的保障和参与机制。首先，一个清晰有效的数据治理框架至关重要。需要制定法律标准，并且鼓励和激励保险公司在设计和使用大数据分析和人工智能方面秉承以人为本的理念。第二，必须有一个明确的问责流程来解释可以使用哪些信息以及如何使用这些信息。第三，对于数据被采用的人员，应该通过积极参与决定如何管理和治理其个人数据的方式为其赋权。第四，保险公司和治理机构，包括监管机构和政策制定者，需携手合作，确保基于人工智能开发的大数据分析是透明且准确的。除非具备有利的道德环境，否则使用此类分析很可能会导致未连接的数据系统的分散，加剧现有的不均衡情况，并降低可靠性和可信度。. 
 Стремительное развитие технологий в области больших данных (то есть больших объемов разнородных данных из самых разных источников, которые можно быстро обрабатывать), обработки и анализа данных и искусственного интеллекта может совершенствовать функции систем здравоохранения и способствовать разработке индивидуального подхода к оказанию медицинской помощи и обеспечению благосостояния общества. Тем не менее использование этих технологий не сможет заменить фундаментальные компоненты системы здравоохранения, такие как этика руководства и управления, или устранить необходимость создания надежной этической и нормативной среды. В этой статье авторы обсуждают, как может выглядеть надежная этическая и нормативная среда для аналитики больших данных в сфере медицинского страхования, и рассматривают примеры защитных мер и механизмов участия, которые необходимо разработать. Во-первых, важнейшее значение имеет разработка четкой и эффективной системы управления данными. Необходимо принять правовые стандарты, которые будут создавать стимулы для страховщиков использовать подход, ориентированный на нужды людей, для разработки и использования аналитики больших данных и искусственного интеллекта. Во-вторых, необходимо разработать четкий и прозрачный метод определения типа данных, которые могут использоваться, а также способ использования таких данных. В-третьих, люди, чьи данные могут использоваться, должны наделяться полномочиями посредством активного участия в определении методов управления их личными данными. В-четвертых, страховщикам и органам управления, включая директивные органы и органы регулирования, необходимо сотрудничать для обеспечения прозрачности и точности аналитики больших данных, полученной на основе разработанного искусственного интеллекта. В отсутствие благоприятной этической среды использование такой аналитики будет скорее способствовать распространению несвязанных систем данных, усугублению существующего неравенства и подрыву авторитетности этой аналитики и доверия к ней. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284650/  |  
------------------------------------------- 
PMID:32074153  |   The use of computers or machines in medicine dates back to the 1960s. Deep learning software programming is a subset of artificial intelligence (AI) based on the ability of a machine to learn from data and adaptively change. Deep learning is creating the next industrial revolution across the economy by replacing repetitive low-skilled tasks with learning algorithms. In medicine, image-based fields such as radiology, dermatology, and pathology have seen an increase in the number of studies using deep learning. However, given the current lack of standardized data sets to train these machines, it is difficult to predict if the present results eventually will be translated to real-life clinical settings. 
  |  https://www.mdedge.com/dermatology/article/215099/practice-management/artificial-intelligence-going-replace-dermatologists  |  
------------------------------------------- 
10.1007/s11948-020-00176-7  |   When the phrase "playing God" is used in debates concerning the use of new technologies, such as cloning or genetic engineering, it is usually interpreted as a warning not to interfere with God's creation or nature. I think that this interpretation of "playing God" arguments as a call to non-interference with nature is too narrow. In this paper, I propose an alternative interpretation of "playing God" arguments. Taking an argumentation theory approach, I provide an argumentation scheme and accompanying critical questions that capture the moral concerns expressed by "playing God" arguments. If I am right, then "playing God" arguments should be understood, not as a warning to leave God's creation or nature alone, but rather as an invitation to think carefully about all the ways in which the use of new technologies could go seriously wrong. 
  |  https://dx.doi.org/10.1007/s11948-020-00176-7  |  
------------------------------------------- 
PMID:32030939  |   Cybersecurity is a real threat in almost all human activity domains. The health sector is a particular vulnerable target for cybercriminals. The first reason is obviously the financial incentive: the value of the content of a personal electronic health record, sold on the darknet, easily exceeds 1000 US dollars. The second reason is the aging Information Technology (IT) infrastructure we are dealing with, both in the hospital sector as well as in the vast majority of private medical practices. There is also an astonishing lack of environmental consciousness and an absence of a real safety culture in the medical profession. Very often there is neither an institutional basic training, nor a continuous and mandatory education in institutional cybersecurity. There is no single magic bullet to solve the problem, but various mechanisms can be put in place to mitigate the risks and limit the hazards as much as possible. 
 La criminalité cybernétique s’attaque à tous les domaines d’activité humaine. Le monde hospitalier est particulièrement vulnérable. En effet, d’une part, un dossier médical personnel (DMP) volé se vend aisément plus de 1.000 dollars sur le «darknet» et, d’autre part, les structures informatiques vieillissantes de nos institutions et de nos cabinets médicaux privés sont particulièrement exposées aux attaques. Viennent s’y ajouter une réelle méconnaissance du danger par les professionnels de soins et un manque de culture de sécurité informatique, illustré par l’absence quasi totale d’une formation initiale et d’une formation continue en la matière. Il n’y a pas une seule réponse simple et définitive à ce fléau, mais différentes solutions peuvent être mises en place rapidement afin de limiter les risques encourus et les dégâts. 
  |  https://www.rmlg.ulg.ac.be/aboel.php?num_id=3245&langue=EN  |  
------------------------------------------- 
10.1016/j.jtos.2020.02.008  |    Objective:  To apply artificial intelligence (AI) for automated identification of corneal condition and prediction of the likelihood of need for future keratoplasty intervention from optical coherence tomography (OCT)-based corneal parameters. 
  Design:  Cohort study. 
  Participants:  We collected 12,242 corneal OCT images from 3162 subjects using CASIA OCT Imaging Systems (Tomey, Japan). We included 3318 measurements collected at the baseline visit of each patient. A total of 333 eyes had post-operative penetrating keratoplasty (PKP), lamellar keratoplasty (LKP), deep anterior keratoplasty (DALK), descemet's stripping automated endothelial keratoplasty (DSAEK) or descemet's membrane endothelial keratoplasty (DMEK) intervention. 
  Method:  We developed a pipeline including linear and nonlinear data transformations followed by unsupervised machine learning and applied on corneal parameters from the baseline visit of each patient. Five non-overlapping clusters of eyes were identified. Post hoc analyses revealed that clusters corresponded to different likelihoods of need for future keratoplasty. These clusters on a 2-dimensional map can be used by clinicians and surgeons to identify patients with higher risk of need for future keratoplasty intervention. 
  Main outcome measures:  The likelihood of the need for future surgery. 
  Results:  The mean age of participants was 69.7 (standard deviation; SD = 16.1) and 59% were female. The normalized likelihood of need for future corneal keratoplasty intervention for eyes mapped onto clusters one to five were 2.2%, 1.0%, 33.1%, 32.7%, and 31.0%, respectively. 
  Conclusions:  The AI system can assist the (cornea) surgeon in identifying those patients who may be at higher risk for future keratoplasty using comprehensive corneal shape, thickness, and elevation parameters. Future research utilizing independent datasets is necessary to validate the proposed system. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1542-0124(20)30027-6  |  
------------------------------------------- 
10.1097/CM9.0000000000000623  |   Artificial intelligence (AI) is now a trendy subject in clinical medicine and especially in gastrointestinal (GI) endoscopy. AI has the potential to improve the quality of GI endoscopy at all levels. It will compensate for humans' errors and limited capabilities by bringing more accuracy, consistency, and higher speed, making endoscopic procedures more efficient and of higher quality. AI showed great results in diagnostic and therapeutic endoscopy in all parts of the GI tract. More studies are still needed before the introduction of this new technology in our daily practice and clinical guidelines. Furthermore, ethical clearance and new legislations might be needed. In conclusion, the introduction of AI will be a big breakthrough in the field of GI endoscopy in the upcoming years. It has the potential to bring major improvements to GI endoscopy at all levels. 
  |  http://dx.doi.org/10.1097/CM9.0000000000000623  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31929362/  |  
------------------------------------------- 
10.18632/aging.102850  |    |  https://www.impactaging.com/full/12/2028  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32039835/  |  
------------------------------------------- 
10.3389/fpubh.2020.00041  |    |  https://doi.org/10.3389/fpubh.2020.00041  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32158740/  |  
------------------------------------------- 
10.1016/j.ejrad.2020.108928  |    Purpose:  To investigate the effective dose (E) and convolution kernel's effects on the detection of pulmonary nodules in different artificial intelligence (AI) software systems. 
  Methods:  Simulated nodules of various sizes and densities in the Lungman phantom were CT scanned at different levels of E (3 - 5, 1 - 3, 0.5 - 1, and &lt;0.5 mSv) and were reconstructed with different kernels (B30f, B60f, and B80f). The number of nodules and corresponding volumes in different images were detected by four AI software systems (A, B, C, and D). Sensitivity, false positives (FPs), false negatives (FNs), and relative volume error (RVE) were calculated and compared to the aspects of the E and convolution kernel. 
  Results:  System B had the highest median sensitivity (100 %). The median FPs of systems B (1) and D (1) was lower than A (11.5) and C (5). System D had the smallest RVE (13.12 %). When the E was &lt;0.5 mSv, system D's sensitivity decreased, while the FPs and FNs of systems A and B increased significantly (P &lt; 0.05). When the kernel was changed from B80f to B30f, the FPs of system A decreased, while that of system C increased, and the RVE of systems A, B, and C increased (P &lt; 0.05). 
  Conclusion:  AI software systems B and D have high detection efficiency under normal or low dose conditions and show better stability. However, the detection efficiency of systems A and C would be affected by the E or convolution kernel, but the E would not affect the volume measurement of four systems. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0720-048X(20)30117-0  |  
------------------------------------------- 
10.1007/s12282-020-01061-8  |    Background:  To compare the breast cancer detection performance in digital mammograms of a panel of three unaided human readers (HR) versus a stand-alone artificial intelligence (AI)-based Transpara system in a population of Japanese women. 
  Methods:  The subjects were 310 Japanese female outpatients who underwent digital mammographic examinations between January 2018 and October 2018. A panel of three HR provided a Breast Imaging Reporting and Data System (BI-RADS) score, and Transpara system provided an interactive decision support score and an examination-based cancer likelihood score. The area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were compared under each of reading conditions. 
  Results:  The AUC was higher for human readers than with stand-alone Transpara system (human readers 0.816; Transpara system 0.706; difference 0.11; P &lt; 0.001). The sensitivity of the unaided HR for diagnosis was 89% and specificity was 86%. The sensitivity of stand-alone Transpara system for cutoff scores of 4 and 7 were 93% and 85%, and specificities were 45% and 67%, respectively. 
  Conclusions:  Although the diagnostic performance of Transpara system was statistically lower than that of HR, the recent advances in AI algorithms are expected to reduce the difference between computers and human experts in detecting breast cancer. 
  |  https://dx.doi.org/10.1007/s12282-020-01061-8  |  
------------------------------------------- 
10.3389/fpsyt.2020.00111  |    |  https://doi.org/10.3389/fpsyt.2020.00111  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32174858/  |  
------------------------------------------- 
10.1007/s00146-020-00978-0  |   This paper provides an early evaluation of Artificial Intelligence (AI) against COVID-19. The main areas where AI can contribute to the fight against COVID-19 are discussed. It is concluded that AI has not yet been impactful against COVID-19. Its use is hampered by a lack of data, and by too much data. Overcoming these constraints will require a careful balance between data privacy and public health, and rigorous human-AI interaction. It is unlikely that these will be addressed in time to be of much help during the present pandemic. In the meantime, extensive gathering of diagnostic data on who is infectious will be essential to save lives, train AI, and limit economic damages. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32346223/  |  
------------------------------------------- 
10.1111/cyt.12828  |    Objective:  Thyroid pathology has great potential for automated/artificial intelligence (AI) algorithm application as the incidence of thyroid nodules is increasing and the indeterminate interpretation rate of fine-needle aspiration remains relatively high. The aim of the study is to review the published literature on automated image analysis and AI applications to thyroid pathology with whole-slide imaging (WSI). 
  Methods:  Systematic search was carried out in electronic databases. Studies dealing with thyroid pathology and use of automated algorithms applied to WSI were included. Quality of studies was assessed with a modified QUADAS-2 tool. 
  Results:  Of 919 retrieved articles, 19 were included. The main themes addressed were the comparison of automated assessment of immunohistochemical staining with manual pathologist's assessment, quantification of differences in cellular and nuclear parameters among tumor entities, and discrimination between benign and malignant nodules. Correlation coefficients with manual assessment were higher than 0.76 and diagnostic performance of automated models was comparable with an expert pathologist diagnosis. Computational difficulties were related to the large size of whole-slide images. 
  Conclusions:  Overall, the results are promising and it is likely that with the resolution of technical issues the application of automated algorithms in thyroid pathology will increase and be adopted following suitable validation studies. 
  |  https://doi.org/10.1111/cyt.12828  |  
------------------------------------------- 
10.1016/j.tips.2020.03.003  |   Gestational diabetes mellitus is a common pregnancy complication associated with significant adverse health outcomes for both women and infants. Effective screening and early prediction tools as part of routine clinical care are needed to reduce the impact of the disease on the baby and mother. Using large-scale electronic health records, Artzi and colleagues developed and evaluated a machine learning driven tool to identify women at high and low risk of GDM. Their findings showcase how artificial intelligence approaches can potentially be embedded in clinical care to enable accurate and rapid risk stratification. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0165-6147(20)30067-5  |  
------------------------------------------- 
10.1002/1878-0261.12685  |   Radiation oncology, a major treatment modality in the care of patients with malignant disease, is a technology- and computer-intensive medical specialty. As such, it should lend itself ideally to data science methods, where computer science, statistics and clinical knowledge are combined to advance state-of-the-art care. Nevertheless, data science methods in radiation oncology research are still in their infancy and successful applications leading to improved patient care remain scarce. Here, we discuss data inter-operability issues within and across organizational boundaries that hamper the introduction of big data and data science techniques in radiation oncology. At the semantic level, creating common underlying models and codification of the data, including the use of data elements with standardized definitions, an ontology, remains a work in progress. Methodological issues in data science and in the use of large population-based health data registries are identified. We show that data science methods and big data cannot replace randomized clinical trials in comparative effectiveness research by reviewing a series of instances where the outcome of big data analyses and randomized trials are at odds. We also discuss the modern wave of machine learning and artificial intelligence as represented by deep learning and convolutional neural networks. Finally, we identify promising research avenues and remain optimistic that the data sources in radiation oncology can be linked to yield important insights in the near future. We argue that data science will be a valuable complement to, but not a replacement of, the traditional hypothesis-driven translational research chain and the randomized clinical trials that form the back-bone of evidence-based medicine. 
  |  https://doi.org/10.1002/1878-0261.12685  |  
------------------------------------------- 
10.1002/jor.24614  |   Up to one-third of total joint replacement (TJR) procedures may be performed inappropriately in a subset of patients who remain dissatisfied with their outcomes, stressing the importance of shared decision-making. Patient-reported outcome measures capture physical, emotional, and social aspects of health and wellbeing from the patient's perspective. Powerful computer systems capable of performing highly sophisticated analysis using different types of data, including patient-derived data, such as patient-reported outcomes, may eliminate guess work, generating impactful metrics to better inform the decision-making process. We have created a shared decision-making tool which generates personalized predictions of risks and benefits from TJR based on patient-reported outcomes as well as clinical and demographic data. We present the protocol for a randomized controlled trial designed to assess the impact of this tool on decision quality, level of shared decision-making, and patient and process outcomes. We also discuss current concepts in this field and highlight opportunities leveraging patient-reported data and artificial intelligence for decision support across the care continuum. 
  |  https://doi.org/10.1002/jor.24614  |  
------------------------------------------- 
10.1016/j.heliyon.2020.e03218  |   Sexual cyberbullying is becoming a serious problem in today's society. In the workplace, this issue is more complex because of the power imbalance between potential perpetrators and victims. Preventing sexual cyberbullying in organizations is very important for a safety and respectful workplace. Occupational Safety and Health (OSH) standards establish certain policies to be considered to create an organizational culture based on zero tolerance to sexual cyberbullying. The research aims to broaden knowledge about personality and sexual cyberbullying. Therefore, this paper proposes a crucial tool to explore potential sexual cyberbullying behaviour. This study analysed how personality traits, particularly those related to the Dark Triad (psychopathy, Machiavellianism and narcissism), might influence this behaviour. Participants (N = 374) were Spanish young adults, using the convenience sampling to recruit them. The methodology focused on the use of structural equation modelling and ensemble classification tree. First, we tested the proposed hypotheses with structural equation method based on covariance using the Lavaan R-package. Second, for the ensemble of classification trees, we applied the package randomForest and Adabag (bagging and boosting) in R. Results proposed high levels of psychopathy and Machiavellianism are more likely to be related to sexual cyberbullying behaviours. Organizations could use the tool proposed in this research to develop internal policies and procedures for detection and deterrence of potential cyberbullying behaviours. By raising awareness about cyberbullying behaviour including its conceptualisation and measurement in training courses, organizations might build an organizational culture based on a respectful workplace without sexual cyberbullying behaviours. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2405-8440(20)30063-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32042968/  |  
------------------------------------------- 
10.1016/j.jns.2020.116723  |    Background:  The evaluation of neurological examination in clinical practice still remains qualitative or semi-quantitative, and the results often vary depending on an examiner's skill level and are less objective. In this study, we developed a smartphone-based application to investigate quantifying neurological examinations using hand-drawn spirals and diagnose patients with tremor using artificial intelligence (AI). 
  Methods:  This study included 24 and 26 patients with essential tremor (ET) and cerebellar disease (CD), respectively, and 41 age-matched normal controls (NCs). We obtained 69, 46, and 56 hand-drawn spirals from the NC, ET, and CD groups, respectively, as image data captured by smartphones. The patients traced a printed reference spiral. The length of this spiral was compared with the reference spiral length (% of spiral length) and the total deviation area between these spirals was calculated. The server also estimates the diagnostic probability through AI. 
  Results:  The quantified spiral analysis (% of spiral length and deviation area) significantly correlated with disease severity in each disease group, and significant differences in the deviation area were observed among all groups. The AI diagnosis showed 79%, 70%, and 73% accuracies for the NC, ET, and CD groups, respectively. 
  Conclusion:  This study indicates the possibility of using a smartphone as a medical examination tool and demonstrates the application of AI in neurological examinations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-510X(20)30059-9  |  
------------------------------------------- 
10.3389/fneur.2020.00270  |   Early detection of brain metastases and differentiation from other neuropathologies is crucial. Although biopsy is often required for definitive diagnosis, imaging can provide useful information. After treatment commences, imaging is also performed to assess the efficacy of treatment. Contrast-enhanced magnetic resonance imaging (MRI) is the traditional imaging method for the evaluation of brain metastases, as it provides information about lesion size, morphology, and macroscopic properties. Newer MRI sequences have been developed to increase the conspicuity of detecting enhancing metastases. Other advanced MRI techniques, that have the capability to probe beyond the anatomic structure, are available to characterize micro-structures, cellularity, physiology, perfusion, and metabolism. Artificial intelligence provides powerful computational tools for detection, segmentation, classification, prediction, and prognosis. We highlight and review a few advanced MRI techniques for the assessment of brain metastases-specifically for (1) diagnosis, including differentiating between malignancy types and (2) evaluation of treatment response, including the differentiation between radiation necrosis and disease progression. 
  |  https://doi.org/10.3389/fneur.2020.00270  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32351445/  |  
------------------------------------------- 
10.3389/fonc.2019.01527  |   Theoretical and empirical work over the past several decades suggests that oncogenesis and disease progression represents an evolutionary story. Despite this knowledge, current anti-resistance strategies to drugs are often managed through treating cancers as independent biological agents divorced from human activity. Yet once drug resistance to cancer treatment is understood as a product of artificial or anthropogenic rather than unconscious selection, oncologists could improve outcomes for their patients by consulting evolutionary studies of oncology prior to clinical trial and treatment plan design. In the setting of multiple cancer types, for example, a machine learning algorithm can predict the genetic changes known to be related to drug resistance. In this way, a unity between technology and theory might have practical clinical implications-and may pave the way for a new paradigm shift in medicine. 
  |  https://doi.org/10.3389/fonc.2019.01527  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32039014/  |  
------------------------------------------- 
10.1038/d41586-020-01002-7  |    |  https://doi.org/10.1038/d41586-020-01002-7  |  
------------------------------------------- 
10.15171/ijhpm.2019.80  |   China's estimated 114 million people with diabetes pose a massive challenge for China's health policy-makers who have significantly extended health insurance coverage over the past decade. What China is doing now, what it has achieved, and what remains to be done should be of interest to health policy-makers, worldwide. We identify the challenges posed by China's two principal strategies to tackle diabetes: (1) A short-term pilot strategy of health promotion, detection and control of chronic diseases in 265 national demonstration areas (NDAs); and (2) A long-term strategy to extend health promotion and strengthen primary care capacity and health system integration throughout China. Finally, we consider how Chinese innovations in artificial intelligence (AI) and Big Data may contribute to improving diagnosis, controlling complications and increasing access to care. Health system integration in China will require overcoming the fragmentation of a system that still places excessive reliance on local government financing. Moreover, what remains to be done resembles deeper challenges faced by healthcare systems worldwide: the need to upgrade primary care and reduce inequalities in access to health services. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32124588/  |  
------------------------------------------- 
10.1016/j.ejrad.2020.108969  |   Research into the possibilities of AI in cardiac CT has been growing rapidly in the last decade. With the rise of publicly available databases and AI algorithms, many researchers and clinicians have started investigations into the use of AI in the clinical workflow. This review is a comprehensive overview on the types of tasks and applications in which AI can aid the clinician in cardiac CT, and can be used as a primer for medical researchers starting in the field of AI. The applications of AI algorithms are explained and recent examples in cardiac CT of these algorithms are further elaborated on. The critical factors for implementation in the future are discussed. 
  |  None  |  
------------------------------------------- 
10.1002/ps.5820  |   'Deep learning' is causing rapid technological changes in many fields of science, and conjectures about its potential for transforming everyone's work and lives is a matter of great debate. Unfortunately, it is all too easy to apply it as a 'black box' tool with little consideration of its potential limitations, especially when the data it is being applied to is less than perfect. In this Perspective, I try to put deep learning into a broader mechanistic and historical context by showing how it relates to older forms of artificial intelligence; by providing a general explanation of how it operates; and by exploring some of the challenges involved in its implementation. Examples wherein it has been applied to pest management problems are provided to illustrate how the technology works and the challenges deep learning faces. At least in the near term, its biggest impact on agrochemical development seems likely to come in automating the tedious work involved in assessing agrochemical efficacy, but getting there will require major investments in building large, well-curated data sets to work from and in providing the expertise required to assess the resulting model predictions in real-world scenarios. Deep learning may also come to complement the machine learning methodologies already available for use in pesticide discovery and development, but it seems unlikely to supplant them. © 2020 The Author. Pest Management Science published by John Wiley &amp; Sons Ltd on behalf of Society of Chemical Industry. 
  |  https://doi.org/10.1002/ps.5820  |  
------------------------------------------- 
10.17219/acem/115083  |   Innovative computer techniques are starting to be employed not only in academic research, but also in commercial production, finding use in many areas of dentistry. This is conducive to the digitalization of dentistry and its increasing treatment and diagnostic demands. In many areas of dentistry, such as orthodontics and maxillofacial surgery, but also periodontics or prosthetics, only a correct diagnosis ensures the correct treatment plan, which is the only way to restore the patient's health. The diagnosis and treatment plan is based on the specialist's knowledge, but is subject to a large, multi-factorial risk of error. Therefore, the introduction of multiparametric pattern recognition methods (statistics, machine learning and artificial intelligence (AI)) is a great hope for both the physicians and the patients. However, the general use of clinical decision support systems (CDSS) in a dental clinic is not yet realistic and requires work in many aspects - methodical, technological and business. The article presents a review of the latest attempts to apply AI, such as CDSS or genetic algorithms (GAs) in research and clinical dentistry, taking under consideration all of the main dental specialties. Work on the introduction of public CDSS has been continued for years. The article presents the latest achievements in this field, analyzing their real-life application and credibility. 
  |  http://www.advances.am.wroc.pl/pdf/2020/29/3/375.pdf  |  
------------------------------------------- 
10.1186/s13244-019-0830-7  |    Objectives:  To explore the attitudes of United Kingdom (UK) medical students regarding artificial intelligence (AI), their understanding, and career intention towards radiology. We also examine the state of education relating to AI amongst this cohort. 
  Methods:  UK medical students were invited to complete an anonymous electronic survey consisting of Likert and dichotomous questions. 
  Results:  Four hundred eighty-four responses were received from 19 UK medical schools. Eighty-eight percent of students believed that AI will play an important role in healthcare, and 49% reported they were less likely to consider a career in radiology due to AI. Eighty-nine percent of students believed that teaching in AI would be beneficial for their careers, and 78% agreed that students should receive training in AI as part of their medical degree. Only 45 students received any teaching on AI; none of the students received such teaching as part of their compulsory curriculum. Statistically, students that did receive teaching in AI were more likely to consider radiology (p = 0.01) and rated more positively to the questions relating to the perceived competence in the post-graduation use of AI (p = 0.01-0.04); despite this, a large proportion of students in the taught group reported a lack of confidence and understanding required for the critical use of healthcare AI tools. 
  Conclusions:  UK medical students understand the importance of AI and are keen to engage. Medical school training on AI should be expanded and improved. Realistic use cases and limitations of AI must be presented to students so they will not feel discouraged from pursuing radiology. 
  |  https://dx.doi.org/10.1186/s13244-019-0830-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32025951/  |  
------------------------------------------- 
10.1007/s11948-020-00213-5  |   The idea of artificial intelligence for social good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This article addresses this gap by identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by 27 case examples of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good. 
  |  https://dx.doi.org/10.1007/s11948-020-00213-5  |  
------------------------------------------- 
10.1186/s40662-020-00183-6  |   In clinical ophthalmology, a variety of image-related diagnostic techniques have begun to offer unprecedented insights into eye diseases based on morphological datasets with millions of data points. Artificial intelligence (AI), inspired by the human multilayered neuronal system, has shown astonishing success within some visual and auditory recognition tasks. In these tasks, AI can analyze digital data in a comprehensive, rapid and non-invasive manner. Bioinformatics has become a focus particularly in the field of medical imaging, where it is driven by enhanced computing power and cloud storage, as well as utilization of novel algorithms and generation of data in massive quantities. Machine learning (ML) is an important branch in the field of AI. The overall potential of ML to automatically pinpoint, identify and grade pathological features in ocular diseases will empower ophthalmologists to provide high-quality diagnosis and facilitate personalized health care in the near future. This review offers perspectives on the origin, development, and applications of ML technology, particularly regarding its applications in ophthalmic imaging modalities. 
  |  https://eandv.biomedcentral.com/articles/10.1186/s40662-020-00183-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32322599/  |  
------------------------------------------- 
10.1186/s13584-020-0361-1  |   Digital healthcare promises to achieve cost-efficiency gains, improve clinical effectiveness, support better public sector governance by enhancing transparency and accountability, and increase confidence in medical diagnoses, especially in the field of oncology. This article aims to discuss the benefits offered by digital technologies in tax-based European healthcare systems against the backdrop of structural bureaucratic rigidities and a slow pace of implementation.Artificial intelligence (AI) will transform the existing delivery of healthcare services, inducing a redesign of public accountability systems and the traditional relationships between professionals and patients. Despite legitimate ethical and accountability concerns, which call for clearer guidance and regulation, digital governance of healthcare is a powerful means of empowering patients and improving their medical treatment in terms of quality and effectiveness. On the path to better health, the use of digital technologies has moved beyond the back office of administrative processes and procedures, and is now being applied to clinical activities and direct patient engagement. 
  |  https://ijhpr.biomedcentral.com/articles/10.1186/s13584-020-0361-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31952548/  |  
------------------------------------------- 
10.1016/j.preteyeres.2020.100841  |   Alterations in ocular blood flow have been identified as important risk factors for the onset and progression of numerous diseases of the eye. In particular, several population-based and longitudinal-based studies have provided compelling evidence of hemodynamic biomarkers as independent risk factors for ocular disease throughout several different geographic regions. Despite this evidence, the relative contribution of blood flow to ocular physiology and pathology in synergy with other risk factors and comorbidities (e.g., age, gender, race, diabetes and hypertension) remains uncertain. There is currently no gold standard for assessing all relevant vascular beds in the eye, and the heterogeneous vascular biomarkers derived from multiple ocular imaging technologies are non-interchangeable and difficult to interpret as a whole. As a result of these disease complexities and imaging limitations, standard statistical methods often yield inconsistent results across studies and are unable to quantify or explain a patient's overall risk for ocular disease. Combining mathematical modeling with artificial intelligence holds great promise for advancing data analysis in ophthalmology and enabling individualized risk assessment from diverse, multi-input clinical and demographic biomarkers. Mechanism-driven mathematical modeling makes virtual laboratories available to investigate pathogenic mechanisms, advance diagnostic ability and improve disease management. Artificial intelligence provides a novel method for utilizing a vast amount of data from a wide range of patient types to diagnose and monitor ocular disease. This article reviews the state of the art and major unanswered questions related to ocular vascular anatomy and physiology, ocular imaging techniques, clinical findings in glaucoma and other eye diseases, and mechanistic modeling predictions, while laying a path for integrating clinical observations with mathematical models and artificial intelligence. Viable alternatives for integrated data analysis are proposed that aim to overcome the limitations of standard statistical approaches and enable individually tailored precision medicine in ophthalmology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1350-9462(20)30013-6  |  
------------------------------------------- 
10.1007/s10926-020-09886-y  |    Purpose:  This paper aims to illustrate an example of how to set up a work injury database: the Smart Work Injury Management (SWIM) system. It is a secure and centralized cloud platform containing a set of management tools for data storage, data analytics, and machine learning. It employs artificial intelligence to perform in-depth analysis via text-mining techniques in order to extract both dynamic and static data from work injury case files. When it is fully developed, this system can provide a more accurate prediction model for cost of work injuries. It can also predict return-to-work (RTW) trajectory and provide advice on medical care and RTW interventions to all RTW stakeholders. The project will comprise three stages. Stage one: to identify human factors in terms of both facilitators and barriers RTW through face-to-face interviews and focus group discussions with different RTW stakeholders in order to collect opinions related to facilitators, barriers, and essential interventions for RTW of injured workers; Stage two: to develop a machine learning model which employs artificial intelligence to perform in-depth analysis. The technologies used will include: 1. Text-mining techniques including English and Chinese work segmentation as well as N-Gram to extract both dynamic and static data from free-style text as well as sociodemographic information from work injury case files; 2. Principle component/independent component analysis to identify features of significant relationships with RTW outcomes or combine raw features into new features; 3. A machine learning model that combines Variational Autoencoder, Long and Short Term Memory, and Neural Turning Machines. Stage two will also include the development of an interactive dashboard and website to query the trained machine learning model. Stage three: to field test the SWIM system. 
  Conclusion:  SWIM ia secure and centralized cloud platform containing a set of management tools for data storage, data analytics, and machine learning. When it is fully developed, SWIM can provide a more accurate prediction model for the cost of work injuries and advice on medical care and RTW interventions to all RTW stakeholders. 
  Ethics:  The project has been approved by the Ethics Committee for Human Subjects at the Hong Kong Polytechnic University and is funded by the Innovation and Technology Commission (Grant # ITS/249/18FX). 
  |  https://doi.org/10.1007/s10926-020-09886-y  |  
------------------------------------------- 
10.3389/fneur.2020.00007  |   <b>Objective:</b> To evaluate various machine learning algorithms in predicting peripheral vestibular dysfunction using the dataset of the center of pressure (COP) sway during foam posturography measured from patients with dizziness. <b>Study Design:</b> Retrospective study. <b>Setting:</b> Tertiary referral center. <b>Patients:</b> Seventy-five patients with vestibular dysfunction and 163 healthy controls were retrospectively recruited. The dataset included the velocity, the envelopment area, the power spectrum of the COP for three frequency ranges and the presence of peripheral vestibular dysfunction evaluated by caloric testing in 75 patients with vestibular dysfunction and 163 healthy controls. <b>Main Outcome Measures:</b> Various forms of machine learning algorithms including the Gradient Boosting Decision Tree, Bagging Classifier, and Logistic Regression were trained. Validation and comparison were performed using the area under the curve (AUC) of the receiver operating characteristic curve (ROC) and the recall of each algorithm using K-fold cross-validation. <b>Results:</b> The AUC (0.90 ± 0.06) and the recall (0.84 ± 0.07) of the Gradient Boosting Decision Tree were the highest among the algorithms tested, and both of them were significantly higher than those of the logistic regression (AUC: 0.85 ± 0.08, recall: 0.78 ± 0.07). The recall of the Bagging Classifier (0.82 ± 0.07) was also significantly higher than that of logistic regression. <b>Conclusion:</b> Machine learning algorithms can be successfully used to predict vestibular dysfunction as identified using caloric testing with the dataset of the COP sway during posturography. The multiple algorithms should be evaluated in each clinical dataset since specific algorithm does not always fit to any dataset. Optimization of the hyperparameters in each algorithm are necessary to obtain the highest accuracy. 
  |  https://doi.org/10.3389/fneur.2020.00007  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32116997/  |  
------------------------------------------- 
10.1007/s11906-019-1006-z  |    Purpose of review:  Aortic stiffness (AS) is widely associated with hypertension and considered as a major predictor of coronary heart disease (CHD). AS is measured using carotid-femoral pulse wave velocity (PWV), particularly when this parameter is associated with an index involving age, gender, heart rate, and mean blood pressure. The present review focuses on the interest of measurement of PWV and the calculation of individual PWV index for the prediction of CHD, in addition with the use of new statistical nonlinear models enabling results with very high levels of accuracy. 
  Recent findings:  PWV index may so constitute a substantial marker of large arteries prediction and damage in CHD and may be also used in cerebrovascular and renal circulations models. PWV index determinations are particularly relevant to consider in angiographic CHD decisions and in the presence of vulnerable plaques with high cardiovascular risk. Due to the variability in symptoms and clinical characteristics of patients, together with some imperfections in results, there is no very simple adequate diagnosis approach enabling to improve the so defined CHD prediction in usual clinical practice. In recent works in relation to "artificial intelligence" and involving "decision tree" models and "artificial neural networks," it has been possible to determine consistent pathways introducing predictive medicine and enabling to obtain efficient algorithm classification models of coronary prediction. 
  |  https://dx.doi.org/10.1007/s11906-019-1006-z  |  
------------------------------------------- 
10.1007/s11892-020-1287-2  |    Purpose of review:  Machine learning (ML) is increasingly being studied for the screening, diagnosis, and management of diabetes and its complications. Although various models of ML have been developed, most have not led to practical solutions for real-world problems. There has been a disconnect between ML developers, regulatory bodies, health services researchers, clinicians, and patients in their efforts. Our aim is to review the current status of ML in various aspects of diabetes care and identify key challenges that must be overcome to leverage ML to its full potential. 
  Recent findings:  ML has led to impressive progress in development of automated insulin delivery systems and diabetic retinopathy screening tools. Compared with these, use of ML in other aspects of diabetes is still at an early stage. The Food &amp; Drug Administration (FDA) is adopting some innovative models to help bring technologies to the market in an expeditious and safe manner. ML has great potential in managing diabetes and the future is in furthering the partnership of regulatory bodies with health service researchers, clinicians, developers, and patients to improve the outcomes of populations and individual patients with diabetes. 
  |  https://dx.doi.org/10.1007/s11892-020-1287-2  |  
------------------------------------------- 
10.3389/fpls.2019.01791  |   The purity of seeds is the most important factor in agriculture that determines crop yield, price, and quality. Rice is a major staple food consumed in different forms globally. The identification of high yielding and good quality paddy seeds is a challenging job and mainly dependent on expensive molecular techniques. The practical and day-to-day usage of the molecular-laboratory based techniques are very costly and time-consuming, and involves several logistical issues too. Moreover, such techniques are not easily accessible to paddy farmers. Thus, there is an unmet need to develop alternative, easily accessible and rapid methods for correct identification of paddy seed varieties, especially of commercial importance. We have developed iRSVPred, deep learning based on seed images, for the identification and differentiation of ten major varieties of basmati rice namely, Pusa basmati 1121 (1121), Pusa basmati 1509 (1509), Pusa basmati 1637 (1637), salt-tolerant basmati rice variety CSR 30 (CSR-30), Dehradoon basmati Type-3 (DHBT-3), Pusa Basmati-1 (PB-1), Pusa Basmati-6 (PB-6), Basmati -370 (BAS-370), Pusa Basmati 1718 (1718) and Pusa Basmati 1728 (1728). The method has an overall accuracy of 100% and 97% on the training set (total 61,632 images) and internal validation set (total 15,408 images), respectively. Furthermore, accuracies of greater than or equal to 80% have been achieved for all the ten varieties on the external validation dataset (642 images) used in the study. The iRSVPred web-server is freely available at http://14.139.62.220/rice/. 
  |  https://doi.org/10.3389/fpls.2019.01791  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32158451/  |  
------------------------------------------- 
10.3390/s20051364  |   Internet of Things (IoT) technologies are undeniably already all around us, as we stand at the cusp of the next generation of IoT technologies. Indeed, the next-generation of IoT technologies are evolving before IoT technologies have been fully adopted, and smart dust IoT technology is one such example. The concept of smart dust IoT technology, which features very small devices with low computing power, is a revolutionary and innovative concept that enables many things that were previously unimaginable, but at the same time creates unresolved problems. One of the biggest problems is the bottlenecks in data transmission that can be caused by this large number of devices. The bottleneck problem was solved with the Dual Plane Development Kit (DPDK) architecture. However, the DPDK solution created an unexpected new problem, which is called the mixed packet problem. The mixed packet problem, which occurs when a large number of data packets and control packets mix and change at a rapid rate, can slow a system significantly. In this paper, we propose a dynamic partitioning algorithm that solves the mixed packet problem by physically separating the planes and using a learning algorithm to determine the ratio of separated planes. In addition, we propose a training data model eXtended Permuted Frame (XPF) that innovatively increases the number of training data to reflect the packet characteristics of the system. By solving the mixed packet problem in this way, it was found that the proposed dynamic partitioning algorithm performed about 72% better than the general DPDK environment, and 88% closer to the ideal environment. 
  |  http://www.mdpi.com/resolver?pii=s20051364  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32131480/  |  
------------------------------------------- 
10.3389/fgene.2020.00200  |   Watson for Oncology (WFO) is a artificial intelligence clinical decision-support system with evidence-based treatment options for oncologists. WFO has been gradually used in China, but limited reports on whether WFO is suitable for Chinese patients. This study aims to investigate the concordance of treatment options between WFO and real clinical practice for Cervical cancer patients retrospectively. We retrospectively enrolled 300 cases of cervical cancer patients. WFO provides treatment options for 246 supported cases. Real clinical practice were defined as concordant if treatment options were designated "recommended" or "for consideration" by WFO. Concordance of treatment option between WFO and real clinical practice was analyzed statistically. The treatment concordance between WFO and real clinical practice occurred in 72.8% (179/246) of cervical cancer cases. Logistic regression analysis showed that rural registration residences, advanced age, poor ECOG performance status, stages II-IV disease have a remarkable impact on consistency. The main reasons attributed to the 27.2% (67/246) of the discordant cases were the substitution of nedaplatin for cisplatin, reimbursement plan of bevacizumab, surgical preference, and absence of neoadjuvant/adjuvant chemotherapy and PD-1/PD-L1 antibodies recommendations. WFO recommendations were in 72.8% of concordant with real clinical practice for cervical cancer patients in China. However, several localization and individual factors limit its wider application. So, WFO could be an essential tool but it cannot currently replace oncologists. To be rapidly and fully apply to cervical cancer patients in China, accelerate localization and improvement were needed for WFO. 
  |  https://doi.org/10.3389/fgene.2020.00200  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32265980/  |  
------------------------------------------- 
10.3390/s20030828  |   Industrial IoT has special communication requirements, including high reliability, low latency, flexibility, and security. These are instinctively provided by the 5G mobile technology, making it a successful candidate for supporting Industrial IoT (IIoT) scenarios. The aim of this paper is to identify current research challenges and solutions in relation to 5G-enabled Industrial IoT, based on the initial requirements and promises of both domains. The methodology of the paper follows the steps of surveying state-of-the art, comparing results to identify further challenges, and drawing conclusions as lessons learned for each research domain. These areas include IIoT applications and their requirements; mobile edge cloud; back-end performance tuning; network function virtualization; and security, blockchains for IIoT, Artificial Intelligence support for 5G, and private campus networks. Beside surveying the current challenges and solutions, the paper aims to provide meaningful comparisons for each of these areas (in relation to 5G-enabled IIoT) to draw conclusions on current research gaps. 
  |  http://www.mdpi.com/resolver?pii=s20030828  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32033076/  |  
------------------------------------------- 
10.1007/s11912-020-00907-w  |    Purpose of review:  We review advances in imaging of liver tumors, by particularly focusing on the utility of novel imaging in diagnosis and management of these lesions. 
  Recent findings:  Contrast-enhanced CT and/or MRI are currently utilized for accurate diagnosis of liver tumors, but several ongoing studies are examining the use of other advanced techniques. Novel CT (i.e., dual-energy CT and perfusion CT), MRI (diffusion-weighted imaging, MR elastography, and T1 mapping), and image processing (texture analysis and artificial intelligence-based methods) techniques have emerged and can be used for precise characterization of liver tumors, quantification of treatment responses, and prediction of overall survival rate of patients. Recent advancements in imaging of liver tumors allowed for a precise assessment of tumor features. These evolving technologies can be utilized for applying individualized treatment based on the presence of specific imaging biomarkers. 
  |  https://dx.doi.org/10.1007/s11912-020-00907-w  |  
------------------------------------------- 
10.3803/EnM.2020.35.1.71  |   Machine learning (ML) applications have received extensive attention in endocrinology research during the last decade. This review summarizes the basic concepts of ML and certain research topics in endocrinology and metabolism where ML principles have been actively deployed. Relevant studies are discussed to provide an overview of the methodology, main findings, and limitations of ML, with the goal of stimulating insights into future research directions. Clear, testable study hypotheses stem from unmet clinical needs, and the management of data quality (beyond a focus on quantity alone), open collaboration between clinical experts and ML engineers, the development of interpretable high-performance ML models beyond the black-box nature of some algorithms, and a creative environment are the core prerequisites for the foreseeable changes expected to be brought about by ML and artificial intelligence in the field of endocrinology and metabolism, with actual improvements in clinical practice beyond hype. Of note, endocrinologists will continue to play a central role in these developments as domain experts who can properly generate, refine, analyze, and interpret data with a combination of clinical expertise and scientific rigor. 
  |  https://e-enm.org/DOIx.php?id=10.3803/EnM.2020.35.1.71  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32207266/  |  
------------------------------------------- 
10.3389/fonc.2020.00121  |   Aqueous solubility is an important physicochemical property of compounds in anti-cancer drug discovery. Artificial intelligence solubility prediction tools have scored impressive performances by employing regression, machine learning, and deep learning methods. The reported performances vary significantly partly because of the different datasets used. Solubility prediction on novel compounds needs to be improved, which may be achieved by going deeper with deep learning. We constructed deeper-net models of ~20-layer modified ResNet convolutional neural network architecture, which were trained and tested with 9,943 compounds encoded by molecular fingerprints. Retrospectively tested by 62 recently-published novel compounds, one deeper-net model outperformed four established tools, shallow-net models, and four human experts. Deeper-net models also outperformed others in predicting the solubility values of a series of novel compounds newly-synthesized for anti-cancer drug discovery. Solubility prediction may be improved by going deeper with deep learning. Our deeper-net models are accessible at http://www.npbdb.net/solubility/index.jsp. 
  |  https://doi.org/10.3389/fonc.2020.00121  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32117768/  |  
------------------------------------------- 
10.3390/ijms21030969  |   A growing body of evidence now suggests that precision psychiatry, an interdisciplinary field of psychiatry, precision medicine, and pharmacogenomics, serves as an indispensable foundation of medical practices by offering the accurate medication with the accurate dose at the accurate time to patients with psychiatric disorders. In light of the latest advancements in artificial intelligence and machine learning techniques, numerous biomarkers and genetic loci associated with psychiatric diseases and relevant treatments are being discovered in precision psychiatry research by employing neuroimaging and multi-omics. In this review, we focus on the latest developments for precision psychiatry research using artificial intelligence and machine learning approaches, such as deep learning and neural network algorithms, together with multi-omics and neuroimaging data. Firstly, we review precision psychiatry and pharmacogenomics studies that leverage various artificial intelligence and machine learning techniques to assess treatment prediction, prognosis prediction, diagnosis prediction, and the detection of potential biomarkers. In addition, we describe potential biomarkers and genetic loci that have been discovered to be associated with psychiatric diseases and relevant treatments. Moreover, we outline the limitations in regard to the previous precision psychiatry and pharmacogenomics studies. Finally, we present a discussion of directions and challenges for future research. 
  |  http://www.mdpi.com/resolver?pii=ijms21030969  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32024055/  |  
------------------------------------------- 
10.1007/s10388-020-00716-x  |    Objectives:  In Japan, endoscopic resection (ER) is often used to treat esophageal squamous cell carcinoma (ESCC) when invasion depths are diagnosed as EP-SM1, whereas ESCC cases deeper than SM2 are treated by surgical operation or chemoradiotherapy. Therefore, it is crucial to determine the invasion depth of ESCC via preoperative endoscopic examination. Recently, rapid progress in the utilization of artificial intelligence (AI) with deep learning in medical fields has been achieved. In this study, we demonstrate the diagnostic ability of AI to measure ESCC invasion depth. 
  Methods:  We retrospectively collected 1751 training images of ESCC at the Cancer Institute Hospital, Japan. We developed an AI-diagnostic system of convolutional neural networks using deep learning techniques with these images. Subsequently, 291 test images were prepared and reviewed by the AI-diagnostic system and 13 board-certified endoscopists to evaluate the diagnostic accuracy. 
  Results:  The AI-diagnostic system detected 95.5% (279/291) of the ESCC in test images in 10 s, analyzed the 279 images and correctly estimated the invasion depth of ESCC with a sensitivity of 84.1% and accuracy of 80.9% in 6 s. The accuracy score of this system exceeded those of 12 out of 13 board-certified endoscopists, and its area under the curve (AUC) was greater than the AUCs of all endoscopists. 
  Conclusions:  The AI-diagnostic system demonstrated a higher diagnostic accuracy for ESCC invasion depth than those of endoscopists and, therefore, can be potentially used in ESCC diagnostics. 
  |  None  |  
------------------------------------------- 
10.1055/s-0039-3400264  |   Artificial intelligence (AI) holds the potential to revolutionize the field of radiology by increasing the efficiency and accuracy of both interpretive and noninterpretive tasks. We have only just begun to explore AI applications in the diagnostic evaluation of knee pathology. Experimental algorithms have already been developed that can assess the severity of knee osteoarthritis from radiographs, detect and classify cartilage lesions, meniscal tears, and ligament tears on magnetic resonance imaging, provide automatic quantitative assessment of tendon healing, detect fractures on radiographs, and predict those at highest risk for recurrent bone tumors. This article reviews and summarizes the most current literature. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400264  |  
------------------------------------------- 
10.2214/AJR.19.21572  |   <b>OBJECTIVE.</b> The purpose of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for fully automated quantification of emphysema on chest CT compared with pulmonary function testing (spirometry). <b>MATERIALS AND METHODS.</b> A total of 141 patients (72 women, mean age ± SD of 66.46 ± 9.7 years [range, 23-86 years]; 69 men, mean age of 66.72 ± 11.4 years [range, 27-91 years]) who underwent both chest CT acquisition and spirometry within 6 months were retrospectively included. The spirometry-based Tiffeneau index (TI; calculated as the ratio of forced expiratory volume in the first second to forced vital capacity) was used to measure emphysema severity; a value less than 0.7 was considered to indicate airway obstruction. Segmentation of the lung based on two different reconstruction methods was carried out by using a deep convolution image-to-image network. This multilayer convolutional neural network was combined with multilevel feature chaining and depth monitoring. To discriminate the output of the network from ground truth, an adversarial network was used during training. Emphysema was quantified using spatial filtering and attenuation-based thresholds. Emphysema quantification and TI were compared using the Spearman correlation coefficient. <b>RESULTS.</b> The mean TI for all patients was 0.57 ± 0.13. The mean percentages of emphysema using reconstruction methods 1 and 2 were 9.96% ± 11.87% and 8.04% ± 10.32%, respectively. AI-based emphysema quantification showed very strong correlation with TI (reconstruction method 1, ρ = -0.86; reconstruction method 2, ρ = -0.85; both <i>p</i> &lt; 0.0001), indicating that AI-based emphysema quantification meaningfully reflects clinical pulmonary physiology. <b>CONCLUSION.</b> AI-based, fully automated emphysema quantification shows good correlation with TI, potentially contributing to an image-based diagnosis and quantification of emphysema severity. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.21572  |  
------------------------------------------- 
10.1055/s-0039-3400267  |   Body composition entails the measurement of muscle and fat mass in the body and has been shown to impact clinical outcomes in various aspects of human health. As a result, the need is growing for reliable and efficient noninvasive tools to measure body composition. Traditional methods of estimating body composition, anthropomorphic measurements, dual-energy X-ray absorptiometry, and bioelectrical impedance, are limited in their application. Cross-sectional imaging remains the reference standard for body composition analysis and is accomplished through segmentation of computed tomography and magnetic resonance imaging studies. However, manual segmentation of images by an expert reader is labor intensive and time consuming, limiting its implementation in large-scale studies and in routine clinical practice. In this review, novel methods to automate the process of body composition measurement are discussed including the application of artificial intelligence and deep learning to tissue segmentation. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400267  |  
------------------------------------------- 
10.5946/ce.2020.046  |   Diagnosis and evaluation of early gastric cancer (EGC) using endoscopic images is significantly important; however, it has some limitations. In several studies, the application of convolutional neural network (CNN) greatly enhanced the effectiveness of endoscopy. To maximize clinical usefulness, it is important to determine the optimal method of applying CNN for each organ and disease. Lesion�-based CNN is a type of deep learning model designed to learn the entire lesion from endoscopic images. This review describes the application of lesion-based CNN technology in diagnosis of EGC. 
  |  https://dx.doi.org/10.5946/ce.2020.046  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32252505/  |  
------------------------------------------- 
10.1016/S1473-3099(20)30241-3  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1473-3099(20)30241-3  |  
------------------------------------------- 
10.1503/cmaj.74722  |    |  http://www.cmaj.ca/cgi/pmidlookup?view=long&pmid=32179541  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32179541/  |  
------------------------------------------- 
10.1097/MD.0000000000019517  |    Introduction:  Radiotherapy is a valid treatment option for nasopharyngeal carcinoma. However, complications can occur following irradiation of the closest anatomical structures, including brainstem radionecrosis (BRN). The rehabilitation is poorly described in patients with BRN, despite its usefulness in improving functional independence in patients with brain tumors. We aimed at testing the usefulness of intensive, robot-assisted neurorehabilitation program to improve functional independence in a 57-year-old male with BRN. 
  Patient concerns:  A 57-year-old male diagnosed with a nasopharyngeal carcinoma, received a radiation total dose of 72 Gy. Owing to the appearance of a severe symptomatology characterized by dysphagia, hearing loss, and left sided hemiparesis, the patient was hospitalized to be provided with intensive pharmacological and neurorehabilitation treatment. 
  Diagnosis:  Follow-up brain magnetic resonance imaging disclosed no residual cancer, but some brainstem lesions compatible with BRN areas were appreciable. 
  Intervention:  The patient underwent a 2-month conventional, respiratory, and speech therapy. Given that the patient only mildly improved, he was provided with intensive robot-aided upper limb and gait training and virtual reality-based cognitive rehabilitation for other 2 months. 
  Outcomes:  The patient reported a significant improvement in functional independence, spasticity, cognitive impairment degree, and balance. 
  Conclusion:  Our case suggests the usefulness of neurorobotic intensive rehabilitation in BRN to reduce functional disability. Future studies should investigate whether an earlier, even multidisciplinary rehabilitative treatment could lead to better functional outcome in patients with BRN. 
  |  http://dx.doi.org/10.1097/MD.0000000000019517  |  
------------------------------------------- 
10.1161/CIRCULATIONAHA.120.046760  |    |  http://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.120.046760?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1186/s12859-020-3390-4  |    Background:  Understanding cellular and molecular heterogeneity in glioblastoma (GBM), the most common and aggressive primary brain malignancy, is a crucial step towards the development of effective therapies. Besides the inter-patient variability, the presence of multiple cell populations within tumors calls for the need to develop modeling strategies able to extract the molecular signatures driving tumor evolution and treatment failure. With the advances in single-cell RNA Sequencing (scRNA-Seq), tumors can now be dissected at the cell level, unveiling information from their life history to their clinical implications. 
  Results:  We propose a classification setting based on GBM scRNA-Seq data, through sparse logistic regression, where different cell populations (neoplastic and normal cells) are taken as classes. The goal is to identify gene features discriminating between the classes, but also those shared by different neoplastic clones. The latter will be approached via the network-based twiner regularizer to identify gene signatures shared by neoplastic cells from the tumor core and infiltrating neoplastic cells originated from the tumor periphery, as putative disease biomarkers to target multiple neoplastic clones. Our analysis is supported by the literature through the identification of several known molecular players in GBM. Moreover, the relevance of the selected genes was confirmed by their significance in the survival outcomes in bulk GBM RNA-Seq data, as well as their association with several Gene Ontology (GO) biological process terms. 
  Conclusions:  We presented a methodology intended to identify genes discriminating between GBM clones, but also those playing a similar role in different GBM neoplastic clones (including migrating cells), therefore potential targets for therapy research. Our results contribute to a deeper understanding on the genetic features behind GBM, by disclosing novel therapeutic directions accounting for GBM heterogeneity. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3390-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32070274/  |  
------------------------------------------- 
10.1001/jama.2020.1039  |    |  https://jamanetwork.com/journals/jama/fullarticle/10.1001/jama.2020.1039  |  
------------------------------------------- 
10.3934/mbe.2020128  |    |  https://www.aimspress.com/article/10.3934/mbe.2020128  |  
------------------------------------------- 
10.2174/138620732301200316112000  |    |  http://www.eurekaselect.com/180227/article  |  
------------------------------------------- 
10.1016/S2589-7500(20)30054-6  |    |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32289116/  |  
------------------------------------------- 
10.3390/v12020142  |   Molecular cluster detection can be used to interrupt HIV transmission but is dependent on identifying clusters where transmission is likely. We characterized molecular cluster detection in Washington State, evaluated the current cluster investigation criteria, and developed a criterion using machine learning. The population living with HIV (PLWH) in Washington State, those with an analyzable genotype sequences, and those in clusters were described across demographic characteristics from 2015 to2018. The relationship between 3- and 12-month cluster growth and demographic, clinical, and temporal predictors were described, and a random forest model was fit using data from 2016 to 2017. The ability of this model to identify clusters with future transmission was compared to Centers for Disease Control and Prevention (CDC) and the Washington state criteria in 2018. The population with a genotype was similar to all PLWH, but people in a cluster were disproportionately white, male, and men who have sex with men. The clusters selected for investigation by the random forest model grew on average 2.3 cases (95% CI 1.1-1.4) in 3 months, which was not significantly larger than the CDC criteria (2.0 cases, 95% CI 0.5-3.4). Disparities in the cases analyzed suggest that molecular cluster detection may not benefit all populations. Jurisdictions should use auxiliary data sources for prediction or continue using established investigation criteria. 
  |  http://www.mdpi.com/resolver?pii=v12020142  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31991877/  |  
------------------------------------------- 
10.1001/jamanetworkopen.2020.0282  |    |  https://jamanetwork.com/journals/jamanetworkopen/fullarticle/10.1001/jamanetworkopen.2020.0282  |  
------------------------------------------- 
10.1038/s41433-020-0855-7  |    |  http://dx.doi.org/10.1038/s41433-020-0855-7  |  
------------------------------------------- 
10.1186/s12903-020-1062-4  |    Background:  Artificial intelligence (AI) is a branch of computer science concerned with building smart software or machines capable of performing tasks that typically require human intelligence. We present a protocol for the use of AI to fabricate implant-supported monolithic zirconia crowns (MZCs) cemented on customized hybrid abutments. 
  Methods:  The study protocol consisted of: (1) intraoral scan of the implant position; (2) design of the individual abutment and temporary crown using computer-aided design (CAD) software; (3) milling of the zirconia abutment and the temporary polymethyl-methacrylate (PMMA) crown, with extraoral cementation of the zirconia abutment on the relative titanium bonding base, to generate an individual hybrid abutment; (4) clinical application of the hybrid abutment and the temporary PMMA crown; (5) intraoral scan of the hybrid abutment; (6) CAD of the final crown with automated margin line design using AI; (7) milling, sintering and characterisation of the final MZC; and (8) clinical application of the MZC. The outcome variables were mathematical (quality of the fabrication of the individual zirconia abutment) and clinical, such as (1) quality of the marginal adaptation, (2) of interproximal contact points and (3) of occlusal contacts, (4) chromatic integration, (5) survival and (6) success of MZCs. A careful statistical analysis was performed. 
  Results:  90 patients (35 males, 55 females; mean age 53.3 ± 13.7 years) restored with 106 implant-supported MZCs were included in the study. The follow-up varied from 6 months to 3 years. The quality of the fabrication of individual hybrid abutments revealed a mean deviation of 44 μm (± 6.3) between the original CAD design of the zirconia abutment, and the mesh of the zirconia abutment captured intraorally at the end of the provisionalization. At the delivery of the MZCs, the marginal adaptation, quality of interproximal and occlusal contacts, and aesthetic integration were excellent. The three-year cumulative survival and success of the MZCs were 99.0% and 91.3%, respectively. 
  Conclusions:  AI seems to represent a reliable tool for the restoration of single implants with MZCs cemented on customised hybrid abutments via a full digital workflow. Further studies are needed to confirm these positive results. 
  |  https://bmcoralhealth.biomedcentral.com/articles/10.1186/s12903-020-1062-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32188431/  |  
------------------------------------------- 
10.1007/s13555-020-00372-0  |   Machine learning (ML) has the potential to improve the dermatologist's practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges. 
  |  https://dx.doi.org/10.1007/s13555-020-00372-0  |  
------------------------------------------- 
10.1097/NCQ.0000000000000412  |    Background:  Electronic health record-derived data and novel analytics, such as machine learning, offer promising approaches to identify high-risk patients and inform nursing practice. 
  Purpose:  The aim was to identify patients at risk for readmissions by applying a machine-learning technique, Classification and Regression Tree, to electronic health record data from our 300-bed hospital. 
  Methods:  We conducted a retrospective analysis of 2165 clinical encounters from August to October 2017 using data from our health system's data store. Classification and Regression Tree was employed to determine patient profiles predicting 30-day readmission. 
  Results:  The 30-day readmission rate was 11.2% (n = 242). Classification and Regression Tree analysis revealed highest risk for readmission among patients who visited the emergency department, had 9 or more comorbidities, were insured through Medicaid, and were 65 years of age and older. 
  Conclusions:  Leveraging information through the electronic health record and Classification and Regression Tree offers a useful way to identify high-risk patients. Findings from our algorithm may be used to improve the quality of nursing care delivery for patients at highest readmission risk. 
  |  http://dx.doi.org/10.1097/NCQ.0000000000000412  |  
------------------------------------------- 
10.1097/MD.0000000000019612  |   Pulmonary arterial hypertension (PAH) is a disease associated with high mortality, but notable sex differences have been observed between males and females. For this reason, further research on the mechanisms underlying sex differences in PAH is required to better understand and treat the disease. This study mainly focused on gene expression levels to investigate potential differences in the pathogenesis and progression of PAH between the male and female sexes.Sex-specific differentially expressed genes (DEGs) were analyzed using the Gene Expression Omnibus datasets GSE117261 and GSE38267. Gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analyses were conducted, and a protein-protein interaction (PPI) network was established based on the identified DEGs to predict potential mechanisms involved in the observed sex differences in the pathogenesis of PAH.We identified 26 female- and 53 male-specific DEGs from lung tissue and 498 female-specific DEGs in blood samples. No male-specific DEGs were identified from blood samples. GO and KEGG pathway analyses revealed that female-specific DEGs in lung tissue were enriched in inflammatory response and cytokine-cytokine receptor interaction, whereas male-specific DEGs were mainly enriched in cellular chemotaxis and the nuclear factor of kappa light polypeptide gene enhancer in B-cell (NF-kappa B) signaling pathway. Lipocalin 2 (LCN2) was the only gene that was differentially expressed in both the lung tissue and the blood of female patients.In conclusion, inflammation and immunity may play key roles in the pathogenesis of female PAH, and LCN2 may act as a serum biomarker of female PAH, whereas the pathogenesis in males is more complicated. 
  |  http://dx.doi.org/10.1097/MD.0000000000019612  |  
------------------------------------------- 
10.1371/journal.pone.0226976  |   A case-control study was conducted in which we evaluated the association between genetic variability of DNA repair proteins belonging to the Rad51 family and breast cancer (BrC) risk. In the study, 132 female BrC cases and 189 healthy control females were genotyped for a total of 14 common single nucleotide polymorphisms (SNPs) within Rad51 and Xrcc3. Moreover, our previously reported Rad51C genetic data were involved to explore the nonlinear interactions among SNPs within the three genes and effect of such interactions on BrC risk. The rare rs5030789 genotype (-4601AA) in Rad51 was found to significantly decrease the BrC risk (OR = 0.5, 95% CI: 0.3-1.0, p&lt;0.05). An interaction between this SNP, rs2619679 and rs2928140 (both in Rad51), was found to result in a two three-locus genotypes -4719AA/-4601AA/2972CG and -4719AT/-4601GA/2972CC, both of which were found to increase the risk of BrC (OR = 8.4, 95% CI: 1.8-38.6, p&lt;0.0001), instead. Furthermore, rare Rad51 rs1801320 (135CC) and heterozygous Xrcc3 rs3212057 (10343GA) genotypes were found to respectively increase (OR = 10.6, 95% CI: 1.9-198, p&lt;0.02) and decrease (OR = 0.0, 95% CI: 0.0-NA, p&lt;0.05) the risk of BrC. Associations between these SNPs and BrC risk were further supported by outcomes of employed machine learning analyses. In Xrcc3, the 4541A/9685A haplotype was found to be significantly associated with reduced BrC risk (OR = 0.5; 95% CI: 0.3-0.9; p&lt;0.05). Concluding, our study indicates a complex role of SNPs within Rad51 (especially rs5030789) and Xrcc3 in BrC, although their significance with respect to the disease needs to be further clarified. 
  |  http://dx.plos.org/10.1371/journal.pone.0226976  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31905201/  |  
------------------------------------------- 
10.1038/s41467-019-13922-8  |   Center-involved diabetic macular edema (ci-DME) is a major cause of vision loss. Although the gold standard for diagnosis involves 3D imaging, 2D imaging by fundus photography is usually used in screening settings, resulting in high false-positive and false-negative calls. To address this, we train a deep learning model to predict ci-DME from fundus photographs, with an ROC-AUC of 0.89 (95% CI: 0.87-0.91), corresponding to 85% sensitivity at 80% specificity. In comparison, retinal specialists have similar sensitivities (82-85%), but only half the specificity (45-50%, p &lt; 0.001). Our model can also detect the presence of intraretinal fluid (AUC: 0.81; 95% CI: 0.81-0.86) and subretinal fluid (AUC 0.88; 95% CI: 0.85-0.91). Using deep learning to make predictions via simple 2D images without sophisticated 3D-imaging equipment and with better than specialist performance, has broad relevance to many other applications in medical imaging. 
  |  http://dx.doi.org/10.1038/s41467-019-13922-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31913272/  |  
------------------------------------------- 
10.12659/MSM.920504  |   BACKGROUND Evidence indicates that there is an important role for long non-coding RNAs (lncRNA) in numerous cellular processes and that lncRNAs dysregulation contributes to tumor progression. Improved insight into the molecular characteristics of bladder cancer is required to predict outcomes and to develop a new rationale for targeted therapeutic strategies. Bioinformatics methods, including functional enrichment and network analysis combined with survival analysis, are required to process a large volume of data to obtain further information about differentially expressed genes (DEGs) in bladder cancer. This study aimed to explore the role of lncRNAs and their regulation network in bladder cancer. MATERIAL AND METHODS We analyzed bladder cancer data by The Cancer Genome Atlas profiling to identify differentially expressed lncRNAs in bladder cancer. The genes involved in the circlncRNAnet database were evaluated using Kyoto Encyclopedia of Genes and Genomes (KEGG), Gene Ontology (GO), evolutionary relationship analysis, and protein-protein interaction (PPI) networks. RESULTS Two new lncRNAs, ADAMTS9-AS1 and LINC00460, were shown to be differentially expressed in bladder cancer. Patients were divided into 2 groups (high expression and low expression) according to their median expression values. The overall survival and disease-free survival of patients with high ADAMTS9-AS1 bladder cancer were significantly shorter; the expression of LINC00460 had no significant correlation with survival. GO and KEGG analysis of the 2 lncRNA-related genes revealed that these lncRNAs played a vital role in tumorigenesis. Bioinformatics analysis showed that key genes related to LINC00460, including CXCL, CCL, and CSF2, may be related to the development of bladder cancer. The low expression of ADAMTS9-AS1 may influence the survival rate of bladder cancer with the hub gene as a target. CONCLUSIONS LncRNA, including LINC00460 and ADAMTS9-AS1, might play a crucial role in the biosynthesis network of bladder cancer. Differential expression results of ADAMTS9-AS1 suggests it may be correlated with a worse prognosis and a shorter survival time. We outlined the biosynthesis network that regulates lncRNAs in bladder cancer. Further experimental data is needed to validate our results. 
  |  https://www.medscimonit.com/download/index/idArt/920504  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32277695/  |  
------------------------------------------- 
10.1097/RLI.0000000000000600  |   The use of artificial intelligence (AI) is a powerful tool for image analysis that is increasingly being evaluated by radiology professionals. However, due to the fact that these methods have been developed for the analysis of nonmedical image data and data structure in radiology departments is not "AI ready", implementing AI in radiology is not straightforward. The purpose of this review is to guide the reader through the pipeline of an AI project for automated image analysis in radiology and thereby encourage its implementation in radiology departments. At the same time, this review aims to enable readers to critically appraise articles on AI-based software in radiology. 
  |  http://dx.doi.org/10.1097/RLI.0000000000000600  |  
------------------------------------------- 
10.1055/s-0039-3400266  |   Artificial intelligence (AI) has the potential to affect every step of the radiology workflow, but the AI application that has received the most press in recent years is image interpretation, with numerous articles describing how AI can help detect and characterize abnormalities as well as monitor disease response. Many AI-based image interpretation tasks for musculoskeletal (MSK) pathologies have been studied, including the diagnosis of bone tumors, detection of osseous metastases, assessment of bone age, identification of fractures, and detection and grading of osteoarthritis. This article explores the applications of AI for image interpretation of MSK pathologies. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400266  |  
------------------------------------------- 
10.1016/j.jaad.2020.04.069  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0190-9622(20)30684-8  |  
------------------------------------------- 
10.1213/ANE.0000000000004728  |   Artificial intelligence-driven anesthesiology and perioperative care may just be around the corner. However, its promises of improved safety and patient outcomes can only become a reality if we take the time to examine its technical, ethical, and moral implications. The aim of perioperative medicine is to diagnose, treat, and prevent disease. As we introduce new interventions or devices, we must take care to do so with a conscience, keeping patient care as the main objective, and understanding that humanism is a core component of our practice. In our article, we outline key principles of artificial intelligence for the perioperative physician and explore limitations and ethical challenges in the field. 
  |  http://dx.doi.org/10.1213/ANE.0000000000004728  |  
------------------------------------------- 
10.1038/s41467-019-13932-6  |   Detailed conductance-based nonlinear neuron models consisting of thousands of synapses are key for understanding of the computational properties of single neurons and large neuronal networks, and for interpreting experimental results. Simulations of these models are computationally expensive, considerably curtailing their utility. Neuron_Reduce is a new analytical approach to reduce the morphological complexity and computational time of nonlinear neuron models. Synapses and active membrane channels are mapped to the reduced model preserving their transfer impedance to the soma; synapses with identical transfer impedance are merged into one NEURON process still retaining their individual activation times. Neuron_Reduce accelerates the simulations by 40-250 folds for a variety of cell types and realistic number (10,000-100,000) of synapses while closely replicating voltage dynamics and specific dendritic computations. The reduced neuron-models will enable realistic simulations of neural networks at unprecedented scale, including networks emerging from micro-connectomics efforts and biologically-inspired "deep networks". Neuron_Reduce is publicly available and is straightforward to implement. 
  |  http://dx.doi.org/10.1038/s41467-019-13932-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31941884/  |  
------------------------------------------- 
10.1055/s-0039-3400269  |   The radiology practice has access to a wealth of data in the radiologist information system, dictation reports, and electronic health records. Although many artificial intelligence applications in radiology have focused on computer vision and the interpretive use cases, many opportunities exist to enhance the radiologist's value proposition through business analytics. This article explores how AI lends an analytical lens to the radiology practice to create value. 
  |  http://www.thieme-connect.com/DOI/DOI?10.1055/s-0039-3400269  |  
------------------------------------------- 
PMID:32285782  |   Artificial intelligence (AI) is a computer-based system, which in diagnostic imaging can improve patient flow, optimise image processing, shorten scan time, reduce radiation dose and be used as decision aid in image interpretation. In this review, we argue that AI algorithms should be based on evidence with initial hypothesis, then a choice of algorithm and development with training on the initial data set; afterwards the algorithms should be tested on a new representative dataset, and finally it should be tested in a prospective study. If the AI is evidence-based and can solve a task better or cheaper than the usual methodology, it can be implemented. 
  |  http://ugeskriftet.dk/videnskab/V10190563  |  
------------------------------------------- 
PMID:32089154  |   Artificial intelligence (AI) and machine learning have become important in medicine as shown in this review. Automatic tools can be trained to analyse patient data and thereby be a great asset to doctors when diagnosing and treating patients. Denmark is a leading country in collecting data. Having large amounts of stored data improves the quality when building a prediction tool. Due to the potential of building AI in the Danish healthcare sector, a significant amount of money has been allocated to foster new innovations in the field. Denmark could play a critical rule utilising data and implementing AI in medicine. 
  |  http://ugeskriftet.dk/videnskab/V09190540  |  
------------------------------------------- 
10.1002/anie.201915777  |   This essay highlights the complex issue of twinning in science publications. Historical accounts present cases where two scientists focused on the same problem and came up with the same solution following different paths. This has changed in the present situation. Birth of twins in research literature has increased post 2010. In early 1900s to 2000s, twinning in research publication was serendipitous, and there was a healthy competition among teams working on similar products. At present, twinning has become a norm rather than serendipity. This can be attributed to the urge to have publications in popular themes, dependence on building research on popular key words, superficial glancing at literature and funding agencies backing thematic areas. As we inch away from human ingenuity towards Artificial Intelligence, twinning may become a norm of the day and this may result in not just twins but multiple clones appearing in scientific literature. 
  |  https://doi.org/10.1002/anie.201915777  |  
------------------------------------------- 
10.7150/jca.43268  |   Lung cancer is one of the main causes of cancer-related death in the world. The identification and characteristics of malignant cells are essential for the diagnosis and treatment of primary or metastatic cancers. Deep learning is a new field of artificial intelligence, which can be used for computer aided diagnosis and scientific research of lung cancer pathology by analyzing and learning through establishment and simulation of human brain. In this review, we will introduce the application, progress and problems of deep learning in pathology of lung cancer and make prospects for its future development. 
  |  http://www.jcancer.org/v11p3615.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284758/  |  
------------------------------------------- 
10.1016/j.acra.2019.09.017  |   Recent years have seen digital technologies increasingly leveraged to multiply conventional imaging modalities' diagnostic power. Artificial intelligence (AI) is most prominent among these in the radiology space, touted as the "stethoscope of the 21st century" for its potential to revolutionize diagnostic precision, provider workflow, and healthcare expenditure. Partially owing to AI's unique characteristics, and partially due to its novelty, existing regulatory paradigms are not well suited to balancing patient safety with furthering the growth of this new sector. The current review examines the historic, current, and proposed regulatory treatment of AI-empowered medical devices by the US Food and Drug Administration (FDA). An innovative framework proposed by the FDA seeks to address these issues by looking to current good manufacturing practices (cGMP) and adopting a total product lifecycle (TPLC) approach. If brought into force, this may reduce the regulatory burden incumbent on developers, while holding them to rigorous quality standards, maximizing safety, and permitting the field to mature. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30450-7  |  
------------------------------------------- 
10.3389/fcvm.2020.00054  |   Imaging and cardiology are the healthcare domains which have seen the greatest number of FDA approvals for novel data-driven technologies, such as artificial intelligence, in recent years. The increasing use of such data-driven technologies in healthcare is presenting a series of important challenges to healthcare practitioners, policymakers, and patients. In this paper, we review ten ethical, social, and political challenges raised by these technologies. These range from relatively pragmatic concerns about data acquisition to potentially more abstract issues around how these technologies will impact the relationships between practitioners and their patients, and between healthcare providers themselves. We describe what is being done in the United Kingdom to identify the principles that should guide AI development for health applications, as well as more recent efforts to convert adherence to these principles into more practical policy. We also consider the approaches being taken by healthcare organizations and regulators in the European Union, the United States, and other countries. Finally, we discuss ways by which researchers and frontline clinicians, in cardiac imaging and more broadly, can ensure that these technologies are acceptable to their patients. 
  |  https://doi.org/10.3389/fcvm.2020.00054  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32351974/  |  
------------------------------------------- 
10.1007/s12194-019-00552-4  |   The third artificial intelligence (AI) boom is coming, and there is an inkling that the speed of its evolution is quickly increasing. In games like chess, shogi, and go, AI has already defeated human champions, and the fact that it is able to achieve autonomous driving is also being realized. Under these circumstances, AI has evolved and diversified at a remarkable pace in medical diagnosis, especially in diagnostic imaging. Therefore, this commentary focuses on AI in medical diagnostic imaging and explains the recent development trends and practical applications of computer-aided detection/diagnosis using artificial intelligence, especially deep learning technology, as well as some topics surrounding it. 
  |  https://dx.doi.org/10.1007/s12194-019-00552-4  |  
------------------------------------------- 
10.1111/his.13991  |   Surgical pathology forms the cornerstone of modern oncological medicine, owing to the wealth of clinically relevant information that can be obtained from tissue morphology. Although several ancillary testing modalities have been added to surgical pathology, the way in which we view and interpret tissue morphology has remained largely unchanged since the inception of our profession. In this review, we discuss new technological advances that promise to transform the way in which we access tissue morphology and how we use it to guide patient care. 
  |  https://doi.org/10.1111/his.13991  |  
------------------------------------------- 
10.1093/europace/euz324  |    Aims :  Although left ventricular hypertrophy (LVH) has a high incidence and clinical importance, the conventional diagnosis criteria for detecting LVH using electrocardiography (ECG) has not been satisfied. We aimed to develop an artificial intelligence (AI) algorithm for detecting LVH. 
  Methods and results:  This retrospective cohort study involved the review of 21 286 patients who were admitted to two hospitals between October 2016 and July 2018 and underwent 12-lead ECG and echocardiography within 4 weeks. The patients in one hospital were divided into a derivation and internal validation dataset, while the patients in the other hospital were included in only an external validation dataset. An AI algorithm based on an ensemble neural network (ENN) combining convolutional and deep neural network was developed using the derivation dataset. And we visualized the ECG area that the AI algorithm used to make the decision. The area under the receiver operating characteristic curve of the AI algorithm based on ENN was 0.880 (95% confidence interval 0.877-0.883) and 0.868 (0.865-0.871) during the internal and external validations. These results significantly outperformed the cardiologist's clinical assessment with Romhilt-Estes point system and Cornell voltage criteria, Sokolov-Lyon criteria, and interpretation of ECG machine. At the same specificity, the AI algorithm based on ENN achieved 159.9%, 177.7%, and 143.8% higher sensitivities than those of the cardiologist's assessment, Sokolov-Lyon criteria, and interpretation of ECG machine. 
  Conclusion :  An AI algorithm based on ENN was highly able to detect LVH and outperformed cardiologists, conventional methods, and other machine learning techniques. 
  |  https://academic.oup.com/europace/article-lookup/doi/10.1093/europace/euz324  |  
------------------------------------------- 
10.1038/s41467-020-14545-0  |   In the future, entire genomes tailored to specific functions and environments could be designed using computational tools. However, computational tools for genome design are currently scarce. Here we present algorithms that enable the use of design-simulate-test cycles for genome design, using genome minimisation as a proof-of-concept. Minimal genomes are ideal for this purpose as they have a simple functional assay whether the cell replicates or not. We used the first (and currently only published) whole-cell model for the bacterium Mycoplasma genitalium. Our computational design-simulate-test cycles discovered novel in silico minimal genomes which, if biologically correct, predict in vivo genomes smaller than JCVI-Syn3.0; a bacterium with, currently, the smallest genome that can be grown in pure culture. In the process, we identified 10 low essential genes and produced evidence for at least two Mycoplasma genitalium in silico minimal genomes. This work brings combined computational and laboratory genome engineering a step closer. 
  |  http://dx.doi.org/10.1038/s41467-020-14545-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32047145/  |  
------------------------------------------- 
10.7507/1001-5515.201904018  |   Lung cancer is a most common malignant tumor of the lung and is the cancer with the highest morbidity and mortality worldwide. For patients with advanced non-small cell lung cancer who have undergone epidermal growth factor receptor (EGFR) gene mutations, targeted drugs can be used for targeted therapy. There are many methods for detecting EGFR gene mutations, but each method has its own advantages and disadvantages. This study aims to predict the risk of EGFR gene mutation by exploring the association between the histological features of the whole slides pathology of non-small cell lung cancer hematoxylin-eosin (HE) staining and the patient's EGFR mutant gene. The experimental results show that the area under the curve (AUC) of the EGFR gene mutation risk prediction model proposed in this paper reached 72.4% on the test set, and the accuracy rate was 70.8%, which reveals the close relationship between histomorphological features and EGFR gene mutations in the whole slides pathological images of non-small cell lung cancer. In this paper, the molecular phenotypes were analyzed from the scale of the whole slides pathological images, and the combination of pathology and molecular omics was used to establish the EGFR gene mutation risk prediction model, revealing the correlation between the whole slides pathological images and EGFR gene mutation risk. It could provide a promising research direction for this field. 
 肺癌是一种常见的肺部恶性肿瘤，是全球发病率和死亡率最高的恶性肿瘤。对于发生了表皮生长因子受体（EGFR）基因突变的晚期非小细胞型肺癌患者，可以使用靶向药物来进行针对性治疗。EGFR 基因突变的检测方法很多，但是各有优缺点。本文拟通过探索非小细胞型肺癌苏木精-伊红（HE）染色的全扫描组织病理图像形态学特征与患者 EGFR 基因突变之间的关联，达到预测 EGFR 基因突变风险的目的。实验结果表明，本文所提出的 EGFR 基因突变风险预测模型的曲线下面积（AUC）在测试集上可达 72.4%，准确率为 70.8%，提示非小细胞型肺癌全扫描组织病理图像中的组织形态学特征与 EGFR 基因突变之间存在密切关联。本文从病理图像的尺度来分析基因分子表型，将病理组学和分子组学相融合，建立 EGFR 基因突变风险预测模型，揭示全扫描组织病理图像和 EGFR 基因突变风险的关联性，或可为该领域提供一个颇具前景的研究方向。. 
  |  None  |  
------------------------------------------- 
10.1016/j.cmpb.2020.105436  |    Background:  Metabolic engineering aims at contriving microbes as biocatalysts for enhanced and cost-effective production of countless secondary metabolites. These secondary metabolites can be treated as the resources of industrial chemicals, pharmaceuticals and fuels. Plants are also crucial targets for metabolic engineers to produce necessary secondary metabolites. Metabolic engineering of both microorganism and plants also contributes towards drug discovery. In order to implement advanced metabolic engineering techniques efficiently, metabolic engineers should have detailed knowledge about cell physiology and metabolism. Principle behind methodologies: Genome-scale mathematical models of integrated metabolic, signal transduction, gene regulatory and protein-protein interaction networks along with experimental validation can provide such knowledge in this context. Incorporation of omics data into these models is crucial in the case of drug discovery. Inverse metabolic engineering and metabolic control analysis (MCA) can help in developing such models. Artificial intelligence methodology can also be applied for efficient and accurate metabolic engineering. 
  Conclusion:  In this review, we discuss, at the beginning, the perspectives of metabolic engineering and its application on microorganism and plant leading to drug discovery. At the end, we elaborate why inverse metabolic engineering and MCA are closely related to modern metabolic engineering. In addition, some crucial steps ensuring efficient and optimal metabolic engineering strategies have been discussed. Moreover, we explore the use of genomics data for the activation of silent metabolic clusters and how it can be integrated with metabolic engineering. Finally, we exhibit a few applications of artificial intelligence to metabolic engineering. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0169-2607(19)32113-3  |  
------------------------------------------- 
10.1177/1460458219900452  |   There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action. 
  |  https://journals.sagepub.com/doi/10.1177/1460458219900452?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  
------------------------------------------- 
10.1098/rsta.2019.0058  |   The case is made for a much closer synergy between climate science, numerical analysis and computer science. This article is part of a discussion meeting issue 'Numerical algorithms for high-performance computational science'. 
  |  https://royalsocietypublishing.org/doi/full/10.1098/rsta.2019.0058?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/cid/ciaa184  |    |  https://academic.oup.com/cid/article-lookup/doi/10.1093/cid/ciaa184  |  
------------------------------------------- 
10.1177/0022034520902128  |   Early diagnosis is the most important determinant of oral and oropharyngeal squamous cell carcinoma (OPSCC) outcomes, yet most of these cancers are detected late, when outcomes are poor. Typically, nonspecialists such as dentists screen for oral cancer risk, and then they refer high-risk patients to specialists for biopsy-based diagnosis. Because the clinical appearance of oral mucosal lesions is not an adequate indicator of their diagnosis, status, or risk level, this initial triage process is inaccurate, with poor sensitivity and specificity. The objective of this study is to provide an overview of emerging optical imaging modalities and novel artificial intelligence-based approaches, as well as to evaluate their individual and combined utility and implications for improving oral cancer detection and outcomes. The principles of image-based approaches to detecting oral cancer are placed within the context of clinical needs and parameters. A brief overview of artificial intelligence approaches and algorithms is presented, and studies that use these 2 approaches singly and together are cited and evaluated. In recent years, a range of novel imaging modalities has been investigated for their applicability to improving oral cancer outcomes, yet none of them have found widespread adoption or significantly affected clinical practice or outcomes. Artificial intelligence approaches are beginning to have considerable impact in improving diagnostic accuracy in some fields of medicine, but to date, only limited studies apply to oral cancer. These studies demonstrate that artificial intelligence approaches combined with imaging can have considerable impact on oral cancer outcomes, with applications ranging from low-cost screening with smartphone-based probes to algorithm-guided detection of oral lesion heterogeneity and margins using optical coherence tomography. Combined imaging and artificial intelligence approaches can improve oral cancer outcomes through improved detection and diagnosis. 
  |  http://journals.sagepub.com/doi/full/10.1177/0022034520902128?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s40257-019-00462-6  |   Although artificial intelligence has been available for some time, it has garnered significant interest recently and has been popularized by major companies with its applications in image identification, speech recognition and problem solving. Artificial intelligence is now being increasingly studied for its potential uses in medicine. A sound understanding of the concepts of this emerging field is essential for the dermatologist as dermatology has abundant medical data and images that can be used to train artificial intelligence for patient care. There are already a number of artificial intelligence studies focusing on skin disorders such as skin cancer, psoriasis, atopic dermatitis and onychomycosis. This article aims to present a basic introduction to the concepts of artificial intelligence as well as present an overview of the current research into artificial intelligence in dermatology, examining both its current applications and its future potential. 
  |  https://dx.doi.org/10.1007/s40257-019-00462-6  |  
------------------------------------------- 
10.1007/s12350-019-01994-4  |    |  https://dx.doi.org/10.1007/s12350-019-01994-4  |  
------------------------------------------- 
10.1109/JBHI.2020.2970807  |   Medicine has entered the digital era, driven by data from new modalities, especially genomics and imaging, as well as new sources such as wearables and Internet of Things. As we gain a deeper understanding of the disease biology and how diseases affect an individual, we are developing targeted therapies to personalize treatments. There is a need for technologies like Artificial Intelligence (AI) to be able to support predictions for personalized treatments. In order to mainstream AI in healthcare we will need to address issues such as explainability, liability and privacy. Developing explainable algorithms and including AI training in medical education are many of the solutions that can help alleviate these concerns. 
  |  https://dx.doi.org/10.1109/JBHI.2020.2970807  |  
------------------------------------------- 
10.1016/j.jaad.2020.03.121  |   Managing the balance between accurately identifying early-stage melanomas while avoiding biopsies of benign lesions (i.e. over-biopsy) is the major challenge of melanoma detection. Decision-making can be especially difficult in patients with extensive atypical nevi. Recognizing that the primary screening modality for melanoma is subjective examination, studies have shown a tendency towards over-biopsy. Even low-risk routine surgical procedures are associated with morbidity, mounting healthcare costs, and patient anxiety. Recent advancements in noninvasive diagnostic modalities have helped to improve diagnostic accuracy, especially when managing melanocytic lesions of uncertain diagnosis. Breakthroughs in artificial intelligence have also shown exciting potential in changing the landscape of melanoma detection. In part I of this continuing medical education article, we review novel diagnostic technologies such as automated 2D and 3D total body imaging with sequential digital dermoscopic imaging, reflectance confocal microscopy, and electrical impedance spectroscopy, and we explore the logistics and implications of potentially integrating artificial intelligence into existing melanoma management paradigms. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0190-9622(20)30738-6  |  
------------------------------------------- 
10.1007/s12149-020-01468-5  |    Objective:  An artificial intelligence (AI)-based algorithm typically requires a considerable amount of training data; however, few training images are available for dementia with Lewy bodies and frontotemporal lobar degeneration. Therefore, this study aims to present the potential of cycle-consistent generative adversarial networks (CycleGAN) to obtain enough number of training images for AI-based computer-aided diagnosis (CAD) algorithms for diagnosing dementia. 
  Methods:  We trained CycleGAN using 43 amyloid-negative and 45 positive images in slice-by-slice. 
  Results:  The CycleGAN can be used to synthesize reasonable amyloid-positive images, and the continuity of slices was preserved. 
  Discussion:  Our results show that CycleGAN has the potential to generate a sufficient number of training images for CAD of dementia. 
  |  https://dx.doi.org/10.1007/s12149-020-01468-5  |  
------------------------------------------- 
10.1097/MD.0000000000018676  |   Adenoid cystic carcinoma (ACC) is one of the most frequent malignancies of salivary glands. The objective of this study was to identify key genes and potential mechanisms during ACC samples.The gene expression profiles of GSE88804 data set were downloaded from Gene Expression Omnibus. The GSE88804 data set contained 22 samples, including 15 ACC samples and 7 normal salivary gland tissues. The gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses were constructed, and protein-protein interaction network of differentially expressed genes (DEGs) was performed by Cytoscape. The top 10 hub genes were analyzed based on Gene Expression Profiling Interactive Analysis. Then, DEGs between ACC samples and normal salivary gland samples were analyzed by gene set enrichment analysis. Furthermore, miRTarBase and Cytoscape were used for visualization of miRNA-mRNA regulatory network. KEGG pathway analysis was undertaken using DIANA-miRPath v3.0.In total, 382 DEGs were identified, including 119 upregulated genes and 263 downregulated genes. GO analysis showed that DEGs were mainly enriched in extracellular matrix organization, extracellular matrix, and calcium ion binding. KEGG pathway analysis showed that DEGs were mainly enriched in p53 signaling pathway and salivary secretion. Expression analysis and survival analysis showed that ANLN, CCNB2, CDK1, CENPF, DTL, KIF11, and TOP2A are all highly expressed, which all may be related to poor overall survival. Predicted miRNAs of 7 hub DEGs mainly enriched in proteoglycans in cancer and pathways in cancer.This study indicated that identified DEGs and hub genes might promote our understanding of molecular mechanisms, which might be used as molecular targets or diagnostic biomarkers for ACC. 
  |  http://dx.doi.org/10.1097/MD.0000000000018676  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31914060/  |  
------------------------------------------- 
10.2196/18828  |    Background:  The recent global outbreak of coronavirus disease (COVID-19) is affecting many countries worldwide. Iran is one of the top 10 most affected countries. Search engines provide useful data from populations, and these data might be useful to analyze epidemics. Utilizing data mining methods on electronic resources' data might provide a better insight into the COVID-19 outbreak to manage the health crisis in each country and worldwide. 
  Objective:  This study aimed to predict the incidence of COVID-19 in Iran. 
  Methods:  Data were obtained from the Google Trends website. Linear regression and long short-term memory (LSTM) models were used to estimate the number of positive COVID-19 cases. All models were evaluated using 10-fold cross-validation, and root mean square error (RMSE) was used as the performance metric. 
  Results:  The linear regression model predicted the incidence with an RMSE of 7.562 (SD 6.492). The most effective factors besides previous day incidence included the search frequency of handwashing, hand sanitizer, and antiseptic topics. The RMSE of the LSTM model was 27.187 (SD 20.705). 
  Conclusions:  Data mining algorithms can be employed to predict trends of outbreaks. This prediction might support policymakers and health care managers to plan and allocate health care resources accordingly. 
  |  https://publichealth.jmir.org/2020/2/e18828/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32234709/  |  
------------------------------------------- 
10.1002/jmri.27035  |    Background:  Classical machine learning (ML) and deep learning (DL) articles have rapidly captured the attention of the radiology research community and comprise an increasing proportion of articles submitted to JMRI, of variable reporting and methodological quality. 
  Purpose:  To identify the most frequent reviewer critiques of classical ML and DL articles submitted to JMRI. 
  Study type:  Qualitative thematic analysis. 
  Population:  In all, 1396 manuscript journal articles submitted to JMRI for consideration in 2018, with thematic analysis performed of reviewer critiques of 38 artificial intelligence (AI) articles, comprised of 24 ML and 14 DL articles, from January 9, 2018 to June 2, 2018. 
  Field strength/sequence:  N/A. 
  Assessment:  After identifying and sampling ML and DL articles, and collecting all reviews, qualitative thematic analysis was performed to identify major and minor themes of reviewer critiques. 
  Statistical tests:  Descriptive statistics provided of article characteristics, and thematic review of major and minor themes. 
  Results:  Thirty-eight articles were sampled for thematic review: 24 (63.2%) focused on classical ML and 14 (36.8%) on DL. The overall acceptance rate of classical ML/DL articles was 28.9%, similar to the overall 2017-2019 acceptance rate of 23.1-28.1%. These articles resulted in 72 reviews analyzed, yielding a total 713 critiques that underwent formal thematic analysis consensus encoding. Ten major themes of critiques were identified, with 1-Lack of Information as the most frequent, comprising 268 (37.6%) of all critiques. Frequent minor themes of critiques concerning ML/DL-specific recommendations included performing basic clinical statistics such as to ensure similarity of training and test groups (N = 26), emphasizing strong clinical Gold Standards for the basis of training labels (N = 19), and ensuring strong radiological relevance of the topic and task performed (N = 16). 
  Data conclusion:  Standardized reporting of ML and DL methods could help address nearly one-third of all reviewer critiques made. 
  Level of evidence:  4 Technical Efficacy Stage: 1 J. Magn. Reson. Imaging 2020. 
  |  https://doi.org/10.1002/jmri.27035  |  
------------------------------------------- 
10.1186/s12967-019-02204-y  |    Background:  Artificial intelligence (AI), with its seemingly limitless power, holds the promise to truly revolutionize patient healthcare. However, the discourse carried out in public does not always correlate with the actual impact. Thus, we aimed to obtain both an overview of how French health professionals perceive the arrival of AI in daily practice and the perception of the other actors involved in AI to have an overall understanding of this issue. 
  Methods:  Forty French stakeholders with diverse backgrounds were interviewed in Paris between October 2017 and June 2018 and their contributions analyzed using the grounded theory method (GTM). 
  Results:  The interviews showed that the various actors involved all see AI as a myth to be debunked. However, their views differed. French healthcare professionals, who are strategically placed in the adoption of AI tools, were focused on providing the best and safest care for their patients. Contrary to popular belief, they are not always seeing the use of these tools in their practice. For healthcare industrial partners, AI is a true breakthrough but legal difficulties to access individual health data could hamper its development. Institutional players are aware that they will have to play a significant role concerning the regulation of the use of these tools. From an external point of view, individuals without a conflict of interest have significant concerns about the sustainability of the balance between health, social justice, and freedom. Health researchers specialized in AI have a more pragmatic point of view and hope for a better transition from research to practice. 
  Conclusion:  Although some hyperbole has taken over the discourse on AI in healthcare, diverse opinions and points of view have emerged among French stakeholders. The development of AI tools in healthcare will be satisfactory for everyone only by initiating a collaborative effort between all those involved. It is thus time to also consider the opinion of patients and, together, address the remaining questions, such as that of responsibility. 
  |  https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-019-02204-y  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31918710/  |  
------------------------------------------- 
10.1007/s12032-020-01368-8  |   Artificial intelligence (AI) is revolutionizing healthcare and transforming the clinical practice of physicians across the world. Radiology has a strong affinity for machine learning and is at the forefront of the paradigm shift, as machines compete with humans for cognitive abilities. AI is a computer science simulation of the human mind that utilizes algorithms based on collective human knowledge and the best available evidence to process various forms of inputs and deliver desired outcomes, such as clinical diagnoses and optimal treatment options. Despite the overwhelmingly positive uptake of the technology, warnings have been published about the potential dangers of AI. Concerns have been expressed reflecting opinions that future medicine based on AI will render radiologists irrelevant. Thus, how much of this is based on reality? To answer these questions, it is important to examine the facts, clarify where AI really stands and why many of these speculations are untrue. We aim to debunk the 6 top myths regarding AI in the future of radiologists. 
  |  https://dx.doi.org/10.1007/s12032-020-01368-8  |  
------------------------------------------- 
10.2214/AJR.19.22145  |   <b>OBJECTIVE.</b> The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. <b>CONCLUSION.</b> A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.22145  |  
------------------------------------------- 
10.1177/1536012120914773  |   Chronic subdural hematomas (CSDHs) frequently affect the elderly population. The postoperative recurrence rate of CSDHs is high, ranging from 3% to 20%. Both qualitative and quantitative analyses have been explored to investigate the mechanisms underlying postoperative recurrence. We surveyed the pathophysiology of CSDHs and analyzed the relative factors influencing postoperative recurrence. Here, we summarize various qualitative methods documented in the literature and present our unique computer-assisted quantitative method, published previously, to assess postoperative recurrence. Imaging features of CSDHs, based on qualitative analysis related to postoperative high recurrence rate, such as abundant vascularity, neomembrane formation, and patent subdural space, could be clearly observed using the proposed quantitative analysis methods in terms of mean hematoma density, brain re-expansion rate, hematoma volume, average distance of subdural space, and brain shifting. Finally, artificial intelligence (AI) device types and applications in current health care are briefly outlined. We conclude that the potential applications of AI techniques can be integrated to the proposed quantitative analysis method to accomplish speedy execution and accurate prediction for postoperative outcomes in the management of CSDHs. 
  |  https://journals.sagepub.com/doi/10.1177/1536012120914773?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  
------------------------------------------- 
10.2214/AJR.19.21872  |   <b>OBJECTIVE.</b> The objective of this study was to assess the impact of artificial intelligence (AI)-based decision support (DS) on breast ultrasound (US) lesion assessment. <b>MATERIALS AND METHODS.</b> A multicenter retrospective review of 900 breast lesions (470/900 [52.2%] benign; 430/900 [47.8%] malignant) on US by 15 physicians (11 radiologists, two surgeons, two obstetrician/gynecologists). An AI system (Koios DS for Breast, Koios Medical) evaluated images and assigned them to one of four categories: benign, probably benign, suspicious, and probably malignant. Each reader reviewed cases twice: 750 cases with US only or with US plus DS; 4 weeks later, cases were reviewed in the opposite format. One hundred fifty additional cases were presented identically in each session. DS and reader sensitivity, specificity, and positive likelihood ratios (PLRs) were calculated as well as reader AUCs with and without DS. The Kendall τ-b correlation coefficient was used to assess intraand interreader variability. <b>RESULTS.</b> Mean reader AUC for cases reviewed with US only was 0.83 (95% CI, 0.78-0.89); for cases reviewed with US plus DS, mean AUC was 0.87 (95% CI, 0.84-0.90). PLR for the DS system was 1.98 (95% CI, 1.78-2.18) and was higher than the PLR for all readers but one. Fourteen readers had better AUC with US plus DS than with US only. Mean Kendall τ-b for US-only interreader variability was 0.54 (95% CI, 0.53-0.55); for US plus DS, it was 0.68 (95% CI, 0.67-0.69). Intrareader variability improved with DS; class switching (defined as crossing from BI-RADS category 3 to BI-RADS category 4A or above) occurred in 13.6% of cases with US only versus 10.8% of cases with US plus DS (<i>p</i> = 0.04). <b>CONCLUSION.</b> AI-based DS improves accuracy of sonographic breast lesion assessment while reducing inter- and intraobserver variability. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.19.21872  |  
------------------------------------------- 
10.1093/ndt/gfz203  |    |  https://academic.oup.com/ndt/article-lookup/doi/10.1093/ndt/gfz203  |  
------------------------------------------- 
10.3389/fnins.2020.00207  |    Introduction:  Deep learning neural networks are especially potent at dealing with structured data, such as images and volumes. Both modified LiviaNET and HyperDense-Net performed well at a prior competition segmenting 6-month-old infant magnetic resonance images, but neonatal cerebral tissue type identification is challenging given its uniquely inverted tissue contrasts. The current study aims to evaluate the two architectures to segment neonatal brain tissue types at term equivalent age. 
  Methods:  Both networks were retrained over 24 pairs of neonatal T1 and T2 data from the Developing Human Connectome Project public data set and validated on another eight pairs against ground truth. We then reported the best-performing model from training and its performance by computing the Dice similarity coefficient (DSC) for each tissue type against eight test subjects. 
  Results:  During the testing phase, among the segmentation approaches tested, the dual-modality HyperDense-Net achieved the best statistically significantly test mean DSC values, obtaining 0.94/0.95/0.92 for the tissue types and took 80 h to train and 10 min to segment, including preprocessing. The single-modality LiviaNET was better at processing T2-weighted images than processing T1-weighted images across all tissue types, achieving mean DSC values of 0.90/0.90/0.88 for gray matter, white matter, and cerebrospinal fluid, respectively, while requiring 30 h to train and 8 min to segment each brain, including preprocessing. 
  Discussion:  Our evaluation demonstrates that both neural networks can segment neonatal brains, achieving previously reported performance. Both networks will be continuously retrained over an increasingly larger repertoire of neonatal brain data and be made available through the Canadian Neonatal Brain Platform to better serve the neonatal brain imaging research community. 
  |  https://doi.org/10.3389/fnins.2020.00207  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32273836/  |  
------------------------------------------- 
10.4103/ijo.IJO_966_19  |    Purpose:  Deep learning is a newer and advanced subfield in artificial intelligence (AI). The aim of our study is to validate a machine-based algorithm developed based on deep convolutional neural networks as a tool for screening to detect referable diabetic retinopathy (DR). 
  Methods:  An AI algorithm to detect DR was validated at our hospital using an internal dataset consisting of 1,533 macula-centered fundus images collected retrospectively and an external validation set using Methods to Evaluate Segmentation and Indexing Techniques in the field of Retinal Ophthalmology (MESSIDOR) dataset. Images were graded by two retina specialists as any DR, prompt referral (moderate nonproliferative diabetic retinopathy (NPDR) or above or presence of macular edema) and sight-threatening DR/STDR (severe NPDR or above) and compared with AI results. Sensitivity, specificity, and area under curve (AUC) for both internal and external validation sets for any DR detection, prompt referral, and STDR were calculated. Interobserver agreement using kappa value was calculated for both the sets and two out of three agreements for DR grading was considered as ground truth to compare with AI results. 
  Results:  In the internal validation set, the overall sensitivity and specificity was 99.7% and 98.5% for Any DR detection and 98.9% and 94.84%for Prompt referral respectively. The AUC was 0.991 and 0.969 for any DR detection and prompt referral respectively. The agreement between two observers was 99.5% and 99.2% for any DR detection and prompt referral with a kappa value of 0.94 and 0.96, respectively. In the external validation set (MESSIDOR 1), the overall sensitivity and specificity was 90.4% and 91.0% for any DR detection and 94.7% and 97.4% for prompt referral, respectively. The AUC was. 907 and. 960 for any DR detection and prompt referral, respectively. The agreement between two observers was 98.5% and 97.8% for any DR detection and prompt referral with a kappa value of 0.971 and 0.980, respectively. 
  Conclusion:  With increasing diabetic population and growing demand supply gap in trained resources, AI is the future for early identification of DR and reducing blindness. This can revolutionize telescreening in ophthalmology, especially where people do not have access to specialized health care. 
  |  http://www.ijo.in/article.asp?issn=0301-4738;year=2020;volume=68;issue=2;spage=398;epage=405;aulast=Shah  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31957737/  |  
------------------------------------------- 
10.3390/s20041031  |   Myoelectric prostheses help amputees to regain independence and a higher quality of life. These prostheses are controlled by electromyography, which measures an electrical signal at the skin surface during muscle contractions. In this contribution, the electromyography is measured with innovative flexible insulated sensors, which separate the skin and the sensor area by a dielectric layer. Electromyography sensors, and biosignal sensors in general, are striving for higher robustness against motion artifacts, which are a major obstacle in real-world environment. The motion artifact suppression algorithms presented in this article, prevent the activation of the prosthesis drive during artifacts, thereby achieving a substantial performance boost. These algorithms classify the signal into muscle contractions and artifacts. Therefore, new time domain features, such as Mean Crossing Rate are introduced and well-established time domain features (e.g., Zero-Crossing Rate, Slope Sign Change) are modified and implemented. Various artificial intelligence models, which require low calculation resources for an application in a wearable device, were investigated. These models are neural networks, recurrent neural networks, decision trees and logistic regressions. Although these models are designed for a low-power real-time embedded system, high accuracies in discriminating artifacts to contractions of up to 99.9% are achieved. The models were implemented and trained for fast response leading to a high performance in real-world environment. For highest accuracies, recurrent neural networks are suggested and for minimum runtime ( 0.99-1.15 μ s), decision trees are preferred. 
  |  http://www.mdpi.com/resolver?pii=s20041031  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32075031/  |  
------------------------------------------- 
10.1093/cvr/cvaa021  |   Rapid technological advances in non-invasive imaging, coupled with the availability of large data sets and the expansion of computational models and power, have revolutionized the role of imaging in medicine. Non-invasive imaging is the pillar of modern cardiovascular diagnostics, with modalities such as cardiac computed tomography (CT) now recognized as first-line options for cardiovascular risk stratification and the assessment of stable or even unstable patients. To date, cardiovascular imaging has lagged behind other fields, such as oncology, in the clinical translational of artificial intelligence (AI)-based approaches. We hereby review the current status of AI in non-invasive cardiovascular imaging, using cardiac CT as a running example of how novel machine learning (ML)-based radiomic approaches can improve clinical care. The integration of ML, deep learning, and radiomic methods has revealed direct links between tissue imaging phenotyping and tissue biology, with important clinical implications. More specifically, we discuss the current evidence, strengths, limitations, and future directions for AI in cardiac imaging and CT, as well as lessons that can be learned from other areas. Finally, we propose a scientific framework in order to ensure the clinical and scientific validity of future studies in this novel, yet highly promising field. Still in its infancy, AI-based cardiovascular imaging has a lot to offer to both the patients and their doctors as it catalyzes the transition towards a more precise phenotyping of cardiovascular disease. 
  |  https://academic.oup.com/cardiovascres/article-lookup/doi/10.1093/cvr/cvaa021  |  
------------------------------------------- 
10.3389/fphar.2019.01526  |   Antimalarial drugs are becoming less effective due to the emergence of drug resistance. Resistance has been reported for all available malaria drugs, including artemisinin, thus creating a perpetual need for alternative drug candidates. The traditional drug discovery approach of high throughput screening (HTS) of large compound libraries for identification of new drug leads is time-consuming and resource intensive. While virtual <i>in silico</i> screening is a solution to this problem, however, the generalization of the models is not ideal. Artificial intelligence (AI), utilizing either structure-based or ligand-based approaches, has demonstrated highly accurate performances in the field of chemical property prediction. Leveraging the existing data, AI would be a suitable alternative to blind-search HTS or fingerprint-based virtual screening. The AI model would learn patterns within the data and help to search for hit compounds efficiently. In this work, we introduce DeepMalaria, a deep-learning based process capable of predicting the anti-<i>Plasmodium falciparum</i> inhibitory properties of compounds using their SMILES. A graph-based model is trained on 13,446 publicly available antiplasmodial hit compounds from GlaxoSmithKline (GSK) dataset that are currently being used to find novel drug candidates for malaria. We validated this model by predicting hit compounds from a macrocyclic compound library and already approved drugs that are used for repurposing. We have chosen macrocyclic compounds as these ligand-binding structures are underexplored in malaria drug discovery. The <i>in silico</i> pipeline for this process also consists of additional validation of an in-house independent dataset consisting mostly of natural product compounds. Transfer learning from a large dataset was leveraged to improve the performance of the deep learning model. To validate the DeepMalaria generated hits, we used a commonly used SYBR Green I fluorescence assay based phenotypic screening. DeepMalaria was able to detect all the compounds with nanomolar activity and 87.5% of the compounds with greater than 50% inhibition. Further experiments to reveal the compounds' mechanism of action have shown that not only does one of the hit compounds, DC-9237, inhibits all asexual stages of <i>Plasmodium falciparum</i>, but is a fast-acting compound which makes it a strong candidate for further optimization. 
  |  https://doi.org/10.3389/fphar.2019.01526  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32009951/  |  
------------------------------------------- 
10.7759/cureus.7124  |   Artificial intelligence (AI) is growing exponentially in various fields, including medicine. This paper reviews the pertinent aspects of AI in obstetrics and gynecology (OB/GYN) and how these can be applied to improve patient outcomes and reduce the healthcare costs and workload for clinicians. Herein, we will address current AI uses in OB/GYN, and the use of AI as a tool to interpret fetal heart rate (FHR) and cardiotocography (CTG) to aid in the detection of preterm labor, pregnancy complications, and review discrepancies in its interpretation between clinicians to reduce maternal and infant morbidity and mortality. AI systems can be used as tools to create algorithms identifying asymptomatic women with short cervical length who are at risk of preterm birth. Additionally, the benefits of using the vast data capacity of AI storage can assist in determining the risk factors for preterm labor using multiomics and extensive genomic data. In the field of gynecological surgery, the use of augmented reality helps surgeons detect vital structures, thus decreasing complications, reducing operative time, and helping surgeons in training to practice in a realistic setting. Using three-dimensional (3D) printers can provide materials that mimic real tissues and also helps trainees to practice on a realistic model. Furthermore, 3D imaging allows better depth perception than its two-dimensional (2D) counterpart, allowing the surgeon to create preoperative plans according to tissue depth and dimensions. Although AI has some limitations, this new technology can improve the prognosis and management of patients, reduce healthcare costs, and help OB/GYN practitioners to reduce their workload and increase their efficiency and accuracy by incorporating AI systems into their daily practice. AI has the potential to guide practitioners in decision-making, reaching a diagnosis, and improving case management. It can reduce healthcare costs by decreasing medical errors and providing more dependable predictions. AI systems can accurately provide information on the large array of patients in clinical settings, although more robust data is required. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32257670/  |  
------------------------------------------- 
10.1007/s10928-020-09685-1  |   The role of artificial intelligence (AI) in healthcare for pregnant women. To assess the role of AI in women's health, discover gaps, and discuss the future of AI in maternal health. A systematic review of English articles using EMBASE, PubMed, and SCOPUS. Search terms included pregnancy and AI. Research articles and book chapters were included, while conference papers, editorials and notes were excluded from the review. Included papers focused on pregnancy and AI methods, and pertained to pharmacologic interventions. We identified 376 distinct studies from our queries. A final set of 31 papers were included for the review. Included papers represented a variety of pregnancy concerns and multidisciplinary applications of AI. Few studies relate to pregnancy, AI, and pharmacologics and therefore, we review carefully those studies. External validation of models and techniques described in the studies is limited, impeding on generalizability of the studies. Our review describes how AI has been applied to address maternal health, throughout the pregnancy process: preconception, prenatal, perinatal, and postnatal health concerns. However, there is a lack of research applying AI methods to understand how pharmacologic treatments affect pregnancy. We identify three areas where AI methods could be used to improve our understanding of pharmacological effects of pregnancy, including: (a) obtaining sound and reliable data from clinical records (15 studies), (b) designing optimized animal experiments to validate specific hypotheses (1 study) to (c) implementing decision support systems that inform decision-making (11 studies). The largest literature gap that we identified is with regards to using AI methods to optimize translational studies between animals and humans for pregnancy-related drug exposures. 
  |  https://doi.org/10.1007/s10928-020-09685-1  |  
------------------------------------------- 
10.1016/S1470-2045(19)30809-5  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1470-2045(19)30809-5  |  
------------------------------------------- 
10.1007/s40506-020-00216-7  |    Purpose of review:  Artificial intelligence (AI) offers huge potential in infection prevention and control (IPC). We explore its potential IPC benefits in epidemiology, laboratory infection diagnosis, and hand hygiene. 
  Recent findings:  AI has the potential to detect transmission events during outbreaks or predict high-risk patients, enabling development of tailored IPC interventions. AI offers opportunities to enhance diagnostics with objective pattern recognition, standardize the diagnosis of infections with IPC implications, and facilitate the dissemination of IPC expertise. AI hand hygiene applications can deliver behavior change, though it requires further evaluation in different clinical settings. However, staff can become dependent on automatic reminders, and performance returns to baseline if feedback is removed. 
  Summary:  Advantages for IPC include speed, consistency, and capability of handling infinitely large datasets. However, many challenges remain; improving the availability of high-quality representative datasets and consideration of biases within preexisting databases are important challenges for future developments. AI in itself will not improve IPC; this requires culture and behavior change. Most studies to date assess performance retrospectively so there is a need for prospective evaluation in the real-life, often chaotic, clinical setting. Close collaboration with IPC experts to interpret outputs and ensure clinical relevance is essential. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32218708/  |  
------------------------------------------- 
10.1093/rheumatology/kez370  |    Objectives:  Manual systematic literature reviews are becoming increasingly challenging due to the sharp rise in publications. The primary objective of this literature review was to compare manual and computer software using artificial intelligence retrieval of publications on the cutaneous manifestations of primary SS, but we also evaluated the prevalence of cutaneous manifestations in primary SS. 
  Methods:  We compared manual searching and searching with the in-house computer software BIbliography BOT (BIBOT) designed for article retrieval and analysis. Both methods were used for a systematic literature review on a complex topic, i.e. the cutaneous manifestations of primary SS. Reproducibility was estimated by computing Cohen's κ coefficients and was interpreted as follows: slight, 0-0.20; fair, 0.21-0.40; moderate, 0.41-0.60; substantial, 0.61-0.80; and almost perfect, 0.81-1. 
  Results:  The manual search retrieved 855 articles and BIBOT 1042 articles. In all, 202 articles were then selected by applying exclusion criteria. Among them, 155 were retrieved by both methods, 33 by manual search only, and 14 by BIBOT only. Reliability (κ = 0.84) was almost perfect. Further selection was performed by reading the 202 articles. Cohort sizes and the nature and prevalence of cutaneous manifestations varied across publications. In all, we found 52 cutaneous manifestations reported in primary SS patients. The most described ones were cutaneous vasculitis (561 patients), xerosis (651 patients) and annular erythema (215 patients). 
  Conclusion:  Among the final selection of 202 articles, 155/202 (77%) were found by the two methods but BIBOT was faster and automatically classified the articles in a chart. Combining the two methods retrieved the largest number of publications. 
  |  https://academic.oup.com/rheumatology/article-lookup/doi/10.1093/rheumatology/kez370  |  
------------------------------------------- 
10.3390/healthcare8010046  |   As the Coronavirus (COVID-19) expands its impact from China, expanding its catchment into surrounding regions and other countries, increased national and international measures are being taken to contain the outbreak. The placing of entire cities in 'lockdown' directly affects urban economies on a multi-lateral level, including from social and economic standpoints. This is being emphasised as the outbreak gains ground in other countries, leading towards a global health emergency, and as global collaboration is sought in numerous quarters. However, while effective protocols in regard to the sharing of health data is emphasised, urban data, on the other hand, specifically relating to urban health and safe city concepts, is still viewed from a nationalist perspective as solely benefiting a nation's economy and its economic and political influence. This perspective paper, written one month after detection and during the outbreak, surveys the virus outbreak from an urban standpoint and advances how smart city networks should work towards enhancing standardization protocols for increased data sharing in the event of outbreaks or disasters, leading to better global understanding and management of the same. 
  |  http://www.mdpi.com/resolver?pii=healthcare8010046  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32120822/  |  
------------------------------------------- 
10.1002/cyto.a.23990  |    |  https://doi.org/10.1002/cyto.a.23990  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32142596/  |  
------------------------------------------- 
10.1371/journal.pone.0228439  |   In recent years, the number of vulnerabilities discovered and publicly disclosed has shown a sharp upward trend. However, the value of exploitation of vulnerabilities varies for attackers, considering that only a small fraction of vulnerabilities are exploited. Therefore, the realization of quick exclusion of the non-exploitable vulnerabilities and optimal patch prioritization on limited resources has become imperative for organizations. Recent works using machine learning techniques predict exploited vulnerabilities by extracting features from open-source intelligence (OSINT). However, in the face of explosive growth of vulnerability information, there is room for improvement in the application of past methods to multiple threat intelligence. A more general method is needed to deal with various threat intelligence sources. Moreover, in previous methods, traditional text processing methods were used to deal with vulnerability related descriptions, which only grasped the static statistical characteristics but ignored the context and the meaning of the words of the text. To address these challenges, we propose an exploit prediction model, which is based on a combination of fastText and LightGBM algorithm and called fastEmbed. We replicate key portions of the state-of-the-art work of exploit prediction and use them as benchmark models. Our model outperforms the baseline model whether in terms of the generalization ability or the prediction ability without temporal intermixing with an average overall improvement of 6.283% by learning the embedding of vulnerability-related text on extremely imbalanced data sets. Besides, in terms of predicting the exploits in the wild, our model also outperforms the baseline model with an F1 measure of 0.586 on the minority class (33.577% improvement over the work using features from darkweb/deepweb). The results demonstrate that the model can improve the ability to describe the exploitability of vulnerabilities and predict exploits in the wild effectively. 
  |  http://dx.plos.org/10.1371/journal.pone.0228439  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32027693/  |  
------------------------------------------- 
10.1248/yakushi.19-00190-4  |   Toxicity testing is critical for new drug and chemical development process. A clinical study, experimental animal models, and in vitro study are performed to evaluate the safety of a new drug. The limitations of these methods include extensive time for toxicity testing, an ethical problem, and high costs of experimentation. Therefore computational methods are considered useful for estimating chemical toxicity. In silico toxicity prediction is one of the toxicity assessments that uses computational methods to predict and stimulate the toxicity of chemicals. In silico study aims to contribute to effective development of new drug and chemical design. In this study, quantitative structure-activity relationship (QSAR) models will be used to predict toxicities based on chemical structural parameters. Because toxicities are complicated physiological phenomena, a similar toxicity expression might cause a different pathway. Also, since many drugs with unknown mechanisms of actions are available, the application of artificial intelligence (AI)-which uses sophisticated algorithms- is increasingly used to predict toxicities. Recently, the QSAR model was applied to determine complex relations between chemical structures and toxicities. However, accuracy of QSAR for toxicity prediction remains an important issue. International competitions funded by public institutions can address this issue. Two important toxicity challenges were organized in the past decade; this article presents issues of toxicity based on these challenges. 
  |  https://dx.doi.org/10.1248/yakushi.19-00190-4  |  
------------------------------------------- 
10.1186/s13643-020-01304-x  |    Background:  Machine learning, a subset of artificial intelligence, is a set of models and methods that can automatically detect patterns in vast amounts of data, extract information and use it to perform various kinds of decision-making under uncertain conditions. This can assist surgeons in clinical decision-making by identifying patient cohorts that will benefit from surgery prior to treatment. The aim of this review is to evaluate the applications of machine learning in plastic and reconstructive surgery. 
  Methods:  A literature review will be undertaken of EMBASE, MEDLINE and CENTRAL (1990 up to September 2019) to identify studies relevant for the review. Studies in which machine learning has been employed in the clinical setting of plastic surgery will be included. Primary outcomes will be the evaluation of the accuracy of machine learning models in predicting a clinical diagnosis and post-surgical outcomes. Secondary outcomes will include a cost analysis of those models. This protocol has been prepared using the Preferred Items for Systematic Review and Meta-Analysis Protocols (PRISMA-P) guidelines. 
  Discussion:  This will be the first systematic review in available literature that summarises the published work on the applications of machine learning in plastic surgery. Our findings will provide the basis of future research in developing artificial intelligence interventions in the specialty. 
  Systematic review registration:  PROSPERO CRD42019140924. 
  |  https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-020-01304-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32111260/  |  
------------------------------------------- 
10.1002/cyto.a.24000  |    |  https://doi.org/10.1002/cyto.a.24000  |  
------------------------------------------- 
10.1111/pan.13792  |   Artificial intelligence and machine learning are rapidly expanding fields with increasing relevance in anesthesia and, in particular, airway management. The ability of artificial intelligence and machine learning algorithms to recognize patterns from large volumes of complex data makes them attractive for use in pediatric anesthesia airway management. The purpose of this review is to introduce artificial intelligence, machine learning, and deep learning to the pediatric anesthesiologist. Current evidence and developments in artificial intelligence, machine learning, and deep learning relevant to pediatric airway management are presented. We critically assess the current evidence on the use of artificial intelligence and machine learning in the assessment, diagnosis, monitoring, procedure assistance, and predicting outcomes during pediatric airway management. Further, we discuss the limitations of these technologies and offer areas for focused research that may bring pediatric airway management anesthesiology into the era of artificial intelligence and machine learning. 
  |  https://doi.org/10.1111/pan.13792  |  
------------------------------------------- 
10.1097/RTI.0000000000000486  |   Artificial intelligence (AI) is a broad field of computational science that includes many subsets. Today the most widely used subset in medical imaging is machine learning (ML). Many articles have focused on the use of ML for pattern recognition to detect and potentially diagnose various pathologies. However, AI algorithm development is now directed toward workflow management. AI can impact patient care at multiple stages of their imaging experience and assist in efficient and effective scheduling, imaging performance, worklist prioritization, image interpretation, and quality assurance. The purpose of this manuscript was to review the potential AI applications in radiology focusing on workflow management and discuss how ML will affect cardiothoracic imaging. 
  |  http://dx.doi.org/10.1097/RTI.0000000000000486  |  
------------------------------------------- 
10.1371/journal.pone.0228289  |    Objective:  To assess the classification performance between Parkinson's disease (PD) and normal control (NC) when semi-quantitative indicators and shape features obtained on dopamine transporter (DAT) single photon emission computed tomography (SPECT) are combined as a feature of machine learning (ML). 
  Methods:  A total of 100 cases of both PD and normal control (NC) from the Parkinson's Progression Markers Initiative database were evaluated. A summed image was generated and regions of interests were set to the left and right striata. Area, equivalent diameter, major axis length, minor axis length, perimeter and circularity were calculated as shape features. Striatum binding ratios (SBRputamen and SBRcaudate) were used as comparison features. The classification performance of the PD and NC groups according to receiver operating characteristic analysis of the shape features was compared in terms of SBRs. Furthermore, we compared the classification performance of ML when shape features or SBRs were used alone and in combination. 
  Results:  The shape features (except minor axis length) and SBRs indicated significant differences between the NC and PD groups (p &lt; 0.05). The top five areas under the curves (AUC) were as follows: circularity (0.972), SBRputamen (0.972), major axis length (0.945), SBRcaudate (0.928) and perimeter (0.896). When classification was done using ML, AUC was as follows: circularity and SBRs (0.995), circularity alone (0.990), and SBRs (0.973). The classification performance was significantly improved by combining SBRs and circularity than by SBRs alone (p = 0.018). 
  Conclusion:  We found that the circularity obtained from DAT-SPECT images could help in distinguishing NC and PD. Furthermore, the classification performance of ML was significantly improved using circularity in SBRs together. 
  |  http://dx.plos.org/10.1371/journal.pone.0228289  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31978154/  |  
------------------------------------------- 
10.3233/WOR-193050  |    Background:  In a real working environment, workers' performance depends on the level of competence, psychological and health condition, motivation, and perceived stress. These are the attributes of actual availability. It is crucial to identify the most influential attributes to develop an adequate level of worker's performance. 
  Objective:  The purpose of this paper is to upgrade the Availability-Humanization-Model (AH-Model) with an implementation of the artificial intelligence classification tree to identify influencing factors of the well-being attributes on human performance, where the identified influencing factors are gripping points for maintaining sustainable performance in real-life conditions of different professions. 
  Methods:  Well-being attributes are collected with the Questionnaire Actual Availability (QAA) from AH-Model and then analysed by implementation of the decision trees classification algorithms. An embedded clustering analysis of QAA ensures an efficient feature construction and selection. It negates the need of applying tree pruning or any other noise reduction algorithms. 
  Results:  An implementation of the machine learning algorithms reflects the real conditions of working environments: (a) real performance of workers depends on the perception of well-being and availability and (b) the most influencing factors explicitly reflect the content of work in a specific domain (Fintech, health, forestry, traffic) with a high level of stress. 
  Conclusions:  The presented approach offers a possibility to identify the most important well-being attributes to determine an adequate efficiency and to improve the performance level in the real working conditions. 
  |  https://content.iospress.com/openurl?genre=article&id=doi:10.3233/WOR-193050  |  
------------------------------------------- 
10.1111/tbj.13858  |   Advances in digital image analysis have the potential to transform the practice of breast pathology. In the near future, a move to a digital workflow offers improvements in efficiency. Coupled with artificial intelligence (AI), digital pathology can assist pathologist interpretation, automate time-consuming tasks, and discover novel morphologic patterns. Opportunities for digital enhancements abound in breast pathology, from increasing reproducibility in grading and biomarker interpretation, to discovering features that correlate with patient outcome and treatment. Our objective is to review the most recent developments in digital pathology with clear impact to breast pathology practice. Although breast pathologists currently undertake limited adoption of digital methods, the field is rapidly evolving. Care is needed to validate emerging technologies for effective patient care. 
  |  https://doi.org/10.1111/tbj.13858  |  
------------------------------------------- 
10.1136/bmjopen-2019-033139  |    Introduction:  Around 70% to 80% of the 19 million visually disabled children in the world are due to a preventable or curable disease, if detected early enough. Vision screening in childhood is an evidence-based and cost-effective way to detect visual disorders. However, current screening programmes face several limitations: training required to perform them efficiently, lack of accurate screening tools and poor collaboration from young children.Some of these limitations can be overcome by new digital tools. Implementing a system based on artificial intelligence systems avoid the challenge of interpreting visual outcomes.The objective of the TrackAI Project is to develop a system to identify children with visual disorders. The system will have two main components: a novel visual test implemented in a digital device, DIVE (Device for an Integral Visual Examination); and artificial intelligence algorithms that will run on a smartphone to analyse automatically the visual data gathered by DIVE. 
  Methods and analysis:  This is a multicentre study, with at least five centres located in five geographically diverse study sites participating in the recruitment, covering Europe, USA and Asia.The study will include children aged between 6 months and 14 years, both with normal or abnormal visual development.The project will be divided in two consecutive phases: design and training of an artificial intelligence (AI) algorithm to identify visual problems, and system development and validation. The study protocol will consist of a comprehensive ophthalmological examination, performed by an experienced paediatric ophthalmologist, and an exam of the visual function using a DIVE.For the first part of the study, diagnostic labels will be given to each DIVE exam to train the neural network. For the validation, diagnosis provided by ophthalmologists will be compared with AI system outcomes. 
  Ethics and dissemination:  The study will be conducted in accordance with the principles of Good Clinical Practice. This protocol was approved by the Clinical Research Ethics Committee of Aragón, CEICA, on January 2019 (Code PI18/346).Results will be published in peer-reviewed journals and disseminated in scientific meetings. 
  Trial registration number:  ISRCTN17316993. 
  |  http://bmjopen.bmj.com/cgi/pmidlookup?view=long&pmid=32071178  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32071178/  |  
------------------------------------------- 
10.1371/journal.pone.0229596  |   Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms' decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4-6 residents) and 22 novice participants (10 PGY 1-3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm. 
  |  http://dx.plos.org/10.1371/journal.pone.0229596  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32106247/  |  
------------------------------------------- 
10.4103/idoj.IDOJ_241_19  |   Teledermatology is one of the most important and commonly employed subsets of telemedicine, a special alternative to face-to-face (FTF) doctor--patient consultation that refers to the use of electronic telecommunication tools to facilitate the provision of healthcare between the "seeker" and "provider." It is used for consultation, education, second opinion, and monitoring medical conditions. This article will review basic concepts, the integration of noninvasive imaging technique images, artificial intelligence, and the current ethical and legal issues. 
  |  http://www.idoj.in/article.asp?issn=2229-5178;year=2020;volume=11;issue=1;spage=12;epage=20;aulast=Pasquali  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32055502/  |  
------------------------------------------- 
10.1007/s11547-020-01205-y  |    Purpose:  To report the results of a nationwide online survey on artificial intelligence (AI) among radiologist members of the Italian Society of Medical and Interventional Radiology (SIRM). 
  Methods and materials:  All members were invited to the survey as an initiative by the Imaging Informatics Chapter of SIRM. The survey consisted of 13 questions about the participants' demographic information, perceived advantages and issues related to AI implementation in radiological practice, and their overall opinion about AI. 
  Results:  In total, 1032 radiologists (equaling 9.5% of active SIRM members for the year 2019) joined the survey. Perceived AI advantages included a lower diagnostic error rate (750/1027, 73.0%) and optimization of radiologists' work (697/1027, 67.9%). The risk of a poorer professional reputation of radiologists compared with non-radiologists (617/1024, 60.3%), and increased costs and workload due to AI system maintenance and data analysis (399/1024, 39.0%) were seen as potential issues. Most radiologists stated that specific policies should regulate the use of AI (933/1032, 90.4%) and were not afraid of losing their job due to it (917/1032, 88.9%). Overall, 77.0% of respondents (794/1032) were favorable to the adoption of AI, whereas 18.0% (186/1032) were uncertain and 5.0% (52/1032) were unfavorable. 
  Conclusions:  Radiologists had a mostly positive attitude toward the implementation of AI in their working practice. They were not concerned that AI will replace them, but rather that it might diminish their professional reputation. 
  |  https://dx.doi.org/10.1007/s11547-020-01205-y  |  
------------------------------------------- 
10.1097/MOU.0000000000000692  |    Purpose of review:  This review aims to draw a road-map to the use of artificial intelligence in an era of robotic surgery and highlight the challenges inherent to this process. 
  Recent findings:  Conventional mechanical robots function by transmitting actions of the surgeon's hands to the surgical target through the tremor-filtered movements of surgical instruments. Similarly, the next iteration of surgical robots conform human-initiated actions to a personalized surgical plan leveraging 3D digital segmentation generated prior to surgery. The advancements in cloud computing, big data analytics, and artificial intelligence have led to increased research and development of intelligent robots in all walks of human life. Inspired by the successful application of deep learning, several surgical companies are joining hands with tech giants to develop intelligent surgical robots. We, hereby, highlight key steps in the handling and analysis of big data to build, define, and deploy deep-learning models for building autonomous robots. 
  Summary:  Despite tremendous growth of autonomous robotics, their entry into the operating room remains elusive. It is time that surgeons actively collaborate for the development of the next generation of intelligent robotic surgery. 
  |  http://dx.doi.org/10.1097/MOU.0000000000000692  |  
------------------------------------------- 
10.1016/j.jbspin.2020.03.016  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1297-319X(20)30063-4  |  
------------------------------------------- 
10.4103/ijo.IJO_1203_19  |    Purpose:  An observational study to assess the sensitivity and specificity of the Medios smartphone-based offline deep learning artificial intelligence (AI) software to detect diabetic retinopathy (DR) compared with the image diagnosis of ophthalmologists. 
  Methods:  Patients attending the outpatient services of a tertiary center for diabetes care underwent 3-field dilated retinal imaging using the Remidio NM FOP 10. Two fellowship-trained vitreoretinal specialists separately graded anonymized images and a patient-level diagnosis was reached based on grading of the worse eye. The images were subjected to offline grading using the Medios integrated AI-based software on the same smartphone used to acquire images. The sensitivity and specificity of the AI in detecting referable DR (moderate non-proliferative DR (NPDR) or worse disease) was compared to the gold standard diagnosis of the retina specialists. 
  Results:  Results include analysis of images from 297 patients of which 176 (59.2%) had no DR, 35 (11.7%) had mild NPDR, 41 (13.8%) had moderate NPDR, and 33 (11.1%) had severe NPDR. In addition, 12 (4%) patients had PDR and 36 (20.4%) had macular edema. Sensitivity and specificity of the AI in detecting referable DR was 98.84% (95% confidence interval [CI], 97.62-100%) and 86.73% (95% CI, 82.87-90.59%), respectively. The area under the curve was 0.92. The sensitivity for vision-threatening DR (VTDR) was 100%. 
  Conclusion:  The AI-based software had high sensitivity and specificity in detecting referable DR. Integration with the smartphone-based fundus camera with offline image grading has the potential for widespread applications in resource-poor settings. 
  |  http://www.ijo.in/article.asp?issn=0301-4738;year=2020;volume=68;issue=2;spage=391;epage=395;aulast=Sosale  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31957735/  |  
------------------------------------------- 
10.1007/s12178-020-09600-8  |    Purpose of review:  With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care. 
  Recent findings:  Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care. 
  |  https://dx.doi.org/10.1007/s12178-020-09600-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31983042/  |  
------------------------------------------- 
10.2214/AJR.20.23112  |   <b>OBJECTIVE.</b> Because CT plays an important role in diagnosis, isolation, treatment, and effective evaluation of coronavirus disease (COVID-19), infection prevention and control management of CT examination rooms is important. <b>CONCLUSION.</b> We describe modifications to the CT examination process, strict disinfection of examination rooms, arrangement of waiting areas, and efforts to increase radiographers' awareness of personal protection made at our institution during the COVID-19 outbreak. In addition, we discuss the potential of using artificial intelligence in imaging patients with contagious diseases. 
  |  http://www.ajronline.org/doi/full/10.2214/AJR.20.23112  |  
------------------------------------------- 
10.1007/s11019-019-09912-8  |   In an analysis of artificially intelligent systems for medical diagnostics and treatment planning we argue that patients should be able to exercise a right to withdraw from AI diagnostics and treatment planning for reasons related to (1) the physician's role in the patients' formation of and acting on personal preferences and values, (2) the bias and opacity problem of AI systems, and (3) rational concerns about the future societal effects of introducing AI systems in the health care sector. 
  |  https://doi.org/10.1007/s11019-019-09912-8  |  
------------------------------------------- 
10.1089/tmj.2019.0182  |   <b><i>Background:</i></b> <i>Research into interventions based on mobile health (m-Health) applications (apps) has attracted considerable attention among researchers; however, most previous studies have focused on research-led apps and their effectiveness when applied to overweight/obese adults. There remains a paucity of research on the attitudes of typical consumers toward the adoption of m-Health apps for weight management. This study adopted the tenets of the extended unified theory of acceptance and use of technology 2 (UTAUT2) as the theoretical foundation in developing a model that integrates personal innovativeness (PI) and network externality (NE) in seeking to identify the factors with the most pronounced effect on one's intention to use an artificial intelligence-powered weight loss and health management app.</i> <b><i>Materials and Methods:</i></b> <i>An online survey was conducted for Taiwanese participants aged ≥21 years from May 23 to June 30, 2018. Hypotheses were tested using structural equation modeling.</i> <b><i>Results:</i></b> <i>In the analysis of 458 responses, the proposed research model explained 75.5% of variance in behavioral intention (BI). Habit was the independent variable with the strongest performance in predicting user intention, followed by PI, NE, and performance expectancy (PE). Social influence weakly affects user intention through PE. In multi-group analysis, education was shown to exert a moderating influence on some of the relationships hypothesized in the model.</i> <b><i>Conclusions:</i></b> <i>The empirically validated model in this study provides insights into the primary determinants of user intention toward the adoption of m-Health app for weight loss and health management. The theoretical and practical implications are relevant to researchers seeking to extend the applicability of the UTAUT2 model to health apps as well as practitioners seeking to promote the adoption of m-Health apps. In the future, researchers could extend the model to assess the effects of BI on actual use behavior.</i> 
  |  https://www.liebertpub.com/doi/full/10.1089/tmj.2019.0182?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.4103/sjg.SJG_377_19  |    Background/aim:  To study the impact of computer-aided detection (CADe) system on the detection rate of polyps and adenomas in colonoscopy. 
  Materials and methods:  A total of 1026 patients were prospectively randomly scheduled for colonoscopy with (the CADe group, CADe) or without (the control group, CON) the aid of the CADe system, together with visual notification and voice alarm, so as to compare the detection rate of polyp. 
  Results:  Compared with group CON, the detection rate of adenomas increased in group CADe, the average number of adenomas increased, the number of small adenomas increased, the number of proliferative polyps increased, and the differences were statistically significant (P &lt; 0.001), but the comparison for the number of larger adenomas showed no significant difference between the groups (P&gt; 0.05). 
  Conclusions:  The CADe system is feasible for increasing the detection of polyps and adenomas in colonoscopy. 
  |  http://www.saudijgastro.com/article.asp?issn=1319-3767;year=2020;volume=26;issue=1;spage=13;epage=19;aulast=Liu  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31898644/  |  
------------------------------------------- 
10.1016/j.canlet.2020.03.032  |   Machine learning (ML) is a branch of artificial intelligence centered on algorithms which do not need explicit prior programming to function but automatically learn from available data, creating decision models to complete tasks. ML-based tools have numerous promising applications in several fields of medicine. Its use has grown following the increased availability of patient data due to technological advances such as digital health records and high-volume information extraction from medical images. Multiple ML algorithms have been proposed for applications in oncology. For instance, they have been employed for oncological risk assessment, automated segmentation, lesion detection, characterization, grading and staging, prediction of prognosis and therapy response. In the near future, ML could become essential part of every step of oncological screening strategies and patients' management thus leading to precision medicine. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0304-3835(20)30165-8  |  
------------------------------------------- 
10.1111/jdv.16371  |   The study by Udrea et al. provides encouraging results showing high sensitivity (95%) of a smartphone app to detect skin cancer.<sup>1</sup> As the authors correctly mentioned, these apps bear the potential to serve as a valuable tool for the early diagnosis of malignant/pre-malignant skin lesions and can also facilitate the proper use of the resources. The specificity (78.3%) of the smartphone app in the study, however, is concerning. 
  |  https://doi.org/10.1111/jdv.16371  |  
------------------------------------------- 
10.2196/17490  |    Background:  It is predicted that digital health technologies that incorporate artificial intelligence will transform health care delivery in the next decade. Little research has explored how emerging trends in artificial intelligence-driven digital health technologies may influence the relationship between nurses and patients. 
  Objective:  The purpose of this scoping review is to summarize the findings from 4 research questions regarding emerging trends in artificial intelligence-driven digital health technologies and their influence on nursing practice across the 5 domains outlined by the Canadian Nurses Association framework: administration, clinical care, education, policy, and research. Specifically, this scoping review will examine how emerging trends will transform the roles and functions of nurses over the next 10 years and beyond. 
  Methods:  Using an established scoping review methodology, MEDLINE, Cumulative Index to Nursing and Allied Health Literature, Embase, PsycINFO, Cochrane Database of Systematic Reviews, Cochrane Central, Education Resources Information Centre, Scopus, Web of Science, and Proquest databases were searched. In addition to the electronic database searches, a targeted website search will be performed to access relevant grey literature. Abstracts and full-text studies will be independently screened by 2 reviewers using prespecified inclusion and exclusion criteria. Included literature will focus on nursing and digital health technologies that incorporate artificial intelligence. Data will be charted using a structured form and narratively summarized. 
  Results:  Electronic database searches have retrieved 10,318 results. The scoping review and subsequent briefing paper will be completed by the fall of 2020. 
  Conclusions:  A symposium will be held to share insights gained from this scoping review with key thought leaders and a cross section of stakeholders from administration, clinical care, education, policy, and research as well as patient advocates. The symposium will provide a forum to explore opportunities for action to advance the future of nursing in a technological world and, more specifically, nurses' delivery of compassionate care in the age of artificial intelligence. Results from the symposium will be summarized in the form of a briefing paper and widely disseminated to relevant stakeholders. 
  International registered report identifier (irrid):  DERR1-10.2196/17490. 
  |  https://www.researchprotocols.org/2020/4/e17490/  |  
------------------------------------------- 
10.1080/14767058.2020.1722995  |   <b>Background:</b> Advances in omics and computational Artificial Intelligence (AI) have been said to be key to meeting the objectives of precision cardiovascular medicine. The focus of precision medicine includes a better assessment of disease risk and understanding of disease mechanisms. Our objective was to determine whether significant epigenetic changes occur in isolated, non-syndromic CoA. Further, we evaluated the AI analysis of DNA methylation for the prediction of CoA.<b>Methods:</b> Genome-wide DNA methylation analysis of newborn blood DNA was performed in 24 isolated, non-syndromic CoA cases and 16 controls using the Illumina HumanMethylation450 BeadChip arrays. Cytosine nucleotide (CpG) methylation changes in CoA in each of 450,000 CpG loci were determined. Ingenuity pathway analysis (IPA) was performed to identify molecular and disease pathways that were epigenetically dysregulated. Using methylation data, six artificial intelligence (AI) platforms including deep learning (DL) was used for CoA detection.<b>Results:</b> We identified significant (FDR <i>p</i>-value ≤ .05) methylation changes in 65 different CpG sites located in 75 genes in CoA subjects. DL achieved an AUC (95% CI) = 0.97 (0.80-1) with 95% sensitivity and 98% specificity. Gene ontology (GO) analysis yielded epigenetic alterations in important cardiovascular developmental genes and biological processes: abnormal morphology of cardiovascular system, left ventricular dysfunction, heart conduction disorder, thrombus formation, and coronary artery disease.<b>Conclusion:</b> In an exploratory study we report the use of AI and epigenomics to achieve important objectives of precision cardiovascular medicine. Accurate prediction of CoA was achieved using a newborn blood spot. Further, we provided evidence of a significant epigenetic etiology in isolated CoA development. 
  |  http://www.tandfonline.com/doi/full/10.1080/14767058.2020.1722995  |  
------------------------------------------- 
10.3390/ijerph17061982  |   The rising prevalence and global burden of diabetes fortify the need for more comprehensive and effective management to prevent, monitor, and treat diabetes and its complications. Applying artificial intelligence in complimenting the diagnosis, management, and prediction of the diabetes trajectory has been increasingly common over the years. This study aims to illustrate an inclusive landscape of application of artificial intelligence in diabetes through a bibliographic analysis and offers future direction for research. Bibliometrics analysis was combined with exploratory factor analysis and latent Dirichlet allocation to uncover emergent research domains and topics related to artificial intelligence and diabetes. Data were extracted from the Web of Science Core Collection database. The results showed a rising trend in the number of papers and citations concerning AI applications in diabetes, especially since 2010. The nucleus driving the research and development of AI in diabetes is centered around developed countries, mainly consisting of the United States, which contributed 44.1% of the publications. Our analyses uncovered the top five emerging research domains to be: (i) use of artificial intelligence in diagnosis of diabetes, (ii) risk assessment of diabetes and its complications, (iii) role of artificial intelligence in novel treatments and monitoring in diabetes, (iv) application of telehealth and wearable technology in the daily management of diabetes, and (v) robotic surgical outcomes with diabetes as a comorbid. Despite the benefits of artificial intelligence, challenges with system accuracy, validity, and confidentiality breach will need to be tackled before being widely applied for patients' benefits. 
  |  http://www.mdpi.com/resolver?pii=ijerph17061982  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32192211/  |  
------------------------------------------- 
10.1038/s41467-020-15728-5  |   We constructed an intelligent cloud lab that integrates lab automation with cloud servers and artificial intelligence (AI) to detect chirality in perovskites. Driven by the materials acceleration operating system in cloud (MAOSIC) platform, on-demand experimental design by remote users was enabled in this cloud lab. By employing artificial intelligence of things (AIoT) technology, synthesis, characterization, and parameter optimization can be autonomously achieved. Through the remote collaboration of researchers, optically active inorganic perovskite nanocrystals (IPNCs) were first synthesized with temperature-dependent circular dichroism (CD) and inversion control. The inter-structure (structural patterns) and intra-structure (screw dislocations) dual-pattern-induced mechanisms detected by MAOSIC were comprehensively investigated, and offline theoretical analysis revealed the thermodynamic mechanism inside the materials. This self-driving cloud lab enables efficient and reliable collaborations across the world, reduces the setup costs of in-house facilities, combines offline theoretic analysis, and is practical for accelerating the speed of material discovery. 
  |  http://dx.doi.org/10.1038/s41467-020-15728-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32341340/  |  
------------------------------------------- 
10.3390/brainsci10030183  |   Alzheimer's disease (AD) imposes a considerable burden on those diagnosed. Faced with a neurodegenerative decline for which there is no effective cure or prevention method, sufferers of the disease are subject to judgement, both self-imposed and otherwise, that can have a great deal of effect on their lives. The burden of this stigma is more than just psychological, as reluctance to face an AD diagnosis can lead people to avoid early diagnosis, treatment, and research opportunities that may be beneficial to them, and that may help progress towards fighting AD and its progression. In this review, we discuss how recent advents in information technology may be employed to help fight this stigma. Using artificial intelligence (AI) technologies, specifically natural language processing (NLP), to classify the sentiment and tone of texts, such as those of online posts on various social media sites, has proven to be an effective tool for assessing the opinions of the general public on certain topics. These tools can be used to analyze the public stigma surrounding AD. Additionally, there is much concern among individuals that an AD diagnosis, or evidence of pre-clinical AD such as a biomarker or imaging test results, may wind up unintentionally disclosed to an entity that may discriminate against them. The lackluster security record of many medical institutions justifies this fear to an extent. Adopting more secure and decentralized methods of data transfer and storage, and giving patients enhanced ability to control their own data, such as a blockchain-based method, may help to alleviate some of these fears. 
  |  http://www.mdpi.com/resolver?pii=brainsci10030183  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32210011/  |  
------------------------------------------- 
10.1007/s00330-020-06672-5  |   Artificial intelligence (AI) has the potential to significantly disrupt the way radiology will be practiced in the near future, but several issues need to be resolved before AI can be widely implemented in daily practice. These include the role of the different stakeholders in the development of AI for imaging, the ethical development and use of AI in healthcare, the appropriate validation of each developed AI algorithm, the development of effective data sharing mechanisms, regulatory hurdles for the clearance of AI algorithms, and the development of AI educational resources for both practicing radiologists and radiology trainees. This paper details these issues and presents possible solutions based on discussions held at the 2019 meeting of the International Society for Strategic Studies in Radiology. KEY POINTS: • Radiologists should be aware of the different types of bias commonly encountered in AI studies, and understand their possible effects. • Methods for effective data sharing to train, validate, and test AI algorithms need to be developed. • It is essential for all radiologists to gain an understanding of the basic principles, potentials, and limits of AI. 
  |  https://dx.doi.org/10.1007/s00330-020-06672-5  |  
------------------------------------------- 
10.3390/s20030625  |   Understanding relationships among multimodal data extracted from a smartphone-based electrochemiluminescence (ECL) sensor is crucial for the development of low-cost point-of-care diagnostic devices. In this work, artificial intelligence (AI) algorithms such as random forest (RF) and feedforward neural network (FNN) are used to quantitatively investigate the relationships between the concentration of Ru(bpy)32+ luminophore and its experimentally measured ECL and electrochemical data. A smartphone-based ECL sensor with Ru(bpy)32+/TPrA was developed using disposable screen-printed carbon electrodes. ECL images and amperograms were simultaneously obtained following 1.2-V voltage application. These multimodal data were analyzed by RF and FNN algorithms, which allowed the prediction of Ru(bpy)32+ concentration using multiple key features. High correlation (0.99 and 0.96 for RF and FNN, respectively) between actual and predicted values was achieved in the detection range between 0.02 µM and 2.5 µM. The AI approaches using RF and FNN were capable of directly inferring the concentration of Ru(bpy)32+ using easily observable key features. The results demonstrate that data-driven AI algorithms are effective in analyzing the multimodal ECL sensor data. Therefore, these AI algorithms can be an essential part of the modeling arsenal with successful application in ECL sensor data modeling. 
  |  http://www.mdpi.com/resolver?pii=s20030625  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31979213/  |  
------------------------------------------- 
10.3390/s20071822  |   Computer-aided diagnosis systems have been developed to assist doctors in diagnosing thyroid nodules to reduce errors made by traditional diagnosis methods, which are mainly based on the experiences of doctors. Therefore, the performance of such systems plays an important role in enhancing the quality of a diagnosing task. Although there have been the state-of-the art studies regarding this problem, which are based on handcrafted features, deep features, or the combination of the two, their performances are still limited. To overcome these problems, we propose an ultrasound image-based diagnosis of the malignant thyroid nodule method using artificial intelligence based on the analysis in both spatial and frequency domains. Additionally, we propose the use of weighted binary cross-entropy loss function for the training of deep convolutional neural networks to reduce the effects of unbalanced training samples of the target classes in the training data. Through our experiments with a popular open dataset, namely the thyroid digital image database (TDID), we confirm the superiority of our method compared to the state-of-the-art methods. 
  |  http://www.mdpi.com/resolver?pii=s20071822  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32218230/  |  
------------------------------------------- 
10.1016/j.acra.2019.08.018  |   As the first step in image interpretation is detection, an error in perception can prematurely end the diagnostic process leading to missed diagnoses. Because perceptual errors of this sort-"failure to detect"-are the most common interpretive error (and cause of litigation) in radiology, understanding the nature of perceptual expertise is essential in decreasing radiology's long-standing error rates. In this article, we review what constitutes a perceptual error, the existing models of radiologic image perception, the development of perceptual expertise and how it can be tested, perceptual learning methods in training radiologists, and why understanding perceptual expertise is still relevant in the era of artificial intelligence. Adding targeted interventions, such as perceptual learning, to existing teaching practices, has the potential to enhance expertise and reduce medical error. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30440-4  |  
------------------------------------------- 
10.1111/joim.13030  |   Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high-value machine learning applications include both model-based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour-infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology. 
  |  https://doi.org/10.1111/joim.13030  |  
------------------------------------------- 
10.1016/j.pcad.2020.03.003  |   There has been a tidal wave of recent interest in artificial intelligence (AI), machine learning and deep learning approaches in cardiovascular (CV) medicine. In the era of modern medicine, AI and electronic health records hold the promise to improve the understanding of disease conditions and bring a personalized approach to CV care. The field of CV imaging (CVI), incorporating echocardiography, cardiac computed tomography, cardiac magnetic resonance imaging and nuclear imaging, with sophisticated imaging techniques and high volumes of imaging data, is primed to be at the forefront of the revolution in precision cardiology. This review provides a contemporary overview of the CVI imaging applications of AI, including a critique of the strengths and potential limitations of deep learning approaches. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0033-0620(20)30060-8  |  
------------------------------------------- 
10.3390/jcm9041107  |   Kidney diseases form part of the major health burdens experienced all over the world. Kidney diseases are linked to high economic burden, deaths, and morbidity rates. The great importance of collecting a large quantity of health-related data among human cohorts, what scholars refer to as "big data", has increasingly been identified, with the establishment of a large group of cohorts and the usage of electronic health records (EHRs) in nephrology and transplantation. These data are valuable, and can potentially be utilized by researchers to advance knowledge in the field. Furthermore, progress in big data is stimulating the flourishing of artificial intelligence (AI), which is an excellent tool for handling, and subsequently processing, a great amount of data and may be applied to highlight more information on the effectiveness of medicine in kidney-related complications for the purpose of more precise phenotype and outcome prediction. In this article, we discuss the advances and challenges in big data, the use of EHRs and AI, with great emphasis on the usage of nephrology and transplantation. 
  |  http://www.mdpi.com/resolver?pii=jcm9041107  |  
------------------------------------------- 
10.4097/kja.20124  |   Machine learning (ML) is revolutionizing anesthesiology research. Unlike classical research methods that are largely inference-based, ML is geared more towards making accurate predictions. ML is a field of artificial intelligence concerned with developing algorithms and models to perform prediction tasks in the absence of explicit instructions. Most ML applications, despite being highly variable in the topics that they deal with, generally follow a common workflow. For classification tasks, a researcher typically tests various ML models and compares the predictive performance with the reference logistic regression model. The main advantage of ML is in its ability to deal with many features with complex interactions and its specific focus on maximizing predictive performance. However, the emphasis on data-driven prediction can sometimes neglect mechanistic understanding. This article mainly focuses on supervised ML as applied to electronic health records (EHR) data. The main limitation of EHR based studies is in the difficulty of establishing causal relationships. However, low cost and rich information content provide great potential to uncover hitherto unknown correlations. In this review, the basic concepts of ML are introduced along with important terms that any ML researcher should know. Practical tips regarding the choice of software and computing devices are provided. Towards the end, several examples of successful application of ML to anesthesiology are discussed. The goal of this article is to provide a basic roadmap to novice ML researchers working in the field of anesthesiology. 
  |  https://ekja.org/journal/view.php?doi=10.4097/kja.20124  |  
------------------------------------------- 
10.1016/j.vgie.2020.01.002  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2468-4481(20)30002-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32258840/  |  
------------------------------------------- 
10.1016/j.soncn.2020.151005  |    Objective:  To discuss the implications of electronic systems and regulations regarding the use of electronic systems implemented during the conduct of a clinical trial and identify the impact of such platforms on oncology nurses' responsible for providing care to the research participant. 
  Data sources:  Peer-reviewed journal articles, internet, book chapters, and white papers. 
  Conclusion:  Electronic systems are being increasingly used in the conduct of clinical research. Electronic systems enable the capability to streamline data transfer, remote enrollment capabilities, greater transparency of the trial conduct, improved research documentation, and clearer audit trails. The oncology nurse is at the center of implementation of electronic systems to support the conduct of clinical research and enables safe and effective care to the research participant. 
  Implications for nursing practice:  Oncology nurses are vital to the successful outcome of clinical research studies and are key members of the clinical research team. Electronic systems move beyond traditional data collection in clinical trials with multiple benefits. Such systems may enhance the successful completion and adherence of the clinical trial and maintain the safety of the individual consented to research trial. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0749-2081(20)30020-6  |  
------------------------------------------- 
10.1016/j.tins.2020.03.013  |   Artificial intelligence advances have led to robots endowed with increasingly sophisticated social abilities. These machines speak to our innate desire to perceive social cues in the environment, as well as the promise of robots enhancing our daily lives. However, a strong mismatch still exists between our expectations and the reality of social robots. We argue that careful delineation of the neurocognitive mechanisms supporting human-robot interaction will enable us to gather insights critical for optimising social encounters between humans and robots. To achieve this, the field must incorporate human neuroscience tools including mobile neuroimaging to explore long-term, embodied human-robot interaction in situ. New analytical neuroimaging approaches will enable characterisation of social cognition representations on a finer scale using sensitive and appropriate categorical comparisons (human, animal, tool, or object). The future of social robotics is undeniably exciting, and insights from human neuroscience research will bring us closer to interacting and collaborating with socially sophisticated robots. 
  |  None  |  
------------------------------------------- 
10.1177/0840470419869032  |   This article discusses the emerging role of Artificial Intelligence (AI) in the learning and professional development of healthcare professionals. It provides a brief history of AI, current and past applications in healthcare education and training, and discusses why and how health leaders can revolutionize education system practices using AI in healthcare education. It also discusses potential implications of AI on human educators like clinical educators and provides recommendations for health leaders to support the application of AI in the learning and professional development of healthcare professionals. 
  |  http://journals.sagepub.com/doi/full/10.1177/0840470419869032?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.clineuro.2020.105718  |    Objectives:  Machine Learning and Artificial Intelligence (AI) are rapidly growing in capability and increasingly applied to model outcomes and complications within medicine. In spinal surgery, post-operative surgical site infections (SSIs) are a rare, yet morbid complication. This paper applied AI to predict SSIs after posterior spinal fusions. 
  Patients and methods:  4046 posterior spinal fusions were identified at a single academic center. A Deep Neural Network DNN classification model was trained using 35 unique input variables The model was trained and tested using cross-validation, in which the data were randomly partitioned into training n = 3034 and testing n = 1012 datasets. Stepwise multivariate regression was further used to identify actual model weights based on predictions from our trained model. 
  Results:  The overall rate of infection was 1.5 %. The mean area under the curve (AUC), representing the accuracy of the model, across all 300 iterations was 0.775 (95 % CI [0.767,0.782]) with a median AUC of 0.787. The positive predictive value (PPV), representing how well the model predicted SSI when a patient had SSI, over all predictions was 92.56 % with a negative predictive value (NPV), representing how well the model predicted absence of SSI when a patient did not have SSI, of 98.45 %. In analyzing relative model weights, the five highest weighted variables were Congestive Heart Failure, Chronic Pulmonary Failure, Hemiplegia/Paraplegia, Multilevel Fusion and Cerebrovascular Disease respectively. Notable factors that were protective against infection were ICU Admission, Increasing Charlson Comorbidity Score, Race (White), and being male. Minimally invasive surgery (MIS) was also determined to be mildly protective. 
  Conclusion:  Machine learning and artificial intelligence are relevant and impressive tools that should be employed in the clinical decision making for patients. The variables with the largest model weights were primarily comorbidity related with the exception of multilevel fusion. Further study is needed, however, in order to draw any definitive conclusions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0303-8467(20)30061-5  |  
------------------------------------------- 
10.1016/j.clineuro.2020.105732  |    Objectives:  Neurosurgical audits are an important part of improving the safety, efficiency and quality of care but require considerable resources, time, and funding. To that end, the advent of the Artificial Intelligence-based algorithms offered a novel, more economically viable solution. The aim of the study was to evaluate whether the algorithm can indeed outperform humans in that task. 
  Patients &amp; methods:  Forty-six human students were invited to inspect the clinical notes of 45 medical outliers on a neurosurgical ward. The aim of the task was to produce a report containing a quantitative analysis of the scale of the problem (e.g. time to discharge) and a qualitative list of suggestions on how to improve the patient flow, quality of care, and healthcare costs. The Artificial Intelligence-based Frideswide algorithm (FwA) was used to analyse the same dataset. 
  Results:  The FwA produced 44 recommendations whilst human students reported an average of 3.89. The mean time to deliver the final report was 5.80 s for the FwA and 10.21 days for humans. The mean relative error for factual inaccuracy for humans was 14.75 % for total waiting times and 81.06 % for times between investigations. The report produced by the FwA was entirely factually correct. 13 out of 46 students submitted an unfinished audit, 3 out of 46 made an overdue submission. Thematic analysis revealed numerous internal contradictions of the recommendations given by human students. 
  Conclusion:  The AI-based algorithm can produce significantly more recommendations in shorter time. The audits conducted by the AI are more factually accurate (0 % error rate) and logically consistent (no thematic contradictions). This study shows that the algorithm can produce reliable neurosurgical audits for a fraction of the resources required to conduct it by human means. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0303-8467(20)30075-5  |  
------------------------------------------- 
10.5935/1518-0557.20200014  |   Based on growing demand for assisted reproduction technology, improved predictive models are required to optimize in vitro fertilization/intracytoplasmatic sperm injection strategies, prioritizing single embryo transfer. There are still several obstacles to overcome for the purpose of improving assisted reproductive success, such as intra- and inter-observer subjectivity in embryonic selection, high occurrence of multiple pregnancies, maternal and neonatal complications. Here, we compare studies that used several variables that impact the success of assisted reproduction, such as blastocyst morphology and morphokinetic aspects of embryo development as well as characteristics of the patients submitted to assisted reproduction, in order to predict embryo quality, implantation or live birth. Thereby, we emphasize the proposal of an artificial intelligence-based platform for a more objective method to predict live birth. 
  |  https://doi.org/10.5935/1518-0557.20200014  |  
------------------------------------------- 
10.1186/s12859-020-3425-x  |    Background:  Imaging mass spectrometry (imaging MS) is an enabling technology for spatial metabolomics of tissue sections with rapidly growing areas of applications in biology and medicine. However, imaging MS data is polluted with off-sample ions caused by sample preparation, particularly by the MALDI (matrix-assisted laser desorption/ionization) matrix application. Off-sample ion images confound and hinder statistical analysis, metabolite identification and downstream analysis with no automated solutions available. 
  Results:  We developed an artificial intelligence approach to recognize off-sample ion images. First, we created a high-quality gold standard of 23,238 expert-tagged ion images from 87 public datasets from the METASPACE knowledge base. Next, we developed several machine and deep learning methods for recognizing off-sample ion images. The following methods were able to reproduce expert judgements with a high agreement: residual deep learning (F1-score 0.97), semi-automated spatio-molecular biclustering (F1-score 0.96), and molecular co-localization (F1-score 0.90). In a test-case study, we investigated off-sample images corresponding to the most common MALDI matrix (2,5-dihydroxybenzoic acid, DHB) and characterized properties of matrix clusters. 
  Conclusions:  Overall, our work illustrates how artificial intelligence approaches enabled by open-access data, web technologies, and machine and deep learning open novel avenues to address long-standing challenges in imaging MS. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3425-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32245392/  |  
------------------------------------------- 
10.3390/ma13040938  |   Artificial Intelligence has found many applications in the last decade due to increased computing power. Artificial Neural Networks are inspired in the brain structure and consist in the interconnection of artificial neurons through artificial synapses in the so-called Deep Neural Networks (DNNs). Training these systems requires huge amounts of data and, after the network is trained, it can recognize unforeseen data and provide useful information. As far as the training is concerned, we can distinguish between supervised and unsupervised learning. The former requires labelled data and is based on the iterative minimization of the output error using the stochastic gradient descent method followed by the recalculation of the strength of the synaptic connections (weights) with the backpropagation algorithm. On the other hand, unsupervised learning does not require data labeling and it is not based on explicit output error minimization. Conventional ANNs can function with supervised learning algorithms (perceptrons, multi-layer perceptrons, convolutional networks, etc.) but also with unsupervised learning rules (Kohonen networks, self-organizing maps, etc.). Besides, another type of neural networks are the so-called Spiking Neural Networks (SNNs) in which learning takes place through the superposition of voltage spikes launched by the neurons. Their behavior is much closer to the brain functioning mechanisms they can be used with supervised and unsupervised learning rules. Since learning and inference is based on short voltage spikes, energy efficiency improves substantially. Up to this moment, all these ANNs (spiking and conventional) have been implemented as software tools running on conventional computing units based on the von Neumann architecture. However, this approach reaches important limits due to the required computing power, physical size and energy consumption. This is particularly true for applications at the edge of the internet. Thus, there is an increasing interest in developing AI tools directly implemented in hardware for this type of applications. The first hardware demonstrations have been based on Complementary Metal-Oxide-Semiconductor (CMOS) circuits and specific communication protocols. However, to further increase training speed andenergy efficiency while reducing the system size, the combination of CMOS neuron circuits with memristor synapses is now being explored. It has also been pointed out that the short time non-volatility of some memristors may even allow fabricating purely memristive ANNs. The memristor is a new device (first demonstrated in solid-state in 2008) which behaves as a resistor with memory and which has been shown to have potentiation and depression properties similar to those of biological synapses. In this Special Issue, we explore the state of the art of neuromorphic circuits implementing neural networks with memristors for AI applications. 
  |  http://www.mdpi.com/resolver?pii=ma13040938  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093164/  |  
------------------------------------------- 
10.1111/jgh.15070  |    Background and aim:  The utility of artificial intelligence (AI) in colonoscopy has gained popularity in current times. Recent trials have evaluated the efficacy of deep convolutional neural network (DCNN)-based AI system in colonoscopy for improving adenoma detection rate (ADR) and polyp detection rate (PDR). We performed a systematic review and meta-analysis of the available studies to assess the impact of DCNN-based AI-assisted colonoscopy in improving the ADR and PDR. 
  Methods:  We queried the following database for this study: PubMed, Embase, Cochrane Library, Web of Sciences, and Computers and Applied Sciences. We only included randomized controlled trials that compared AI colonoscopy to standard colonoscopy (SC). Our outcomes included ADR and PDR. Risk ratios (RR) with 95% confidence interval (CI) were calculated using random effects model and DerSimonian-Laird approach for each outcome. 
  Results:  A total of three studies with 2815 patients (1415 in SC group and 1400 in AI group) were included. AI colonoscopy resulted in significantly improved ADR (32.9% vs 20.8%, RR: 1.58, 95% CI 1.39-1.80, P = &lt; 0.001) and PDR (43.0% vs 27.8%, RR: 1.55, 95% CI 1.39-1.72, P = &lt; 0.001) compared with SC. 
  Conclusion:  Given the results and limitations, the utility of AI colonoscopy holds promise and should be evaluated in more randomized controlled trials across different population, especially in patients solely undergoing colonoscopy for screening purpose as improved ADR will ultimately help in reducing incident colorectal cancer. 
  |  https://doi.org/10.1111/jgh.15070  |  
------------------------------------------- 
10.3390/jcm9010248  |   The use of artificial intelligence (AI) to support clinical medical decisions is a rather promising concept. There are two important factors that have driven these advances: the availability of data from electronic health records (EHR) and progress made in computational performance. These two concepts are interrelated with respect to complex mathematical functions such as machine learning (ML) or neural networks (NN). Indeed, some published articles have already demonstrated the potential of these approaches in medicine. When considering the diagnosis and management of pneumonia, the use of AI and chest X-ray (CXR) images primarily have been indicative of early diagnosis, prompt antimicrobial therapy, and ultimately, better prognosis. Coupled with this is the growing research involving empirical therapy and mortality prediction, too. Maximizing the power of NN, the majority of studies have reported high accuracy rates in their predictions. As AI can handle large amounts of data and execute mathematical functions such as machine learning and neural networks, AI can be revolutionary in supporting the clinical decision-making processes. In this review, we describe and discuss the most relevant studies of AI in pneumonia. 
  |  http://www.mdpi.com/resolver?pii=jcm9010248  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31963480/  |  
------------------------------------------- 
10.1016/j.breast.2019.12.014  |   Although computers have had a role in interpretation of mammograms for at least two decades, their impact on performance has not lived up to expectations. However, in the last five years, the field of medical image analysis has undergone a revolution due to the introduction of deep learning convolutional neural networks - a form of artificial intelligence (AI). Because of their considerably higher performance compared to conventional computer aided detection methods, these AI algorithms have resulted in renewed interest in their potential for interpreting breast images in stand-alone mode. For this, first the actual capability of the algorithms, compared to breast radiologists, needs to be well understood. Although early studies have pointed to the comparable performance between AI systems and breast radiologists in interpreting mammograms, these comparisons have been performed in laboratory conditions with limited, enriched datasets. AI algorithms with performance comparable to breast radiologists could be used in a number of different ways, the most impactful being pre-selection, or triaging, of normal screening mammograms that would not need human interpretation. Initial studies evaluating this proposed use have shown very promising results, with the resulting accuracy of the complete screening process not being affected, but with a significant reduction in workload. There is a need to perform additional studies, especially prospective ones, with large screening data sets, to both gauge the actual stand-alone performance of these new algorithms, and the impact of the different implementation possibilities on screening programs. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9776(19)31221-4  |  
------------------------------------------- 
10.1016/j.wneu.2020.03.029  |    Background:  Artificial intelligence (AI) may favorably support surgeons but can result in concern among patients and their relatives. The aim of this study was to evaluate attitudes of patients and their relatives regarding use of AI in neurosurgery. 
  Methods:  In a 2-stage cross-sectional survey, a qualitative survey was administered to a focus group of former patients to investigate their perception of AI and its role in neurosurgery. Five themes were identified and used to generate a case-based quantitative survey administered to inpatients and their relatives over a 2-week period. Presented AI platforms were rated appropriate and acceptable using 5-point Likert scales. Demographic data were collected. χ<sup>2</sup> test was used to determine whether demographics influenced participants' attitudes. 
  Results:  In the first stage, 20 participants responded. Five themes were identified: interpretation of imaging (4/20; 20%), operative planning (5/20; 25%), real-time alert of potential complications (10/20; 50%), partially autonomous surgery (6/20; 30%), and fully autonomous surgery (3/20; 15%). In the second stage, 107 participants responded. Most thought it appropriate and acceptable to use AI for imaging interpretation (76.7%; 66.3%), operative planning (76.7%; 75.8%), real-time alert of potential complications (82.2%; 72.9%), and partially autonomous surgery (58%; 47.7%). Conversely, most did not think that fully autonomous surgery was appropriate (27.1%) or acceptable (17.7%). Demographics did not have a significant influence on perception. 
  Conclusions:  Most patients and their relatives believed that AI has a role in neurosurgery and found it acceptable. Notable exceptions were fully autonomous systems, with most wanting the neurosurgeon ultimately to remain in control. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1878-8750(20)30497-6  |  
------------------------------------------- 
10.1371/journal.pone.0232391  |   The 2019 novel coronavirus (renamed SARS-CoV-2, and generally referred to as the COVID-19 virus) has spread to 184 countries with over 1.5 million confirmed cases. Such major viral outbreaks demand early elucidation of taxonomic classification and origin of the virus genomic sequence, for strategic planning, containment, and treatment. This paper identifies an intrinsic COVID-19 virus genomic signature and uses it together with a machine learning-based alignment-free approach for an ultra-fast, scalable, and highly accurate classification of whole COVID-19 virus genomes. The proposed method combines supervised machine learning with digital signal processing (MLDSP) for genome analyses, augmented by a decision tree approach to the machine learning component, and a Spearman's rank correlation coefficient analysis for result validation. These tools are used to analyze a large dataset of over 5000 unique viral genomic sequences, totalling 61.8 million bp, including the 29 COVID-19 virus sequences available on January 27, 2020. Our results support a hypothesis of a bat origin and classify the COVID-19 virus as Sarbecovirus, within Betacoronavirus. Our method achieves 100% accurate classification of the COVID-19 virus sequences, and discovers the most relevant relationships among over 5000 viral genomes within a few minutes, ab initio, using raw DNA sequence data alone, and without any specialized biological knowledge, training, gene or genome annotations. This suggests that, for novel viral and pathogen genome sequences, this alignment-free whole-genome machine-learning approach can provide a reliable real-time option for taxonomic classification. 
  |  http://dx.plos.org/10.1371/journal.pone.0232391  |  
------------------------------------------- 
10.1038/s41467-019-14225-8  |   Infections have become the major cause of morbidity and mortality among patients with chronic lymphocytic leukemia (CLL) due to immune dysfunction and cytotoxic CLL treatment. Yet, predictive models for infection are missing. In this work, we develop the CLL Treatment-Infection Model (CLL-TIM) that identifies patients at risk of infection or CLL treatment within 2 years of diagnosis as validated on both internal and external cohorts. CLL-TIM is an ensemble algorithm composed of 28 machine learning algorithms based on data from 4,149 patients with CLL. The model is capable of dealing with heterogeneous data, including the high rates of missing data to be expected in the real-world setting, with a precision of 72% and a recall of 75%. To address concerns regarding the use of complex machine learning algorithms in the clinic, for each patient with CLL, CLL-TIM provides explainable predictions through uncertainty estimates and personalized risk factors. 
  |  http://dx.doi.org/10.1038/s41467-019-14225-8  |  http://europepmc.org/abstract/MED/31953409  |  
------------------------------------------- 
10.1016/j.earlhumdev.2020.105017  |   Artificial Intelligence (AI) is based on accurate decision-making processes which can be carried out independently by a machine. AI may be subdivided into strong AI (with consciousness and intentionality) and weak AI (lacking both and programmed to perform specific tasks only). With AI currently making rapid progress in all domains, including those of healthcare, physicians face possible competitors. Various types of AI programs are already available as consultants to the physician, and these help in medical diagnostics and treatment. At the time of writing, extant programs constitute weak AI. This paper will explore the development of AI and robotics in medicine, and will refer to Star Trek's "Emergency Medical Hologram", who is portrayed as a strong AI program. This paper will also briefly explore the issues pertaining to AI in the medical field and will show that weak AI should not only suffice in the demesne of healthcare, but may actually be more desirable than strong AI. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-3782(20)30148-1  |  
------------------------------------------- 
10.1371/journal.pone.0227481  |    Background:  Ischemic Stroke (IS) is a major disease which greatly threatens human health. Recent studies showed sex-specific outcomes and mechanisms of cerebral ischemic stroke. This study aimed to identify the key changes of gene expression between male and female IS in humans. 
  Methods:  Gene expression dataset GSE22255, including peripheral blood samples, was downloaded from the Gene Expression Omnibus (GEO) dataset. Differentially Expressed Genes (DEGs) with a LogFC&gt;1, and a P-value &lt;0.05 were screened by BioConductor R package and grouped in female, male and overlap DEGs for further bioinformatic analysis. Gene Ontology (GO) functional annotation, Protein-Protein Interaction (PPI) network, "Molecular Complex Detection" (MCODE) modules, CytoNCA (cytoscape network centrality analysis) essential genes and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway interrelation analysis were performed. 
  Results:  In a total of 54,665 genes, 185 (73 ups and 112 downs) DEGs in the female dataset, 461 DEGs (297 ups and 164 downs) in the male dataset, within which 118 DEGs overlapped (7 similar changes in female and male, 111 opposite changes in female and male) were obtained from the GSE22255 dataset. Female, male and overlapping DEGs enriched for similar cellular components and molecular function. Male DEGs enriched for divergent biological processes from female and overlapping DEGs. Sex-specific and overlapping DEGs were put into the PPI network. Overlapping genes such as IL6, presented opposite changes and were mainly involved in cytokine-cytokine receptor interactions, the TNF-signalling pathway, etc. 
  Conclusion:  The analysis of sex-specific DEGs from GEO human blood samples showed that not only specific but also opposite DEG alterations in the female and male stroke genome wide dataset. The results provided an overview of sex-specific mechanisms, which might provide insight into stroke and its biomarkers and lead to sex-specific prognosis and treatment strategies in future clinical practice. 
  |  http://dx.plos.org/10.1371/journal.pone.0227481  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31899762/  |  
------------------------------------------- 
10.1128/mSphere.00481-19  |   We conducted a global characterization of the microbial communities of shipping ports to serve as a novel system to investigate microbial biogeography. The community structures of port microbes from marine and freshwater habitats house relatively similar phyla, despite spanning large spatial scales. As part of this project, we collected 1,218 surface water samples from 604 locations across eight countries and three continents to catalogue a total of 20 shipping ports distributed across the East and West Coast of the United States, Europe, and Asia to represent the largest study of port-associated microbial communities to date. Here, we demonstrated the utility of machine learning to leverage this robust system to characterize microbial biogeography by identifying trends in biodiversity across broad spatial scales. We found that for geographic locations sharing similar environmental conditions, subpopulations from the dominant phyla of these habitats (<i>Actinobacteria</i>, <i>Bacteroidetes</i>, <i>Cyanobacteria</i>, and <i>Proteobacteria</i>) can be used to differentiate 20 geographic locations distributed globally. These results suggest that despite the overwhelming diversity within microbial communities, members of the most abundant and ubiquitous microbial groups in the system can be used to differentiate a geospatial location across global spatial scales. Our study provides insight into how microbes are dispersed spatially and robust methods whereby we can interrogate microbial biogeography.<b>IMPORTANCE</b> Microbes are ubiquitous throughout the world and are highly diverse. Characterizing the extent of variation in the microbial diversity across large geographic spatial scales is a challenge yet can reveal a lot about what biogeography can tell us about microbial populations and their behavior. Machine learning approaches have been used mostly to examine the human microbiome and, to some extent, microbial communities from the environment. Here, we display how supervised machine learning approaches can be useful to understand microbial biodiversity and biogeography using microbes from globally distributed shipping ports. Our findings indicate that the members of globally dominant phyla are important for differentiating locations, which reduces the reliance on rare taxa to probe geography. Further, this study displays how global biogeographic patterning of aquatic microbial communities (and other systems) can be assessed through populations of the highly abundant and ubiquitous taxa that dominant the system. 
  |  https://doi.org/10.1128/mSphere.00481-19  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31996419/  |  
------------------------------------------- 
10.1371/journal.pone.0227243  |   Low-phosphorus (LP) stress is a major factor limiting the growth and yield of soybean. Circular RNAs (circRNAs) are novel noncoding RNAs that play a crucial role in plant responses to abiotic stress. However, how LP stress mediates the biogenesis of circRNAs in soybean remains unclear. Here, to explore the response mechanisms of circRNAs to LP stress, the roots of two representative soybean genotypes with different P-use efficiency, Bogao (a LP-sensitive genotype) and Nannong 94156 (a LP-tolerant genotype), were used for the construction of RNA sequencing (RNA-seq) libraries and circRNA identification. In total, 371 novel circRNA candidates, including 120 significantly differentially expressed (DE) circRNAs, were identified across different P levels and genotypes. More DE circRNAs were significantly regulated by LP stress in Bogao than in NN94156, suggesting that the tolerant genotype was less affected by LP stress than the sensitive genotype was; in other words, NN94156 may have a better ability to maintain P homeostasis under LP stress. Moreover, a positive correlation was observed between the expression patterns of P stress-induced circRNAs and their circRNA-host genes. Gene Ontology (GO) enrichment analysis of these circRNA-host genes and microRNA (miRNA)-targeted genes indicated that these DE circRNAs were involved mainly in defense responses, ADP binding, nucleoside binding, organic substance catabolic processes, oxidoreductase activity, and signal transduction. Together, our results revealed that LP stress can significantly alter the genome-wide profiles of circRNAs and indicated that the regulation of circRNAs was both genotype and environment specific in response to LP stress. LP-induced circRNAs might provide a rich resource for LP-responsive circRNA candidates for future studies. 
  |  http://dx.plos.org/10.1371/journal.pone.0227243  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31961887/  |  
------------------------------------------- 
10.1016/S2468-1253(20)30049-2  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2468-1253(20)30049-2  |  
------------------------------------------- 
10.1371/journal.pntd.0007969  |    Background:  Dengue, chikungunya, and Zika are arboviruses of major global health concern. Decisions regarding the clinical management of suspected arboviral infection are challenging in resource-limited settings, particularly when deciding on patient hospitalization. The objective of this study was to determine if hospitalization of individuals with suspected arboviral infections could be predicted using subject intake data. 
  Methodology/principal findings:  Two prediction models were developed using data from a surveillance study in Machala, a city in southern coastal Ecuador with a high burden of arboviral infections. Data were obtained from subjects who presented at sentinel medical centers with suspected arboviral infection (November 2013 to September 2017). The first prediction model-called the Severity Index for Suspected Arbovirus (SISA)-used only demographic and symptom data. The second prediction model-called the Severity Index for Suspected Arbovirus with Laboratory (SISAL)-incorporated laboratory data. These models were selected by comparing the prediction ability of seven machine learning algorithms; the area under the receiver operating characteristic curve from the prediction of a test dataset was used to select the final algorithm for each model. After eliminating those with missing data, the SISA dataset had 534 subjects, and the SISAL dataset had 98 subjects. For SISA, the best prediction algorithm was the generalized boosting model, with an AUC of 0.91. For SISAL, the best prediction algorithm was the elastic net with an AUC of 0.94. A sensitivity analysis revealed that SISA and SISAL are not directly comparable to one another. 
  Conclusions/significance:  Both SISA and SISAL were able to predict arbovirus hospitalization with a high degree of accuracy in our dataset. These algorithms will need to be tested and validated on new data from future patients. Machine learning is a powerful prediction tool and provides an excellent option for new management tools and clinical assessment of arboviral infection. 
  |  http://dx.plos.org/10.1371/journal.pntd.0007969  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32059026/  |  
------------------------------------------- 
10.3389/fcvm.2020.00001  |   Cardiac imaging plays an important role in the diagnosis of cardiovascular disease (CVD). Until now, its role has been limited to visual and quantitative assessment of cardiac structure and function. However, with the advent of big data and machine learning, new opportunities are emerging to build artificial intelligence tools that will directly assist the clinician in the diagnosis of CVDs. This paper presents a thorough review of recent works in this field and provide the reader with a detailed presentation of the machine learning methods that can be further exploited to enable more automated, precise and early diagnosis of most CVDs. 
  |  https://doi.org/10.3389/fcvm.2020.00001  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32039241/  |  
------------------------------------------- 
10.1016/j.semcancer.2019.12.011  |   Knowledge of the underpinnings of cancer initiation, progression and metastasis has increased exponentially in recent years. Advanced "omics" coupled with machine learning and artificial intelligence (deep learning) methods have helped elucidate targets and pathways critical to those processes that may be amenable to pharmacologic modulation. However, the current anti-cancer therapeutic armamentarium continues to lag behind. As the cost of developing a new drug remains prohibitively expensive, repurposing of existing approved and investigational drugs is sought after given known safety profiles and reduction in the cost barrier. Notably, successes in oncologic drug repurposing have been infrequent. Computational in-silico strategies have been developed to aid in modeling biological processes to find new disease-relevant targets and discovering novel drug-target and drug-phenotype associations. Machine and deep learning methods have especially enabled leaps in those successes. This review will discuss these methods as they pertain to cancer biology as well as immunomodulation for drug repurposing opportunities in oncologic diseases. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1044-579X(19)30409-2  |  
------------------------------------------- 
10.3390/genes11020131  |   Inflammasomes are intracellular multiprotein complexes in the cytoplasm that regulate inflammation activation in the innate immune system in response to pathogens and to host self-derived molecules. Recent advances greatly improved our understanding of the activation of nucleotide-binding oligomerization domain-like receptor (NLR) family pyrin domain containing 3 (NLRP3) inflammasomes at the molecular level. The NLRP3 belongs to the subfamily of NLRP which activates caspase 1, thus causing the production of proinflammatory cytokines (interleukin 1β and interleukin 18) and pyroptosis. This inflammasome is involved in multiple neurodegenerative and metabolic disorders including Alzheimer's disease, multiple sclerosis, type 2 diabetes mellitus, and gout. Therefore, therapeutic targeting to the NLRP3 inflammasome complex is a promising way to treat these diseases. Recent research advances paved the way toward drug research and development using a variety of machine learning-based and artificial intelligence-based approaches. These state-of-the-art approaches will lead to the discovery of better drugs after the training of such a system. 
  |  http://www.mdpi.com/resolver?pii=genes11020131  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32012695/  |  
------------------------------------------- 
10.1001/jama.2020.2547  |    |  https://jamanetwork.com/journals/jama/fullarticle/10.1001/jama.2020.2547  |  
------------------------------------------- 
10.1136/bmj.m1312  |    |  http://www.bmj.com/cgi/pmidlookup?view=long&pmid=32238345  |  
------------------------------------------- 
10.1007/s00482-020-00468-8  |    Background:  The objective recording of subjectively experienced pain is a problem that has not been sufficiently solved to date. In recent years, data sets have been created to train artificial intelligence algorithms to recognize patterns of pain intensity. The multimodal recognition of pain with machine learning could provide a way to reduce an over- or undersupply of analgesics, explicitly in patients with limited communication skills. 
  Objectives:  This study investigated the methodology of automated multimodal recognition of pain intensity and modality using machine-learning techniques of artificial intelligence. Multimodal recognition rates of experimentally induced phasic electrical and heat pain stimuli were compared with uni- and bimodal recognition rates. 
  Material and methods:  On the basis of the X‑ITE Pain Database, healthy subjects were stimulated with phasic electro-induced pain and heat pain, and their corresponding pain responses were recorded with multimodal sensors (acoustic, video-based, physiological). After complex signal processing, machine-learning methods were used to calculate recognition rates with respect to pain intensity (baseline vs. pain threshold, pain tolerance, mean value of pain threshold and tolerance) and pain modality (electrical vs. heat). Finally, a statistical comparison of uni- vs. multimodal and bi- vs. multimodal detection rates was performed. 
  Results:  With few exceptions, multimodal recognition of pain intensity rates was statistically superior to unimodal recognition rates, regardless of the pain modality. Multimodal pain recognition distinguished significantly better between heat and electro-induced pain. Further, multimodal recognition rates were predominantly superior to bimodal recognition rates. 
  Conclusion:  Priority should be given to the multimodal approach to the recognition of pain intensity and modality compared with unimodality. Further clinical studies should clarify whether multimodal automated recognition of pain intensity and modality is in fact superior to bimodal recognition. 
  |  https://dx.doi.org/10.1007/s00482-020-00468-8  |  
------------------------------------------- 
10.1016/j.ebiom.2020.102710  |    Background:  We developed and validated an artificial intelligence (AI)-assisted prediction of preeclampsia applied to a nationwide health insurance dataset in Indonesia. 
  Methods:  The BPJS Kesehatan dataset have been preprocessed using a nested case-control design into preeclampsia/eclampsia (n = 3318) and normotensive pregnant women (n = 19,883) from all women with one pregnancy. The dataset provided 95 features consisting of demographic variables and medical histories started from 24 months to event and ended by delivery as the event. Six algorithms were compared by area under the receiver operating characteristics curve (AUROC) with a subgroup analysis by time to the event. We compared our model to similar prediction models from systematically reviewed studies. In addition, we conducted a text mining analysis based on natural language processing techniques to interpret our modeling results. 
  Findings:  The best model consisted of 17 predictors extracted by a random forest algorithm. Nine∼12 months to the event was the period that had the best AUROC in external validation by either geographical (0.88, 95% confidence interval (CI) 0.88-0.89) or temporal split (0.86, 95% CI 0.85-0.86). We compared this model to prediction models in seven studies from 869 records in PUBMED, EMBASE, and SCOPUS. This model outperformed the previous models in terms of the precision, sensitivity, and specificity in all validation sets. 
  Interpretation:  Our low-cost model improved preliminary prediction to decide pregnant women that will be predicted by the models with high specificity and advanced predictors. 
  Funding:  This work was supported by grant no. MOST108-2221-E-038-018 from the Ministry of Science and Technology of Taiwan. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2352-3964(20)30085-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32283530/  |  
------------------------------------------- 
10.1503/cmaj.191634  |    |  http://www.cmaj.ca/cgi/pmidlookup?view=long&pmid=31907234  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31907234/  |  
------------------------------------------- 
10.1007/s11547-020-01197-9  |   The COVID-19 pandemic started in Italy in February 2020 with an exponential growth that has exceeded the number of cases reported in China. Italian radiology departments found themselves at the forefront in the management of suspected and positive COVID cases, both in diagnosis, in estimating the severity of the disease and in follow-up. In this context SIRM recommends chest X-ray as first-line imaging tool, CT as additional tool that shows typical features of COVID pneumonia, and ultrasound of the lungs as monitoring tool. SIRM recommends, as high priority, to ensure appropriate sanitation procedures on the scan equipment after detecting any suspected or positive COVID-19 patients. In this emergency situation, several expectations have been raised by the scientific community about the role that artificial intelligence can have in improving the diagnosis and treatment of coronavirus infection, and SIRM wishes to deliver clear statements to the radiological community, on the usefulness of artificial intelligence as a radiological decision support system in COVID-19 positive patients. (1) SIRM supports the research on the use of artificial intelligence as a predictive and prognostic decision support system, especially in hospitalized patients and those admitted to intensive care, and welcomes single center of multicenter studies for a clinical validation of the test. (2) SIRM does not support the use of CT with artificial intelligence for screening or as first-line test to diagnose COVID-19. (3) Chest CT with artificial intelligence cannot replace molecular diagnosis tests with nose-pharyngeal swab (rRT-PCR) in suspected for COVID-19 patients. 
  |  https://dx.doi.org/10.1007/s11547-020-01197-9  |  
------------------------------------------- 
10.1007/s00234-020-02424-w  |    Purpose:  To conduct a systematic review of the possibilities of artificial intelligence (AI) in neuroradiology by performing an objective, systematic assessment of available applications. To analyse the potential impacts of AI applications on the work of neuroradiologists. 
  Methods:  We identified AI applications offered on the market during the period 2017-2019. We systematically collected and structured information in a relational database and coded for the characteristics of the applications, their functionalities for the radiology workflow and their potential impacts in terms of 'supporting', 'extending' and 'replacing' radiology tasks. 
  Results:  We identified 37 AI applications in the domain of neuroradiology from 27 vendors, together offering 111 functionalities. The majority of functionalities 'support' radiologists, especially for the detection and interpretation of image findings. The second-largest group of functionalities 'extends' the possibilities of radiologists by providing quantitative information about pathological findings. A small but noticeable portion of functionalities seek to 'replace' certain radiology tasks. 
  Conclusion:  Artificial intelligence in neuroradiology is not only in the stage of development and testing but also available for clinical practice. The majority of functionalities support radiologists or extend their tasks. None of the applications can replace the entire radiology profession, but a few applications can do so for a limited set of tasks. Scientific validation of the AI products is more limited than the regulatory approval. 
  |  https://dx.doi.org/10.1007/s00234-020-02424-w  |  
------------------------------------------- 
10.1007/s11019-020-09939-2  |   Should we be allowed to refuse any involvement of artificial intelligence (AI) technology in diagnosis and treatment planning? This is the relevant question posed by Ploug and Holm in a recent article in Medicine, Health Care and Philosophy. In this article, I adhere to their conclusions, but not necessarily to the rationale that supports them. First, I argue that the idea that we should recognize this right on the basis of a rational interest defence is not plausible, unless we are willing to judge each patient's ideology or religion. Instead, I consider that the right must be recognized by virtue of values such as social pluralism or individual autonomy. Second, I point out that the scope of such a right should be limited at least under three circumstances: (1) if it is against a physician's obligation to not cause unnecessary harm to a patient or to not provide futile treatment, (2) in cases where the costs of implementing this right are too high, or (3) if recognizing the right would deprive other patients of their own rights to adequate health care. 
  |  https://doi.org/10.1007/s11019-020-09939-2  |  
------------------------------------------- 
10.3389/fonc.2020.00330  |   An increasing body of evidence supports the association of immune genes with tumorigenesis and prognosis of breast cancer (BC). This research aims at exploring potential regulatory mechanisms and identifying immunogenic prognostic markers for BC, which were used to construct a prognostic signature for disease-free survival (DFS) of BC based on artificial intelligence algorithms. Differentially expressed immune genes were identified between normal tissues and tumor tissues. Univariate Cox regression identified potential prognostic immune genes. Thirty-four transcription factors and 34 immune genes were used to develop an immune regulatory network. The artificial intelligence survival prediction system was developed based on three artificial intelligence algorithms. Multivariate Cox analyses determined 17 immune genes (ADAMTS8, IFNG, XG, APOA5, SIAH2, C2CD2, STAR, CAMP, CDH19, NTSR1, PCDHA1, AMELX, FREM1, CLEC10A, CD1B, CD6, and LTA) as prognostic biomarkers for BC. A prognostic nomogram was constructed on these prognostic genes. Concordance indexes were 0.782, 0.734, and 0.735 for 1-, 3-, and 5- year DFS. The DFS in high-risk group was significantly worse than that in low-risk group. Artificial intelligence survival prediction system provided three individual mortality risk predictive curves based on three artificial intelligence algorithms. In conclusion, comprehensive bioinformatics identified 17 immune genes as potential prognostic biomarkers, which might be potential candidates of immunotherapy targets in BC patients. The current study depicted regulatory network between transcription factors and immune genes, which was helpful to deepen the understanding of immune regulatory mechanisms for BC cancer. Two artificial intelligence survival predictive systems are available at https://zhangzhiqiao7.shinyapps.io/Smart_Cancer_Survival_Predictive_System_16_BC_C1005/ and https://zhangzhiqiao8.shinyapps.io/Gene_Survival_Subgroup_Analysis_16_BC_C1005/. These novel artificial intelligence survival predictive systems will be helpful to improve individualized treatment decision-making. 
  |  https://doi.org/10.3389/fonc.2020.00330  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296631/  |  
------------------------------------------- 
10.1016/j.hrthm.2020.01.034  |    Background:  Detection of atrial fibrillation (AF) occurrence over a long duration has been a challenge in the screening and follow-up of AF patients. Wearable devices may be an ideal solution. 
  Objective:  The purpose of this study was to measure the sensitivity, specificity, and accuracy of a recently developed smart wristband device that is equipped with both photoplethysmographic (PPG) and single-channel electrocardiogram (ECG) systems and an AF-identifying, artificial intelligence (AI) algorithm, used in the short term. 
  Methods:  Use of the Amazfit Health Band 1S, which records both PPG and single-channel ECG data, was assessed in 401 patients (251 normal individuals and 150 ECG-diagnosed AF patients). 
  Results:  ECG and PPG readings could not be judged in 15 and 18 subjects, respectively. Subjects who were unable to be judged were defined as either false negative or false positive. The sensitivity, specificity, and accuracy of wristband PPG readings were 88.00%, 96.41%, and 93.27%, respectively, and those of wristband ECG readings were 87.33%, 99.20%, and 94.76%, respectively. When the original wristband ECG records were judged by physicians, the sensitivity, specificity, and accuracy were 96.67%, 98.01%, and 97.51%, respectively. 
  Conclusion:  This promising new combination of PPG, ECG, and AI algorithm has the potential to facilitate AF detection. 
  |  None  |  
------------------------------------------- 
10.3389/fphar.2020.00069  |   Predicting protein-ligand interactions using artificial intelligence (AI) models has attracted great interest in recent years. However, data-driven AI models unequivocally suffer from a lack of sufficiently large and unbiased datasets. Here, we systematically investigated the data biases on the PDBbind and DUD-E datasets. We examined the model performance of atomic convolutional neural network (ACNN) on the PDBbind core set and achieved a Pearson R<sup>2</sup> of 0.73 between experimental and predicted binding affinities. Strikingly, the ACNN models did not require learning the essential protein-ligand interactions in complex structures and achieved similar performance even on datasets containing only ligand structures or only protein structures, while data splitting based on similarity clustering (protein sequence or ligand scaffold) significantly reduced the model performance. We also identified the property and topology biases in the DUD-E dataset which led to the artificially increased enrichment performance of virtual screening. The property bias in DUD-E was reduced by enforcing the more stringent ligand property matching rules, while the topology bias still exists due to the use of molecular fingerprint similarity as a decoy selection criterion. Therefore, we believe that sufficiently large and unbiased datasets are desirable for training robust AI models to accurately predict protein-ligand interactions. 
  |  https://doi.org/10.3389/fphar.2020.00069  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32161539/  |  
------------------------------------------- 
10.12968/hmed.2019.0322  |   The significance of early diagnosis for melanoma prognosis and survival cannot be understated. The public health benefits of melanoma prevention and detection have driven advances in diagnostics for skin cancer, particularly in the field of artificial intelligence. Evaluating the benefits and limitations of artificial intelligence in dermatology is paramount to its future development and clinical application. 
  |  http://www.magonlinelibrary.com/doi/full/10.12968/hmed.2019.0322?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2147/PGPM.S205082  |   The complexity of orphan diseases, which are those that do not have an effective treatment, together with the high dimensionality of the genetic data used for their analysis and the high degree of uncertainty in the understanding of the mechanisms and genetic pathways which are involved in their development, motivate the use of advanced techniques of artificial intelligence and in-depth knowledge of molecular biology, which is crucial in order to find plausible solutions in drug design, including drug repositioning. Particularly, we show that the use of robust deep sampling methodologies of the altered genetics serves to obtain meaningful results and dramatically decreases the cost of research and development in drug design, influencing very positively the use of precision medicine and the outcomes in patients. The target-centric approach and the use of strong prior hypotheses that are not matched against reality (disease genetic data) are undoubtedly the cause of the high number of drug design failures and attrition rates. Sampling and prediction under uncertain conditions cannot be avoided in the development of precision medicine. 
  |  https://dx.doi.org/10.2147/PGPM.S205082  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32256101/  |  
------------------------------------------- 
10.4103/jpi.jpi_63_19  |   Health care is undergoing a profound transformation driven by an increase in new types of diagnostic data, increased data sharing enabled by interoperability, and improvements in our ability to interpret data through the application of artificial intelligence and machine learning. Paradoxically, we are also discovering that our current paradigms for implementing electronic health-care records and our ability to create new models for reforming the health-care system have fallen short of expectations. This article traces these shortcomings to two basic issues. The first is a reliance on highly centralized quality improvement and measurement strategies that fail to account for the high level of variation and complexity found in human disease. The second is a reliance on legacy payment systems that fail to reward the sharing of data and knowledge across the health-care system. To address these issues, and to better harness the advances in health care noted above, the health-care system must undertake a phased set of reforms. First, efforts must focus on improving both the diagnostic process and data sharing at the local level. These efforts should include the formation of diagnostic management teams and increased collaboration between pathologists and radiologists. Next, building off current efforts to develop national federated research databases, providers must be able to query national databases when information is needed to inform the care of a specific complex patient. In addition, providers, when treating a specific complex patient, should be enabled to consult nationally with other providers who have experience with similar patient issues. The goal of these efforts is to build a health-care system that is funded in part by a novel fee-for-knowledge-sharing paradigm that fosters a collaborative decentralized approach to patient care and financially incentivizes large-scale data and knowledge sharing. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32175171/  |  
------------------------------------------- 
10.4097/kja.19475  |   Biosignals like electrocardiogram or photoplethysmogram have been widely used for monitoring and determining status of patients. However, it has been recently discovered that more information than that we have used traditionally were included in the biosignals after artificial intelligence (AI) was applied. Most meaningful advancement of current AI was in deep learning. The deep learning-based models show the best performance in most area in current due to the distinguished characteristic that it is able to extract important features from raw data. For that, deep learning extracts features in data by itself without feature engineering by human, if amount of data is enough for that. These AI-enabled feature give us opportunities to have a chance to see novel information which was hidden for many decades. It will be able to be used as digital biomarker for detecting or for predicting clinical outcome or event without further or more invasive evaluation. However, because the characteristics of deep learning is black box model, it is difficult to understand to use if users have the traditional view on the biosignals. For properly use of the novel information which is being discovered by AI and for adopting that in real clinical practice, clinicians need to basic knowledge on the AI and machine learning. This review covers from the basis of AI and machine learning for clinicians and its feasibilities in real practice within near future. 
  |  https://ekja.org/journal/view.php?doi=10.4097/kja.19475  |  
------------------------------------------- 
10.1016/j.hrcr.2019.12.013  |    |  https://linkinghub.elsevier.com/retrieve/pii/S2214-0271(19)30194-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32322497/  |  
------------------------------------------- 
10.1177/1359105320913954  |   Healthcare consumers are increasingly turning to online sources such as educational websites, forums and social media platforms to share their experiences with medical services and to demystify the uncertainties associated with undergoing various procedures. This study demonstrates a non-invasive way of understanding the feelings and emotions that consumers share via electronic word of mouth. By using IBM Watson, a content analysis tool that harnesses artificial intelligence, we show how a large amount of unstructured qualitative data can be transformed into quantitative data that can be subsequently analysed to generate novel insights into what patients are sharing about their healthcare experiences online. 
  |  http://journals.sagepub.com/doi/full/10.1177/1359105320913954?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1111/cyt.12799  |   Artificial intelligence (AI) technologies have the potential to transform cytopathology practice, and it is important for cytopathologists to embrace this and place themselves at the forefront of implementing these technologies in cytopathology. This review illustrates an archetypal AI workflow from project conception to implementation in a diagnostic setting and illustrates the cytopathologist's role and level of involvement at each stage of the process. Cytopathologists need to develop and maintain a basic understanding of AI, drive decisions regarding the development and implementation of AI in cytopathology, participate in the generation of datasets used to train and evaluate AI algorithms, understand how the performance of these algorithms is assessed, participate in the validation of these algorithms (either at a regulatory level or in the laboratory setting), and ensure continuous quality assurance of algorithms deployed in a diagnostic setting. In addition, cytopathologists should ensure that these algorithms are developed, trained, tested and deployed in an ethical manner. Cytopathologists need to become informed consumers of these AI algorithms by understanding their workings and limitations, how their performance is assessed and how to validate and verify their output in clinical practice. 
  |  https://doi.org/10.1111/cyt.12799  |  
------------------------------------------- 
10.1039/d0tb00061b  |   There is currently no effective treatment for acute myeloid leukemia, and surgery is also ineffective as an important treatment for most tumors. Rapidly developing artificial intelligence technology can be applied to different aspects of drug development, and it plays a key role in drug discovery. Based on network pharmacology and virtual screening, candidates were selected from the molecular database. Nine artificial intelligence algorithm models were used to further verify the candidates' potential. The 350 training results of the deep learning model showed higher credibility, and the R-square of the training set and test set of the optimal model reached 0.89 and 0.84, respectively. The random forest model has an R-square of 0.91 and a mean square error of only 0.003. The R-square of the Adaptive Boosting model and the Bagging model reached 0.92 and 0.88, respectively. Molecular dynamics simulation evaluated the stability of the ligand-protein complex and achieved good results. Artificial intelligence models had unearthed the promising candidates for STAT3 inhibitors, and the good performance of most models showed that they still had practical value on small data sets. 
  |  https://doi.org/10.1039/d0tb00061b  |  
------------------------------------------- 
10.1177/2192568219878133  |   As exponential expansion of computing capacity converges with unsustainable health care spending, a hopeful opportunity has emerged: the use of artificial intelligence to enhance health care quality and safety. These computer-based algorithms can perform the intricate and extremely complex mathematical operations of classification or regression on immense amounts of data to detect intricate and potentially previously unknown patterns in that data, with the end result of creating predictive models that can be utilized in clinical practice. Such models are designed to distinguish relevant from irrelevant data regarding a particular patient; choose appropriate perioperative care, intervention or surgery; predict cost of care and reimbursement; and predict future outcomes on a variety of anchored measures. If and when one is brought to fruition, an artificial intelligence platform could serve as the first legitimate clinical decision-making tool in spine care, delivering on the value equation while serving as a source for improving physician performance and promoting appropriate, efficient care in this era of financial uncertainty in health care. 
  |  https://journals.sagepub.com/doi/10.1177/2192568219878133?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0www.ncbi.nlm.nih.gov  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31934528/  |  
------------------------------------------- 
10.1016/j.autrev.2020.102508  |   The past decade has witnessed a significant paradigm shift in the clinical approach to autoimmune diseases, lead primarily by initiatives in precision medicine, precision health and precision public health initiatives. An understanding and pragmatic implementation of these approaches require an understanding of the drivers, gaps and limitations of precision medicine. Gaining the trust of the public and patients is paramount but understanding that technologies such as artificial intelligences and machine learning still require context that can only be provided by human input or what is called augmented machine learning. The role of genomics, the microbiome and proteomics, such as autoantibody testing, requires continuing refinement through research and pragmatic approaches to their use in applied precision medicine. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1568-9972(20)30060-4  |  
------------------------------------------- 
10.1007/s11899-020-00575-4  |    Purpose of review:  Artificial intelligence (AI), and in particular its subcategory machine learning, is finding an increasing number of applications in medicine, driven in large part by an abundance of data and powerful, accessible tools that have made AI accessible to a larger circle of investigators. 
  Recent findings:  AI has been employed in the analysis of hematopathological, radiographic, laboratory, genomic, pharmacological, and chemical data to better inform diagnosis, prognosis, treatment planning, and foundational knowledge related to benign and malignant hematology. As more widespread implementation of clinical AI nears, attention has also turned to the effects this will have on other areas in medicine. AI offers many promising tools to clinicians broadly, and specifically in the practice of hematology. Ongoing research into its various applications will likely result in an increasing utilization of AI by a broader swath of clinicians. 
  |  https://dx.doi.org/10.1007/s11899-020-00575-4  |  
------------------------------------------- 
10.1177/0956797620904985  |   Although more individuals are relying on information provided by nonhuman agents, such as artificial intelligence and robots, little research has examined how persuasion attempts made by nonhuman agents might differ from persuasion attempts made by human agents. Drawing on construal-level theory, we posited that individuals would perceive artificial agents at a low level of construal because of the agents' lack of autonomous goals and intentions, which directs individuals' focus toward <i>how</i> these agents implement actions to serve humans rather than <i>why</i> they do so. Across multiple studies (total <i>N</i> = 1,668), we showed that these construal-based differences affect compliance with persuasive messages made by artificial agents. These messages are more appropriate and effective when the message represents low-level as opposed to high-level construal features. These effects were moderated by the extent to which an artificial agent could independently learn from its environment, given that learning defies people's lay theories about artificial agents. 
  |  http://journals.sagepub.com/doi/full/10.1177/0956797620904985?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2174/1389201020666190904163003  |    Background &amp; objective:  Series of synthesized molecular compounds were considered as anti-breast cancer. The molecular descriptors which describe the microbial activities of the studied compounds were calculated using theoretical approach. 
  Methods:  The calculated parameters obtained EHOMO (eV), ELUMO (eV), dipole moment (Debye), log P, molecular weight (amu), HBA, HBD, Vol and Ovality were screened. The obtained calculated descriptors were used to develop QSAR model for prediction of experimental inhibition concentration (IC50) using SPSS and Gretl software packages for multiple linear regression (MLR) and MATLAB for the artificial neural network (ANN). 
  Results:  From this statistical analysis, MLR and ANN were observed to be predictive, however, ANNQSAR model predicted more efficiently than MLR. 
  Conclusion:  Furthermore, molecular docking study was executed with breast cancer cell line (PDB ID: 1hi7); it was observed that BS20 with binding energy of -7.0 kcal/mol bounded more efficiently than other compounds also, it inhibited more than the standard used (5-FU). 
  |  http://www.eurekaselect.com/174702/article  |  
------------------------------------------- 
10.1371/journal.pbio.3000583  |   We present Knowledge Engine for Genomics (KnowEnG), a free-to-use computational system for analysis of genomics data sets, designed to accelerate biomedical discovery. It includes tools for popular bioinformatics tasks such as gene prioritization, sample clustering, gene set analysis, and expression signature analysis. The system specializes in "knowledge-guided" data mining and machine learning algorithms, in which user-provided data are analyzed in light of prior information about genes, aggregated from numerous knowledge bases and encoded in a massive "Knowledge Network." KnowEnG adheres to "FAIR" principles (findable, accessible, interoperable, and reuseable): its tools are easily portable to diverse computing environments, run on the cloud for scalable and cost-effective execution, and are interoperable with other computing platforms. The analysis tools are made available through multiple access modes, including a web portal with specialized visualization modules. We demonstrate the KnowEnG system's potential value in democratization of advanced tools for the modern genomics era through several case studies that use its tools to recreate and expand upon the published analysis of cancer data sets. 
  |  http://dx.plos.org/10.1371/journal.pbio.3000583  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31971940/  |  
------------------------------------------- 
10.1007/978-3-030-34461-0_36  |   In neonatal intensive care units (NICUs), 87.5% of alarms by the monitoring system are false alarms, often caused by the movements of the neonates. Such false alarms are not only stressful for the neonates as well as for their parents and caregivers, but may also lead to longer response times in real critical situations. The aim of this project was to reduce the rates of false alarms by employing machine learning algorithms (MLA), which intelligently analyze data stemming from standard physiological monitoring in combination with cerebral oximetry data (in-house built, OxyPrem). 
  Materials &amp; methods:  Four popular MLAs were selected to categorize the alarms as false or real: (i) decision tree (DT), (ii) 5-nearest neighbors (5-NN), (iii) naïve Bayes (NB) and (iv) support vector machine (SVM). We acquired and processed monitoring data (median duration (SD): 54.6 (± 6.9) min) of 14 preterm infants (gestational age: 26 6/7 (± 2 5/7) weeks). A hybrid method of filter and wrapper feature selection generated the candidate subset for training these four MLAs. 
  Results:  A high specificity of &gt;99% was achieved by all four approaches. DT showed the highest sensitivity (87%). The cerebral oximetry data improved the classification accuracy. 
  Discussion &amp; conclusion:  Despite a (as yet) low amount of data for training, the four MLAs achieved an excellent specificity and a promising sensitivity. Presently, the current sensitivity is insufficient since, in the NICU, it is crucial that no real alarms are missed. This will most likely be improved by including more subjects and data in the training of the MLAs, which makes pursuing this approach worthwhile. 
  |  https://dx.doi.org/10.1007/978-3-030-34461-0_36  |  
------------------------------------------- 
10.1037/ccp0000451  |    Objective:  Research on predictors of treatment outcome in depression has largely derived from randomized clinical trials involving strict standardization of treatments, stringent patient exclusion criteria, and careful selection and supervision of study clinicians. The extent to which findings from such studies generalize to naturalistic psychiatric settings is unclear. This study sought to predict depression outcomes for patients seeking treatment within an intensive psychiatric hospital setting and while comparing the performance of a range of machine learning approaches. 
  Method:  Depressed patients (N = 484; ages 18-72; 89% White) receiving treatment within a psychiatric partial hospital program delivering pharmacotherapy and cognitive behavioral therapy were split into a training sample and holdout sample. First, within the training sample, 51 pretreatment variables were submitted to 13 machine learning algorithms to predict, via cross-validation, posttreatment Patient Health Questionnaire-9 depression scores. Second, the best performing modeling approach (lowest mean squared error; MSE) from the training sample was selected to predict outcome in the holdout sample. 
  Results:  The best performing model in the training sample was elastic net regularization (ENR; MSE = 20.49, R2 = .28), which had comparable performance in the holdout sample (MSE = 11.26; R2 = .38). There were 14 pretreatment variables that predicted outcome. To demonstrate the translation of an ENR model to personalized prediction of treatment outcome, a patient-specific prognosis calculator is presented. 
  Conclusions:  Informed by pretreatment patient characteristics, such predictive models could be used to communicate prognosis to clinicians and to guide treatment planning. Identified predictors of poor prognosis may suggest important targets for intervention. (PsycINFO Database Record (c) 2019 APA, all rights reserved). 
  |  http://content.apa.org/journals/ccp/88/1/25  |  
------------------------------------------- 
10.1038/s41467-020-14286-0  |   Chromosome arm aneuploidies (CAAs) are pervasive in cancers. However, how they affect cancer development, prognosis and treatment remains largely unknown. Here, we analyse CAA profiles of 23,427 tumours, identifying aspects of tumour evolution including probable orders in which CAAs occur and CAAs predicting tissue-specific metastasis. Both haematological and solid cancers initially gain chromosome arms, while only solid cancers subsequently preferentially lose multiple arms. 72 CAAs and 88 synergistically co-occurring CAA pairs multivariately predict good or poor survival for 58% of 6977 patients, with negligible impact of whole-genome doubling. Additionally, machine learning identifies 31 CAAs that robustly alter response to 56 chemotherapeutic drugs across cell lines representing 17 cancer types. We also uncover 1024 potential synthetic lethal pharmacogenomic interactions. Notably, in predicting drug response, CAAs substantially outperform mutations and focal deletions/amplifications combined. Thus, CAAs predict cancer prognosis, shape tumour evolution, metastasis and drug response, and may advance precision oncology. 
  |  http://dx.doi.org/10.1038/s41467-020-14286-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31974379/  |  
------------------------------------------- 
10.1097/MD.0000000000019512  |   To investigate the relationships between grip strengths and self-care activities in stroke patients using a non-linear support vector machine (SVM).Overall, 177 inpatients with poststroke hemiparesis were enrolled. Their grip strengths were measured using the Jamar dynamometer on the first day of rehabilitation training. Self-care activities were assessed by therapists using Functional Independence Measure (FIM), including items for eating, grooming, dressing the upper body, dressing the lower body, and bathing at the time of discharge. When each FIM item score was ≥6 points, the subject was considered independent. One thousand bootstrap grip strength datasets for each independence and dependence in self-care activities were generated from the actual grip strength. Thereafter, we randomly assigned the total bootstrap datasets to 90% training and 10% testing datasets and inputted the bootstrap training data into a non-linear SVM. After training, we used the SVM algorithm to predict a testing dataset for cross-validation. This validation procedure was repeated 10 times.The SVM with grip strengths more accurately predicted independence or dependence in self-care activities than the chance level (mean ± standard deviation of accuracy rate: eating, 0.71 ± 0.04, P &lt; .0001; grooming, 0.77 ± 0.03, P &lt; .0001; upper-body dressing, 0.75 ± 0.03, P &lt; .0001; lower-body dressing, 0.72 ± 0.05, P &lt; .0001; bathing, 0.68 ± 0.03, P &lt; .0001).Non-linear SVM based on grip strengths can prospectively predict self-care activities. 
  |  http://dx.doi.org/10.1097/MD.0000000000019512  |  
------------------------------------------- 
10.1037/ccp0000476  |    Objective:  Depression is a highly common mental disorder and a major cause of disability worldwide. Several psychological interventions are available, but there is a lack of evidence to decide which treatment works best for whom. This study aimed to identify subgroups of patients who respond differentially to cognitive-behavioral therapy (CBT) or person-centered counseling for depression (CfD). 
  Method:  This was a retrospective analysis of archival routine practice data for 1,435 patients who received either CBT (N = 1,104) or CfD (N = 331) in primary care. The main outcome was posttreatment reliable and clinically significant improvement (RCSI) in the PHQ-9 depression measure. A targeted prescription algorithm was developed in a training sample (N = 1,085) using a supervised machine learning approach (elastic net with optimal scaling). The clinical utility of the algorithm was examined in a statistically independent test sample (N = 350) using chi-square analysis and odds ratios. 
  Results:  Cases in the test sample that received their model-indicated "optimal" treatment had a significantly higher RCSI rate (62.5%) compared to those who received the "suboptimal" treatment (41.7%); χ2(df = 1) = 4.79, p = .03, OR = 2.33 (95% CI [1.09, 5.02]). 
  Conclusion:  Targeted prescription has the potential to make best use of currently available evidence-based treatments, improving outcomes for patients at no additional cost to psychological services. (PsycINFO Database Record (c) 2019 APA, all rights reserved). 
  |  http://content.apa.org/journals/ccp/88/1/14  |  
------------------------------------------- 
10.7150/ijms.42078  |   Artificial intelligence (AI), as an advanced science technology, has been widely used in medical fields to promote medical development, mainly applied to early detections, disease diagnoses, and management. Owing to the huge number of patients, kidney disease remains a global health problem. Challenges remain in its diagnosis and treatment. AI could take individual conditions into account, produce suitable decisions and promise to make great strides in kidney disease management. Here, we review the current studies of AI applications in kidney disease in alerting systems, diagnostic assistance, guiding treatment and evaluating prognosis. Although the number of studies related to AI applications in kidney disease is small, the potential of AI in the management of kidney disease is well recognized by clinicians; AI will greatly enhance clinicians' capacity in their clinical practice in the future. 
  |  http://www.medsci.org/v17p0970.htm  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32308551/  |  
------------------------------------------- 
10.1007/s12032-020-01374-w  |   Recent advances in computing capability allowed the development of sophisticated predictive models to assess complex relationships within observational data, described as Artificial Intelligence. Medicine is one of the several fields of application and Radiation oncology could benefit from these approaches, particularly in patients' medical records, imaging, baseline pathology, planning or instrumental data. Artificial Intelligence systems could simplify many steps of the complex workflow of radiotherapy such as segmentation, planning or delivery. However, Artificial Intelligence could be considered as a "black box" in which human operator may only understand input and output predictions and its application to the clinical practice remains a challenge. The low transparency of the overall system is questionable from manifold points of view (ethical included). Given the complexity of this issue, we collected the basic definitions to help the clinician to understand current literature, and overviewed experiences regarding implementation of AI within radiotherapy clinical workflow, aiming to describe this field from the clinician perspective. 
  |  https://dx.doi.org/10.1007/s12032-020-01374-w  |  
------------------------------------------- 
10.1016/j.gene.2020.144451  |   Cucumber mosaic virus (CMV) can cause serious losses in Luffa cylindrica (L.) Roem. Chemical application to control CMV is ineffective and environmentally unfriendly. The development of resistant hybrids is the best way to control CMV disease. Elucidating the virus-host interaction of CMV and molecular basis underlying Luffa spp. resistance against CMV would undoubtedly facilitate breeding for resistance against CMV disease. Transcriptome sequencing was used to analyze differentially expressed genes (DEGs) caused by CMV infection. A total of 138,336 unigenes were assembled, and 74,525 unigenes were annotated. Gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis indicated that the three major enrichment pathways (according to the p-values) were flavonoid biosynthesis, sulfur metabolism, and photosynthesis. Genes involved in basal defenses, probably R genes, were determined to be related to CMV resistance. Using quantitative real-time PCR, we validated the differential expression of 8 genes. A number of genes associated with CMV resistance were found in this study. This study provides transcriptomic information regarding CMV-Luffa spp. interactions and will shed light on our understanding of host-virus interactions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30120-7  |  
------------------------------------------- 
10.1007/s11906-019-1010-3  |   Artificial Intelligence (AI), although well established in many areas of everyday life, has only recently been trialed in the diagnosis and management of common clinical conditions. This editorial review highlights progress to date and suggests further improvements in and trials of AI in the management of conditions such as hypertension. 
  |  https://dx.doi.org/10.1007/s11906-019-1010-3  |  
------------------------------------------- 
10.2471/BLT.19.237487  |   The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed. 
 La perspective que les décisions prises par un outil clinique basé sur l'intelligence artificielle puissent porter préjudice aux patients est un concept dont les bonnes pratiques de responsabilité et de sécurité actuelles ne tiennent pas encore compte à travers le monde. Nous nous concentrons sur deux aspects qui caractérisent les décisions de l'intelligence artificielle à usage clinique : la responsabilité morale des préjudices aux patients, et la garantie de sécurité pour protéger les patients contre de tels préjudices. Les outils fondés sur l'intelligence artificielle remettent en cause les pratiques cliniques conventionnelles d'attribution des responsabilités et de garantie de la sécurité. Les décisions formulées par les systèmes d'intelligence artificielle sont de moins en moins soumises au contrôle des médecins et spécialistes de la sécurité, qui ne comprennent et ne maîtrisent pas toujours les subtilités régissant cette prise de décision. Nous illustrons notre analyse en l'appliquant à un exemple de système d'intelligence artificielle développé dans le cadre du traitement des infections. Le présent document se termine par une série de suggestions concrètes servant à identifier de nouveaux moyens de tempérer ces inquiétudes. Nous estimons qu'il est nécessaire d'inclure les développeurs à l'origine de l'intelligence artificielle ainsi que les spécialistes de la sécurité des systèmes dans notre évaluation de la responsabilité morale des préjudices causés aux patients. Car pour l'instant, aucun des acteurs impliqués dans le modèle ne remplit pleinement les conditions traditionnelles de responsabilité morale pour les décisions prises par un dispositif d'intelligence artificielle. Dans ce contexte, il est donc essentiel revoir notre conception de la responsabilité morale. Nous devons également passer d'un modèle de garantie statique à un modèle de garantie dynamique, et accepter que certains impératifs de sécurité ne puissent être entièrement résolus durant l'élaboration du système d'intelligence artificielle, avant sa mise en œuvre. 
 La perspectiva de que los pacientes sufran daños a causa de por las decisiones tomadas por un instrumento clínico de inteligencia artificial es un aspecto al que todavía no se han ajustado las prácticas actuales de responsabilidad y seguridad en todo el mundo. El presente documento se centra en dos aspectos de la inteligencia artificial clínica utilizada para la toma de decisiones: la responsabilidad moral por el daño causado a los pacientes y la garantía de seguridad para proteger a los pacientes contra dicho daño. Las herramientas de inteligencia artificial están desafiando las prácticas clínicas estándar de asignación de responsabilidades y de garantía de seguridad. Los médicos clínicos y los ingenieros de seguridad de las personas tienen menos control sobre las decisiones que adoptan por los sistemas de inteligencia artificial y menos conocimiento y comprensión de la forma precisa en que los sistemas de inteligencia artificial adoptan sus decisiones. Este análisis se ilustra aplicándolo a un ejemplo de un sistema de inteligencia artificial desarrollado para su uso en el tratamiento de la sepsis. El documento termina con sugerencias prácticas sobre las vías de acción para mitigar estas preocupaciones. Se sostiene la necesidad de incluir a los desarrolladores de inteligencia artificial y a los ingenieros de seguridad de sistemas en las evaluaciones de la responsabilidad moral por los daños causados a los pacientes. Entretanto, ninguno de los actores del modelo cumple sólidamente las condiciones tradicionales de responsabilidad moral por las decisiones de un sistema de inteligencia artificial. En consecuencia, se debería actualizar nuestra concepción de la responsabilidad moral en este contexto. También es preciso pasar de un modelo de garantía estático a uno dinámico, aceptando que las consideraciones de seguridad no se pueden resolver plenamente durante el diseño del sistema de inteligencia artificial antes de que el sistema sea implementado. 
 إن احتمال حدوث ضرر للمريض نتيجة للقرارات التي يتم اتخاذها بواسطة أداة سريرية تعتمد على الذكاء الاصطناعي، هو أمر لم يتم بعد ضبط ممارسات المساءلة والسلامة بالنسبة له في جميع أنحاء العالم. نحن نقوم بالتركيز على جانبين من الذكاء الاصطناعي السريري يتم استخدامهما لصنع القرار: المساءلة الأخلاقية عن الضرر الذي يلحق بالمرضى؛ وضمان السلامة لحماية المرضى من هذا الضرر. تتحدى الأدوات التي تعتمد على الذكاء الاصطناعي الممارسات السريرية القياسية الخاصة بتوجيه اللوم وضمان السلامة. الأطباء البشريون ومهندسو السلامة لديهم سيطرة أضعف على القرارات التي توصلت إليها أنظمة الذكاء الاصطناعي، كما أن لديهم معرفة وفهم أقل لكيفية وصول أنظمة الذكاء الاصطناعي بشكل محدد إلى هذه القرارات. نقوم بتوضيح هذا التحليل من خلال تطبيقه على مثال لنظام يعتمد على الذكاء الاصطناعي، تم تطويره للاستخدام في علاج الإنتان. يوجد بخاتمة الورقة اقتراحات عملية لطرق المضي قدما لتخفيف هذه المخاوف. نحن ندافع عن الحاجة لتضمين مطوري الذكاء الاصطناعي، ومهندسي سلامة النظم، في تقييماتنا للمساءلة الأخلاقية تجاه الأذى الذي يلحق بالمريض. وفي نفس الوقت، لا تفي أي من الجهات الفاعلة في هذا النموذج، بشكل فعال بالشروط التقليدية للمساءلة الأخلاقية عن القرارات التي يتخذها نظام الذكاء الاصطناعي. وبالتالي، يجب علينا تحديث ما لدينا من مفاهيم بشأن المساءلة الأخلاقية في هذا السياق. كما نحتاج كذلك إلى الانتقال من نموذج استاتيكي إلى نموذج ديناميكي للتأكيد، وقبول أن هذه الاعتبارات الخاصة بالسلامة ليست قابلة للحل بشكل كامل أثناء تصميم نظام الذكاء الاصطناعي، قبل نشر هذا النظام. 
 基于人工智能的临床工具所做的决定可能会对患者造成伤害，而目前世界范围内的问责和安全实践尚未对此作出调整。我们重点关注人工智能制定临床决策的两个方面问题：对患者造成伤害的道德问责；以及安全确保保护患者免受此类伤害。基于人工智能的工具正在挑战追究责任和确保安全的标准临床实践。临床医生和安全工程师对人工智能系统做出的决策的控制力较弱，在人工智能系统如何准确地做出决策方面的知识和理解水平也较低。我们通过一个用于治疗败血症的人工智能系统作为示例，来阐述这一分析。文章最后对如何缓解这些担忧提出了切实可行的建议。我们认为十分有必要让人工智能开发人员和系统安全工程师参与我们对患者伤害的道德问责评估中。同时，模型中没有任何参与者可以很好地满足人工智能系统决策中道德问责的传统条件。因此，我们应该在这方面更新我们的道德问责观念。我们还需要从静态的保障模式转变为动态的保障模式，尽管部署人工智能系统之前，人工智能系统设计过程中安全方面的顾虑尚未完全解决。. 
 Вероятность причинения вреда пациентам в результате принятия решений с помощью клинических инструментов, основанных на использовании искусственного интеллекта, пока что не учитывается в нынешних мировых практиках обеспечения прозрачности и безопасности. Авторы уделяют особое внимание двум аспектам применения искусственного интеллекта в клинической практике для принятия решений: моральной ответственности за вред, причиненный пациентам, и обеспечению безопасности для защиты пациентов от такого вреда. Инструменты, основанные на использовании искусственного интеллекта, бросают вызов стандартной клинической практике распределения ответственности и обеспечения безопасности. Лечащие врачи и инженеры по технике безопасности имеют небольшое влияние на решения, принимаемые с использованием систем искусственного интеллекта, и не обладают полными знаниями и пониманием тонкостей процесса принятия решений системами искусственного интеллекта. С целью наглядной демонстрации такого анализа авторы приводят пример системы на основе искусственного интеллекта, разработанной для использования при лечении сепсиса. В конце документа приводятся практические предложения по решению этих проблем. Авторы настаивают на необходимости включения разработчиков искусственного интеллекта и инженеров систем безопасности в процесс оценки моральной ответственности за вред, причиненный пациенту. Между тем ни один из участников модели полностью не удовлетворяет традиционным условиям, предъявляемым к моральной ответственности за решения, принимаемые системой искусственного интеллекта. В связи с этим необходимо пересмотреть понятие моральной ответственности в данном контексте. Требуется также перейти от статической к динамической модели обеспечения гарантий, признав, что невозможно полностью учесть соображения безопасности при разработке системы искусственного интеллекта до ее развертывания. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284648/  |  
------------------------------------------- 
10.1097/MD.0000000000019725  |   The aim of this study was to discriminate malignant and benign clinical T1 renal masses on routinely acquired computed tomography (CT) images using radiomics and machine learning techniques.Adult patients undergoing surgical resection and histopathological analysis of clinical T1 renal masses were included. Preoperative CT studies in venous phase from multiple referring centers were included, without restriction to specific CT scanners, slice thickness, or degrees of artifacts. Renal masses were segmented and 120 standardized radiomic features extracted. Machine learning algorithms were used to predict malignancy of renal masses using radiomics features and cross-validation. Diagnostic accuracy of machine learning models and assessment by independent blinded radiologists were compared based on the gold standard of histopathologic diagnosis.A total of 94 patients met inclusion criteria (benign renal masses: n = 18; malignant: n = 76). CT studies from 18 different scanners were assessed with median slice thickness of 2.5 mm and artifacts in 15 cases (15.9%).Area under the receiver-operating-characteristics curve (AUC) of random forest (random forest [RF], AUC = 0.83) was significantly higher compared to the radiologists (AUC = 0.68, P = .047). Sensitivity was significantly higher for RF versus radiologists (0.88 vs 0.80, P = .045), whereas specificity was numerically higher for RF (0.67 vs 0.50, P = .083).Although limited by an overall small sample size and few benign renal tumors, a radiomic features and machine learning approach suggests a high diagnostic accuracy for discrimination of malignant and benign clinical T1 renal masses on venous phase CT. The presented algorithm robustly outperforms human readers in a real-life scenario with nonstandardized imaging studies from various referring centers. 
  |  http://dx.doi.org/10.1097/MD.0000000000019725  |  
------------------------------------------- 
10.1038/s41591-019-0729-3  |   Disrupted molecular pathways are often robustly associated with disease outcome in cancer<sup>1-3</sup>. Although biologically informative transcriptional pathways can be revealed by RNA sequencing (RNA-seq) at up to hundreds of folds reduction in conventionally used coverage<sup>4-6</sup>, it remains unknown how low-depth sequencing datasets perform in the challenging context of developing transcriptional signatures to predict clinical outcomes. Here we assessed the possibility of cancer prognosis with shallow tumor RNA-seq, which would potentially enable cost-effective assessment of much larger numbers of samples for deeper biological and predictive insights. By statistically modeling the relative risk of an adverse outcome for thousands of subjects in The Cancer Genome Atlas<sup>7-13</sup>, we present evidence that subsampled tumor RNA-seq data with a few hundred thousand reads per sample provide sufficient information for outcome prediction in several types of cancer. Analysis of predictive models revealed robust contributions from pathways known to be associated with outcomes. Our findings indicate that predictive models of outcomes in cancer may be developed with dramatically increases in sample numbers at low cost, thus potentially enabling the development of more realistic predictive models that incorporate diverse variables and their interactions. This strategy could also be used, for example, in longitudinal analysis of multiple regions of a tumor alongside treatment for quantitative modeling and prediction of outcome in personalized oncology. 
  |  http://dx.doi.org/10.1038/s41591-019-0729-3  |  
------------------------------------------- 
10.1016/j.revmed.2019.12.014  |   Clinical reasoning is at the heart of physicians' competence, as it allows them to make diagnoses. However, diagnostic errors are common, due to the existence of reasoning biases. Artificial intelligence is undergoing unprecedented development in this context. It is increasingly seen as a solution to improve the diagnostic performance of physicians, or even to perform this task for them, in a totally autonomous and more efficient way. In order to understand the challenges associated with the development of artificial intelligence, it is important to understand how the machine works to make diagnoses, what are the similarities and differences with the physician's diagnostic reasoning, and what are the consequences for medical training and practice. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0248-8663(20)30002-3  |  
------------------------------------------- 
10.1080/17476348.2020.1743181  |   <b>Introduction</b>: The application of artificial intelligence (AI) and machine learning (ML) in medicine and in particular in respiratory medicine is an increasingly relevant topic.<b>Areas covered</b>: We aimed to identify and describe the studies published on the use of AI and ML in the field of respiratory diseases. The string '(((pulmonary) OR respiratory)) AND ((artificial intelligence) OR machine learning)' was used in PubMed as a search strategy. The majority of studies identified corresponded to the area of chronic obstructive pulmonary disease (COPD), in particular to COPD and chest computed tomography scans, interpretation of pulmonary function tests, exacerbations and treatment. Another field of interest is the application of AI and ML to the diagnosis of interstitial lung disease, and a few other studies were identified on the fields of mechanical ventilation, interpretation of images on chest X-ray and diagnosis of bronchial asthma.<b>Expert opinion</b>: ML may help to make clinical decisions but will not replace the physician completely. Human errors in medicine are associated with large financial losses, and many of them could be prevented with the help of AI and ML. AI is particularly useful in the absence of conclusive evidence of decision-making. 
  |  http://www.tandfonline.com/doi/full/10.1080/17476348.2020.1743181  |  
------------------------------------------- 
10.1007/s00261-020-02471-0  |   Artificial intelligence is a technique that holds promise for helping radiologists improve the care of our patients. At the same time, implementation decisions we make now can have a long-lasting effect on patient outcomes. In the following article, we discuss four areas with unique considerations for implementation of AI: bias, trust, risk, and design. In each section, we highlight applications of AI to abdominal imaging and prostate cancer specifically. 
  |  https://dx.doi.org/10.1007/s00261-020-02471-0  |  
------------------------------------------- 
10.1007/s43465-019-00023-3  |   Orthopaedics as a surgical discipline requires a combination of good clinical acumen, good surgical skill, a reasonable physical strength and most of all, good understanding of technology. The last few decades have seen rapid adoption of new technologies into orthopaedic practice, power tools, new implants, CAD-CAM design, 3-D printing, additive manufacturing just to name a few. The new disruption in orthopaedics in the current time and era is undoubtedly the advent of artificial intelligence and robotics. As these technologies take root and innovative applications continue to be incorporated into the main-stream orthopedics, as we know it today, it is imperative to look at and understand the basics of artificial intelligence and what work is being done in the field today. This article takes the form of a loosely structured narrative review and will introduce the reader to key concepts in the field of artificial intelligence as well as some of the directions in application of the same in orthopaedics. Some of the recent work has been summarised and we present our viewpoint at the conclusion as to why we must consider artificial intelligence as a disrupting positive influence on orthopaedic surgery. 
  |  None  |  
------------------------------------------- 
10.1016/j.acra.2019.04.024  |   As artificial intelligence (AI) is finding its place in radiology, it is important to consider how to guide the research and clinical implementation in a way that will be most beneficial to patients. Although there are multiple aspects of this issue, I consider a specific one: a potential misalignment of the self-interests of radiologists and AI developers with the best interests of the patients. Radiologists know that supporting research into AI and advocating for its adoption in clinical settings could diminish their employment opportunities and reduce respect for their profession. This provides an incentive to oppose AI in various ways. AI developers have an incentive to hype their discoveries to gain attention. This could provide short-term personal gains, however, it could also create a distrust toward the field if it became apparent that the state of the art was far from where it was promised to be. The future research and clinical implementation of AI in radiology will be partially determined by radiologist and AI researchers. Therefore, it is very important that we recognize our own personal motivations and biases and act responsibly to ensure the highest benefit of the AI transformation to the patients. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30444-1  |  
------------------------------------------- 
10.7507/1001-5515.201903052  |   The segmentation of organs at risk is an important part of radiotherapy. The current method of manual segmentation depends on the knowledge and experience of physicians, which is very time-consuming and difficult to ensure the accuracy, consistency and repeatability. Therefore, a deep convolutional neural network (DCNN) is proposed for the automatic and accurate segmentation of head and neck organs at risk. The data of 496 patients with nasopharyngeal carcinoma were reviewed. Among them, 376 cases were randomly selected for training set, 60 cases for validation set and 60 cases for test set. Using the three-dimensional (3D) U-NET DCNN, combined with two loss functions of Dice Loss and Generalized Dice Loss, the automatic segmentation neural network model for the head and neck organs at risk was trained. The evaluation parameters are Dice similarity coefficient and Jaccard distance. The average Dice Similarity coefficient of the 19 organs at risk was 0.91, and the Jaccard distance was 0.15. The results demonstrate that 3D U-NET DCNN combined with Dice Loss function can be better applied to automatic segmentation of head and neck organs at risk. 
 勾画危及器官是放射治疗中的重要环节。目前人工勾画的方式依赖于医生的知识和经验，非常耗时且难以保证勾画准确性、一致性和重复性。为此，本研究提出一种深度卷积神经网络，用于头颈部危及器官的自动和精确勾画。研究回顾了 496 例鼻咽癌患者数据，随机选择 376 例用于训练集，60 例用于验证集，60 例作为测试集。使用三维（3D）U-NET 深度卷积神经网络结构，结合 Dice Loss 和 Generalized Dice Loss 两种损失函数训练头颈部危及器官自动勾画深度卷积神经网络模型，评估参数为 Dice 相似性系数和 Jaccard 距离。19 种危及器官 Dice 相似性指数平均达到 0.91，Jaccard 距离平均值为 0.15。研究结果显示基于 3D U-NET 深度卷积神经网络结合 Dice 损失函数可以较好地应用于头颈部危及器官的自动勾画。. 
  |  None  |  
------------------------------------------- 
10.1007/s00216-019-02345-5  |   The study assessed the feasibility of merging data acquired from hyperspectral imaging (HSI) and electronic nose (e-nose) to develop a robust method for the rapid prediction of intramuscular fat (IMF) and peroxide value (PV) of pork meat affected by temperature and NaCl treatments. Multivariate calibration models for prediction of IMF and PV using median spectra features (MSF) and image texture features (ITF) from HSI data and mean signal values (MSV) from e-nose signals were established based on support vector machine regression (SVMR). Optimum wavelengths highly related to IMF and PV were selected from the MSF and ITF. Next, recurring optimum wavelengths from the two feature groups were manually obtained and merged to constitute "combined attribute features" (CAF) which yielded acceptable results with (R<sub>c</sub><sup>2</sup> = 0.877, 0.891; RMSEC = 2.410, 1.109; R<sub>p</sub><sup>2</sup> = 0.790, 0.858; RMSEP = 3.611, 2.013) respectively for IMF and PV. MSV yielded relatively low results with (R<sub>c</sub><sup>2</sup> = 0.783, 0.877; RMSEC = 4.591, 0.653; R<sub>p</sub><sup>2</sup> = 0.704, 0.797; RMSEP = 3.991, 0.760) respectively for IMF and PV. Finally, data fusion of CAF and MSV was performed which yielded relatively improved prediction results with (R<sub>c</sub><sup>2</sup> = 0.936, 0.955; RMSEC = 1.209, 0.997; R<sub>p</sub><sup>2</sup> = 0.895, 0.901; RMSEP = 2.099, 1.008) respectively for IMF and PV. The results obtained demonstrate that it is feasible to mutually integrate spectral and image features with volatile information to quantitatively monitor IMF and PV in processed pork meat. Graphical abstract. 
  |  https://dx.doi.org/10.1007/s00216-019-02345-5  |  
------------------------------------------- 
10.1371/journal.pone.0227791  |   The objective investigation of the dynamic properties of vocal fold vibrations demands the recording and further quantitative analysis of laryngeal high-speed video (HSV). Quantification of the vocal fold vibration patterns requires as a first step the segmentation of the glottal area within each video frame from which the vibrating edges of the vocal folds are usually derived. Consequently, the outcome of any further vibration analysis depends on the quality of this initial segmentation process. In this work we propose for the first time a procedure to fully automatically segment not only the time-varying glottal area but also the vocal fold tissue directly from laryngeal high-speed video (HSV) using a deep Convolutional Neural Network (CNN) approach. Eighteen different Convolutional Neural Network (CNN) network configurations were trained and evaluated on totally 13,000 high-speed video (HSV) frames obtained from 56 healthy and 74 pathologic subjects. The segmentation quality of the best performing Convolutional Neural Network (CNN) model, which uses Long Short-Term Memory (LSTM) cells to take also the temporal context into account, was intensely investigated on 15 test video sequences comprising 100 consecutive images each. As performance measures the Dice Coefficient (DC) as well as the precisions of four anatomical landmark positions were used. Over all test data a mean Dice Coefficient (DC) of 0.85 was obtained for the glottis and 0.91 and 0.90 for the right and left vocal fold (VF) respectively. The grand average precision of the identified landmarks amounts 2.2 pixels and is in the same range as comparable manual expert segmentations which can be regarded as Gold Standard. The method proposed here requires no user interaction and overcomes the limitations of current semiautomatic or computational expensive approaches. Thus, it allows also for the analysis of long high-speed video (HSV)-sequences and holds the promise to facilitate the objective analysis of vocal fold vibrations in clinical routine. The here used dataset including the ground truth will be provided freely for all scientific groups to allow a quantitative benchmarking of segmentation approaches in future. 
  |  http://dx.plos.org/10.1371/journal.pone.0227791  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32040514/  |  
------------------------------------------- 
10.1097/MOU.0000000000000707  |    Purpose of review:  To investigate the application of artificial intelligence in the management of nephrolithiasis. 
  Recent findings:  Although rising, the number of publications on artificial intelligence for the management of urinary stone disease is still low. Most publications focus on diagnostic tools and prediction of outcomes after clinical interventions. Artificial intelligence can, however, play a major role in development of surgical skills and automated data extraction to support clinical research. 
  Summary:  The combination of artificial intelligence with new technological developments in the field of endourology will create new possibilities in the management of urinary stones. The implication of artificial intelligence can lead to better patient selection, higher success rates, and furthermore improve patient safety. 
  |  http://dx.doi.org/10.1097/MOU.0000000000000707  |  
------------------------------------------- 
10.1080/19396368.2020.1749727  |   The existing flaws in both conducting and reporting of research have been outlined and criticized in the past. Weak research design, poor methodology, lack of fresh ideas and poor reporting are the main points to blame. Issues have been continually raised on the types of results published, review process, sponsorship, notion, ethics, and incentives in publishing, the role of regulatory agencies and stakeholders, the role of funding, and the cooperation between funders and academic institutions and the training of both clinicians and methodologists or statisticians. As a result, there is loss of the utmost goal: the production of robust research to form recommendations to support pragmatic decision in a real-world context. We propose the construction of a model based on artificial intelligence that could assist stakeholders, clinicians, and patients to guide conducting the best quality of research. We briefly describe the levels of the workflow, including the input and output data collection, the feature extraction/selection, the architecture, and parameterization of the model, along with its training, operation, and refinement. 
  |  http://www.tandfonline.com/doi/full/10.1080/19396368.2020.1749727  |  
------------------------------------------- 
10.3389/fmed.2020.00027  |   Artificial intelligence-powered medical technologies are rapidly evolving into applicable solutions for clinical practice. Deep learning algorithms can deal with increasing amounts of data provided by wearables, smartphones, and other mobile monitoring sensors in different areas of medicine. Currently, only very specific settings in clinical practice benefit from the application of artificial intelligence, such as the detection of atrial fibrillation, epilepsy seizures, and hypoglycemia, or the diagnosis of disease based on histopathological examination or medical imaging. The implementation of augmented medicine is long-awaited by patients because it allows for a greater autonomy and a more personalized treatment, however, it is met with resistance from physicians which were not prepared for such an evolution of clinical practice. This phenomenon also creates the need to validate these modern tools with traditional clinical trials, debate the educational upgrade of the medical curriculum in light of digital medicine as well as ethical consideration of the ongoing connected monitoring. The aim of this paper is to discuss recent scientific literature and provide a perspective on the benefits, future opportunities and risks of established artificial intelligence applications in clinical practice on physicians, healthcare institutions, medical education, and bioethics. 
  |  https://doi.org/10.3389/fmed.2020.00027  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32118012/  |  
------------------------------------------- 
10.3969/j.issn.1671-7104.2020.01.007  |   Accompanied by changes in modern work and lifestyle, the incidence of cervical spondylosis has increased year by year. In view of the fact long-term fixed posture of the head and neck is one of the main causes of cervical spondylosis, a set of wearable cervical spondylosis prevention system is developed. The system comprises a head and neck movement collection module based on the acceleration sensor and a head and neck motion recognition module based on artificial intelligence. Experimental results showed that the system can accurately identify long-term posture of the head and neck, and guide users to complete effective exercise therapy under the supervision of motion recognition module. Using this system can be beneficial for the prevention of cervical spondylosis. 
  |  None  |  
------------------------------------------- 
10.7759/cureus.7202  |   This technical report describes the methods undertaken by a US-based Digital Health company (X2AI or X2 for short) to develop an ethical code for startup environments and other organizations delivering emotional artificial intelligence (AI) services, especially for mental health support. With a growing demand worldwide for scalable, affordable, and accessible health care solutions, the use of AI offers tremendous potential to improve emotional well-being. To realize this potential, it is imperative that AI service providers prioritize clear and consistent ethical guidelines that align with global considerations regarding user safety and privacy. This report offers a template for an ethical code that can be implemented by other emotional AI services and their affiliates. It includes practical guidelines for integrating support from clients, collaborators, and research partners. It also shows how existing ethical systems can inform the development of AI ethics. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32269881/  |  
------------------------------------------- 
10.1016/j.revmed.2020.02.010  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0248-8663(20)30052-7  |  
------------------------------------------- 
10.3390/s20072034  |   The electroencephalogram (EEG) has great attraction in emotion recognition studies due to its resistance to deceptive actions of humans. This is one of the most significant advantages of brain signals in comparison to visual or speech signals in the emotion recognition context. A major challenge in EEG-based emotion recognition is that EEG recordings exhibit varying distributions for different people as well as for the same person at different time instances. This nonstationary nature of EEG limits the accuracy of it when subject independency is the priority. The aim of this study is to increase the subject-independent recognition accuracy by exploiting pretrained state-of-the-art Convolutional Neural Network (CNN) architectures. Unlike similar studies that extract spectral band power features from the EEG readings, raw EEG data is used in our study after applying windowing, pre-adjustments and normalization. Removing manual feature extraction from the training system overcomes the risk of eliminating hidden features in the raw data and helps leverage the deep neural network's power in uncovering unknown features. To improve the classification accuracy further, a median filter is used to eliminate the false detections along a prediction interval of emotions. This method yields a mean cross-subject accuracy of 86.56% and 78.34% on the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) for two and three emotion classes, respectively. It also yields a mean cross-subject accuracy of 72.81% on the Database for Emotion Analysis using Physiological Signals (DEAP) and 81.8% on the Loughborough University Multimodal Emotion Dataset (LUMED) for two emotion classes. Furthermore, the recognition model that has been trained using the SEED dataset was tested with the DEAP dataset, which yields a mean prediction accuracy of 58.1% across all subjects and emotion classes. Results show that in terms of classification accuracy, the proposed approach is superior to, or on par with, the reference subject-independent EEG emotion recognition studies identified in literature and has limited complexity due to the elimination of the need for feature extraction. 
  |  http://www.mdpi.com/resolver?pii=s20072034  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32260445/  |  
------------------------------------------- 
10.3389/fchem.2020.00275  |    |  https://doi.org/10.3389/fchem.2020.00275  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32328481/  |  
------------------------------------------- 
10.1093/geront/gnz190  |   Increasing awareness of errors and harms in institutional care settings, combined with rapid advancements in artificial intelligence, have resulted in a widespread push for implementing monitoring technologies in institutional settings. There has been limited critical reflection in gerontology regarding the ethical, social, and policy implications of using these technologies. We critically review current scholarship regarding use of monitoring technology in institutional care, and identify key gaps in knowledge and important avenues for future research and development. 
  |  https://academic.oup.com/gerontologist/article-lookup/doi/10.1093/geront/gnz190  |  
------------------------------------------- 
10.7507/1001-5515.201910055  |   Recently, artificial intelligence (AI) has been widely applied in the diagnosis and treatment of urinary diseases with the development of data storage, image processing, pattern recognition and machine learning technologies. Based on the massive biomedical big data of imaging and histopathology, many urinary system diseases (such as urinary tumor, urological calculi, urinary infection, voiding dysfunction and erectile dysfunction) will be diagnosed more accurately and will be treated more individualizedly. However, most of the current AI diagnosis and treatment are in the pre-clinical research stage, and there are still some difficulties in the wide application of AI. This review mainly summarizes the recent advances of AI in the diagnosis of prostate cancer, bladder cancer, kidney cancer, urological calculi, frequent micturition and erectile dysfunction, and discusses the future potential and existing problems. 
 近年来，随着数据储存、图像处理、模式识别和机器学习等技术的进步，人工智能在泌尿疾病的诊疗方面得到了广泛的应用。基于影像学和组织病理学等海量的生物医学大数据，人工智能技术可以让医务工作者对泌尿系肿瘤、泌尿系结石、泌尿系感染、泌尿功能异常和勃起功能障碍等几类泌尿疾病的诊断更为精准，让治疗更加个性化。然而，目前人工智能诊疗大多处于研究阶段，在实际的应用中尚存在一些问题。本文以辅助诊断为线索，对人工智能方法在前列腺癌、膀胱癌、肾癌、尿路结石、尿频、勃起功能障碍等常见泌尿疾病的应用和研究情况予以综述，并进一步探讨其存在的问题和未来发展方向。. 
  |  None  |  
------------------------------------------- 
10.1016/j.jacr.2019.07.019  |   Radiologists today are under increasing work pressure. We surveyed radiologists in the United States across practice settings, and the overwhelming majority reported an increased workload. Artificial intelligence (AI), which includes machine learning, can help address these issues. It also has the potential to improve clinical outcomes and raise further the value of medical imaging in ways yet to be defined. In this article, we report on recent McKinsey &amp; Company work to understand the growth of AI in medical imaging. We highlight progress in its clinical application, the investments that are backing it, and the barriers to broader adoption. We also offer a view on how the market will develop. AI is set to have a big impact on the medical imaging market and hence on how radiologists work, helping them to speed up scan time, make more accurate diagnoses, and ease their workload. As AI in medical imaging increasingly proves its worth, it is hard to imagine that AI will not ultimately transform radiology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1546-1440(19)30863-4  |  
------------------------------------------- 
10.1371/journal.pone.0228324  |   To solve the problem of low accuracy in traditional fault diagnosis methods, a novel method of combining generalized frequency response function(GFRF) and convolutional neural network(CNN) is proposed. In order to accurately characterize system state information, this paper proposed a variable step size least mean square (VSSLMS) adaptive algorithm to calculate the second-order GFRF spectrum values under normal and fault states; In order to improve the ability of fault feature extraction, a convolution neural network (CNN) with gradient descent learning rate and alternate convolution layer and pooling layer is designed to extract the fault features from GFRF spectrum. In the proposed method, the second-order GFRF spectrum of each state of Permanent Magnet Synchronous Motor (PMSM) is obtained by VSSLMS; Then, the two-dimension GFRF spectrum, which is regarded as the gray value of the image,will be further transformed into image. Finally, the CNN is trained with learning rate by gradient descent way to realize the fault diagnosis of PMSM. Experimental results indicate that the accuracy of proposed method is 98.75%, which verifies the reliability of the proposed method in application of PMSM fault diagnosis. 
  |  http://dx.plos.org/10.1371/journal.pone.0228324  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32017780/  |  
------------------------------------------- 
10.1007/s00438-019-01642-z  |   As knowledge of genetics and genome elements increases, the demand for the development of bioinformatics tools for analyzing these data is raised. Riboswitches are genetic components, usually located in the untranslated regions of mRNAs, that regulate gene expression. Additionally, their interaction with antibiotics has been recently suggested, implying a role in antibiotic effects and resistance. Following a previously published sequential block finding algorithm, herein, we report the development of a new block location-based feature extraction strategy (BLBFE). This procedure utilizes the locations of family-specific sequential blocks on riboswitch sequences as features. Furthermore, the performance of other feature extraction strategies, including mono- and dinucleotide frequencies, k-mer, DAC, DCC, DACC, PC-PseDNC-General and SC-PseDNC-General methods, was investigated. KNN, LDA, naïve Bayes, PNN and decision tree classifiers accompanied by V-fold cross-validation were applied for all methods of feature extraction, and their performances based on the defined feature extraction strategies were compared. Performance measures of accuracy, sensitivity, specificity and F-score for each method of feature extraction were studied. The proposed feature extraction strategy resulted in classification of riboswitches with an average correct classification rate (CCR) of 90.8%. Furthermore, the obtained data confirmed the performance of the developed feature extraction method with an average accuracy of 96.1%, an average sensitivity of 90.8%, an average specificity of 97.52% and an average F-score of 90.69%. Our results implied that the proposed feature extraction (BLBFE) method can classify and discriminate riboswitch families with high CCR, accuracy, sensitivity, specificity and F-score values. 
  |  https://doi.org/10.1007/s00438-019-01642-z  |  
------------------------------------------- 
10.1136/bmj.m958  |    Objective:  To provide focused evaluation of predictive modeling of electronic medical record (EMR) data to predict 30 day hospital readmission. 
  Design:  Systematic review. 
  Data source:  Ovid Medline, Ovid Embase, CINAHL, Web of Science, and Scopus from January 2015 to January 2019. 
  Eligibility criteria for selecting studies:  All studies of predictive models for 28 day or 30 day hospital readmission that used EMR data. 
  Outcome measures:  Characteristics of included studies, methods of prediction, predictive features, and performance of predictive models. 
  Results:  Of 4442 citations reviewed, 41 studies met the inclusion criteria. Seventeen models predicted risk of readmission for all patients and 24 developed predictions for patient specific populations, with 13 of those being developed for patients with heart conditions. Except for two studies from the UK and Israel, all were from the US. The total sample size for each model ranged between 349 and 1 195 640. Twenty five models used a split sample validation technique. Seventeen of 41 studies reported C statistics of 0.75 or greater. Fifteen models used calibration techniques to further refine the model. Using EMR data enabled final predictive models to use a wide variety of clinical measures such as laboratory results and vital signs; however, use of socioeconomic features or functional status was rare. Using natural language processing, three models were able to extract relevant psychosocial features, which substantially improved their predictions. Twenty six studies used logistic or Cox regression models, and the rest used machine learning methods. No statistically significant difference (difference 0.03, 95% confidence interval -0.0 to 0.07) was found between average C statistics of models developed using regression methods (0.71, 0.68 to 0.73) and machine learning (0.74, 0.71 to 0.77). 
  Conclusions:  On average, prediction models using EMR data have better predictive performance than those using administrative data. However, this improvement remains modest. Most of the studies examined lacked inclusion of socioeconomic features, failed to calibrate the models, neglected to conduct rigorous diagnostic testing, and did not discuss clinical impact. 
  |  http://www.bmj.com/cgi/pmidlookup?view=long&pmid=32269037  |  
------------------------------------------- 
10.7507/1002-1892.201906105  |    Objective:  To compare short-term effectiveness between robot-guided percutaneous minimally invasive pedicle screw internal fixation and traditional open internal fixation in the treatment of thoracolumbar fractures. 
  Methods:  The clinical data of 52 cases of thoracolumbar fracture without neurological injury symptoms admitted between January 2018 and May 2018 were retrospectively analyzed. According to the different surgical methods, they were divided into minimally invasive group (24 cases, treated with robot-assisted percutaneous minimally invasive pedicle screw internal fixation) and open group (28 cases, treated with traditional open internal fixation). There was no significant difference between the two groups in the general data such as gender, age, cause of injury, fracture segment, thoracolumbar injury classification and severity score (TLICS), preoperative back pain visual analogue scale (VAS) score, Oswestry disability index (ODI) score, fixed segment height, and fixed segment kyphosis Cobb angle ( <i>P</i>&gt;0.05). The operation time, intraoperative blood loss, and hospitalization time of the two groups were recorded and compared; as well as the VAS score, ODI score, fixed segment height, and fixed segment kyphosis Cobb angle of the two groups before operation and at 3 days, 1 month, 6 months, and 10 months after operation. CT scan was reexamined at 1-3 days after operation, and the pedicle screw insertion accuracy rate was determined and calculated according to Gertzbein-Robbins classification standard. 
  Results:  The operation time of the minimally invasive group was significantly longer than that of the open group, but the intraoperative blood loss and hospitalization time were significantly shorter than those of the open group ( <i>P</i>&lt;0.05). There were 132 pedicle screws and 158 pedicle screws implanted in the minimally invasive group and the open group respectively. According to the Gertzbein-Robbins classification standard, the accuracy of pedicle screws was 97.7% (129/132) and 96.8% (153/158), respectively, showing no significant difference between the two groups ( <i>χ</i> <sup>2</sup>=0.505, <i>P</i>=0.777). The patients in both groups were followed up 10 months, and there was no rejection or internal fixation fracture. In the minimally invasive group, the internal fixator was removed at 10 months after operation, but not in the open group. The VAS score, ODI score, fixed segment heigh, and fixed segment kyphotic Cobb angle of the two groups were improved in different degrees when compared with preoperative ones ( <i>P</i>&lt;0.05). Except that the VAS score and ODI score of the minimally invasive group were significantly better than those of the open group at 3 days after operation ( <i>P</i>&lt;0.05), there was no significant difference between the two groups at other time points ( <i>P</i>&gt;0.05). 
  Conclusion:  Robot-assisted percutaneous minimally invasive pedicle screw internal fixation for thoracolumbar fractures has significant advantages in intraoperative blood loss, hospitalization time, and early postoperative effectiveness and other aspects, and the effect of fracture reduction is good. 
  目的:  比较机器人辅助下经皮微创椎弓根螺钉内固定与传统开放内固定治疗胸腰椎骨折的短期疗效。. 
  方法:  回顾分析 2018 年 1 月—5 月收治的 52 例无神经损伤症状胸腰椎骨折患者临床资料，根据手术方式不同分为微创组（24 例，采用机器人辅助下经皮微创椎弓根螺钉内固定治疗）和开放组（28 例，采用传统开放内固定治疗）。两组患者性别、年龄、致伤原因、骨折节段、胸腰椎损伤分类和损伤程度评分（TLICS）及术前腰痛视觉模拟评分（VAS）、Oswestry 功能障碍指数（ODI）、固定节段高度及固定节段后凸 Cobb 角等一般资料比较差异均无统计学意义（ <i>P</i>&gt;0.05），具有可比性。记录并比较两组患者手术时间、术中出血量、住院时间，以及两组患者术前，术后 3 d、1 个月、6 个月、10 个月腰痛 VAS 评分、ODI 评分、固定节段高度及固定节段后凸 Cobb 角。术后 1～3 d 复查 CT，根据 Gertzbein-Robbins 分类标准判断并计算植钉准确率。. 
  结果:  微创组手术时间显著长于开放组，但术中出血量和住院时间均显著短于开放组，差异均有统计学意义（ <i>P</i>&lt;0.05）。微创组和开放组分别植入椎弓根螺钉 132 枚和 158 枚，根据 Gertzbein-Robbins 分类标准，植钉准确率分别为 97.7%（129/132）和 96.8%（153/158），比较差异无统计学意义（ <i>χ</i> <sup>2</sup>=0.505， <i>P</i>=0.777）。两组患者均获随访 10 个月，术后均未出现排斥反应、内固定物断裂等情况。微创组于术后 10 个月取出内固定物，开放组不取出。术后各时间点两组腰痛 VAS 评分、ODI 评分、固定节段高度及固定节段后凸 Cobb 角均较术前有不同程度改善（ <i>P</i>&lt;0.05）。除术后 3 d 微创组腰痛 VAS 评分和 ODI 评分显著优于开放组（ <i>P</i>&lt;0.05）外，其余各时间点各指标两组间比较差异均无统计学意义（ <i>P</i>&gt;0.05）。. 
  结论:  机器人辅助下经皮微创椎弓根螺钉内固定治疗胸腰椎骨折在术中出血量、住院时间、术后早期临床疗效等方面优势显著，骨折复位效果良好。. 
  |  None  |  
------------------------------------------- 
10.1007/s10661-020-8173-x  |   Dissolved oxygen (DO) as one of the most fundamental parameters of water quality plays a vital role in aquatic life. This study was conducted to predict DO, biological oxygen demand (BOD), and chemical oxygen demand (COD) in an intensive rainbow trout rearing system with different biomass (B). The multilayer perceptron (MLP) and the radial basis function (RBF) neural networks were employed for evaluating the impacts of food parameters (crude protein (CP), consumed feed (CF)), fish parameters (different values of B, and weight gain (WG)), and water quality parameters including temperature (T) and flow rate (Q) on variation of DO, BOD, and COD concentrations. This study's results showed that although both MLP and RBF neural networks are capable to estimate DO, BOD, and COD concentrations, RBF neural network showed better performance compared to MLP neural network. The results of sensitivity analysis indicated that the parameter CF has the highest effect on DO concentration estimation. Independent variables CF, CP, WG, and B showed the highest to the lowest rank of impacts on BOD estimation, respectively. The results also illustrated a decreasing trend of the effects on the estimation error of COD changes simulation by all independent variables, including B, T, WG, CF, CP, and Q, respectively. RBF neural network based on better stability and generalization ability with average root mean square error (RMSE) and mean absolute percentage error (MAPE) values of less than 0.12 and 3% was superior to MLP in DO, BOD, and COD concentration prediction. Moreover, CF was identified as the most effective factor in estima12tion process. Based on the present study results, there are direct relationships between DO, BOD, and COD concentrations and water quality parameters, fish parameters, and food parameters. Food parameters relative to fish and water quality parameters imposed the greatest effects. Improvement in feeding process such as application of intelligence feeding methods and change in fish diet and feeding time can considerably reduce losses in production system. Graphical abstract. 
  |  https://doi.org/10.1007/s10661-020-8173-x  |  
------------------------------------------- 
10.7507/1002-1892.201905057  |    Objective:  To compare the effectiveness and screw planting accuracy of percutaneous reduction and internal fixation with robot and traditional fluoroscopy-assisted in the treatment of single-level thoracolumbar fractures without neurological symptoms. 
  Methods:  The clinical data of 58 patients with single-level thoracolumbar fractures without neurological symptoms between December 2016 and January 2018 were retrospectively analysed. According to different surgical methods, the patients were divided into group A (28 cases underwent robot-assisted percutaneous reduction and internal fixation) and group B (30 cases underwent fluoroscopy-assisted percutaneous reduction and internal fixation). There was no neurological symptoms, other fractures or organ injuries in the two groups. There was no significant difference in general data of age, gender, fracture location, AO classification, time from injury to surgery, and preoperative vertebral anterior height ratio, sagittal Cobb angle, visual analogue scale (VAS) score, and Oswestry disability index (ODI) score between the two groups ( <i>P</i>&gt;0.05). The screw placement time, operation time, intraoperative blood loss, intraoperative fluoroscopy frequency, hospitalization time, operation cost, postoperative complications, VAS score, ODI score, anterior vertebral height ratio, and sagittal Cobb angle before operation, at 3 days, 6 months after operation, and at last follow-up were recorded and compared between the two groups. The accuracy of the pedicle screw placement was evaluated by Neo's criteria. 
  Results:  The screw placement time, operation time, and intraoperative fluoroscopy frequency of group A were significantly less than those of group B, and the operation cost was significantly higher than that of group B ( <i>P</i>&lt;0.05). But there was no significant difference in intraoperative blood loss and hospitalization time between the two groups ( <i>P</i>&gt;0.05). Both groups were followed up 12-24 months, with an average of 15.2 months. The accuracy rate of screw placement in groups A and B was 93.75% (150/160) and 84.71% (144/170), respectively, and the difference was significant ( <i>χ</i> <sup>2</sup>=5.820, <i>P</i>=0.008). Except for 1 case of postoperative superficial infection in group A and wound healing after dressing change, there was no complication such as neurovascular injury, screw loosening and fracture in both groups, and there was no significant difference in the incidence of complications between the two groups ( <i>χ</i> <sup>2</sup>=0.625, <i>P</i>=0.547). The anterior vertebral height ratio, sagittal Cobb angle, VAS score, and ODI score of the two groups were significantly improved ( <i>P</i>&lt;0.05); there was no significant difference between the two groups at all time points after operation ( <i>P</i>&gt;0.05). 
  Conclusion:  The spinal robot and traditional fluoroscopy-assisted percutaneous reduction and internal fixation can both achieve satisfactory effectiveness in the treatment of single-level thoracolumbar fractures without neurological symptoms. However, the former has higher accuracy, fewer fluoroscopy times, shorter time of screw placement, and lower technical requirements for the operator. It has wide application potential. 
  目的:  比较脊柱机器人辅助和传统透视辅助下微创经皮椎弓根螺钉复位内固定术治疗单节段无神经症状胸腰椎骨折的临床疗效及植钉准确性。. 
  方法:  回顾分析 2016 年 12 月—2018 年 1 月收治的 58 例单节段无神经症状的胸腰椎骨折患者临床资料，根据手术方式不同分为 A 组（脊柱机器人辅助经皮复位内固定组，28 例）和 B 组（传统透视辅助经皮复位内固定组，30 例）。两组患者均无神经症状，不合并其他骨折和脏器损伤。两组患者年龄、性别、骨折部位、骨折 AO 分型、受伤至手术时间及术前椎体前缘高度百分比、矢状面 Cobb 角、疼痛视觉模拟评分（VAS）和 Oswestry 功能障碍指数（ODI）等一般资料比较差异均无统计学意义（ <i>P</i>&gt;0.05），具有可比性。记录并比较两组患者植钉时间、手术时间、术中出血量、术中透视次数、住院时间、手术费用、术后并发症，以及术前、术后 3 d、术后 6 个月及末次随访时的 VAS 评分、ODI 评分、椎体前缘高度百分比、矢状面 Cobb 角。参考 Neo 等的方法评估植入椎弓根螺钉的精确性。. 
  结果:  A 组植钉时间、手术时间、术中透视次数均显著少于 B 组，手术费用高于 B 组（ <i>P</i>&lt;0.05）；但两组术中出血量、住院时间比较差异无统计学意义（ <i>P</i>&gt;0.05）。两组患者均获随访，随访时间 12～18 个月，平均 15.2 个月。A、B 组术后精确植钉百分率分别为 93.75%（150/160）和 84.71%（144/170），比较差异有统计学意义（ <i>χ</i> <sup>2</sup>=5.820， <i>P</i>=0.008）。除 A 组发生术后切口浅部感染 1 例，经换药后切口愈合外，两组均未出现神经血管损伤、螺钉松动断裂等并发症，两组并发症发生率比较差异无统计学意义（ <i>χ</i> <sup>2</sup>=0.625， <i>P</i>=0.547）。两组术后各时间点椎体前缘高度百分比、矢状面 Cobb 角、VAS 评分、ODI 评分均较术前显著改善（ <i>P</i>&lt;0.05）；术后各时间点两组间比较差异均无统计学意义（ <i>P</i>&gt;0.05）。. 
  结论:  脊柱机器人和传统透视辅助下经皮复位内固定术治疗单节段无神经症状胸腰椎骨折均可获得满意的临床效果，但前者植钉准确性更高，术中透视次数更少，植钉时间更短，对术者技术要求较低，具有广泛的应用潜力。. 
  |  None  |  
------------------------------------------- 
10.1016/j.jelectrocard.2020.02.008  |    Background:  Screening and early diagnosis of mitral regurgitation (MR) are crucial for preventing irreversible progression of MR. In this study, we developed and validated an artificial intelligence (AI) algorithm for detecting MR using electrocardiography (ECG). 
  Methods:  This retrospective cohort study included data from two hospital. An AI algorithm was trained using 56,670 ECGs from 24,202 patients. Internal validation of the algorithm was performed with 3174 ECGs of 3174 patients from one hospital, while external validation was performed with 10,865 ECGs of 10,865 patients from another hospital. The endpoint was the diagnosis of significant MR, moderate to severe, confirmed by echocardiography. We used 500 Hz ECG raw data as predictive variables. Additionally, we showed regions of ECG that have the most significant impact on the decision-making of the AI algorithm using a sensitivity map. 
  Results:  During the internal and external validation, the area under the receiver operating characteristic curve of the AI algorithm using a 12-lead ECG for detecting MR was 0.816 and 0.877, respectively, while that using a single-lead ECG was 0.758 and 0.850, respectively. In the 3157 non-MR individuals, those patients that the AI defined as high risk had a significantly higher chance of development of MR than the low risk group (13.9% vs. 2.6%, p &lt; 0.001) during the follow-up period. The sensitivity map showed the AI algorithm focused on the P-wave and T-wave for MR patients and QRS complex for non-MR patients. 
  Conclusions:  The proposed AI algorithm demonstrated promising results for MR detecting using 12-lead and single-lead ECGs. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-0736(19)30704-6  |  
------------------------------------------- 
10.1107/S2052252519016476  |   AI is no magic dust: for it to become a true discovery accelerator, much work is needed to make it transparent and robust. 
  |  http://scripts.iucr.org/cgi-bin/paper?me6061  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31949897/  |  
------------------------------------------- 
10.1177/1932296820914287  |   The study by Shah et al published in this issue of the <i>Journal of Diabetes Science and Technology</i> validates the IDx autonomous diabetic retinopathy (DR) screening program in a real-world setting. The study found high sensitivity (100%) but low specificity (82%) for referable DR. The resulting positive predictive value of 19% means that four out of five patients without referable DR would be referred to ophthalmology causing a significant burden to ophthalmologists, primary care clinics, and patients. Artificial intelligence programs that provide better specificity, multiple levels of DR, and annotations of where lesions are located in the retina may function better than a simple referral/no referral output. This will allow for better engagement of patients through the difficult process of adhering to treatment recommendations and control their diabetes. 
  |  http://journals.sagepub.com/doi/full/10.1177/1932296820914287?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s00146-020-00968-2  |   "My first long haul flight that didn't fill up and an empty row for me. I have been blessed by the algorithm ". The phrase 'blessed by the algorithm' expresses the feeling of having been fortunate in what appears on your feed on various social media platforms, or in the success or virality of your content as a creator, or in what gig economy jobs you are offered. However, we can also place it within wider public discourse employing theistic conceptions of AI. Building on anthropological fieldwork into the 'entanglements of AI and Religion' (Singler 2017a), this article will explore how 'blessed by the algorithm' tweets are indicative of the impact of theistic AI narratives: modes of thinking about AI in an implicitly religious way. This thinking also represents continuities that push back against the secularisation thesis and other grand narratives of disenchantment that claim secularity occurs because of technological and intellectual progress. This article will also explore new religious movements, where theistic conceptions of AI entangle technological aspirations with religious ones. 
  |  None  |  
------------------------------------------- 
10.1016/j.dsx.2020.04.012  |    Background and aims:  Healthcare delivery requires the support of new technologies like Artificial Intelligence (AI), Internet of Things (IoT), Big Data and Machine Learning to fight and look ahead against the new diseases. We aim to review the role of AI as a decisive technology to analyze, prepare us for prevention and fight with COVID-19 (Coronavirus) and other pandemics. 
  Methods:  The rapid review of the literature is done on the database of Pubmed, Scopus and Google Scholar using the keyword of COVID-19 or Coronavirus and Artificial Intelligence or AI. Collected the latest information regarding AI for COVID-19, then analyzed the same to identify its possible application for this disease. 
  Results:  We have identified seven significant applications of AI for COVID-19 pandemic. This technology plays an important role to detect the cluster of cases and to predict where this virus will affect in future by collecting and analyzing all previous data. 
  Conclusions:  Healthcare organizations are in an urgent need for decision-making technologies to handle this virus and help them in getting proper suggestions in real-time to avoid its spread. AI works in a proficient way to mimic like human intelligence. It may also play a vital role in understanding and suggesting the development of a vaccine for COVID-19. This result-driven technology is used for proper screening, analyzing, prediction and tracking of current patients and likely future patients. The significant applications are applied to tracks data of confirmed, recovered and death cases. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1871-4021(20)30077-1  |  
------------------------------------------- 
10.4166/kjg.2020.75.3.120  |   Artificial intelligence using deep learning has been applied to gastrointestinal disorders for the detection, classification, and delineation of various lesion images. With the accumulation of enormous medical records, the evolution of computation power with graphic processing units, and the widespread use of open-source libraries in large-scale machine learning processes, medical artificial intelligence is overcoming its traditional limitations. This paper explains the basic concepts of deep learning model establishment and summarizes previous studies on upper gastrointestinal disorders. The limitations and perspectives on future development are also discussed. 
  |  http://www.kjg.or.kr/journal/view.html?doi=10.4166/kjg.2020.75.3.120  |  
------------------------------------------- 
10.1371/journal.pone.0226483  |   Modern societies are exposed to a myriad of risks ranging from disease to natural hazards and technological disruptions. Exploring how the awareness of risk spreads and how it triggers a diffusion of coping strategies is prominent in the research agenda of various domains. It requires a deep understanding of how individuals perceive risks and communicate about the effectiveness of protective measures, highlighting learning and social interaction as the core mechanisms driving such processes. Methodological approaches that range from purely physics-based diffusion models to data-driven environmental methods rely on agent-based modeling to accommodate context-dependent learning and social interactions in a diffusion process. Mixing agent-based modeling with data-driven machine learning has become popularity. However, little attention has been paid to the role of intelligent learning in risk appraisal and protective decisions, whether used in an individual or a collective process. The differences between collective learning and individual learning have not been sufficiently explored in diffusion modeling in general and in agent-based models of socio-environmental systems in particular. To address this research gap, we explored the implications of intelligent learning on the gradient from individual to collective learning, using an agent-based model enhanced by machine learning. Our simulation experiments showed that individual intelligent judgement about risks and the selection of coping strategies by groups with majority votes were outperformed by leader-based groups and even individuals deciding alone. Social interactions appeared essential for both individual learning and group learning. The choice of how to represent social learning in an agent-based model could be driven by existing cultural and social norms prevalent in a modeled society. 
  |  http://dx.plos.org/10.1371/journal.pone.0226483  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31905206/  |  
------------------------------------------- 
10.1089/thy.2019.0752  |   <b><i>Background:</i></b> Current classification systems for thyroid nodules are very subjective. Artificial intelligence (AI) algorithms have been used to decrease subjectivity in medical image interpretation. One out of 2 women over the age of 50 years may have a thyroid nodule and at present the only way to exclude malignancy is through invasive procedures for those that are suspicious on ultrasonography. Hence, there exists a need for noninvasive objective classification of thyroid nodules. Some cancers have benign appearance on ultrasonogram. Hence, we decided to create an image similarity algorithm rather than image classification algorithm. <b><i>Materials and Methods:</i></b> Ultrasound images of thyroid nodules from patients who underwent either biopsy or thyroid surgery from February 2012 to February 2017 in our institution were used to create AI models. Nodules were excluded if there was no definitive diagnosis of it being benign or malignant. A total of 482 nodules met the inclusion criteria and all available images from these nodules were used to create the AI models. Later, these AI models were used to test 103 thyroid nodules that underwent biopsy or surgery from March 2017 to July 2018. <b><i>Results:</i></b> Negative predictive value (NPV) of the image similarity model was 93.2%. Sensitivity, specificity, positive predictive value (PPV), and accuracy of the model were 87.8%, 78.5%, 65.9%, and 81.5%, respectively. <b><i>Conclusions:</i></b> When compared with published results of ultrasound thyroid cancer risk stratification systems, our image similarity model had comparable NPV with better sensitivity, specificity, and PPV. By using image similarity AI models, we can decrease subjectivity and decrease the number of unnecessary biopsies. Using image similarity AI model, we were able to create an explainable AI model that increases physician's confidence in the predictions. 
  |  https://www.liebertpub.com/doi/full/10.1089/thy.2019.0752?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2471/BLT.19.237198  |   Empathy, compassion and trust are fundamental values of a patient-centred, relational model of health care. In recent years, the quest for greater efficiency in health care, including economic efficiency, has often resulted in the side-lining of these values, making it difficult for health-care professionals to incorporate them in practice. Artificial intelligence is increasingly being used in health care. This technology promises greater efficiency and more free time for health-care professionals to focus on the human side of care, including fostering trust relationships and engaging with patients with empathy and compassion. This article considers the vision of efficient, empathetic and trustworthy health care put forward by the proponents of artificial intelligence. The paper suggests that artificial intelligence has the potential to fundamentally alter the way in which empathy, compassion and trust are currently regarded and practised in health care. Moving forward, it is important to re-evaluate whether and how these values could be incorporated and practised within a health-care system where artificial intelligence is increasingly used. Most importantly, society needs to re-examine what kind of health care it ought to promote. 
 L'empathie, la compassion et la confiance sont des valeurs fondamentales d'un modèle de soins de santé centré sur les relations avec le patient. Mais ces dernières années, la quête d'efficacité dans le secteur, y compris au niveau économique, a souvent relégué ces valeurs au second plan et les professionnels de la santé ont donc eu du mal à les intégrer à leur pratique. De son côté, l'intelligence artificielle gagne en importance. Cette technologie devrait accroître l'efficacité tout en libérant du temps pour les professionnels de la santé, qui pourront ainsi se concentrer sur l'aspect humain des soins, notamment en établissant une relation de confiance et en faisant preuve d'empathie et de compassion envers les patients. Le présent article s'intéresse à l'idée d'un système de soins de santé efficace, qui repose sur l'empathie et la confiance, et à laquelle adhèrent les adeptes de l'intelligence artificielle. Il suggère que l'intelligence artificielle a le potentiel nécessaire pour transformer radicalement la manière dont l'empathie, la compassion et la confiance sont considérées et appliquées aujourd'hui dans le secteur de la santé. À l'avenir, il est essentiel de réexaminer l'importance de ces valeurs et la façon dont elles pourraient être incorporées et mises en œuvre dans un système de santé où l'intelligence artificielle devient peu à peu incontournable. Et surtout, la société a besoin de se demander quel modèle de soins de santé elle souhaite promouvoir. 
 La empatía, la compasión y la confianza son valores fundamentales de un modelo relacional de atención sanitaria centrado en el paciente. En los últimos años, la búsqueda de una mayor eficiencia en la atención sanitaria, incluida la eficiencia económica, ha dado lugar con frecuencia a que estos valores se vean relegados a un segundo plano, lo que dificulta que los profesionales sanitarios los incorporen en la práctica. La inteligencia artificial se utiliza cada vez más en la atención sanitaria. Esta tecnología promete una mayor eficiencia y más tiempo libre para que los profesionales sanitarios se centren en el lado humano de la atención, lo que incluye el fomento de las relaciones de confianza y el trato a los pacientes con empatía y compasión. En este artículo se examina la visión de una atención sanitaria eficiente, empática y confiable que proponen los defensores de la inteligencia artificial. El artículo sugiere que la inteligencia artificial tiene el potencial de alterar fundamentalmente la forma en que la empatía, la compasión y la confianza se consideran y practican actualmente en la atención sanitaria. Para avanzar, es importante volver a evaluar si dichos valores se podrían incorporar y practicar en un sistema de atención sanitaria en el que se utiliza cada vez más la inteligencia artificial, y de qué manera. Lo más importante es que la sociedad debe reconsiderar qué tipo de atención sanitaria debe promover. 
 التعاطف والشفقة والثقة هي القيم الأساسية لنموذج الرعاية الصحية المرتكزة على المريض. في السنوات الأخيرة، أدى السعي لتحقيق المزيد من الفعالية في الرعاية الصحية، بما في ذلك الفعالية الاقتصادية، في الغالب إلى تباعد هذه القيم، مما جعل من الصعب على أخصائيي الرعاية الصحية دمجها في الممارسة. يُستخدم الذكاء الاصطناعي بشكل متزايد في الرعاية الصحية. وتقدم هذه التكنولوجيا وعوداً بمزيد من الفعالية والوقت الحر لأخصائيي الرعاية الصحية للتركيز على الجانب الإنساني من الرعاية، بما في ذلك تعزيز علاقات الثقة والاندماج مع المرضى من خلال التعاطف والشفقة. يناقش هذا المقال رؤية تتميز بالفعالية والتعاطف لرعاية صحية جديرة بالثقة، يطرحها مؤيدي الذكاء الاصطناعي. تشير الورقة إلى أن الذكاء الاصطناعي لديه إمكانية تغيير الطريقة التي ينظر بها إلى التعاطف والشفقة والثقة، وكيفية ممارسة كل منها، بشكل جذري في مجال الرعاية الصحية. ومع المضي قدما، من الهام إعادة تقييم ما إذا كان يمكن دمج وممارسة هذه القيم، داخل نظام الرعاية الصحية، حيث يستخدم الذكاء الاصطناعي بشكل متزايد، وكيفية القيام بذلك. والأهم من ذلك، يحتاج المجتمع إلى التحقق من نوع الرعاية الصحية الذي يمكن لهذه القيم أن ترتقي به. 
 理解、同情和信任是以患者为中心的、关系型医疗保健模式的基本价值。近年来，为了提高医疗保健的效率，包括经济效率，往往导致对这些价值观的背离，使医疗保健专业人员难以将其纳入实践。人工智能正越来越多地被应用于医疗保健。这项技术为医疗保健专业人员提供了更高的效率和更多的空闲时间，使他们能够专注于人性化的护理，包括培养信任关系，理解并同情患者。本文探讨了人工智能倡导者提出的高效、理解、可信赖的医疗保健理念。本文表明，人工智能有可能从根本上改变目前人们在医疗保健中看待和实践理解、同情和信任的方式。展望未来，重新评估这些价值观是否以及如何能够在越来越多地使用人工智能的医疗保健系统中纳入和实施是一件十分重要的事。最重要的是，社会需要重新审视什么类型的医疗保健值得推广。. 
 Эмпатия, сочувствие и доверие — это основополагающие ценности ориентированной на пациента реляционной модели здравоохранения. В последнее время стремление повысить эффективность систем здравоохранения, в том числе их рентабельность, приводит к тому, что этим ценностям часто не уделяется должного внимания, что в свою очередь значительно осложняет их использование на практике работниками сферы здравоохранения. Применение искусственного интеллекта в сфере здравоохранения неуклонно растет. Эта технология привлекательна перспективой повышенной эффективности и тем, что она оставляет медицинским работникам больше свободного времени для непосредственной работы с пациентами, в том числе для налаживания доверительных отношений и применения эмпатии и сочувствия в профессиональном общении с пациентами. В этой статье рассматривается представление об эффективной системе здравоохранения, построенной на основе эмпатии и доверия, которое предлагается специалистами, продвигающими внедрение технологий ИИ в сфере здравоохранения. В статье выдвигается предположение о том, что искусственный интеллект потенциально способен коренным образом изменить сегодняшнее представление о применении эмпатии, сочувствия и доверия в сфере здравоохранения и внедрении соответствующих практик. В дальнейшем важно заново оценить возможность включения этих ценностей в систему здравоохранения, все чаще использующую технологию искусственного интеллекта, и их применения на практике. Что наиболее важно, общество нуждается в пересмотре того, развитие какого типа системы здравоохранения следует поощрять. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284647/  |  
------------------------------------------- 
10.1038/s41467-019-13866-z  |   Data-independent acquisition (DIA) is an emerging technology for quantitative proteomic analysis of large cohorts of samples. However, sample-specific spectral libraries built by data-dependent acquisition (DDA) experiments are required prior to DIA analysis, which is time-consuming and limits the identification/quantification by DIA to the peptides identified by DDA. Herein, we propose DeepDIA, a deep learning-based approach to generate in silico spectral libraries for DIA analysis. We demonstrate that the quality of in silico libraries predicted by instrument-specific models using DeepDIA is comparable to that of experimental libraries, and outperforms libraries generated by global models. With peptide detectability prediction, in silico libraries can be built directly from protein sequence databases. We further illustrate that DeepDIA can break through the limitation of DDA on peptide/protein detection, and enhance DIA analysis on human serum samples compared to the state-of-the-art protocol using a DDA library. We expect this work expanding the toolbox for DIA proteomics. 
  |  http://dx.doi.org/10.1038/s41467-019-13866-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31919359/  |  
------------------------------------------- 
10.1007/s13181-020-00769-5  |   Artificial intelligence (AI) refers to machines or software that process information and interact with the world as understanding beings. Examples of AI in medicine include the automated reading of chest X-rays and the detection of heart dysrhythmias from wearables. A key promise of AI is its potential to apply logical reasoning at the scale of data too vast for the human mind to comprehend. This scaling up of logical reasoning may allow clinicians to bring the entire breadth of current medical knowledge to bear on each patient in real time. It may also unearth otherwise unreachable knowledge in the attempt to integrate knowledge and research across disciplines. In this review, we discuss two complementary aspects of artificial intelligence: deep learning and knowledge representation. Deep learning recognizes and predicts patterns. Knowledge representation structures and interprets those patterns or predictions. We frame this review around how deep learning and knowledge representation might expand the reach of Poison Control Centers and enhance syndromic surveillance from social media. 
  |  https://dx.doi.org/10.1007/s13181-020-00769-5  |  
------------------------------------------- 
10.1007/s00104-019-01089-3  |    Background:  The application of artificial intelligence is a relatively new option to enable improved patient treatment in modern medicine and is therefore currently the focus of many research projects. In the clinical practice the application of artificial intelligence so far seems to be confined to the analysis of medical imaging. 
  Objective:  In which form is the use of artificial intelligence possible in routine daily work in thoracic surgery and is already being practiced? 
  Material and methods:  A search of the currently available literature was performed. 
  Results:  Under current conditions artificial intelligence can best be used as part of diagnostics and treatment planning; however, in order to enable a comprehensive use standardization and evaluation of the centralized data collection are necessary. 
  Conclusion:  At the present time promising study results are available but the implementation into the surgical routine has so far been very difficult. 
  |  https://dx.doi.org/10.1007/s00104-019-01089-3  |  
------------------------------------------- 
10.1007/s11547-020-01135-9  |   The aim of the paper is to find an answer to the question "Who or what is responsible for the benefits and harms of using artificial intelligence in radiology?" When human beings make decisions, the action itself is normally connected with a direct responsibility by the agent who generated the action. You have an effect on others, and therefore, you are responsible for what you do and what you decide to do. But if you do not do this yourself, but an artificial intelligence system, it becomes difficult and important to be able to ascribe responsibility when something goes wrong. The manuscript addresses the following statements: (1) using AI, the radiologist is responsible for the diagnosis; (2) radiologists must be trained on the use of AI since they are responsible for the actions of machines; (3) radiologists involved in R&amp;D have the responsibility to guide the respect of rules for a trustworthy AI; (4) radiologist responsibility is at risk of validating the unknown (black box); (5) radiologist decision may be biased by the AI automation; (6)risk of a paradox: increasing AI tools to compensate the lack of radiologists; (7) need of informed consent and quality measures. Future legislation must outline the contours of the professional's responsibility, with respect to the provision of the service performed autonomously by AI, balancing the professional's ability to influence and therefore correct the machine, limiting the sphere of autonomy that instead technological evolution would like to recognize to robots. 
  |  https://dx.doi.org/10.1007/s11547-020-01135-9  |  
------------------------------------------- 
10.1016/j.jaad.2020.01.028  |    Background:  The use of artificial intelligence (AI) for skin cancer assessment has been an emerging topic in dermatology. Leadership of dermatologists is necessary in defining how these technologies fit into clinical practice. 
  Objective:  To characterize the evolution of AI in skin cancer assessment and characterize the involvement of dermatologists in developing these technologies. 
  Methods:  An electronic literature search was performed using PubMed by searching machine learning or artificial intelligence combined with skin cancer or melanoma. Articles were included if they used AI for screening and diagnosis of skin cancer using data sets consisting of dermoscopic images or photographs of gross lesions. 
  Results:  Fifty-one articles were included, and 41% of these had dermatologists included as authors. Articles that included dermatologists described algorithms built with more images versus articles that did not include dermatologists (mean, 12,111 vs 660 images, respectively). In terms of underlying technology, AI used for skin cancer assessment has followed trends in the field of image recognition. 
  Limitations:  This review focused on models described in the medical literature and did not account for those described elsewhere. 
  Conclusions:  Greater involvement of dermatologists is needed in thinking through issues in data collection, data set biases, and applications of technology. Dermatologists can provide access to large, diverse data sets that are increasingly important for building these models. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0190-9622(20)30079-7  |  
------------------------------------------- 
10.1093/schbul/sbz105  |   The rapid embracing of artificial intelligence in psychiatry has a flavor of being the current "wild west"; a multidisciplinary approach that is very technical and complex, yet seems to produce findings that resonate. These studies are hard to review as the methods are often opaque and it is tricky to find the suitable combination of reviewers. This issue will only get more complex in the absence of a rigorous framework to evaluate such studies and thus nurture trustworthiness. Therefore, our paper discusses the urgency of the field to develop a framework with which to evaluate the complex methodology such that the process is done honestly, fairly, scientifically, and accurately. However, evaluation is a complicated process and so we focus on three issues, namely explainability, transparency, and generalizability, that are critical for establishing the viability of using artificial intelligence in psychiatry. We discuss how defining these three issues helps towards building a framework to ensure trustworthiness, but show how difficult definition can be, as the terms have different meanings in medicine, computer science, and law. We conclude that it is important to start the discussion such that there can be a call for policy on this and that the community takes extra care when reviewing clinical applications of such models.. 
  |  https://academic.oup.com/schizophreniabulletin/article-lookup/doi/10.1093/schbul/sbz105  |  
------------------------------------------- 
10.1007/s00101-020-00764-z  |   The application of artificial intelligence (AI) is currently changing very different areas of life. Artificial intelligence involves the emulation of human behavior with the aid of methods from mathematics and informatics. Machine learning (ML) represents a subdivision of AI. Algorithms for ML have the potential to optimize patient care, in that they can be utilized in a supportive way in personalized medicine, decision making and risk prediction. Although the majority of the applications in medicine are still limited to data analysis and research, it is certain that ML will become increasingly more important in scientific and clinical aspects in this supportive function. Therefore, it is necessary for clinicians to have at least a basic understanding of the functional principles, strengths and weaknesses of ML. 
  |  https://dx.doi.org/10.1007/s00101-020-00764-z  |  
------------------------------------------- 
10.1080/09546634.2019.1708239  |   <b>Background:</b> Automatic skin lesion image identification is of utmost importance to develop a fully automatized computer-aided skin analysis system. This will be helping the medical practitioners to provide skin lesions disease treatment more efficiently and effectively.<b>Material and method:</b> In this article, two image processing techniques for accurate detection of skin lesions have been proposed. In first technique, the optimization of edge detection has been carried out by using a branch of artificial intelligence called nature inspired algorithm. Ant colony optimization (ACO) is used to increase effectiveness of edge detection in skin lesion. The second technique deals with the color space-based split-and-merge process in combination with global thresholding segmentation and edge smoothing operations.<b>Result:</b> The performance of both techniques has been measured by entropy performance evaluation parameter. The results show remarkable improvement in output images obtained by Canny edge detection technique optimized by ACO in comparison with ACO-Sobel, ACO-Prewitt and Edge Smoothing-Color Space techniques.<b>Conclusion:</b> ACO-Canny Edge detection technique shows far better effieciency for skin lesion detection as compared to ACO-Sobel, ACO-Prewitt and Edge Smoothing Color Space technique. 
  |  http://www.tandfonline.com/doi/full/10.1080/09546634.2019.1708239  |  
------------------------------------------- 
10.1093/rap/rkaa005  |   Machine learning as a field of artificial intelligence is increasingly applied in medicine to assist patients and physicians. Growing datasets provide a sound basis with which to apply machine learning methods that learn from previous experiences. This review explains the basics of machine learning and its subfields of supervised learning, unsupervised learning, reinforcement learning and deep learning. We provide an overview of current machine learning applications in rheumatology, mainly supervised learning methods for e-diagnosis, disease detection and medical image analysis. In the future, machine learning will be likely to assist rheumatologists in predicting the course of the disease and identifying important disease factors. Even more interestingly, machine learning will probably be able to make treatment propositions and estimate their expected benefit (e.g. by reinforcement learning). Thus, in future, shared decision-making will not only include the patient's opinion and the rheumatologist's empirical and evidence-based experience, but it will also be influenced by machine-learned evidence. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296743/  |  
------------------------------------------- 
10.1371/journal.pone.0226634  |    Purpose:  The objective of this study was to assess the classification capability of Breast Imaging Reporting and Data System (BI-RADS) ultrasound feature descriptors targeting established commercial transcriptomic gene signatures that guide management of breast cancer. 
  Materials and methods:  This retrospective, single-institution analysis of 219 patients involved two cohorts using one of two FDA approved transcriptome-based tests that were performed as part of the clinical care of breast cancer patients at Harbor-UCLA Medical Center between April 2008 and January 2013. BI-RADS descriptive terminology was collected from the corresponding ultrasound reports for each patient in conjunction with transcriptomic test results. Recursive partitioning and regression trees were used to test and validate classification of the two cohorts. 
  Results:  The area under the curve (AUC) of the receiver operator curves (ROC) for the regression classifier between the two FDA approved tests and ultrasound features were 0.77 and 0.65, respectively; they employed the 'margins', 'retrotumoral', and 'internal echoes' feature descriptors. Notably, the 'retrotumoral' and mass 'margins' features were used in both classification trees. The identification of sonographic correlates of gene tests provides added value to the ultrasound exam without incurring additional procedures or testing. 
  Conclusions:  The predictive capability using structured language from diagnostic ultrasound reports (BI-RADS) was moderate for the two tests, and provides added value from ultrasound imaging without incurring any additional costs. Incorporation of additional measures, such as ultrasound contrast enhancement, with validation in larger, prospective studies may further substantiate these results and potentially demonstrate even greater predictive utility. 
  |  http://dx.plos.org/10.1371/journal.pone.0226634  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31923222/  |  
------------------------------------------- 
10.1016/j.ad.2018.08.017  |    |  http://www.elsevier.es/en/linksolver/pdf/pii/S0001-7310(20)30007-7  |  
------------------------------------------- 
10.1371/journal.pone.0228645  |    Background:  As an essential component in reducing anthropogenic CO2 emissions to the atmosphere, tree planting is the key to keeping carbon dioxide emissions under control. In 1992, the United Nations agreed to take action at the Earth Summit to stabilize and reduce net zero global anthropogenic CO2 emissions. Tree planting was identified as an effective method to offset CO2 emissions. A high net photosynthetic rate (Pn) with fast-growing trees could efficiently fulfill the goal of CO2 emission reduction. Net photosynthetic rate model can provide refernece for plant's stability of photosynthesis productivity. 
  Methods and results:  Using leaf phenotype data to predict the Pn can help effectively guide tree planting policies to offset CO2 release into the atmosphere. Tree planting has been proposed as one climate change solution. One of the most popular trees to plant are poplars. This study used a Populus simonii (P. simonii) dataset collected from 23 artificial forests in northern China. The samples represent almost the entire geographic distribution of P. simonii. The geographic locations of these P. simonii trees cover most of the major provinces of northern China. The northwestern point reaches (36°30'N, 98°09'E). The northeastern point reaches (40°91'N, 115°83'E). The southwestern point reaches (32°31'N, 108°90'E). The southeastern point reaches (34°39'N, 113°74'E). The collected data on leaf phenotypic traits are sparse, noisy, and highly correlated. The photosynthetic rate data are nonnormal and skewed. Many machine learning algorithms can produce reasonably accurate predictions despite these data issues. Influential outliers are removed to allow an accurate and precise prediction, and cluster analysis is implemented as part of a data exploratory analysis to investigate further details in the dataset. We select four regression methods, extreme gradient boosting (XGBoost), support vector machine (SVM), random forest (RF) and generalized additive model (GAM), which are suitable to use on the dataset given in this study. Cross-validation and regularization mechanisms are implemented in the XGBoost, SVM, RF, and GAM algorithms to ensure the validity of the outputs. 
  Conclusions:  The best-performing approach is XGBoost, which generates a net photosynthetic rate prediction that has a 0.77 correlation with the actual rates. Moreover, the root mean square error (RMSE) is 2.57, which is approximately 35 percent smaller than the standard deviation of 3.97. The other metrics, i.e., the MAE, R2, and the min-max accuracy are 1.12, 0.60, and 0.93, respectively. This study demonstrates the ability of machine learning models to use noisy leaf phenotype data to predict the net photosynthetic rate with significant accuracy. Most net photosynthetic rate prediction studies are conducted on herbaceous plants. The net photosynthetic rate prediction of P. simonii, a kind of woody plant, illustrates significant guidance for plant science or environmental science regarding the predictive relationship between leaf phenotypic characteristics and the Pn for woody plants in northern China. 
  |  http://dx.plos.org/10.1371/journal.pone.0228645  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32045452/  |  
------------------------------------------- 
10.1371/journal.pcbi.1007511  |   Antimicrobial resistance (AMR) is an increasing threat to public health. Current methods of determining AMR rely on inefficient phenotypic approaches, and there remains incomplete understanding of AMR mechanisms for many pathogen-antimicrobial combinations. Given the rapid, ongoing increase in availability of high-density genomic data for a diverse array of bacteria, development of algorithms that could utilize genomic information to predict phenotype could both be useful clinically and assist with discovery of heretofore unrecognized AMR pathways. To facilitate understanding of the connections between DNA variation and phenotypic AMR, we developed a new bioinformatics tool, variant mapping and prediction of antibiotic resistance (VAMPr), to (1) derive gene ortholog-based sequence features for protein variants; (2) interrogate these explainable gene-level variants for their known or novel associations with AMR; and (3) build accurate models to predict AMR based on whole genome sequencing data. We curated the publicly available sequencing data for 3,393 bacterial isolates from 9 species that contained AMR phenotypes for 29 antibiotics. We detected 14,615 variant genotypes and built 93 association and prediction models. The association models confirmed known genetic antibiotic resistance mechanisms, such as blaKPC and carbapenem resistance consistent with the accurate nature of our approach. The prediction models achieved high accuracies (mean accuracy of 91.1% for all antibiotic-pathogen combinations) internally through nested cross validation and were also validated using external clinical datasets. The VAMPr variant detection method, association and prediction models will be valuable tools for AMR research for basic scientists with potential for clinical applicability. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007511  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31929521/  |  
------------------------------------------- 
10.1016/j.jmir.2020.01.008  |   Artificial intelligence (AI) and machine learning (ML) approaches have caught the attention of many in health care. Current literature suggests there are many potential benefits that could transform future clinical workflows and decision making. Embedding AI and ML concepts in radiation therapy education could be a fundamental step in equipping radiation therapists (RTs) to engage in competent and safe practice as they utilise clinical technologies. In this discussion paper, the authors provide a brief review of some applications of AI and ML in radiation therapy and discuss pertinent considerations for radiation therapy curriculum enhancement. As the current literature suggests, AI and ML approaches will impose changes to routine clinical radiation therapy tasks. The emphasis in RT education could be on critical evaluation of AI and ML application in routine clinical workflows and gaining an understanding of the impact on quality assurance, provision of quality of care and safety in radiation therapy as well as research. It is also imperative RTs have a broader understanding of AI/ML impact on health care, including ethical and legal considerations. The paper concludes with recommendations and suggestions to deliberately embed AI and ML aspects in RT education to empower future RT practitioners. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1939-8654(20)30008-4  |  
------------------------------------------- 
10.3390/cancers12040797  |   Digital Pathology is the process of converting histology glass slides to digital images using sophisticated computerized technology to facilitate acquisition, evaluation, storage, and portability of histologic information. By its nature, digitization of analog histology data renders it amenable to analysis using deep learning/artificial intelligence (DL/AI) techniques. The application of DL/AI to digital pathology data holds promise, even if the scope of use cases and regulatory framework for deploying such applications in the clinical environment remains in the early stages. Recent studies using whole-slide images and DL/AI to detect histologic abnormalities in general and cancer in particular have shown encouraging results. In this review, we focus on these emerging technologies intended for use in diagnostic hematology and the evaluation of lymphoproliferative diseases. 
  |  http://www.mdpi.com/resolver?pii=cancers12040797  |  
------------------------------------------- 
10.1080/21507740.2020.1740352  |   Clinical neuroscience is increasingly relying on the collection of large volumes of differently structured data and the use of intelligent algorithms for data analytics. In parallel, the ubiquitous collection of unconventional data sources (e.g. mobile health, digital phenotyping, consumer neurotechnology) is increasing the variety of data points. Big data analytics and approaches to Artificial Intelligence (AI) such as advanced machine learning are showing great potential to make sense of these larger and heterogeneous data flows. AI provides great opportunities for making new discoveries about the brain, improving current preventative and diagnostic models in both neurology and psychiatry and developing more effective assistive neurotechnologies. Concurrently, it raises many new methodological and ethical challenges. Given their transformative nature, it is still largely unclear how AI-driven approaches to the study of the human brain will meet adequate standards of scientific validity and affect normative instruments in neuroethics and research ethics. This manuscript provides an overview of current AI-driven approaches to clinical neuroscience and an assessment of the associated key methodological and ethical challenges. In particular, it will discuss what ethical principles are primarily affected by AI approaches to human neuroscience, and what normative safeguards should be enforced in this domain. 
  |  None  |  
------------------------------------------- 
10.1017/S0022215120000444  |    Objective:  Convolutional neural networks are a subclass of deep learning or artificial intelligence that are predominantly used for image analysis and classification. This proof-of-concept study attempts to train a convolutional neural network algorithm that can reliably determine if the middle turbinate is pneumatised (concha bullosa) on coronal sinus computed tomography images. 
  Method:  Consecutive high-resolution computed tomography scans of the paranasal sinuses were retrospectively collected between January 2016 and December 2018 at a tertiary rhinology hospital in Australia. The classification layer of Inception-V3 was retrained in Python using a transfer learning method to interpret the computed tomography images. Segmentation analysis was also performed in an attempt to increase diagnostic accuracy. 
  Results:  The trained convolutional neural network was found to have diagnostic accuracy of 81 per cent (95 per cent confidence interval: 73.0-89.0 per cent) with an area under the curve of 0.93. 
  Conclusion:  A trained convolutional neural network algorithm appears to successfully identify pneumatisation of the middle turbinate with high accuracy. Further studies can be pursued to test its ability in other clinically important anatomical variants in otolaryngology and rhinology. 
  |  https://www.cambridge.org/core/product/identifier/S0022215120000444/type/journal_article  |  
------------------------------------------- 
10.5946/ce.2020.038  |   Artificial intelligence (AI) is rapidly integrating into modern technology and clinical practice. Although in its nascency, AI has become a hot topic of investigation for applications in clinical practice. Multiple fields of medicine have embraced the possibility of a future with AI assisting in diagnosis and pathology applications. In the field of gastroenterology, AI has been studied as a tool to assist in risk stratification, diagnosis, and pathologic identification. Specifically, AI has become of great interest in endoscopy as a technology with substantial potential to revolutionize the practice of a modern gastroenterologist. From cancer screening to automated report generation, AI has touched upon all aspects of modern endoscopy. Here, we review landmark AI developments in endoscopy. Starting with broad definitions to develop understanding, we will summarize the current state of AI research and its potential applications. With innovation developing rapidly, this article touches upon the remarkable advances in AI-assisted endoscopy since its initial evaluation at the turn of the millennium, and the potential impact these AI models may have on the modern clinical practice. As with any discussion of new technology, its limitations must also be understood to apply clinical AI tools successfully. 
  |  https://dx.doi.org/10.5946/ce.2020.038  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32252506/  |  
------------------------------------------- 
10.3758/s13428-019-01312-3  |   Analogical reasoning is an active topic of investigation across education, artificial intelligence (AI), cognitive psychology, and related fields. In all fields of inquiry, explicit analogy problems provide useful tools for investigating the mechanisms underlying analogical reasoning. Such sets have been developed by researchers working in the fields of educational testing, AI, and cognitive psychology. However, these analogy tests have not been systematically made accessible across all the relevant fields. The present paper aims to remedy this situation by presenting a working inventory of verbal analogy problem sets, intended to capture and organize sets from diverse sources. 
  |  https://dx.doi.org/10.3758/s13428-019-01312-3  |  
------------------------------------------- 
10.1007/s12124-020-09523-6  |   Can artificial intelligence (AI) develop the potential to be our partner, and will we be as sensitive to its social signals as we are to those of human beings? I examine both of these questions and how cultural psychology might add such questions to its research agenda. There are three areas in which I believe there is a need for both a better understanding and added perspective. First, I will present some important concepts and ideas from the world of AI that might be beneficial for pursuing research topics focused on AI within the cultural psychology research agenda. Second, there are some very interesting questions that must be answered with respect to central notions in cultural psychology as these are tested through human interactions with AI. Third, I claim that social robots are parasitic to deeply ingrained human social behaviour, in the sense that they exploit and feed upon processes and mechanisms that evolved for purposes that were originally completely alien to human-computer interactions. 
  |  https://dx.doi.org/10.1007/s12124-020-09523-6  |  
------------------------------------------- 
10.1016/j.acra.2019.09.010  |    Rationale and objectives:  Artificial intelligence (AI) is playing a growing role in the field of radiology. This article seeks to help readers quantify its impact when put into practice, using a lung nodule flagger as an example. 
  Materials and methods:  The one-time and ongoing costs associated with AI are explored. Costs are divided into three categories: direct costs, costs associated with operational changes, and downstream costs. Examples of each are provided. 
  Results:  A framework for estimating the financial impact of AI is provided. 
  Conclusion:  The impact of AI is quantifiable, but estimates of its financial impact may not be portable across contexts. Different organizations may implement AI in different ways due to differences in clinical practices. Furthermore, different organizations have different hurdle rates for their investments. Finally, international cost-effectiveness analyses may not be generalizable due to differences in both practice patterns and the valuation placed upon quality. When quantifying the impact of AI, organizations should consider relying upon pilots and data from other similarly-situated organizations. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30437-4  |  
------------------------------------------- 
10.15252/embr.202050036  |   Synthetic biology and artificial intelligence naturally converge in the biofoundry. Navigating the ethical and societal issues of the biofoundry's potential remains a major challenge. 
  |  https://doi.org/10.15252/embr.202050036  |  
------------------------------------------- 
10.1161/CIRCULATIONAHA.120.045967  |    |  http://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.120.045967?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1007/s12350-020-02057-9  |    |  https://dx.doi.org/10.1007/s12350-020-02057-9  |  
------------------------------------------- 
10.1089/end.2019.0509  |   <b><i>Introduction:</i></b> Nonmuscle-invasive bladder cancer has a relatively high postoperative recurrence rate despite the implementation of conventional treatment methods. Cystoscopy is essential for diagnosing and monitoring bladder cancer, but lesions are overlooked while using white-light imaging. Using cystoscopy, tumors with a small diameter; flat tumors, such as carcinoma <i>in situ</i>; and the extent of flat lesions associated with the elevated lesions are difficult to identify. In addition, the accuracy of diagnosis and treatment using cystoscopy varies according to the skill and experience of physicians. Therefore, to improve the quality of bladder cancer diagnosis, we aimed to support the cystoscopic diagnosis of bladder cancer using artificial intelligence (AI). <b><i>Materials and Methods:</i></b> A total of 2102 cystoscopic images, consisting of 1671 images of normal tissue and 431 images of tumor lesions, were used to create a dataset with an 8:2 ratio of training and test images. We constructed a tumor classifier based on a convolutional neural network (CNN). The performance of the trained classifier was evaluated using test data. True-positive rate and false-positive rate were plotted when the threshold was changed as the receiver operating characteristic (ROC) curve. <b><i>Results:</i></b> In the test data (tumor image: 87, normal image: 335), 78 images were true positive, 315 true negative, 20 false positive, and 9 false negative. The area under the ROC curve was 0.98, with a maximum Youden index of 0.837, sensitivity of 89.7%, and specificity of 94.0%. <b><i>Conclusion:</i></b> By objectively evaluating the cystoscopic image with CNN, it was possible to classify the image, including tumor lesions and normality. The objective evaluation of cystoscopic images using AI is expected to contribute to improvement in the accuracy of the diagnosis and treatment of bladder cancer. 
  |  https://www.liebertpub.com/doi/full/10.1089/end.2019.0509?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31808367/  |  
------------------------------------------- 
10.1186/s13049-020-0713-4  |    Background:  In emergency medical services (EMSs), accurately predicting the severity of a patient's medical condition is important for the early identification of those who are vulnerable and at high-risk. In this study, we developed and validated an artificial intelligence (AI) algorithm based on deep learning to predict the need for critical care during EMS. 
  Methods:  We conducted a retrospective observation cohort study. The algorithm was established using development data from the Korean national emergency department information system, which were collected during visits in real time from 151 emergency departments (EDs). We validated the algorithm using EMS run sheets from two EDs. The study subjects comprised adult patients who visited EDs. The endpoint was critical care, and we used age, sex, chief complaint, symptom onset to arrival time, trauma, and initial vital signs as the predicted variables. 
  Results:  The number of patients in the development data was 8,981,181, and the validation data comprised 2604 EMS run sheets from two hospitals. The area under the receiver operating characteristic curve of the algorithm to predict the critical care was 0.867 (95% confidence interval, [0.864-0.871]). This result outperformed the Emergency Severity Index (0.839 [0.831-0.846]), Korean Triage and Acuity System (0.824 [0.815-0.832]), National Early Warning Score (0.741 [0.734-0.748]), and Modified Early Warning Score (0.696 [0.691-0.699]). 
  Conclusions:  The AI algorithm accurately predicted the need for the critical care of patients using information during EMS and outperformed the conventional triage tools and early warning scores. 
  |  https://sjtrem.biomedcentral.com/articles/10.1186/s13049-020-0713-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32131867/  |  
------------------------------------------- 
10.1259/dmfr.20190441  |    Objectives:  This study aimed to develop five different supervised machine learning (ML) classifier models using artificial intelligence (AI) techniques and to compare their performance for cervical vertebral maturation (CVM) analysis. A clinical decision support system (CDSS) was developed for more objective results. 
  Methods:  A total of 647 digital lateral cephalometric radiographs with visible C2, C3, C4 and C5 vertebrae were chosen. Newly developed software was used for manually labelling the samples, with the integrated CDSS developed by evaluation of 100 radiographs. On each radiograph, 26 points were marked, and the CDSS generated a suggestion according to the points and CVM analysis performed by the human observer. For each sample, 54 features were saved in text format and classified using logistic regression (LR), support vector machine, random forest, artificial neural network (ANN) and decision tree (DT) models. The weighted κ coefficient was used to evaluate the concordance of classification and expert visual evaluation results. 
  Results:  Among the CVM stage classifier models, the best result was achieved using the ANN model (κ = 0.926). Among cervical vertebrae morphology classifier models, the best result was achieved using the LR model (κ = 0.968) for the presence of concavity, and the DT model (κ = 0.949) for vertebral body shapes. 
  Conclusions:  This study has proposed ML models for CVM assessment on lateral cephalometric radiographs, which can be used for the prediction of cervical vertebrae morphology. Further studies should be done especially of forensic applications of AI models through CVM evaluations. 
  |  http://www.birpublications.org/doi/full/10.1259/dmfr.20190441?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3390/jcm9030678  |   Acute kidney injury (AKI) is a frequent complication in hospitalized patients, which is associated with worse short and long-term outcomes. It is crucial to develop methods to identify patients at risk for AKI and to diagnose subclinical AKI in order to improve patient outcomes. The advances in clinical informatics and the increasing availability of electronic medical records have allowed for the development of artificial intelligence predictive models of risk estimation in AKI. In this review, we discussed the progress of AKI risk prediction from risk scores to electronic alerts to machine learning methods. 
  |  http://www.mdpi.com/resolver?pii=jcm9030678  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32138284/  |  
------------------------------------------- 
10.1073/pnas.1907373117  |   Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals. 
  |  http://www.pnas.org/cgi/pmidlookup?view=long&pmid=31992643  |  
------------------------------------------- 
10.1016/j.retram.2020.01.002  |    Motivation:  As a result of the worldwide health care system digitalization trend, the produced healthcare data is estimated to reach as much as 2314 Exabytes of new data generated in 2020. The ongoing development of intelligent systems aims to provide better reasoning and to more efficiently use the data collected. This use is not restricted retrospective interpretation, that is, to provide diagnostic conclusions. It can also be extended to prospective interpretation providing early prognosis. That said, physicians who could be assisted by these systems find themselves standing in the gap between clinical case and deep technical reviews. What they lack is a clear starting point from which to approach the world of machine learning in medicine. 
  Methodology and main structure:  This article aims at providing interested physicians with an easy-to-follow insight of Artificial Intelligence (AI) and Machine Learning (ML) use in the medical field, primarily over the last few years. To this end, we first discuss the general developmental paths concerning AI and ML concept usage in healthcare systems. We then list fields where these technologies are already being put to the test or even applied such as in Hematology, Neurology, Cardiology, Oncology, Radiology, Ophthalmology, Cell Biology and Cell Therapy. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S2452-3186(20)30019-2  |  
------------------------------------------- 
10.1111/cas.14377  |   Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade. 
  |  https://doi.org/10.1111/cas.14377  |  
------------------------------------------- 
10.1007/s00104-019-01051-3  |    Background:  Artificial intelligence, automatization and digital transformation increasingly dominate the business models of almost all enterprises. Even in medicine and medical technology, companies also no longer close their minds to this development as the advantages provided by the new ideas and processes in medicine and particularly in compact disciplines, such as pediatric surgery have occasionally been recognized. 
  Objective:  This article gives a status quo analysis of artificial intelligence in international pediatric surgery with a discussion of future perspectives and suggestions from the authors' perspective. 
  Material and methods:  Appraisal and discussion of international publications, external expert opinions and personal experiences of the authors. 
  Results:  A wide spectrum of applications using artificial intelligence in surgery is internationally available. Many of these developments can also be further adapted for use in pediatric surgery. The experience using artificial intelligence for special pediatric surgical indications is currently limited to isolated cases. 
  Conclusion:  Disciplines such as pediatric surgery cannot disregard the trend towards the application of artificial intelligence in daily practice. In addition to the establishment of current developments, the requirements of pediatric surgery should also be taken into account. These were some of the impulses that led to the founding of the working group on digitalization of the German Association for Pediatric Surgery in September 2019. 
  |  https://dx.doi.org/10.1007/s00104-019-01051-3  |  
------------------------------------------- 
10.1016/j.acra.2019.07.030  |   Artificial intelligence and deep learning are areas of high interest for radiology investigators at present. However, the field of machine learning encompasses multiple statistics-based techniques useful for investigators, which may be complementary to deep learning approaches. After a refresher in basic statistical concepts, relevant considerations for machine learning practitioners are reviewed: regression, classification, decision boundaries, and bias-variance tradeoff. Regularization, ground truth, and populations are discussed along with compute and data management principles. Advanced statistical machine learning techniques including bootstrapping, bagging, boosting, decision trees, random forest, XGboost, and support vector machines are reviewed along with relevant examples from the radiology literature. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30439-8  |  
------------------------------------------- 
10.1111/jop.13013  |   Oral cancer is easily detectable by physical (self) examination. However, many cases of oral cancer are detected late, which causes unnecessary morbidity and mortality. Screening of high-risk populations seems beneficial, but these populations are commonly located in regions with limited access to healthcare. The advent of information technology and its modern derivative Artificial Intelligence (AI) promises to improve oral cancer screening but to date, few efforts have been made to apply these techniques and relatively little research has been conducted to retrieve meaningful information from AI data. In this paper, we discuss the promise of AI to improve the quality and reach of oral cancer screening and its potential effect on improving mortality and unequal access to health care around the world. 
  |  https://doi.org/10.1111/jop.13013  |  
------------------------------------------- 
10.1002/bab.1936  |   COVID-19 threatens millions of lives especially elderly population and people with chronic diseases including diabetes, hypertension, cancer and cardiovascular diseases. Rapid and effective diagnosis are vital for the isolation of infected people and starting treatment immediately to stop the spread of COVID-19 virus. Bioinformatics techniques such as artificial intelligence should be used for collecting the hemogram and serum biochemistry data of all infected people by COVID-19 worldwide even they do not show severe symptoms. These data may be help to find a biomarker which can be used in combination with the CT results for rapid and accurate diagnosis of COVID-19. This article is protected by copyright. All rights reserved. 
  |  None  |  
------------------------------------------- 
10.1002/mus.26819  |   Neuromuscular ultrasound is an accepted and valuable element in the evaluation of peripheral nerve and muscle disease. However, ultrasound has several limitations to consider, including operator dependency and lack of a viable contrast agent. Fortunately, new technological advances show promise in resolving these issues. Ultra-high-resolution ultrasound enables imaging of the nerve at the fascicular level. Shear wave elastography imaging can provide measures of tissue stiffness that can act as a surrogate measure of nerve and muscle health. Photoacoustic imaging may overcome neuromuscular ultrasound's current lack of contrast agents to detect inflammation and other functional changes within nerve and muscle, while artificial intelligence stands to address operator dependency and improve diagnostic imaging. The basic principles of each of these technologies are discussed along with current research and potential future applications in neuromuscular imaging. This article is protected by copyright. All rights reserved. 
  |  https://doi.org/10.1002/mus.26819  |  
------------------------------------------- 
10.7861/fhj.2020-0001  |   As the surgical workforce, surgical techniques and patient expectations change, the Royal College of Surgeons of England is actively engaged in taking forward the recommendations of its Future of Surgery Commission. Here the commission's chair articulates the implications for smaller hospitals and the need for achieving interoperability and safe sharing of patient data across different systems, so enabling immediate access to patients' records across healthcare organisations; extension of regulation to surgical care practitioners, reflecting the recent decision to regulate physician associates and physician assistants; introducing a UK-wide registry of surgical devices, with tracking for implantable devices; implementing a robotics strategy to help the NHS plan and purchase new surgical robotics, as well as monitor their use and the effect on outcomes; and investing in genomic medicine and artificial intelligence for diagnostics, and in stem-cell research for treatment. 
  |  https://www.rcpjournals.org/lookup/pmid/32104765  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32104765/  |  
------------------------------------------- 
10.1016/j.acra.2019.09.009  |   There is a plethora of Artificial Intelligence (AI) tools that are being developed around the world aiming at either speeding up or improving the accuracy of radiologists. It is essential for radiologists to work with the developers of such algorithms to determine true clinical utility and risks associated with these algorithms. We present a framework, called an Algorithmic Audit, for working with the developers of such algorithms to test and improve the performance of the algorithms. The framework includes concepts of true independent validation on data that the algorithm has not seen before, curating datasets for such testing, deep examination of false positives and false negatives (to examine implications of such errors) and real-world deployment and testing of algorithms. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30435-0  |  
------------------------------------------- 
10.3348/kjr.2019.0312  |   Artificial intelligence has been applied to many industries, including medicine. Among the various techniques in artificial intelligence, deep learning has attained the highest popularity in medical imaging in recent years. Many articles on deep learning have been published in radiologic journals. However, radiologists may have difficulty in understanding and interpreting these studies because the study methods of deep learning differ from those of traditional radiology. This review article aims to explain the concepts and terms that are frequently used in deep learning radiology articles, facilitating general radiologists' understanding. 
  |  https://www.kjronline.org/DOIx.php?id=10.3348/kjr.2019.0312  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31920027/  |  
------------------------------------------- 
10.1186/s13244-020-00866-7  |    Objectives:  To analyze all artificial intelligence abstracts presented at the European Congress of Radiology (ECR) 2019 with regard to their topics and their adherence to the Standards for Reporting Diagnostic accuracy studies (STARD) checklist. 
  Methods:  A total of 184 abstracts were analyzed with regard to adherence to the STARD criteria for abstracts as well as the reported modality, body region, pathology, and use cases. 
  Results:  Major topics of artificial intelligence abstracts were classification tasks in the abdomen, chest, and brain with CT being the most commonly used modality. Out of the 10 STARD for abstract criteria analyzed in the present study, on average, 5.32 (SD = 1.38) were reported by the 184 abstracts. Specifically, the highest adherence with STARD for abstracts was found for general interpretation of results of abstracts (100.0%, 184 of 184), clear study objectives (99.5%, 183 of 184), and estimates of diagnostic accuracy (96.2%, 177 of 184). The lowest STARD adherence was found for eligibility criteria for participants (9.2%, 17 of 184), type of study series (13.6%, 25 of 184), and implications for practice (20.7%, 44 of 184). There was no significant difference in the number of reported STARD criteria between abstracts accepted for oral presentation (M = 5.35, SD = 1.31) and abstracts accepted for the electronic poster session (M = 5.39, SD = 1.45) (p = .86). 
  Conclusions:  The adherence with STARD for abstract was low, indicating that providing authors with the related checklist may increase the quality of abstracts. 
  |  https://dx.doi.org/10.1186/s13244-020-00866-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32335763/  |  
------------------------------------------- 
10.1093/jamia/ocz192  |   As the efficacy of artificial intelligence (AI) in improving aspects of healthcare delivery is increasingly becoming evident, it becomes likely that AI will be incorporated in routine clinical care in the near future. This promise has led to growing focus and investment in AI medical applications both from governmental organizations and technological companies. However, concern has been expressed about the ethical and regulatory aspects of the application of AI in health care. These concerns include the possibility of biases, lack of transparency with certain AI algorithms, privacy concerns with the data used for training AI models, and safety and liability issues with AI application in clinical environments. While there has been extensive discussion about the ethics of AI in health care, there has been little dialogue or recommendations as to how to practically address these concerns in health care. In this article, we propose a governance model that aims to not only address the ethical and regulatory issues that arise out of the application of AI in health care, but also stimulate further discussion about governance of AI in health care. 
  |  https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocz192  |  
------------------------------------------- 
10.1007/s43441-019-00030-4  |    Background:  Delays in clinical trial enrollment and difficulties enrolling representative samples continue to vex sponsors, sites, and patient populations. Here we investigated use of an artificial intelligence-powered technology, Mendel.ai, as a means of overcoming bottlenecks and potential biases associated with standard patient prescreening processes in an oncology setting. 
  Methods:  Mendel.ai was applied retroactively to 2 completed oncology studies (1 breast, 1 lung), and 1 study that failed to enroll (lung), at the Comprehensive Blood and Cancer Center, allowing direct comparison between results achieved using standard prescreening practices and results achieved with Mendel.ai. Outcome variables included the number of patients identified as potentially eligible and the elapsed time between eligibility and identification. 
  Results:  For each trial that enrolled, use of Mendel.ai resulted in a 24% to 50% increase over standard practices in the number of patients correctly identified as potentially eligible. No patients correctly identified by standard practices were missed by Mendel.ai. For the nonenrolling trial, both approaches failed to identify suitable patients. An average of 19 days for breast and 263 days for lung cancer patients elapsed between actual patient eligibility (based on clinical chart information) and identification when the standard prescreening practice was used. In contrast, ascertainment of potential eligibility using Mendel.ai took minutes. 
  Conclusions:  This study suggests that augmentation of human resources with artificial intelligence could yield sizable improvements over standard practices in several aspects of the patient prescreening process, as well as in approaches to feasibility, site selection, and trial selection. 
  |  None  |  
------------------------------------------- 
10.1002/jmrs.385  |   Studies have shown that the use of artificial intelligence can reduce errors in medical image assessment. The diagnosis of breast cancer is an essential task; however, diagnosis can include 'detection' and 'interpretation' errors. Studies to reduce these errors have shown the feasibility of using convolution neural networks (CNNs). This narrative review presents recent studies in diagnosing mammographic malignancy investigating the accuracy and reliability of these CNNs. Databases including ScienceDirect, PubMed, MEDLINE, British Medical Journal and Medscape were searched using the terms 'convolutional neural network or artificial intelligence', 'breast neoplasms [MeSH] or breast cancer or breast carcinoma' and 'mammography [MeSH Terms]'. Articles collected were screened under the inclusion and exclusion criteria, accounting for the publication date and exclusive use of mammography images, and included only literature in English. After extracting data, results were compared and discussed. This review included 33 studies and identified four recurring categories of studies: the differentiation of benign and malignant masses, the localisation of masses, cancer-containing and cancer-free breast tissue differentiation and breast classification based on breast density. CNN's application in detecting malignancy in mammography appears promising but requires further standardised investigations before potentially becoming an integral part of the diagnostic routine in mammography. 
  |  https://doi.org/10.1002/jmrs.385  |  
------------------------------------------- 
10.1177/0846537120918338  |   Emergency and trauma radiologists, emergency department's physicians and nurses, researchers, departmental leaders, and health policymakers have attempted to discover efficient approaches to enhance the provision of quality patient care. There are increasing expectations for radiology practices to deliver a dedicated emergency radiology service providing 24/7/365 on-site attending radiologist coverage. Emergency radiologists (ERs) are pressed to meet the demand of increased imaging volume, provide accurate reports, maintain a lower proportion of discrepancy rate, and with a rapid report turnaround time of finalized reports. Thus, rendering the radiologists overburdened. The demand for an increased efficiency in providing quality care to acute patients has led to the emergence of artificial intelligence (AI) in the field. AI can be used to assist emergency and trauma radiologists deal with the ever-increasing imaging volume and workload, as AI methods have typically demonstrated a variety of applications in medical image analysis and interpretation, albeit most programs are in a training or validation phase. This article aims to offer an evidence-based discourse about the evolving role of artificial intelligence in assisting the imaging pathway in an emergency and trauma radiology department. We hope to generate a multidisciplinary discourse that addresses the technical processes, the challenges in the labour-intensive process of training, validation and testing of an algorithm, the need for emphasis on ethics, and how an emergency radiologist's role is pivotal in the execution of AI-guided systems within the context of an emergency and trauma radiology department. This exploratory narrative serves the present-day health leadership's information needs by proposing an AI supported and radiologist centered framework depicting the work flow within a department. It is suspected that the use of such a framework, if efficacious, could provide considerable benefits for patient safety and quality of care provided. Additionally, alleviating radiologist burnout and decreasing healthcare costs over time. 
  |  http://journals.sagepub.com/doi/full/10.1177/0846537120918338?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.acra.2020.01.012  |   We deem a computer to exhibit artificial intelligence (AI) when it performs a task that would normally require intelligent action by a human. Much of the recent excitement about AI in the medical literature has revolved around the ability of AI models to recognize anatomy and detect pathology on medical images, sometimes at the level of expert physicians. However, AI can also be used to solve a wide range of noninterpretive problems that are relevant to radiologists and their patients. This review summarizes some of the newer noninterpretive uses of AI in radiology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30039-8  |  
------------------------------------------- 
10.1016/j.acra.2020.01.035  |    Rationale and objectives:  Misdiagnosis of intracranial hemorrhage (ICH) can adversely impact patient outcomes. The increasing workload on the radiologists may increase the chance of error and compromise the quality of care provided by the radiologists. 
  Materials and methods:  We used an FDA approved artificial intelligence (AI) solution based on a convolutional neural network to assess the prevalence of ICH in scans, which were reported as negative for ICH. We retrospectively applied the AI solution to all consecutive noncontrast computed tomography (CT) head scans performed at eight imaging sites affiliated to our institution. 
  Results:  In the 6565 noncontrast CT head scans, which met the inclusion criteria, 5585 scans were reported to have no ICH ("negative-by-report" cases). We applied AI solution to these "negative-by-report" cases. AI solution suggested there were ICH in 28 of these scans ("negative-by-report" and "positive-by-AI solution"). After consensus review by three neuroradiologists, 16 of these scans were found to have ICH, which was not reported (missed diagnosis by radiologists), with a false-negative rate of radiologists for ICH detection at 1.6%. Most commonly missed ICH was overlying the cerebral convexity and in the parafalcine regions. 
  Conclusion:  Our study demonstrates that an AI solution can help radiologists to diagnose ICH and thus decrease the error rate. AI solution can serve as a prospective peer review tool for non-contrast head CT scans to identify ICH and thus minimize false negatives. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30084-2  |  
------------------------------------------- 
10.1007/s11356-020-08666-8  |   The discussers wish to thank the authors of the original paper for investigating the comparing accuracy of artificial intelligence techniques trained to predict chlorophyll-a in US lakes. In the original paper (Luo et al., Environ Sci Pollut Res 26: 30524-30532, 2019), four data-driven models were established to estimate the chlorophyll-a (CHLA) values in natural and man-made lakes. Three of these models are adaptive neuro-fuzzy inference system (ANFIS)-based, while one is (artificial neural network) ANN-based. The authors used total phosphorus (TP), total nitrogen (TN), turbidity (TB), and the Secchi depth (SD) as independent variables in order to predict CHLA. They stated that ANFIS with subtractive clustering method (ANFIS_SC) models and multilayer perceptron neural network (MLPNN) models gives higher accuracy in the prediction of CHLA values for natural lakes and man-made lakes, respectively. In this letter, some of the missing points in the original publication, which is important for the estimation and comparison of CHLA values in two different lake sets that differ according to the type of formation, are highlighted. In addition, several points are mentioned in order to make these points more clarified for potential readers. 
  |  https://dx.doi.org/10.1007/s11356-020-08666-8  |  
------------------------------------------- 
10.3390/ani10010132  |   Camera trapping has become an increasingly reliable and mainstream tool for surveying a diversity of wildlife species. Concurrent with this has been an increasing effort to involve the wider public in the research process, in an approach known as 'citizen science'. To date, millions of people have contributed to research across a wide variety of disciplines as a result. Although their value for public engagement was recognised early on, camera traps were initially ill-suited for citizen science. As camera trap technology has evolved, cameras have become more user-friendly and the enormous quantities of data they now collect has led researchers to seek assistance in classifying footage. This has now made camera trap research a prime candidate for citizen science, as reflected by the large number of camera trap projects now integrating public participation. Researchers are also turning to Artificial Intelligence (AI) to assist with classification of footage. Although this rapidly-advancing field is already proving a useful tool, accuracy is variable and AI does not provide the social and engagement benefits associated with citizen science approaches. We propose, as a solution, more efforts to combine citizen science with AI to improve classification accuracy and efficiency while maintaining public involvement. 
  |  http://www.mdpi.com/resolver?pii=ani10010132  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31947586/  |  
------------------------------------------- 
10.1007/s11904-020-00490-6  |    Purpose of review:  We review applications of artificial intelligence (AI), including machine learning (ML), in the field of HIV prevention. 
  Recent findings:  ML approaches have been used to identify potential candidates for preexposure prophylaxis (PrEP) in healthcare settings in the USA and Denmark and in a population-based research setting in Eastern Africa. Although still in the proof-of-concept stage, other applications include ML with smartphone-collected and social media data to promote real-time HIV risk reduction, virtual reality tools to facilitate HIV serodisclosure, and chatbots for HIV education. ML has also been used for causal inference in HIV prevention studies. ML has strong potential to improve delivery of PrEP, with this approach moving from development to implementation. Development and evaluation of AI and ML strategies for HIV prevention may benefit from an implementation science approach, including qualitative assessments with end users, and should be developed and evaluated with attention to equity. 
  |  https://dx.doi.org/10.1007/s11904-020-00490-6  |  
------------------------------------------- 
10.1007/s00266-019-01592-2  |   New developments in artificial intelligence (AI) offer opportunities to enhance plastic surgery practice, research, and education. In this article, we review relevant AI tools and applications, including machine learning, reinforcement learning, and natural language processing. Our own Markov decision process for keloid treatment illustrates how these models are developed and can be used to enhance decision-making in clinical practice. Finally, we discuss challenges of implementing AI and knowledge gaps that must be addressed to successfully apply AI in plastic surgery. Level of Evidence V This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266. 
  |  https://dx.doi.org/10.1007/s00266-019-01592-2  |  
------------------------------------------- 
10.1002/advs.201901957  |   A Materials Acceleration Operation System (MAOS) is designed, with unique language and compiler architecture. MAOS integrates with virtual reality (VR), collaborative robots, and a reinforcement learning (RL) scheme for autonomous materials synthesis, properties investigations, and self-optimized quality assurance. After training through VR, MAOS can work independently for labor and intensively reduces the time cost. Under the RL framework, MAOS also inspires the improved nucleation theory, and feedback for the optimal strategy, which can satisfy the demand on both of the CdSe quantum dots (QDs) emission wavelength and size distribution quality. Moreover, it can work well for extensive coverages of inorganic nanomaterials. MAOS frees the experimental researchers out of the tedious labor as well as the extensive exploration of optimal reaction conditions. This work provides a walking example for the "On-Demand" materials synthesis system, and demonstrates how artificial intelligence technology can reshape traditional materials science research in the future. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32274293/  |  
------------------------------------------- 
10.3389/fcvm.2019.00195  |   Cardiovascular conditions remain the leading cause of mortality and morbidity worldwide, with genotype being a significant influence on disease risk. Cardiac imaging-genetics aims to identify and characterize the genetic variants that influence functional, physiological, and anatomical phenotypes derived from cardiovascular imaging. High-throughput DNA sequencing and genotyping have greatly accelerated genetic discovery, making variant interpretation one of the key challenges in contemporary clinical genetics. Heterogeneous, low-fidelity phenotyping and difficulties integrating and then analyzing large-scale genetic, imaging and clinical datasets using traditional statistical approaches have impeded process. Artificial intelligence (AI) methods, such as deep learning, are particularly suited to tackle the challenges of scalability and high dimensionality of data and show promise in the field of cardiac imaging-genetics. Here we review the current state of AI as applied to imaging-genetics research and discuss outstanding methodological challenges, as the field moves from pilot studies to mainstream applications, from one dimensional global descriptors to high-resolution models of whole-organ shape and function, from univariate to multivariate analysis and from candidate gene to genome-wide approaches. Finally, we consider the future directions and prospects of AI imaging-genetics for ultimately helping understand the genetic and environmental underpinnings of cardiovascular health and disease. 
  |  https://doi.org/10.3389/fcvm.2019.00195  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32039240/  |  
------------------------------------------- 
10.3390/s20051350  |   One of the popular candidates in wireless technology for indoor positioning is Bluetooth Low Energy (BLE). However, this technology faces challenges related to Received Signal Strength Indicator (RSSI) fluctuations due to the behavior of the different advertising channels and the effect of human body shadowing among other effects. In order to mitigate these effects, the paper proposes and implements a dynamic Artificial Intelligence (AI) model that uses the three different BLE advertising channels to detect human body shadowing and compensate the RSSI values accordingly. An experiment in an indoor office environment is conducted. 70% of the observations are randomly selected and used for training and the remaining 30% are used to evaluate the algorithm. The results show that the AI model can properly detect and significantly compensate RSSI values for a dynamic blockage caused by a human body. This can significantly improve the RSSI-based ranges and the corresponding positioning accuracies. 
  |  http://www.mdpi.com/resolver?pii=s20051350  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32121466/  |  
------------------------------------------- 
10.1080/21507740.2020.1740354  |   The ethics of robots and artificial intelligence (AI) typically centers on "giving ethics" to as-yet imaginary AI with human-levels of autonomy in order to protect us from their potentially destructive power. It is often assumed that to do that, we should program AI with the true moral theory (whatever that might be), much as we teach morality to our children. This paper argues that the focus on AI with human-level autonomy is misguided. The robots and AI that we have now and in the near future are "semi-autonomous" in that their ability to make choices and to act is limited across a number of dimensions. Further, it may be morally problematic to create AI with human-level autonomy, even if it becomes possible. As such, any useful approach to AI ethics should begin with a theory of giving ethics to semi-autonomous agents (SAAs). In this paper, we work toward such a theory by evaluating our obligations to and for "natural" SAAs, including nonhuman animals and humans with developing and diminished capacities. Drawing on research in neuroscience, bioethics, and philosophy, we identify the ways in which AI semi-autonomy differs from semi-autonomy in humans and nonhuman animals. We conclude on the basis of these comparisons that when giving ethics to SAAs, we should focus on principles and restrictions that protect human interests, but that we can only permissibly maintain this approach so long as we do not aim at developing technology with human-level autonomy. 
  |  None  |  
------------------------------------------- 
10.21106/ijma.296  |   Artificial Intelligence (AI) applications in medicine have grown considerably in recent years. AI in the forms of Machine Learning, Natural Language Processing, Expert Systems, Planning and Logistics methods, and Image Processing networks provide great analytical aptitude. While AI methods were first conceptualized for radiology, investigations today are established across all medical specialties. The necessity for proper infrastructure, skilled labor, and access to large, well-organized data sets has kept the majority of medical AI applications in higher-income countries. However, critical technological improvements, such as cloud computing and the near-ubiquity of smartphones, have paved the way for use of medical AI applications in resource-poor areas. Global health initiatives (GHI) have already begun to explore ways to leverage medical AI technologies to detect and mitigate public health inequities. For example, AI tools can help optimize vaccine delivery and community healthcare worker routes, thus enabling limited resources to have a maximal impact. Other promising AI tools have demonstrated an ability to: predict burn healing time from smartphone photos; track regions of socioeconomic disparity combined with environmental trends to predict communicable disease outbreaks; and accurately predict pregnancy complications such as birth asphyxia in low resource settings with limited patient clinical data. In this commentary, we discuss the current state of AI-driven GHI and explore relevant lessons from past technology-centered GHI. Additionally, we propose a conceptual framework to guide the development of sustainable strategies for AI-driven GHI, and we outline areas for future research. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32123635/  |  
------------------------------------------- 
10.1007/s11019-020-09948-1  |   Artificial intelligence (AI) is among the fastest developing areas of advanced technology in medicine. The most important qualia of AI which makes it different from other advanced technology products is its ability to improve its original program and decision-making algorithms via deep learning abilities. This difference is the reason that AI technology stands out from the ethical issues of other advanced technology artifacts. The ethical issues of AI technology vary from privacy and confidentiality of personal data to ethical status and value of AI entities in a wide spectrum, depending on their capability of deep learning and scope of the domains in which they operate. Developing ethical norms and guidelines for planning, development, production, and usage of AI technology has become an important issue to overcome these problems. In this respect three outstanding documents have been produced:1. The Montréal Declaration for Responsible Development of Artificial Intelligence2. Ethics Guidelines for Trustworthy AI3. Asilomar Artificial Intelligence PrinciplesIn this study, these three documents will be analyzed with respect to the ethical principles and values they involve, their perspectives for approaching ethical issues, and their prospects for ethical reasoning when one or more of these values and principles are in conflict. Then, the sufficiency of these guidelines for addressing current or prospective ethical issues emerging from the existence of AI technology in medicine will be evaluated. The discussion will be pursued in terms of the ambiguity of interlocutors and efficiency for working out ethical dilemmas occurring in practical life. 
  |  https://doi.org/10.1007/s11019-020-09948-1  |  
------------------------------------------- 
10.1093/cvr/cvaa056  |    |  https://academic.oup.com/cardiovascres/article-lookup/doi/10.1093/cvr/cvaa056  |  
------------------------------------------- 
10.1097/WCO.0000000000000773  |    Purpose of review:  The aim of this review is to highlight novel artificial intelligence-based methods for the detection of optic disc abnormalities, with particular focus on neurology and neuro-ophthalmology. 
  Recent findings:  Methods for detection of optic disc abnormalities on retinal fundus images have evolved considerably over the last few years, from classical ophthalmoscopy to artificial intelligence-based identification methods being applied to retinal imaging with the aim of predicting sight and life-threatening complications of underlying brain or optic nerve conditions. 
  Summary:  Artificial intelligence and in particular newly developed deep-learning systems are playing an increasingly important role for the detection and classification of acquired neuro-ophthalmic optic disc abnormalities on ocular fundus images. The implementation of automatic deep-learning methods for detection of abnormal optic discs, coupled with innovative hardware solutions for fundus imaging, could revolutionize the practice of neurologists and other non-ophthalmic healthcare providers. 
  |  http://dx.doi.org/10.1097/WCO.0000000000000773  |  
------------------------------------------- 
10.1021/acs.jctc.9b01054  |   Protein backbone torsion angles (Phi and Psi) are crucial for protein local conformation description. In this paper, we propose a general postprocessing method for all prediction methods, namely, OPUS-Refine, which may contribute to the field in a different way. OPUS-Refine is a sampling-based method, therefore, the results of other prediction methods can be used as its constraints. After OPUS-Refine refinement, for instance, the accuracy of Phi/Psi predicted by SPIDER3 and SPOT-1D are both increased. In addition, to facilitate the sampling efficiency, we construct a neighbor-dependent statistical torsion angles sampling database, namely, OPUS-TA, which may be useful for other sampling-based methods. Furthermore, we also introduce the contact map predicted by RaptorX to OPUS-Refine as a global structural constraint. After refinement, compared to the predicted structures obtained from RaptorX online server, the accuracy of both global structural configurations (measured by TM-score and RMSD) and local structural configurations (measured by Phi/Psi) results are improved. OPUS-Refine is a highly efficient framework, it takes only about 4 s to refine the torsion angles and 30 s to refine the global structure of a protein with 100 residues in length on a typical desktop personal computer. Therefore, the sampling-based feature and the efficiency of OPUS-Refine offer greater potentiality for it to take advantage of any other method to achieve better performance. 
  |  None  |  
------------------------------------------- 
10.1515/bmt-2018-0136  |   B-mode ultrasonography and sonoelastography are used in the clinical diagnosis of prostate cancer (PCa). A combination of the two ultrasound (US) modalities using computer aid may be helpful for improving the diagnostic performance. A technique for computer-aided diagnosis (CAD) of PCa is presented based on multimodal US. Firstly, quantitative features are extracted from both B-mode US images and sonoelastograms, including intensity statistics, regional percentile features, gray-level co-occurrence matrix (GLCM) texture features and binary texture features. Secondly, a deep network named PGBM-RBM2 is proposed to learn and fuse multimodal features, which is composed of the point-wise gated Boltzmann machine (PGBM) and two layers of the restricted Boltzmann machines (RBMs). Finally, the support vector machine (SVM) is used for prostatic disease classification. Experimental evaluation was conducted on 313 multimodal US images of the prostate from 103 patients with prostatic diseases (47 malignant and 56 benign). Under five-fold cross-validation, the classification sensitivity, specificity, accuracy, Youden's index and area under the receiver operating characteristic (ROC) curve with the PGBM-RBM2 were 87.0%, 88.8%, 87.9%, 75.8% and 0.851, respectively. The results demonstrate that multimodal feature learning and fusion using the PGBM-RBM2 can assist in the diagnosis of PCa. This deep network is expected to be useful in the clinical diagnosis of PCa. 
  |  https://www.degruyter.com/doi/10.1515/bmt-2018-0136  |  
------------------------------------------- 
10.3390/s20072027  |   Due to the difficulties and complications in the quantitative assessment of traumatic brain injury (TBI) and its increasing relevance in today's world, robust detection of TBI has become more significant than ever. In this work, we investigate several machine learning approaches to assess their performance in classifying electroencephalogram (EEG) data of TBI in a mouse model. Algorithms such as decision trees (DT), random forest (RF), neural network (NN), support vector machine (SVM), K-nearest neighbors (KNN) and convolutional neural network (CNN) were analyzed based on their performance to classify mild TBI (mTBI) data from those of the control group in wake stages for different epoch lengths. Average power in different frequency sub-bands and alpha:theta power ratio in EEG were used as input features for machine learning approaches. Results in this mouse model were promising, suggesting similar approaches may be applicable to detect TBI in humans in practical scenarios. 
  |  http://www.mdpi.com/resolver?pii=s20072027  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32260320/  |  
------------------------------------------- 
10.1016/j.transproceed.2019.10.019  |   Prediction models of post-liver transplant mortality are crucial so that donor organs are not allocated to recipients with unreasonably high probabilities of mortality. Machine learning algorithms, particularly deep neural networks (DNNs), can often achieve higher predictive performance than conventional models. In this study, we trained a DNN to predict 90-day post-transplant mortality using preoperative variables and compared the performance to that of the Survival Outcomes Following Liver Transplantation (SOFT) and Balance of Risk (BAR) scores, using United Network of Organ Sharing data on adult patients who received a deceased donor liver transplant between 2005 and 2015 (n = 57,544). The DNN was trained using 202 features, and the best DNN's architecture consisted of 5 hidden layers with 110 neurons each. The area under the receiver operating characteristics curve (AUC) of the best DNN model was 0.703 (95% CI: 0.682-0.726) as compared to 0.655 (95% CI: 0.633-0.678) and 0.688 (95% CI: 0.667-0.711) for the BAR score and SOFT score, respectively. In conclusion, despite the complexity of DNN, it did not achieve a significantly higher discriminative performance than the SOFT score. Future risk models will likely benefit from the inclusion of other data sources, including high-resolution clinical features for which DNNs are particularly apt to outperform conventional statistical methods. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0041-1345(19)30931-5  |  
------------------------------------------- 
10.1259/bjr.20190420  |    Objective:  For real-time markerless tumour tracking in stereotactic lung radiotherapy, we propose a different approach which uses patient-specific deep learning (DL) using a personalised data generation strategy, avoiding the need for collection of a large patient data set. We validated our strategy with digital phantom simulation and epoxy phantom studies. 
  Methods:  We developed lung tumour tracking for radiotherapy using a convolutional neural network trained for each phantom's lesion by using multiple digitally reconstructed radiographs (DRRs) generated from each phantom's treatment planning four-dimensional CT. We trained tumour-bone differentiation using large numbers of training DRRs generated with various projection geometries to simulate tumour motion. We solved the problem of using DRRs for training and X-ray images for tracking using the training DRRs with random contrast transformation and random noise addition. 
  Results:  We defined adequate tracking accuracy as the percentage frames satisfying &lt;1 mm tracking error of the isocentre. In the simulation study, we achieved 100% tracking accuracy in 3 cm spherical and 1.5×2.25×3 cm ovoid masses. In the phantom study, we achieved 100 and 94.7% tracking accuracy in 3 cm and 2 cm spherical masses, respectively. This required 32.5 ms/frame (30.8 fps) real-time processing. 
  Conclusions:  We proved the potential feasibility of a real-time markerless tumour tracking framework for stereotactic lung radiotherapy based on patient-specific DL with personalised data generation with digital phantom and epoxy phantom studies. 
  Advances in knowledge:  Using DL with personalised data generation is an efficient strategy for real-time lung tumour tracking. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190420?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1097/SCS.0000000000005905  |    Background:  Advances in deep learning (DL) have been transformative in computer vision and natural language processing, as well as in healthcare. The authors present a novel application of DL to plastic surgery. Here, the authors describe and demonstrate the mobile deployment of a deep neural network that predicts rhinoplasty status, assess model accuracy compared to surgeons, and describe future directions for such applications in plastic surgery. 
  Methods:  A deep convolutional neural network ("RhinoNet") was developed to classify rhinoplasty images using only pixels and rhinoplasty status labels ("before"/"after") as inputs. RhinoNet was trained using a dataset of 22,686 before and after photos which were collected from publicly available sites. Network classification was compared to that of plastic surgery attendings and residents on 2269 previously-unseen test-set images. 
  Results:  RhinoNet correctly predicted rhinoplasty status in 85% of the test-set images. Sensitivity and specificity of model predictions were 0.840 (0.79-0.89) and 0.826 (0.77-0.88), respectively; the corresponding values for expert consensus predictions were 0.814 (0.76-0.87) and 0.867 (0.82-0.91). RhinoNet and humans performed with effectively equivalent accuracy in this classification task. 
  Conclusion:  The authors describe the development of DL applications to identify the presence of superficial surgical procedures solely from images and labels. DL is especially well suited for unstructured, high-fidelity visual and auditory data that does not lend itself to classical statistical analysis, and may be deployed as mobile applications for potentially unbridled use, so the authors expect DL to play a key role in many areas of plastic surgery. 
  |  http://dx.doi.org/10.1097/SCS.0000000000005905  |  
------------------------------------------- 
10.7507/1001-5515.201905074  |   Aiming at the problem that the small samples of critical disease in clinic may lead to prognostic models with poor performance of overfitting, large prediction error and instability, the long short-term memory transferring algorithm (transLSTM) was proposed. Based on the idea of transfer learning, the algorithm leverages the correlation between diseases to transfer information of different disease prognostic models, constructs the effictive model of target disease of small samples with the aid of large data of related diseases, hence improves the prediction performance and reduces the requirement for target training sample quantity. The transLSTM algorithm firstly uses the related disease samples to pretrain partial model parameters, and then further adjusts the whole network with the target training samples. The testing results on MIMIC-Ⅲ database showed that compared with traditional LSTM classification algorithm, the transLSTM algorithm had 0.02-0.07 higher AUROC and 0.05-0.14 larger AUPRC, while its number of training iterations was only 39%-64% of the traditional algorithm. The results of application on sepsis revealed that the transLSTM model of only 100 training samples had comparable mortality prediction performance to the traditional model of 250 training samples. In small sample situations, the transLSTM algorithm has significant advantages with higher prediciton accuracy and faster training speed. It realizes the application of transfer learning in the prognostic model of critical disease with small samples. 
 针对临床上重症疾病样本数量少容易导致预后模型过拟合、预测误差大、不稳定的问题，本文提出迁移长短时程记忆算法（transLSTM）。该算法基于迁移学习思想，利用疾病间的相关性实现不同疾病预后模型的信息迁移，借助相关疾病的大数据辅助构建小样本目标病种有效模型，提升模型预测性能，降低对目标训练样本量的要求。transLSTM 算法先利用相关疾病数据预训练部分模型参数，再用目标训练样本进一步调整整个网络。基于 MIMIC-Ⅲ数据库的测试结果显示，相比传统的 LSTM 分类算法，transLSTM 算法的 AUROC 指标高出 0.02～0.07，AUPRC 指标超过 0.05～0.14，训练迭代次数仅为传统算法的 39%～64%。应用于脓毒症疾病的结果显示，仅 100 个训练样本的 transLSTM 模型死亡率预测性能与 250 个训练样本的传统模型相当。在小样本情况下，transLSTM 算法预测精度更高、训练速度更快，具有显著优势。它实现了迁移学习在小样本重症疾病预后模型中的应用。. 
  |  None  |  
------------------------------------------- 
10.1371/journal.pone.0227613  |   Recent studies aiming to facilitate mathematical skill development in primary school children have explored the electrophysiological characteristics associated with different levels of arithmetic achievement. The present work introduces an alternative EEG signal characterization using graph metrics and, based on such features, a classification analysis using a decision tree model. This proposal aims to identify group differences in brain connectivity networks with respect to mathematical skills in elementary school children. The methods of analysis utilized were signal-processing (EEG artifact removal, Laplacian filtering, and magnitude square coherence measurement) and the characterization (Graph metrics) and classification (Decision Tree) of EEG signals recorded during performance of a numerical comparison task. Our results suggest that the analysis of quantitative EEG frequency-band parameters can be used successfully to discriminate several levels of arithmetic achievement. Specifically, the most significant results showed an accuracy of 80.00% (α band), 78.33% (δ band), and 76.67% (θ band) in differentiating high-skilled participants from low-skilled ones, averaged-skilled subjects from all others, and averaged-skilled participants from low-skilled ones, respectively. The use of a decision tree tool during the classification stage allows the identification of several brain areas that seem to be more specialized in numerical processing. 
  |  http://dx.plos.org/10.1371/journal.pone.0227613  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31951604/  |  
------------------------------------------- 
10.1016/j.ypmed.2019.105969  |   Tumin and Bhalla mentioned challenges associated with the use of population-based survey and machine learning (ML) results on adolescent opioid misuse to clinical settings. In a clinical setting, medical providers do know patient's identity. So, it is not surprising that drug misuse is rarely identified through patient's self-report especially if it involves illicit drug. Even though self-report is susceptible to bias, it is a valid and affordable tool to gather data on illicit drug use at the population level. Use of audio computer-assisted self-interviewing (ACASI) and computer-assisted personal interviewing (CAPI) in NSDUH provides the respondent with a highly private and confidential mode for responding to questions, which helps increase the level of honest reporting of illicit drug use and other sensitive behaviors. As acknowledged in the paper, opioid misuse should not be inferred at the individual level from our ML models. Such interpretations may lead to ecological fallacy. Predicting opioid misuse at the population level is different from identifying opioid misuse in individual patients. Nonetheless, we believe that coordinated multisectoral collaborations that leverage the expertise and resources of both public health and clinical sectors would offer a promising model for addressing the opioid crisis. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0091-7435(19)30452-9  |  
------------------------------------------- 
10.1007/s11356-019-06972-4  |   The working conditions of underground mining are complex and variable, and roof fall and rib spalling are one of the main types of accidents that can occur. Building an integrated model to evaluate the risk of roof fall and rib spalling is the foundation of mine safety. On the basis of the inherent attributes of event risk, the fuzzy evaluation set and probability of basic events are obtained by using the fuzzy fault tree analysis method based on the sample's fuzzy information. Subsequently, the likelihood of roof fall and rib spalling is determined. Consequence severity data are obtained by using the dynamic fuzzy logic method, and the consequence severity grade of roof fall and rib spalling is evaluated via the dynamic fuzzy comprehensive evaluation method. The event risk level is determined by the risk matrix method. Roof fall and rib spalling in a non-coal mine is analyzed and evaluated by using fuzzy fault tree analysis and dynamic fuzzy comprehensive evaluation. The weak links in the operation of an underground mine are identified by fuzzy fault tree analysis as "mining process, roof management, support and reinforcement." Then, the risk development trend is determined by the dynamic fuzzy comprehensive evaluation method. The risk matrix method is integrated to determine whether the risk level of the mine is "high risk, unacceptable" and expected to deteriorate in the future. The results show the validity and feasibility of the risk analysis and prediction model for roof fall and rib spalling. 
  |  https://dx.doi.org/10.1007/s11356-019-06972-4  |  
------------------------------------------- 
10.12116/j.issn.1004-5619.2020.01.008  |   Objective To analyze the differences among electrical damage, burns and abrasions in pig skin using Fourier transform infrared microspectroscopy （FTIR-MSP） combined with machine learning algorithm, to construct three kinds of skin injury determination models and select characteristic markers of electric injuries, in order to provide a new method for skin electric mark identification. Methods Models of electrical damage, burns and abrasions in pig skin were established. Morphological changes of different injuries were examined using traditional HE staining. The FTIR-MSP was used to detect the epidermal cell spectrum. Principal component method and partial least squares method were used to analyze the injury classification. Linear discriminant and support vector machine were used to construct the classification model, and factor loading was used to select the characteristic markers. Results Compared with the control group, the epidermal cells of the electrical damage group, burn group and abrasion group showed polarization, which was more obvious in the electrical damage group and burn group. Different types of damage was distinguished by principal component and partial least squares method. Linear discriminant and support vector machine models could effectively diagnose different damages. The absorption peaks at 2 923 cm<sup>-1</sup>, 2 854 cm<sup>-1</sup>, 1 623 cm<sup>-1</sup>, and 1 535 cm<sup>-1</sup> showed significant differences in different injury groups. The peak intensity of electrical injury's 2 923 cm<sup>-1</sup> absorption peak was the highest. Conclusion FTIR-MSP combined with machine learning algorithm provides a new technique to diagnose skin electrical damage and identification electrocution. 
  题目:  傅里叶变换显微红外光谱结合机器学习算法鉴定电击伤. 
  摘要:  目的 基于傅里叶变换显微红外光谱结合机器学习算法分析猪皮肤电击伤、烧伤及擦伤的差异，构建3种皮肤损伤鉴定模型，筛选电击伤特征性标志物，为皮肤电流斑鉴定提供新方法。 方法 建立猪皮肤电击伤、烧伤及擦伤的模型，使用传统HE染色检验不同损伤的形态学改变。运用傅里叶变换显微红外技术检测表皮细胞光谱，运用主成分、偏最小二乘法分析损伤的分类情况，运用线性判别和支持向量机构建分类模型，因子载荷筛选特征性标志物。 结果 与对照组相比，电击伤、烧伤及擦伤组的表皮细胞均呈现出极化现象，以电击伤、烧伤组更为明显。通过主成分和偏最小二乘法分析可区分不同类型损伤，线性判别、支持向量机模型均能够有效诊断不同损伤。2 923、2 854、1 623、1 535 cm<sup>-1</sup>吸收峰在不同损伤组显示出明显的差异，电击伤的2 923 cm<sup>-1</sup>吸收峰峰强最高。 结论 傅里叶变换显微红外光谱结合机器学习算法为诊断皮肤电击伤、鉴定电击死提供了新技术。. 
  关键词:  法医病理学；电击伤；谱学，傅里叶变换红外；机器学习；皮肤；猪. 
  |  https://doi.org/10.12116/j.issn.1004-5619.2020.01.008  |  
------------------------------------------- 
10.1371/journal.pone.0228421  |   We develop a dynamic matched sample estimation algorithm to distinguish peer influence and homophily effects on item adoption decisions in dynamic networks, with numerous items diffusing simultaneously. We infer preferences using a machine learning algorithm applied to previous adoption decisions, and we match agents using those inferred preferences. We show that ignoring previous adoption decisions leads to significantly overestimating the role of peer influence in the diffusion of information, mistakenly confounding influence-based contagion with diffusion driven by common preferences. Our matching-on-preferences algorithm with machine learning reduces the relative effect of peer influence on item adoption decisions in this network significantly more than matching on earlier adoption decisions, as well other observable characteristics. We also show significant and intuitive heterogeneity in the relative effect of peer influence. 
  |  http://dx.plos.org/10.1371/journal.pone.0228421  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32027659/  |  
------------------------------------------- 
10.1016/j.chemosphere.2020.126169  |   Water resources are the foundation of people's life and economic development, and are closely related to health and the environment. Accurate prediction of water quality is the key to improving water management and pollution control. In this paper, two novel hybrid decision tree-based machine learning models are proposed to obtain more accurate short-term water quality prediction results. The basic models of the two hybrid models are extreme gradient boosting (XGBoost) and random forest (RF), which respectively introduce an advanced data denoising technique - complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). Taking the water resources of Gales Creek site in Tualatin River (one of the most polluted rivers in the world) Basin as an example, a total of 1875 data (hourly data) from May 1, 2019 to July 20, 2019 are collected. Two hybrid models are used to predict six water quality indicators, including water temperature, dissolved oxygen, pH value, specific conductance, turbidity, and fluorescent dissolved organic matter. Six error metrics are introduced as the basis of performance evaluation, and the results of the two models are compared with the other four conventional models. The results reveal that: (1) CEEMDAN-RF performs best in the prediction of temperature, dissolved oxygen and specific conductance, the mean absolute percentage errors (MAPEs) are 0.69%, 1.05%, and 0.90%, respectively. CEEMDAN-XGBoost performs best in the prediction of pH value, turbidity, and fluorescent dissolved organic matter, the MAPEs are 0.27%, 14.94%, and 1.59%, respectively. (2) The average MAPEs of CEEMDAN-RF and CEEMMDAN-XGBoost models are the smallest, which are 3.90% and 3.71% respectively, indicating that their overall prediction performance is the best. In addition, the stability of the prediction model is also discussed in this paper. The analysis shows that the prediction stability of CEEMDAN-RF and CEEMDAN-XGBoost is higher than other benchmark models. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0045-6535(20)30362-3  |  
------------------------------------------- 
10.7507/1001-5515.201904028  |   Inferior myocardial infarction is an acute ischemic heart disease with high mortality, which is easy to induce life-threatening complications such as arrhythmia, heart failure and cardiogenic shock. Therefore, it is of great clinical value to carry out accurate and efficient early diagnosis of inferior myocardial infarction. Electrocardiogram is the most sensitive means for early diagnosis of inferior myocardial infarction. This paper proposes a method for detecting inferior myocardial infarction based on densely connected convolutional neural network. The method uses the original electrocardiogram (ECG) signals of serially connected Ⅱ, Ⅲ and aVF leads as the input of the model and extracts the robust features of the ECG signals by using the scale invariance of the convolutional layers. The characteristic transmission of ECG signals is enhanced by the dense connectivity between different layers, so that the network can automatically learn the effective features with strong robustness and high recognition, so as to achieve accurate detection of inferior myocardial infarction. The Physikalisch Technische Bundesanstalt diagnosis public ECG database was used for verification. The accuracy, sensitivity and specificity of the model reached 99.95%, 100% and 99.90%, respectively. The accuracy, sensitivity and specificity of the model are also over 99% even though the noise exists. Based on the results of this study, it is expected that the method can be introduced in the clinical environment to help doctors quickly diagnose inferior myocardial infarction in the future. 
 下壁心肌梗死是一种病死率高的急性缺血性心脏病，易诱发恶性心律失常、心率衰竭、心源性休克等危及生命的并发症。因而，开展对下壁心肌梗死准确高效的早期诊断研究具有重要的临床价值。心电图是早期诊断下壁心肌梗死最敏感的手段。本文提出了一种基于密集连接卷积神经网络的下壁心肌梗死检测方法。该方法将Ⅱ、Ⅲ和 aVF 导联的原始心电信号串接数据作为模型的输入，利用卷积层的尺度不变性提取心电信号中具有鲁棒性的特征，并通过不同层间密集连接的方式加强了心电信号特征的传递，使得网络能够自动学习心电信号中鲁棒性强且辨识度高的有效特征，从而实现下壁心肌梗死的准确检测。本文还采用德国国家计量学研究所诊断公共心电数据库进行验证，本文模型的准确率、敏感性和特异性分别达到了 99.95%、100% 和 99.90%。在含有噪声的情况下，模型的准确率、敏感性和特异性也均超过 99%。基于本文研究结果，期望今后可在临床环境中引入本文方法，以帮助医生快速诊断下壁心肌梗死。. 
  |  None  |  
------------------------------------------- 
10.1371/journal.pone.0228434  |   The service quality and system dependability of real-time communication networks strongly depends on the analysis of monitored data, to identify concrete problems and their causes. Many of these can be described by either their structural or temporal properties, or a combination of both. As current research is short of approaches sufficiently addressing both properties simultaneously, we propose a new feature space specifically suited for this task, which we analyze for its theoretical properties and its practical relevance. We evaluate its classification performance when used on real-world data sets of structural-temporal mobile communication data, and compare it to the performance achieved of feature representations used in related work. For this purpose we propose a system which allows the automatic detection and prediction of classes of pre-defined sequence behavior, greatly reducing costs caused by the otherwise required manual analysis. With our proposed feature spaces this system achieves a precision of more than 93% at recall values of 100%, with an up to 6.7% higher effective recall than otherwise similarly performing alternatives, notably outperforming alternative deep learning, kernel learning and ensemble learning approaches of related work. Furthermore the supported system calibration allows separating reliable from unreliable predictions more effectively, which is highly relevant for any practical application. 
  |  http://dx.plos.org/10.1371/journal.pone.0228434  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32027668/  |  
------------------------------------------- 
10.1016/j.biortech.2020.122781  |   To establish the models of microbial lipid production from cellulosic ethanol wastewater by R. glutinis, the biomass, lipid yield, and COD removal rate were investigated under different conditions. Subsequently, the genetic algorithm based on SVM was adopted to optimize parameters for obtaining the maximum biomass. The results demonstrated that the initial COD and glucose content had a significant effect on lipids synthesis. Most of the organic matter in the wastewater was consumed with the production of lipid. Compared with BP-ANN, SVM had better fitting and generalization ability for small amount of experimental data. By genetic algorithm optimization based on SVM, the maximum biomass and lipid yield could reach 11.87 g/L and 2.18 g/L, respectively. The results suggest that the SVM model could be used as an effective tool to optimize fermentation conditions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-8524(20)30050-X  |  
------------------------------------------- 
10.1007/s00484-019-01856-1  |   Disease and pest alert models are able to generate information for agrochemical applications only when needed, reducing costs and environmental impacts. With machine learning algorithms, it is possible to develop models to be used in disease and pest warning systems as a function of the weather in order to improve the efficiency of chemical control of pests of the coffee tree. Thus, we correlated the infection rates with the weather variables and also calibrated and tested machine learning algorithms to predict the incidence of coffee rust, cercospora, coffee miner, and coffee borer. We used weather and field data obtained from coffee plantations in production in the southern regions of the State of Minas Gerais (SO<sub>MG</sub>) and from the region of the Cerrado Mineiro; these crops did not receive phytosanitary treatments. The algorithms calibrated and tested for prediction were (a) Multiple linear regression (RLM); (b) K Neighbors Regressor (KNN); (c) Random Forest Regressor (RFT), and (d) Artificial Neural Networks (MLP). As dependent variables, we considered the monthly rates of coffee rust, cercospora, coffee miner, and coffee tree borer, and the weather elements were considered as independent (predictor) variables. Pearson correlation analyses were performed considering three different time periods, 1-10 d (from 1 to 10 days before the incidence evaluation), 11-20 d, and 21-30 d, and used to evaluate the unit correlations between the weather variables and infection rates of coffee diseases and pests. The models were calibrated in years of high and low yields, because the biannual variation of harvest yield of coffee beans influences the severity of the diseases. The models were compared by the Willmott's 'd', RMSE (root mean square error), and coefficient of determination (R<sup>2</sup>) indices. The result of the more accurate algorithm was specialized for the SO<sub>MG</sub> and Cerrado Mineiro regions using the kriging method. The weather variables that showed significant correlations with coffee rust disease were maximum air temperature, number of days with relative humidity above 80%, and relative humidity. RFT was more accurate in the prediction of coffee rust, cercospora, coffee miner, and coffee borer using weather conditions. In the SO<sub>MG</sub>, RFT showed a greater accuracy in the predictions for the Cerrado Mineiro in years of high and low yields and for all diseases. In SO<sub>MG</sub>, the RMSE values ranged from 0.227 to 0.853 for high-yield and 0.147 and 0.827 for low-yield coffee in the coffee borer forecasting. 
  |  https://dx.doi.org/10.1007/s00484-019-01856-1  |  
------------------------------------------- 
10.1371/journal.pone.0227438  |   Liberia and Gabon joined the Gaborone Declaration for Sustainability in Africa (GDSA), established in 2012, with the goal of incorporating the value of nature into national decision making by estimating the multiple services obtained from ecosystems using the natural capital accounting framework. In this study, we produced 30-m resolution 10 classes land cover maps for the 2015 epoch for Liberia and Gabon using the Google Earth Engine (GEE) cloud platform to support the ongoing natural capital accounting efforts in these nations. We propose an integrated method of pixel-based classification using Landsat 8 data, the Random Forest (RF) classifier and ancillary data to produce high quality land cover products to fit a broad range of applications, including natural capital accounting. Our approach focuses on a pre-classification filtering (Masking Phase) based on spectral signature and ancillary data to reduce the number of pixels prone to be misclassified; therefore, increasing the quality of the final product. The proposed approach yields an overall accuracy of 83% and 81% for Liberia and Gabon, respectively, outperforming prior land cover products for these countries in both thematic content and accuracy. Our approach, while relatively simple and highly replicable, was able to produce high quality land cover products to fill an observational gap in up to date land cover data at national scale for Liberia and Gabon. 
  |  http://dx.plos.org/10.1371/journal.pone.0227438  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31923284/  |  
------------------------------------------- 
10.1371/journal.pone.0228446  |   We investigated whether the integration of machine learning (ML) into MRI interpretation can provide accurate decision rules for the management of suspicious breast masses. A total of 173 consecutive patients with suspicious breast masses upon complementary assessment (BI-RADS IV/V: n = 100/76) received standardized breast MRI prior to histological verification. MRI findings were independently assessed by two observers (R1/R2: 5 years of experience/no experience in breast MRI) using six (semi-)quantitative imaging parameters. Interobserver variability was studied by ICC (intraclass correlation coefficient). A polynomial kernel function support vector machine was trained to differentiate between benign and malignant lesions based on the six imaging parameters and patient age. Ten-fold cross-validation was applied to prevent overfitting. Overall diagnostic accuracy and decision rules (rule-out criteria) to accurately exclude malignancy were evaluated. Results were integrated into a web application and published online. Malignant lesions were present in 107 patients (60.8%). Imaging features showed excellent interobserver variability (ICC: 0.81-0.98) with variable diagnostic accuracy (AUC: 0.65-0.82). Overall performance of the ML algorithm was high (AUC = 90.1%; BI-RADS IV: AUC = 91.6%). The ML algorithm provided decision rules to accurately rule-out malignancy with a false negative rate &lt;1% in 31.3% of the BI-RADS IV cases. Thus, integration of ML into MRI interpretation can provide objective and accurate decision rules for the management of suspicious breast masses, and could help to reduce the number of potentially unnecessary biopsies. 
  |  http://dx.plos.org/10.1371/journal.pone.0228446  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31999755/  |  
------------------------------------------- 
10.1259/bjr.20190535  |    Objectives:  Radiotherapy plan quality may vary considerably depending on planner's experience and time constraints. The variability in treatment plans can be assessed by calculating the difference between achieved and the optimal dose distribution. The achieved treatment plans may still be suboptimal if there is further scope to reduce organs-at-risk doses without compromising target coverage and deliverability. This study aims to develop a knowledge-based planning (KBP) model to reduce variability of volumetric modulated arc therapy (VMAT) lung plans by predicting minimum achievable lung volume-dose metrics. 
  Methods:  Dosimetric and geometric data collected from 40 retrospective plans were used to develop KBP models aiming to predict the minimum achievable lung dose metrics via calculating the ratio of the residual lung volume to the total lung volume. Model accuracy was verified by replanning 40 plans. Plan complexity metrics were calculated using locally developed script and their effect on treatment delivery was assessed via measurement. 
  Results:  The use of KBP resulted in significant reduction in plan variability in all three studied dosimetric parameters V5, V20 and mean lung dose by 4.9% (<i>p</i> = 0.007, 10.8 to 5.9%), 1.3% (<i>p</i> = 0.038, 4.0 to 2.7%) and 0.9 Gy (<i>p</i> = 0.012, 2.5 to 1.6Gy), respectively. It also increased lung sparing without compromising the overall plan quality. The accuracy of the model was proven as clinically acceptable. Plan complexity increased compared to original plans; however, the implication on delivery errors was clinically insignificant as demonstrated by plan verification measurements. 
  Conclusion:  Our in-house model for VMAT lung plans led to a significant reduction in plan variability with concurrent decrease in lung dose. Our study also demonstrated that treatment delivery verifications are important prior to clinical implementation of KBP models. 
  Advances in knowledge:  In-house KBP models can predict minimum achievable lung dose-volume constraints for advance-stage lung cancer patients treated with VMAT. The study demonstrates that plan complexity could increase and should be assessed prior to clinical implementation. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190535?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/ajcp/aqz150  |    Objectives:  Peripheral blood flow cytometry (PBFC) is useful for evaluating circulating hematologic malignancies (HM) but has limited diagnostic value for screening. We used machine learning to evaluate whether clinical history and CBC/differential parameters could improve PBFC utilization. 
  Methods:  PBFC cases with concurrent/recent CBC/differential were split into training (n = 626) and test (n = 159) cohorts. We classified PBFC results with abnormal blast/lymphoid populations as positive and used two models to predict results. 
  Results:  Positive PBFC results were seen in 58% and 21% of training cases with and without prior HM (P &lt; .001). % neutrophils, absolute lymphocyte count, and % blasts/other cells differed significantly between positive and negative PBFC groups (areas under the curve [AUC] &gt; 0.7). Among test cases, a decision tree model achieved 98% sensitivity and 65% specificity (AUC = 0.906). A logistic regression model achieved 100% sensitivity and 54% specificity (AUC = 0.919). 
  Conclusions:  We outline machine learning-based triaging strategies to decrease unnecessary utilization of PBFC by 35% to 40%. 
  |  https://academic.oup.com/ajcp/article-lookup/doi/10.1093/ajcp/aqz150  |  
------------------------------------------- 
10.1186/s12859-020-3378-0  |    Background:  Predicting of chemical compounds is one of the fundamental tasks in bioinformatics and chemoinformatics, because it contributes to various applications in metabolic engineering and drug discovery. The recent rapid growth of the amount of available data has enabled applications of computational approaches such as statistical modeling and machine learning method. Both a set of chemical interactions and chemical compound structures are represented as graphs, and various graph-based approaches including graph convolutional neural networks have been successfully applied to chemical network prediction. However, there was no efficient method that can consider the two different types of graphs in an end-to-end manner. 
  Results:  We give a new formulation of the chemical network prediction problem as a link prediction problem in a graph of graphs (GoG) which can represent the hierarchical structure consisting of compound graphs and an inter-compound graph. We propose a new graph convolutional neural network architecture called dual graph convolutional network that learns compound representations from both the compound graphs and the inter-compound network in an end-to-end manner. 
  Conclusions:  Experiments using four chemical networks with different sparsity levels and degree distributions shows that our dual graph convolution approach achieves high prediction performance in relatively dense networks, while the performance becomes inferior on extremely-sparse networks. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3378-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32321421/  |  
------------------------------------------- 
10.1515/bmt-2019-0001  |   Electroencephalography (EEG) is a common tool used for the detection of epileptic seizures. However, the visual analysis of long-term EEG recordings is characterized by its subjectivity, time-consuming procedure and its erroneous detection. Various epileptic seizure detection algorithms have been proposed to deal with such issues. In this study, a novel automatic seizure-detection approach is proposed. Three different strategies are suggested to the user whereby he/she could choose the appropriate one for a given classification problem. Indeed, the feature extraction step, including both linear and nonlinear measures, is performed either directly from the EEG signals, or from the derived sub-bands of tunable-Q wavelet transform (TQWT), or even from the intrinsic mode functions (IMFs) of multivariate empirical mode decomposition (MEMD). The classification procedure is executed using a support vector machine (SVM). The performance of the proposed method is evaluated through a publicly available database from which six binary classification cases are formulated to discriminate between healthy, seizure and non-seizure EEG signals. Our results show high performance in terms of accuracy (ACC), sensitivity (SEN) and specificity (SPE) compared to the state-of-the-art approaches. Thus, the proposed approach for automatic seizure detection can be considered as a valuable alternative to existing methods, able to alleviate the overload of visual analysis and accelerate the seizure detection. 
  |  https://www.degruyter.com/doi/10.1515/bmt-2019-0001  |  
------------------------------------------- 
10.1248/cpb.c19-00625  |   The goal of drug design is to discover molecular structures that have suitable pharmacological properties in vast chemical space. In recent years, the use of deep generative models (DGMs) is getting a lot of attention as an effective method of generating new molecules with desired properties. However, most of the properties do not have three-dimensional (3D) information, such as shape and pharmacophore. In drug discovery, pharmacophores are valuable clues in finding active compounds. In this study, we propose a computational strategy based on deep reinforcement learning for generating molecular structures with a desired pharmacophore. In addition, to extract selective molecules against a target protein, chemical genomics-based virtual screening (CGBVS) is used as post-processing method of deep reinforcement learning. As an example study, we have employed this strategy to generate molecular structures of selective TIE2 inhibitors. This strategy can be adopted into general use for generating selective molecules with a desired pharmacophore. 
  |  https://dx.doi.org/10.1248/cpb.c19-00625  |  
------------------------------------------- 
10.1016/j.watres.2020.115471  |   Lagoon has been widely used to treat animal wastewater. However, because lagoon effluent often fluctuates in water quality, land application of the effluent may pose a risk to the environment and/or public health. It is necessary to monitor the quality of lagoon water to reduce the risk of its land application. This paper proposes an innovative monitoring method for animal wastewater in lagoons. We implemented spectral processing techniques to analyze the reflectivity of wastewater samples from lagoons, and applied machine learning methods to estimate the water quality parameters of the effluents, including the levels of nitrogen, phosphorus, bacteria (total coliform and E. Coli), and total solids. This study found significant correlations between the spectral rate of emission and above water quality parameters. We used machine learning to train three types of estimators, normal equation linear regression (LR), stochastic gradient descent (SGD), and Ridge regression to quantify these relations. The model performance was evaluated by weight coefficient, function intercept, and mean squared error (MSE). The model showed that TS level and the blue band of spectral reflectance of samples have a relatively good linear relationship, and the MSE of prediction set and decision coefficient were 0.57 and 0.98, respectively. For bacteria level, the MSE of prediction set was 0.63, and coefficient R<sup>2</sup> was 0.96. The results from this study could provide a versatile method for remote sensing of animal waste water. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0043-1354(20)30007-5  |  
------------------------------------------- 
10.1186/s12984-019-0630-9  |    Background:  In clinical practice, therapists choose the amount of assistance for robot-assisted training. This can result in outcomes that are influenced by subjective decisions and tuning of training parameters can be time-consuming. Therefore, various algorithms to automatically tune the assistance have been developed. However, the assistance applied by these algorithms has not been directly compared to manually-tuned assistance yet. In this study, we focused on subtask-based assistance and compared automatically-tuned (AT) robotic assistance with manually-tuned (MT) robotic assistance. 
  Methods:  Ten people with neurological disorders (six stroke, four spinal cord injury) walked in the LOPES II gait trainer with AT and MT assistance. In both cases, assistance was adjusted separately for various subtasks of walking (in this study defined as control of: weight shift, lateral foot placement, trailing and leading limb angle, prepositioning, stability during stance, foot clearance). For the MT approach, robotic assistance was tuned by an experienced therapist and for the AT approach an algorithm that adjusted the assistance based on performances for the different subtasks was used. Time needed to tune the assistance, assistance levels and deviations from reference trajectories were compared between both approaches. In addition, participants evaluated safety, comfort, effect and amount of assistance for the AT and MT approach. 
  Results:  For the AT algorithm, stable assistance levels were reached quicker than for the MT approach. Considerable differences in the assistance per subtask provided by the two approaches were found. The amount of assistance was more often higher for the MT approach than for the AT approach. Despite this, the largest deviations from the reference trajectories were found for the MT algorithm. Participants did not clearly prefer one approach over the other regarding safety, comfort, effect and amount of assistance. 
  Conclusion:  Automatic tuning had the following advantages compared to manual tuning: quicker tuning of the assistance, lower assistance levels, separate tuning of each subtask and good performance for all subtasks. Future clinical trials need to show whether these apparent advantages result in better clinical outcomes. 
  |  https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-019-0630-9  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31992322/  |  
------------------------------------------- 
10.1371/journal.pone.0226880  |   Haptic exploration is a key skill for both robots and humans to discriminate and handle unknown objects or to recognize familiar objects. Its active nature is evident in humans who from early on reliably acquire sophisticated sensory-motor capabilities for active exploratory touch and directed manual exploration that associates surfaces and object properties with their spatial locations. This is in stark contrast to robotics. In this field, the relative lack of good real-world interaction models-along with very restricted sensors and a scarcity of suitable training data to leverage machine learning methods-has so far rendered haptic exploration a largely underdeveloped skill. In robot vision however, deep learning approaches and an abundance of available training data have triggered huge advances. In the present work, we connect recent advances in recurrent models of visual attention with previous insights about the organisation of human haptic search behavior, exploratory procedures and haptic glances for a novel architecture that learns a generative model of haptic exploration in a simulated three-dimensional environment. This environment contains a set of rigid static objects representing a selection of one-dimensional local shape features embedded in a 3D space: an edge, a flat and a convex surface. The proposed algorithm simultaneously optimizes main perception-action loop components: feature extraction, integration of features over time, and the control strategy, while continuously acquiring data online. Inspired by the Recurrent Attention Model, we formalize the target task of haptic object identification in a reinforcement learning framework and reward the learner in the case of success only. We perform a multi-module neural network training, including a feature extractor and a recurrent neural network module aiding pose control for storing and combining sequential sensory data. The resulting haptic meta-controller for the rigid 16 × 16 tactile sensor array moving in a physics-driven simulation environment, called the Haptic Attention Model, performs a sequence of haptic glances, and outputs corresponding force measurements. The resulting method has been successfully tested with four different objects. It achieved results close to 100% while performing object contour exploration that has been optimized for its own sensor morphology. 
  |  http://dx.plos.org/10.1371/journal.pone.0226880  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31896135/  |  
------------------------------------------- 
10.1186/s12859-020-3395-z  |    Background:  CRISPR/Cas9 system, as the third-generation genome editing technology, has been widely applied in target gene repair and gene expression regulation. Selection of appropriate sgRNA can improve the on-target knockout efficacy of CRISPR/Cas9 system with high sensitivity and specificity. However, when CRISPR/Cas9 system is operating, unexpected cleavage may occur at some sites, known as off-target. Presently, a number of prediction methods have been developed to predict the off-target propensity of sgRNA at specific DNA fragments. Most of them use artificial feature extraction operations and machine learning techniques to obtain off-target scores. With the rapid expansion of off-target data and the rapid development of deep learning theory, the existing prediction methods can no longer satisfy the prediction accuracy at the clinical level. 
  Results:  Here, we propose a prediction method named CnnCrispr to predict the off-target propensity of sgRNA at specific DNA fragments. CnnCrispr automatically trains the sequence features of sgRNA-DNA pairs with GloVe model, and embeds the trained word vector matrix into the deep learning model including biLSTM and CNN with five hidden layers. We conducted performance verification on the data set provided by DeepCrispr, and found that the auROC and auPRC in the "leave-one-sgRNA-out" cross validation could reach 0.957 and 0.429 respectively (the Pearson value and spearman value could reach 0.495 and 0.151 respectively under the same settings). 
  Conclusion:  Our results show that CnnCrispr has better classification and regression performance than the existing states-of-art models. The code for CnnCrispr can be freely downloaded from https://github.com/LQYoLH/CnnCrispr. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3395-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32041517/  |  
------------------------------------------- 
10.1371/journal.pone.0220677  |   Diabetic Macular Edema (DME) is an advanced stage of Diabetic Retinopathy (DR) and can lead to permanent vision loss. Currently, it affects 26.7 million people globally and on account of such a huge number of DME cases and the limited number of ophthalmologists, it is desirable to automate the diagnosis process. Computer-assisted, deep learning based diagnosis could help in early detection, following which precision medication can help to mitigate the vision loss. 
  Method:  In order to automate the screening of DME, we propose a novel DMENet Algorithm which is built on the pillars of Convolutional Neural Networks (CNNs). DMENet analyses the preprocessed color fundus images and passes it through a two-stage pipeline. The first stage detects the presence or absence of DME whereas the second stage takes only the positive cases and grades the images based on severity. In both the stages, we use a novel Hierarchical Ensemble of CNNs (HE-CNN). This paper uses two of the popular publicly available datasets IDRiD and MESSIDOR for classification. Preprocessing on the images is performed using morphological opening and gaussian kernel. The dataset is augmented to solve the class imbalance problem for better performance of the proposed model. 
  Results:  The proposed methodology achieved an average Accuracy of 96.12%, Sensitivity of 96.32%, Specificity of 95.84%, and F-1 score of 0.9609 on MESSIDOR and IDRiD datasets. 
  Conclusion:  These excellent results establish the validity of the proposed methodology for use in DME screening and solidifies the applicability of the HE-CNN classification technique in the domain of biomedical imaging. 
  |  http://dx.plos.org/10.1371/journal.pone.0220677  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32040475/  |  
------------------------------------------- 
10.1186/s12859-020-3379-z  |    Background:  Computational prediction of drug-target interactions (DTI) is vital for drug discovery. The experimental identification of interactions between drugs and target proteins is very onerous. Modern technologies have mitigated the problem, leveraging the development of new drugs. However, drug development remains extremely expensive and time consuming. Therefore, in silico DTI predictions based on machine learning can alleviate the burdensome task of drug development. Many machine learning approaches have been proposed over the years for DTI prediction. Nevertheless, prediction accuracy and efficiency are persisting problems that still need to be tackled. Here, we propose a new learning method which addresses DTI prediction as a multi-output prediction task by learning ensembles of multi-output bi-clustering trees (eBICT) on reconstructed networks. In our setting, the nodes of a DTI network (drugs and proteins) are represented by features (background information). The interactions between the nodes of a DTI network are modeled as an interaction matrix and compose the output space in our problem. The proposed approach integrates background information from both drug and target protein spaces into the same global network framework. 
  Results:  We performed an empirical evaluation, comparing the proposed approach to state of the art DTI prediction methods and demonstrated the effectiveness of the proposed approach in different prediction settings. For evaluation purposes, we used several benchmark datasets that represent drug-protein networks. We show that output space reconstruction can boost the predictive performance of tree-ensemble learning methods, yielding more accurate DTI predictions. 
  Conclusions:  We proposed a new DTI prediction method where bi-clustering trees are built on reconstructed networks. Building tree-ensemble learning models with output space reconstruction leads to superior prediction results, while preserving the advantages of tree-ensembles, such as scalability, interpretability and inductive setting. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3379-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32033537/  |  
------------------------------------------- 
10.1007/s10916-020-01561-2  |   Optic disc (OD) and optic cup (OC) segmentation are important steps for automatic screening and diagnosing of optic nerve head abnormalities such as glaucoma. Many recent works formulated the OD and OC segmentation as a pixel classification task. However, it is hard for these methods to explicitly model the spatial relations between the labels in the output mask. Furthermore, the proportion of the background, OD and OC are unbalanced which also may result in a biased model as well as introduce more noise. To address these problems, we developed an approach that follows a coarse-to-fine segmentation process. We start with a U-Net to obtain a rough segmenting boundary and then crop the area around the boundary to form a boundary contour centered image. Second, inspired by sequence labeling tasks in natural language processing, we regard the OD and OC segmentation as a sequence labeling task and propose a novel fully convolutional network called SU-Net and combine it with the Viterbi algorithm to jointly decode the segmentation boundary. We also introduced a geometric parameter-based data augmentation method to generate more training samples in order to minimize the differences between training and test sets and reduce overfitting. Experimental results show that our method achieved state-of-the-art results on 2 datasets for both OD and OC segmentation and our method outperforms most of the ophthalmologists in terms of achieving agreement out of 6 ophthalmologists on the MESSIDOR dataset for both OD and OC segmentation. In terms of glaucoma screening, we achieved the best cup-to-disc ratio (CDR) error and area under the ROC curve (AUC) for glaucoma classification on the Drishti-GS dataset. 
  |  https://dx.doi.org/10.1007/s10916-020-01561-2  |  
------------------------------------------- 
10.1371/journal.pone.0227754  |   Aesthetic perception is a human instinct that is responsive to multimedia stimuli. Giving computers the ability to assess human sensory and perceptual experience of aesthetics is a well-recognized need for the intelligent design industry and multimedia intelligence study. In this work, we constructed a novel database for the aesthetic evaluation of design, using 2,918 images collected from the archives of two major design awards, and we also present a method of aesthetic evaluation that uses machine learning algorithms. Reviewers' ratings of the design works are set as the ground-truth annotations for the dataset. Furthermore, multiple image features are extracted and fused. The experimental results demonstrate the validity of the proposed approach. Primary screening using aesthetic computing can be an intelligent assistant for various design evaluations and can reduce misjudgment in art and design review due to visual aesthetic fatigue after a long period of viewing. The study of computational aesthetic evaluation can provide positive effect on the efficiency of design review, and it is of great significance to aesthetic recognition exploration and applications development. 
  |  http://dx.plos.org/10.1371/journal.pone.0227754  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31961909/  |  
------------------------------------------- 
10.1371/journal.pcbi.1007549  |   Advanced brain imaging analysis methods, including multivariate pattern analysis (MVPA), functional connectivity, and functional alignment, have become powerful tools in cognitive neuroscience over the past decade. These tools are implemented in custom code and separate packages, often requiring different software and language proficiencies. Although usable by expert researchers, novice users face a steep learning curve. These difficulties stem from the use of new programming languages (e.g., Python), learning how to apply machine-learning methods to high-dimensional fMRI data, and minimal documentation and training materials. Furthermore, most standard fMRI analysis packages (e.g., AFNI, FSL, SPM) focus on preprocessing and univariate analyses, leaving a gap in how to integrate with advanced tools. To address these needs, we developed BrainIAK (brainiak.org), an open-source Python software package that seamlessly integrates several cutting-edge, computationally efficient techniques with other Python packages (e.g., Nilearn, Scikit-learn) for file handling, visualization, and machine learning. To disseminate these powerful tools, we developed user-friendly tutorials (in Jupyter format; https://brainiak.org/tutorials/) for learning BrainIAK and advanced fMRI analysis in Python more generally. These materials cover techniques including: MVPA (pattern classification and representational similarity analysis); parallelized searchlight analysis; background connectivity; full correlation matrix analysis; inter-subject correlation; inter-subject functional connectivity; shared response modeling; event segmentation using hidden Markov models; and real-time fMRI. For long-running jobs or large memory needs we provide detailed guidance on high-performance computing clusters. These notebooks were successfully tested at multiple sites, including as problem sets for courses at Yale and Princeton universities and at various workshops and hackathons. These materials are freely shared, with the hope that they become part of a pool of open-source software and educational materials for large-scale, reproducible fMRI analysis and accelerated discovery. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007549  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31940340/  |  
------------------------------------------- 
10.1186/s13058-020-1255-4  |    |  https://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-020-1255-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32041655/  |  
------------------------------------------- 
10.1371/journal.pone.0227222  |   The stock market is known for its extreme complexity and volatility, and people are always looking for an accurate and effective way to guide stock trading. Long short-term memory (LSTM) neural networks are developed by recurrent neural networks (RNN) and have significant application value in many fields. In addition, LSTM avoids long-term dependence issues due to its unique storage unit structure, and it helps predict financial time series. Based on LSTM and an attention mechanism, a wavelet transform is used to denoise historical stock data, extract and train its features, and establish the prediction model of a stock price. We compared the results with the other three models, including the LSTM model, the LSTM model with wavelet denoising and the gated recurrent unit(GRU) neural network model on S&amp;P 500, DJIA, HSI datasets. Results from experiments on the S&amp;P 500 and DJIA datasets show that the coefficient of determination of the attention-based LSTM model is both higher than 0.94, and the mean square error of our model is both lower than 0.05. 
  |  http://dx.plos.org/10.1371/journal.pone.0227222  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31899770/  |  
------------------------------------------- 
10.1038/s41467-019-13996-4  |   Visual context facilitates perception, but how this is neurally implemented remains unclear. One example of contextual facilitation is found in reading, where letters are more easily identified when embedded in a word. Bottom-up models explain this word advantage as a post-perceptual decision bias, while top-down models propose that word contexts enhance perception itself. Here, we arbitrate between these accounts by presenting words and nonwords and probing the representational fidelity of individual letters using functional magnetic resonance imaging. In line with top-down models, we find that word contexts enhance letter representations in early visual cortex. Moreover, we observe increased coupling between letter information in visual cortex and brain activity in key areas of the reading network, suggesting these areas may be the source of the enhancement. Our results provide evidence for top-down representational enhancement in word recognition, demonstrating that word contexts can modulate perceptual processing already at the earliest visual regions. 
  |  http://dx.doi.org/10.1038/s41467-019-13996-4  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31949153/  |  
------------------------------------------- 
10.1111/ggi.13871  |    |  https://doi.org/10.1111/ggi.13871  |  
------------------------------------------- 
10.1371/journal.pcbi.1007607  |   Prediction of clinical drug response (CDR) of cancer patients, based on their clinical and molecular profiles obtained prior to administration of the drug, can play a significant role in individualized medicine. Machine learning models have the potential to address this issue but training them requires data from a large number of patients treated with each drug, limiting their feasibility. While large databases of drug response and molecular profiles of preclinical in-vitro cancer cell lines (CCLs) exist for many drugs, it is unclear whether preclinical samples can be used to predict CDR of real patients. We designed a systematic approach to evaluate how well different algorithms, trained on gene expression and drug response of CCLs, can predict CDR of patients. Using data from two large databases, we evaluated various linear and non-linear algorithms, some of which utilized information on gene interactions. Then, we developed a new algorithm called TG-LASSO that explicitly integrates information on samples' tissue of origin with gene expression profiles to improve prediction performance. Our results showed that regularized regression methods provide better prediction performance. However, including the network information or common methods of including information on the tissue of origin did not improve the results. On the other hand, TG-LASSO improved the predictions and distinguished resistant and sensitive patients for 7 out of 13 drugs. Additionally, TG-LASSO identified genes associated with the drug response, including known targets and pathways involved in the drugs' mechanism of action. Moreover, genes identified by TG-LASSO for multiple drugs in a tissue were associated with patient survival. In summary, our analysis suggests that preclinical samples can be used to predict CDR of patients and identify biomarkers of drug sensitivity and survival. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007607  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31967990/  |  
------------------------------------------- 
10.1371/journal.pone.0227922  |   Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such an adaptation property relies heavily on cellular neuromodulation, the biological mechanism that dynamically controls intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-reinforcement learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems. 
  |  http://dx.plos.org/10.1371/journal.pone.0227922  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31986189/  |  
------------------------------------------- 
10.1186/s12859-020-3459-0  |    Background:  Nanopore sequencing is a rapidly developing third-generation sequencing technology, which can generate long nucleotide reads of molecules within a portable device in real-time. Through detecting the change of ion currency signals during a DNA/RNA fragment's pass through a nanopore, genotypes are determined. Currently, the accuracy of nanopore basecalling has a higher error rate than the basecalling of short-read sequencing. Through utilizing deep neural networks, the-state-of-the art nanopore basecallers achieve basecalling accuracy in a range from 85% to 95%. 
  Result:  In this work, we proposed a novel basecalling approach from a perspective of instance segmentation. Different from previous approaches of doing typical sequence labeling, we formulated the basecalling problem as a multi-label segmentation task. Meanwhile, we proposed a refined U-net model which we call UR-net that can model sequential dependencies for a one-dimensional segmentation task. The experiment results show that the proposed basecaller URnano achieves competitive results on the in-species data, compared to the recently proposed CTC-featured basecallers. 
  Conclusion:  Our results show that formulating the basecalling problem as a one-dimensional segmentation task is a promising approach, which does basecalling and segmentation jointly. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3459-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32321433/  |  
------------------------------------------- 
10.1186/s12859-020-3400-6  |    Background:  Feature selection is a crucial step in machine learning analysis. Currently, many feature selection approaches do not ensure satisfying results, in terms of accuracy and computational time, when the amount of data is huge, such as in 'Omics' datasets. 
  Results:  Here, we propose an innovative implementation of a genetic algorithm, called GARS, for fast and accurate identification of informative features in multi-class and high-dimensional datasets. In all simulations, GARS outperformed two standard filter-based and two 'wrapper' and one embedded' selection methods, showing high classification accuracies in a reasonable computational time. 
  Conclusions:  GARS proved to be a suitable tool for performing feature selection on high-dimensional data. Therefore, GARS could be adopted when standard feature selection approaches do not provide satisfactory results or when there is a huge amount of data to be analyzed. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3400-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046651/  |  
------------------------------------------- 
10.1093/sysbio/syz060  |   Reconstructing the phylogenetic relationships between species is one of the most formidable tasks in evolutionary biology. Multiple methods exist to reconstruct phylogenetic trees, each with their own strengths and weaknesses. Both simulation and empirical studies have identified several "zones" of parameter space where accuracy of some methods can plummet, even for four-taxon trees. Further, some methods can have undesirable statistical properties such as statistical inconsistency and/or the tendency to be positively misleading (i.e. assert strong support for the incorrect tree topology). Recently, deep learning techniques have made inroads on a number of both new and longstanding problems in biological research. In this study, we designed a deep convolutional neural network (CNN) to infer quartet topologies from multiple sequence alignments. This CNN can readily be trained to make inferences using both gapped and ungapped data. We show that our approach is highly accurate on simulated data, often outperforming traditional methods, and is remarkably robust to bias-inducing regions of parameter space such as the Felsenstein zone and the Farris zone. We also demonstrate that the confidence scores produced by our CNN can more accurately assess support for the chosen topology than bootstrap and posterior probability scores from traditional methods. Although numerous practical challenges remain, these findings suggest that the deep learning approaches such as ours have the potential to produce more accurate phylogenetic inferences. 
  |  https://academic.oup.com/sysbio/article-lookup/doi/10.1093/sysbio/syz060  |  
------------------------------------------- 
10.1371/journal.pone.0224445  |   Availability of trained radiologists for fast processing of CXRs in regions burdened with tuberculosis always has been a challenge, affecting both timely diagnosis and patient monitoring. The paucity of annotated images of lungs of TB patients hampers attempts to apply data-oriented algorithms for research and clinical practices. The TB Portals Program database (TBPP, https://TBPortals.niaid.nih.gov) is a global collaboration curating a large collection of the most dangerous, hard-to-cure drug-resistant tuberculosis (DR-TB) patient cases. TBPP, with 1,179 (83%) DR-TB patient cases, is a unique collection that is well positioned as a testing ground for deep learning classifiers. As of January 2019, the TBPP database contains 1,538 CXRs, of which 346 (22.5%) are annotated by a radiologist and 104 (6.7%) by a pulmonologist-leaving 1,088 (70.7%) CXRs without annotations. The Qure.ai qXR artificial intelligence automated CXR interpretation tool, was blind-tested on the 346 radiologist-annotated CXRs from the TBPP database. Qure.ai qXR CXR predictions for cavity, nodule, pleural effusion, hilar lymphadenopathy was successfully matching human expert annotations. In addition, we tested the 12 Qure.ai classifiers to find whether they correlate with treatment success (information provided by treating physicians). Ten descriptors were found as significant: abnormal CXR (p = 0.0005), pleural effusion (p = 0.048), nodule (p = 0.0004), hilar lymphadenopathy (p = 0.0038), cavity (p = 0.0002), opacity (p = 0.0006), atelectasis (p = 0.0074), consolidation (p = 0.0004), indicator of TB disease (p = &lt; .0001), and fibrosis (p = &lt; .0001). We conclude that applying fully automated Qure.ai CXR analysis tool is useful for fast, accurate, uniform, large-scale CXR annotation assistance, as it performed well even for DR-TB cases that were not used for initial training. Testing artificial intelligence algorithms (encapsulating both machine learning and deep learning classifiers) on diverse data collections, such as TBPP, is critically important toward progressing to clinically adopted automatic assistants for medical data analysis. 
  |  http://dx.plos.org/10.1371/journal.pone.0224445  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31978149/  |  
------------------------------------------- 
10.1371/journal.pone.0228035  |    Background:  Invoices had been used in food product traceability, however, none have addressed the automated alarm system for food safety by utilizing electronic invoice big data. In this paper, we present an alarm system for edible oil manufacture that can prevent a food safety crisis rather than trace problematic sources post-crisis. 
  Materials and methods:  Using nearly 100 million labeled e-invoices from the 2013‒2014 of 595 edible oil manufacturers provided by Ministry of Finance, we applied text-mining, statistical and machine learning techniques to "train" the system for two functions: (1) to sieve edible oil-related e-invoices of manufacturers who may also produce other merchandise and (2) to identify suspicious edible oil manufacture based on irrational transactions from the e-invoices sieved. 
  Results:  The system was able to (1) accurately sieve the correct invoices with sensitivity &gt;95% and specificity &gt;98% via text classification and (2) identify problematic manufacturers with 100% accuracy via Random Forest machine learning method, as well as with sensitivity &gt;70% and specificity &gt;99% through simple decision-tree method. 
  Conclusion:  E-invoice has bright future on the application of food safety. It can not only be used for product traceability, but also prevention of adverse events by flag suspicious manufacturers. Compulsory usage of e-invoice for food producing can increase the accuracy of this alarm system. 
  |  http://dx.plos.org/10.1371/journal.pone.0228035  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31978198/  |  
------------------------------------------- 
10.1097/RCT.0000000000000958  |    Objective:  The purpose of this study was to determine whether computed tomography (CT) angiography with machine learning (ML) can be used to predict the rapid growth of abdominal aortic aneurysm (AAA). 
  Materials and methods:  This retrospective study was approved by our institutional review board. Fifty consecutive patients (45 men, 5 women, 73.5 years) with small AAA (38.5 ± 6.2 mm) had undergone CT angiography. To be included, patients required at least 2 CT scans a minimum of 6 months apart. Abdominal aortic aneurysm growth, estimated by change per year, was compared between patients with baseline infrarenal aortic minor axis. For each axial image, major axis of AAA, minor axis of AAA, major axis of lumen without intraluminal thrombi (ILT), minor axis of lumen without ILT, AAA area, lumen area without ILT, ILT area, maximum ILT area, and maximum ILT thickness were measured. We developed a prediction model using an ML method (to predict expansion &gt;4 mm/y) and calculated the area under the receiver operating characteristic curve of this model via 10-fold cross-validation. 
  Results:  The median aneurysm expansion was 3.0 mm/y. Major axis of AAA and AAA area correlated significantly with future AAA expansion (r = 0.472, 0.416 all P &lt; 0.01). Machine learning and major axis of AAA were a strong predictor of significant AAA expansion (&gt;4 mm/y) (area under the receiver operating characteristic curve were 0.86 and 0.78). 
  Conclusions:  Machine learning is an effective method for the prediction of expansion risk of AAA. Abdominal aortic aneurysm area and major axis of AAA are the important factors to reflect AAA expansion. 
  |  http://dx.doi.org/10.1097/RCT.0000000000000958  |  
------------------------------------------- 
10.1371/journal.pone.0228167  |   A key challenge in the field of cognitive neuroscience is to identify discriminable cognitive functions, and then map these functions to brain activity. In the current study, we set out to explore the relationships between performance arising from different cognitive tasks thought to tap different domains of cognition, and then to test whether these distinct latent cognitive abilities also are subserved by corresponding "latent" brain substrates. To this end, we tested a large sample of adults under the age of 40 on twelve cognitive tasks as they underwent fMRI scanning. Exploratory factor analysis revealed 4-factor model, dissociating tasks into processes corresponding to episodic memory retrieval, reasoning, speed of processing and vocabulary. An analysis of the topographic covariance patterns of the BOLD-response acquired during each task similarity also converged on four neural networks that corresponded to the 4 latent factors. These results suggest that distinct ontologies of cognition are subserved by corresponding distinct neural networks. 
  |  http://dx.plos.org/10.1371/journal.pone.0228167  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32040518/  |  
------------------------------------------- 
10.1371/journal.pone.0228422  |   This paper focuses on the application of machine learning algorithms for predicting spinal abnormalities. As a data preprocessing step, univariate feature selection as a filter based feature selection, and principal component analysis (PCA) as a feature extraction algorithm are considered. A number of machine learning approaches namely support vector machine (SVM), logistic regression (LR), bagging ensemble methods are considered for the diagnosis of spinal abnormality. The SVM, LR, bagging SVM and bagging LR models are applied on a dataset of 310 samples publicly available in Kaggle repository. The performance of classification of abnormal and normal spinal patients is evaluated in terms of a number of factors including training and testing accuracy, recall, and miss rate. The classifier models are also evaluated by optimizing the kernel parameters, and by using the results of receiver operating characteristic (ROC) and precision-recall curves. Results indicate that when 78% data are used for training, the observed training accuracies for SVM, LR, bagging SVM and bagging LR are 86.30%, 85.47%, 86.72% and 85.06%, respectively. On the other hand, the accuracies for the test dataset for SVM, LR, bagging SVM and bagging LR are the same being 86.96%. However, bagging SVM is the most attractive as it has a higher recall value and a lower miss rate compared to others. Hence, bagging SVM is suitable for the classification of spinal patients when applied on the most five important features of spinal samples. 
  |  http://dx.plos.org/10.1371/journal.pone.0228422  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32027680/  |  
------------------------------------------- 
10.1186/s12859-020-3375-3  |    Background:  Automated biomedical named entity recognition and normalization serves as the basis for many downstream applications in information management. However, this task is challenging due to name variations and entity ambiguity. A biomedical entity may have multiple variants and a variant could denote several different entity identifiers. 
  Results:  To remedy the above issues, we present a novel knowledge-enhanced system for protein/gene named entity recognition (PNER) and normalization (PNEN). On one hand, a large amount of entity name knowledge extracted from biomedical knowledge bases is used to recognize more entity variants. On the other hand, structural knowledge of entities is extracted and encoded as identifier (ID) embeddings, which are then used for better entity normalization. Moreover, deep contextualized word representations generated by pre-trained language models are also incorporated into our knowledge-enhanced system for modeling multi-sense information of entities. Experimental results on the BioCreative VI Bio-ID corpus show that our proposed knowledge-enhanced system achieves 0.871 F1-score for PNER and 0.445 F1-score for PNEN, respectively, leading to a new state-of-the-art performance. 
  Conclusions:  We propose a knowledge-enhanced system that combines both entity knowledge and deep contextualized word representations. Comparison results show that entity knowledge is beneficial to the PNER and PNEN task and can be well combined with contextualized information in our system for further improvement. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3375-3  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32000677/  |  
------------------------------------------- 
10.1097/IJG.0000000000001411  |    Purpose:  To develop a software package for automated measuring of the trabecular-iris angle (TIA) using ultrasound biomicroscopy. 
  Methods:  Ultrasound biomicroscopy images were collected and the TIA was manually measured by specialists. Different models were used as the convolutional neural network for the automatic TIA measurement. The root-mean-squared error, explained variance, and mean absolute percentage error were used to evaluate the performance of these models. The interobserver reproducibility, coefficient of variation, and intraclass correlation coefficient were calculated to evaluate the consistency between the manual measured and the model predicted values. 
  Results:  ResNet-18 had the best performance in root-mean-squared error, explained variance, and mean absolute percentage error among all 5 models. The average difference between the angles measured manually and by the model is -0.46±3.97 degrees for all eyes, -1.67±5.19 degrees for open angles, and 0.75±1.43 degrees for narrow angles. The coefficient of variation, intraclass correlation coefficient, and reproducibility of the total TIA measurements are 6.8%, 0.95, and 6.1 degrees for all angles; 6.4%, 0.99, and 7.7 degrees for open angles; and 8.8%, 0.93, and 4 degrees for narrow angles, respectively. 
  Conclusions:  Preliminary results show that this fully automated anterior chamber angle measurement method can achieve high accuracy and have good consistency with the manual measurement results, this has great significance for future clinical practice. 
  |  http://dx.doi.org/10.1097/IJG.0000000000001411  |  
------------------------------------------- 
10.1371/journal.pone.0227794  |   Peony is a famous ornamental and medicinal plant in China, and peony hybrid breeding is an important means of germplasm innovation. However, research on the genome of this species is limited, thereby hindering the genetic and breeding research on peony. In the present study, simple sequence repeat (SSR) locus analysis was performed on expressed sequence tags obtained by the transcriptome sequencing of Paeonia using Microsatellite software. Primers with polymorphism were obtained via polymerase chain reaction amplification and electrophoresis. As a result, a total of 86,195 unigenes were obtained by assembling the transcriptome data of Paeonia. Functional annotations were obtained in seven functional databases including 49,172 (Non-Redundant Protein Sequence Database: 57.05%), 38,352 (Nucleotide Sequence Database: 44.49%), 36,477 (Swiss Prot: 42.32%), 38,905 (Clusters of Orthologous Groups for Eukaryotic Complete Genomes: 45.14%), 37,993 (Kyoto Encyclopedia of Genes and Genomes: 44.08%), 26,832 (Gene Ontology: 31.13%) and 37,758 (Pfam: 43.81%) unigenes. Meanwhile, 21,998 SSR loci were distributed in 17,567 unigenes containing SSR sequences, and the SSR distribution frequency was 25.52%, with an average of one SSR sequence per 4.66 kb. Mononucleotide, dinucleotide, and trinucleotide were the main repeat types, accounting for 55.74%, 25.58%, and 13.21% of the total repeat times, respectively. Forty-five pairs of the 100 pairs of primers selected randomly could amplify clear polymorphic bands. The polymorphic primers of these 45 pairs were used to cluster and analyze 16 species of peony. The new SSR molecular markers can be useful for the study of genetic diversity and marker-assisted breeding of peony. 
  |  http://dx.plos.org/10.1371/journal.pone.0227794  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31999761/  |  
------------------------------------------- 
10.1038/s41467-019-14018-z  |   A fundamental problem in biomedical research is the low number of observations available, mostly due to a lack of available biosamples, prohibitive costs, or ethical reasons. Augmenting few real observations with generated in silico samples could lead to more robust analysis results and a higher reproducibility rate. Here, we propose the use of conditional single-cell generative adversarial neural networks (cscGAN) for the realistic generation of single-cell RNA-seq data. cscGAN learns non-linear gene-gene dependencies from complex, multiple cell type samples and uses this information to generate realistic cells of defined types. Augmenting sparse cell populations with cscGAN generated cells improves downstream analyses such as the detection of marker genes, the robustness and reliability of classifiers, the assessment of novel analysis algorithms, and might reduce the number of animal experiments and costs in consequence. cscGAN outperforms existing methods for single-cell RNA-seq data generation in quality and hold great promise for the realistic generation and augmentation of other biomedical data types. 
  |  http://dx.doi.org/10.1038/s41467-019-14018-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31919373/  |  
------------------------------------------- 
10.1111/ele.13462  |   Neural networks are increasingly being used in science to infer hidden dynamics of natural systems from noisy observations, a task typically handled by hierarchical models in ecology. This article describes a class of hierarchical models parameterised by neural networks - neural hierarchical models. The derivation of such models analogises the relationship between regression and neural networks. A case study is developed for a neural dynamic occupancy model of North American bird populations, trained on millions of detection/non-detection time series for hundreds of species, providing insights into colonisation and extinction at a continental scale. Flexible models are increasingly needed that scale to large data and represent ecological processes. Neural hierarchical models satisfy this need, providing a bridge between deep learning and ecological modelling that combines the function representation power of neural networks with the inferential capacity of hierarchical models. 
  |  https://doi.org/10.1111/ele.13462  |  
------------------------------------------- 
10.1371/journal.pone.0227307  |   Z-numbers can generate a more flexible structure to model the real information because of capturing expert's reliability. Moreover, various semantics can flexibly be reflected by linguistic terms under various circumstances. Thus, this study aims to model the portfolio selection problems based on aggregation operators under linguistic Z-number environment. Therefore, a multi-stage methodology is proposed and linguistic Z-numbers are applied to describe the assessment information. Moreover, the weighted averaging (WA) aggregation operator, the ordered weighted averaging (OWA) aggregation operator and the hybrid weighted averaging (HWA) aggregation operator are developed to fuse the input arguments under the linguistic Z-number environment. Then, using the max-score rule and the score-accuracy trade-off rule, three qualitative portfolio models are presented to allocate the optimal assets. These models are suitable for general investors and risky investors. Finally, to illustrate the validity of the proposed qualitative approach, a real case including 20 corporations of Tehran stock exchange market in Iran is provided and the obtained results are analyzed. The results show that combining linguistic Z-numbers with portfolio selection processes can increase the tendencies and capabilities of investors in the capital market and it helps them manage their portfolios efficiently. 
  |  http://dx.plos.org/10.1371/journal.pone.0227307  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31971992/  |  
------------------------------------------- 
10.3346/jkms.2020.35.e78  |    Background:  Human leukocyte antigen (HLA) typing is important for transplant patients to prevent a severe mismatch reaction, and the result can also support the diagnosis of various disease or prediction of drug side effects. However, such secondary applications of HLA typing results are limited because they are typically provided in free-text format or PDFs on electronic medical records. We here propose a method to convert HLA genotype information stored in an unstructured format into a reusable structured format by extracting serotype/allele information. 
  Methods:  We queried HLA typing reports from the clinical data warehouse of Seoul National University Hospital (SUPPREME) from 2000 to 2018 as a rule-development data set (64,024 reports) and from the most recent year (6,181 reports) as a test set. We used a rule-based natural language approach using a Python regex function to extract the 1) number of patients in the report, 2) clinical characteristics such as indication of the HLA testing, and 3) precise HLA genotypes. The performance of the rules and codes was evaluated by comparison between the extracted results from the test set and a validation set generated by manual curation. 
  Results:  Among 11,287 reports for development set and 1,107 for the test set describing HLA typing for a single patient, iterative rule generation developed 124 extracting rules and 8 cleaning rules for HLA genotypes. Application of these rules extracted HLA genotypes with 0.892-0.999 precision and 0.795-0.998 recall for the five HLA genes. The precision and recall of the extracting rules for the number of patients in a report were 0.997 and 0.994 and those for the clinical variable extraction were 0.997 and 0.992, respectively. All extracted HLA alleles and serotypes were transformed according to formal HLA nomenclature by the cleaning rules. 
  Conclusion:  The rule-based HLA genotype extraction method shows reliable accuracy. We believe that there are significant number of patients who takes profit when this under-used genetic information will be return to them. 
  |  https://jkms.org/DOIx.php?id=10.3346/jkms.2020.35.e78  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32233158/  |  
------------------------------------------- 
10.1371/journal.pone.0226963  |   As a low-end computed tomography (CT) system, translational CT (TCT) is in urgent demand in developing countries. Under some circumstances, in order to reduce the scan time, decrease the X-ray radiation or scan long objects, furthermore, to avoid the inconsistency of the detector for the large angle scanning, we use the limited-angle TCT scanning mode to scan an object within a limited angular range. However, this scanning mode introduces some additional noise and limited-angle artifacts that seriously degrade the imaging quality and affect the diagnosis accuracy. To reconstruct a high-quality image for the limited-angle TCT scanning mode, we develop a limited-angle TCT image reconstruction algorithm based on a U-net convolutional neural network (CNN). First, we use the SART method to the limited-angle TCT projection data, then we import the image reconstructed by SART method to a well-trained CNN which can suppress the artifacts and preserve the structures to obtain a better reconstructed image. Some simulation experiments are implemented to demonstrate the performance of the developed algorithm for the limited-angle TCT scanning mode. Compared with some state-of-the-art methods, the developed algorithm can effectively suppress the noise and the limited-angle artifacts while preserving the image structures. 
  |  http://dx.plos.org/10.1371/journal.pone.0226963  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31905225/  |  
------------------------------------------- 
10.1515/bmt-2018-0007  |   The first part of this study investigated pattern recognition of head movements based on mechanomyography (MMG) signals. Four channel MMG signals were collected from the sternocleidomastoid (SCM) muscles and the splenius capitis (SPL) muscles in the subjects' neck when they bowed the head, raised the head, side-bent to left, side-bent to right, turned to left and turned to right. The MMG signals were then filtered, normalized and divided using an unequal length segmentation algorithm into a single action frame. After extracting the energy features of the wavelet packet coefficients and the feature of the principal diagonal slices of the bispectrum, the dimension of the energy features were reduced by the Fisher linear discriminant analysis (FLDA). Finally, all the features were classified through the support vector machine (SVM) classifier. The recognition rate was up to 95.92%. On this basis, the second part of this study used the head movements to control a car model for simulating the control of a wheelchair, and the success rate was 85.74%. 
  |  https://www.degruyter.com/doi/10.1515/bmt-2018-0007  |  
------------------------------------------- 
10.1097/MD.0000000000019218  |   To develop a classification model for accurately discriminating common infectious diseases in Zhejiang province, China.Symptoms and signs, abnormal lab test results, epidemiological features, as well as the incidence rates were treated as predictors, and were collected from the published literature and a national surveillance system of infectious disease. A classification model was established using naïve Bayesian classifier. Dataset from historical outbreaks was applied for model validation, while sensitivity, specificity, accuracy, area under the receiver operating characteristic curve (AUC) and M-index were presented.A total of 146 predictors were included in the classification model, for discriminating 25 common infectious diseases. The sensitivity ranged from 44.44% for hepatitis E to 96.67% for measles. The specificity varied from 96.36% for dengue fever to 100% for 5 diseases. The median of total accuracy was 97.41% (range: 93.85%-99.04%). The AUCs exceeded 0.98 in 11 of 12 diseases, except in dengue fever (0.613). The M-index was 0.960 (95%CI 0.941-0.978).A novel classification model was constructed based on Bayesian approach to discriminate common infectious diseases in Zhejiang province, China. After entering symptoms and signs, abnormal lab test results, epidemiological features and city of disease origin, an output list of possible diseases ranked according to the calculated probabilities can be provided. The discrimination performance was reasonably good, making it useful in epidemiological applications. 
  |  http://dx.doi.org/10.1097/MD.0000000000019218  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32080115/  |  
------------------------------------------- 
10.1007/s10661-019-7987-x  |   Owing to the rise in population, lifestyle changes, high traffic rates in urban areas and environmental pollution, respiratory diseases have become much more prevalent on both regional and urban scales. Respiratory diseases affect over 300 million people worldwide and are thus among the major threats to humans' general well-being. The identification of underlying factors and the specification of accompanying risk areas for the temporal exacerbation of respiratory diseases are effective steps in managing the damage caused by such disorders. Here, we demonstrate a strategy for modelling the risk zone of respiratory diseases temporally, using a location-based social network (LBSN) and an artificial neural network (ANN). The main contribution of this paper is to consider the environmental and infrastructural factors and identify their relationships with the geographical locations of respiratory attacks. The study also utilizes Telegram, which is the most popular and conventional social media platform, in order to observe temporal changes in the location of respiratory attacks in Iran, in the form of a developed Telegram bot known as @respiratoryassociation. The relations between the factors behind and the location of respiratory attacks are determined using a multilayer perceptron (MLP) ANN. All the required data have been collected on a daily basis over a 5-year period from December 2013 to December 2018 in Tehran, Iran. The results indicated air pollution, especially pollution from carbon monoxide (CO) and suspended particulate matter (PM) as the most decisive factors. Following air pollution, the amount of exposure to the polluted area was determined as the second most decisive factor, which in turn increased as a result of escalations in traffic jams. Land use was determined as the third most decisive factor. Furthermore, the results revealed that the ANN performed satisfactorily, implying that the model can be used to examine the spatio-temporal behaviour of the time series of respiratory diseases with respect to environmental and infrastructural factors. 
  |  https://doi.org/10.1007/s10661-019-7987-x  |  
------------------------------------------- 
10.7507/1001-5515.201902023  |   In order to stimulate the patients' active participation in the process of robot-assisted rehabilitation training of stroke patients, the rehabilitation robots should provide assistant torque to patients according to their rehabilitation needs. This paper proposed an assist-as-needed control strategy for wrist rehabilitation robots. Firstly, the ability evaluation rules were formulated and the patient's ability was evaluated according to the rules. Then the controller was designed. Based on the evaluation results, the controller can calculate the assistant torque needed by the patient to complete the rehabilitation training task and send commands to motor. Finally, the motor is controlled to output the commanded value, which assists the patient to complete the rehabilitation training task. The control strategy was implemented to the wrist function rehabilitation robot, which could achieve the training effect of assist-as-needed and could avoid the surge of assistance torque. In addition, therapists can adjust multiple parameters in the ability evaluation rules online to customize the difficulty of tasks for patients with different rehabilitation status. The method proposed in this paper does not rely on the information from force sensor, which reduces development costs and is easy to implement. 
 在康复机器人辅助脑卒中患者进行康复训练时，为激发患者的主动参与意识，康复机器人应按照患者康复需求提供其所需的辅助力矩。本文针对腕功能康复机器人提出一种按需辅助控制策略：首先制定能力评估规则，并依据该规则评估患者能力；然后设计控制器，控制器可基于评估结果求解出患者完成康复训练任务所需的辅助力矩，并下发指令至电机；最后控制电机输出指令值，辅助患者完成康复训练任务。将该控制策略应用于腕功能康复机器人，不仅实现了按需辅助的训练模式，而且能够避免辅助力矩激增，同时康复治疗师可在线调节能力评估规则中的多个参数，为不同康复状态的患者定制任务难度。本文所提方法不依赖于力学传感器信息，降低了开发成本且易于实现，具有一定的工程应用价值。. 
  |  None  |  
------------------------------------------- 
10.1007/s10661-019-7968-0  |   Landslide susceptibility maps can be developed with artificial neural networks (ANNs). In order to train our ANNs, a digital elevation model (DEM) and a scar map of one previous event were used. Eleven attributes are generated, possibly containing redundant information. Our base model is formed by, essentially, one input (the DEM), eleven attributes, 30 neurons, and one output (susceptibility). Principal components (PCs) group information in the first projected variables, the last ones can be expendable. In the present paper, four groups of models were trained: one with eleven attributes generated from the DEM; one with 8 out of 11 attributes, in which 3 were eliminated by their high correlation with others; other, with the data projected over its PCs; and another, using 8 out of 11 PCs. The used number of neurons in hidden layer is 30, calibrated based on a complexity analysis that is an in-house developed method. The ANN models trained with the original data generated better statistical results than their counterparts trained with the PC transformed input. Keeping the original 11 attributes calculated provided the best metrics among all models, showing that eliminating attributes also eliminates information used by the model. Using 11 PC transformed attributes hindered trained. However, for the model with eight PCs, training was much faster than its counterpart with little accuracy loss. The metrics and maps achieved were considered acceptable, conveying the power of our model based on ANNs, which uses essentially one input (the DEM) for mapping areas susceptible to mass movements. 
  |  https://doi.org/10.1007/s10661-019-7968-0  |  
------------------------------------------- 
10.3390/s20072030  |   Driving is a task that puts heavy demands on visual information, thereby the human visual system plays a critical role in making proper decisions for safe driving. Understanding a driver's visual attention and relevant behavior information is a challenging but essential task in advanced driver-assistance systems (ADAS) and efficient autonomous vehicles (AV). Specifically, robust prediction of a driver's attention from images could be a crucial key to assist intelligent vehicle systems where a self-driving car is required to move safely interacting with the surrounding environment. Thus, in this paper, we investigate a human driver's visual behavior in terms of computer vision to estimate the driver's attention locations in images. First, we show that feature representations at high resolution improves visual attention prediction accuracy and localization performance when being fused with features at low-resolution. To demonstrate this, we employ a deep convolutional neural network framework that learns and extracts feature representations at multiple resolutions. In particular, the network maintains the feature representation with the highest resolution at the original image resolution. Second, attention prediction tends to be biased toward centers of images when neural networks are trained using typical visual attention datasets. To avoid overfitting to the center-biased solution, the network is trained using diverse regions of images. Finally, the experimental results verify that our proposed framework improves the prediction accuracy of a driver's attention locations. 
  |  http://www.mdpi.com/resolver?pii=s20072030  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32260397/  |  
------------------------------------------- 
10.1186/s12859-019-3296-1  |    Background:  In recent years, biomedical ontologies have become important for describing existing biological knowledge in the form of knowledge graphs. Data mining approaches that work with knowledge graphs have been proposed, but they are based on vector representations that do not capture the full underlying semantics. An alternative is to use machine learning approaches that explore semantic similarity. However, since ontologies can model multiple perspectives, semantic similarity computations for a given learning task need to be fine-tuned to account for this. Obtaining the best combination of semantic similarity aspects for each learning task is not trivial and typically depends on expert knowledge. 
  Results:  We have developed a novel approach, evoKGsim, that applies Genetic Programming over a set of semantic similarity features, each based on a semantic aspect of the data, to obtain the best combination for a given supervised learning task. The approach was evaluated on several benchmark datasets for protein-protein interaction prediction using the Gene Ontology as the knowledge graph to support semantic similarity, and it outperformed competing strategies, including manually selected combinations of semantic aspects emulating expert knowledge. evoKGsim was also able to learn species-agnostic models with different combinations of species for training and testing, effectively addressing the limitations of predicting protein-protein interactions for species with fewer known interactions. 
  Conclusions:  evoKGsim can overcome one of the limitations in knowledge graph-based semantic similarity applications: the need to expertly select which aspects should be taken into account for a given application. Applying this methodology to protein-protein interaction prediction proved successful, paving the way to broader applications. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3296-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31900127/  |  
------------------------------------------- 
10.1186/s12859-020-3342-z  |    Background:  Protein succinylation has recently emerged as an important and common post-translation modification (PTM) that occurs on lysine residues. Succinylation is notable both in its size (e.g., at 100 Da, it is one of the larger chemical PTMs) and in its ability to modify the net charge of the modified lysine residue from + 1 to - 1 at physiological pH. The gross local changes that occur in proteins upon succinylation have been shown to correspond with changes in gene activity and to be perturbed by defects in the citric acid cycle. These observations, together with the fact that succinate is generated as a metabolic intermediate during cellular respiration, have led to suggestions that protein succinylation may play a role in the interaction between cellular metabolism and important cellular functions. For instance, succinylation likely represents an important aspect of genomic regulation and repair and may have important consequences in the etiology of a number of disease states. In this study, we developed DeepSuccinylSite, a novel prediction tool that uses deep learning methodology along with embedding to identify succinylation sites in proteins based on their primary structure. 
  Results:  Using an independent test set of experimentally identified succinylation sites, our method achieved efficiency scores of 79%, 68.7% and 0.48 for sensitivity, specificity and MCC respectively, with an area under the receiver operator characteristic (ROC) curve of 0.8. In side-by-side comparisons with previously described succinylation predictors, DeepSuccinylSite represents a significant improvement in overall accuracy for prediction of succinylation sites. 
  Conclusion:  Together, these results suggest that our method represents a robust and complementary technique for advanced exploration of protein succinylation. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3342-z  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32321437/  |  
------------------------------------------- 
10.1186/s12859-020-3376-2  |    Background:  Biomedical event extraction is a fundamental and in-demand technology that has attracted substantial interest from many researchers. Previous works have heavily relied on manual designed features and external NLP packages in which the feature engineering is large and complex. Additionally, most of the existing works use the pipeline process that breaks down a task into simple sub-tasks but ignores the interaction between them. To overcome these limitations, we propose a novel event combination strategy based on hybrid deep neural networks to settle the task in a joint end-to-end manner. 
  Results:  We adapted our method to several annotated corpora of biomedical event extraction tasks. Our method achieved state-of-the-art performance with noticeable overall F1 score improvement compared to that of existing methods for all of these corpora. 
  Conclusions:  The experimental results demonstrated that our method is effective for biomedical event extraction. The combination strategy can reconstruct complex events from the output of deep neural networks, while the deep neural networks effectively capture the feature representation from the raw text. The biomedical event extraction implementation is available online;http://www. 
  Predictor:  xin/event_extraction;http://2013.bionlp-st.org/tasks;http://nactem.ac.uk/MLEE/.Otherwise, please provide alternatives. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3376-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32028883/  |  
------------------------------------------- 
10.1007/s10661-020-8064-1  |   Wastewater treatment plants use many sensors to control energy consumption and discharge quality. These sensors produce a vast amount of data which can be efficiently monitored by automatic systems. Consequently, several different statistical and learning methods are proposed in the literature which can automatically detect faults. While these methods have shown promising results, the nonlinear dynamics and complex interactions of the variables in wastewater data necessitate more powerful methods with higher learning capacities. In response, this study focusses on modelling faults in the oxidation and nitrification process. Specifically, this study investigates a method based on deep neural networks (specifically, long short-term memory) compared with statistical and traditional machine-learning methods. The network is specifically designed to capture temporal behaviour of sensor data. The proposed method is evaluated on a real-life dataset containing over 5.1 million sensor data points. The method achieved a fault detection rate (recall) of over 92%, thus outperforming traditional methods and enabling timely detection of collective faults. 
  |  https://doi.org/10.1007/s10661-020-8064-1  |  
------------------------------------------- 
10.1371/journal.pone.0226990  |   This study proposes a deep learning model that effectively suppresses the false alarms in the intensive care units (ICUs) without ignoring the true alarms using single- and multi- modal biosignals. Most of the current work in the literature are either rule-based methods, requiring prior knowledge of arrhythmia analysis to build rules, or classical machine learning approaches, depending on hand-engineered features. In this work, we apply convolutional neural networks to automatically extract time-invariant features, an attention mechanism to put more emphasis on the important regions of the segmented input signal(s) that are more likely to contribute to an alarm, and long short-term memory units to capture the temporal information presented in the signal segments. We trained our method efficiently using a two-step training algorithm (i.e., pre-training and fine-tuning the proposed network) on the dataset provided by the PhysioNet computing in cardiology challenge 2015. The evaluation results demonstrate that the proposed method obtains better results compared to other existing algorithms for the false alarm reduction task in ICUs. The proposed method achieves a sensitivity of 93.88% and a specificity of 92.05% for the alarm classification, considering three different signals. In addition, our experiments for 5 separate alarm types leads significant results, where we just consider a single-lead ECG (e.g., a sensitivity of 90.71%, a specificity of 88.30%, an AUC of 89.51 for alarm type of Ventricular Tachycardia arrhythmia). 
  |  http://dx.plos.org/10.1371/journal.pone.0226990  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31923226/  |  
------------------------------------------- 
10.1186/s12859-020-3488-8  |    Background:  G protein-coupled receptors (GPCRs) mediate a variety of important physiological functions, are closely related to many diseases, and constitute the most important target family of modern drugs. Therefore, the research of GPCR analysis and GPCR ligand screening is the hotspot of new drug development. Accurately identifying the GPCR-drug interaction is one of the key steps for designing GPCR-targeted drugs. However, it is prohibitively expensive to experimentally ascertain the interaction of GPCR-drug pairs on a large scale. Therefore, it is of great significance to predict the interaction of GPCR-drug pairs directly from the molecular sequences. With the accumulation of known GPCR-drug interaction data, it is feasible to develop sequence-based machine learning models for query GPCR-drug pairs. 
  Results:  In this paper, a new sequence-based method is proposed to identify GPCR-drug interactions. For GPCRs, we use a novel bag-of-words (BoW) model to extract sequence features, which can extract more pattern information from low-order to high-order and limit the feature space dimension. For drug molecules, we use discrete Fourier transform (DFT) to extract higher-order pattern information from the original molecular fingerprints. The feature vectors of two kinds of molecules are concatenated and input into a simple prediction engine distance-weighted K-nearest-neighbor (DWKNN). This basic method is easy to be enhanced through ensemble learning. Through testing on recently constructed GPCR-drug interaction datasets, it is found that the proposed methods are better than the existing sequence-based machine learning methods in generalization ability, even an unconventional method in which the prediction performance was further improved by post-processing procedure (PPP). 
  Conclusions:  The proposed methods are effective for GPCR-drug interaction prediction, and may also be potential methods for other target-drug interaction prediction, or protein-protein interaction prediction. In addition, the new proposed feature extraction method for GPCR sequences is the modified version of the traditional BoW model and may be useful to solve problems of protein classification or attribute prediction. The source code of the proposed methods is freely available for academic research at https://github.com/wp3751/GPCR-Drug-Interaction. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3488-8  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32312232/  |  
------------------------------------------- 
10.1103/PhysRevLett.124.108301  |   Deep neural networks are workhorse models in machine learning with multiple layers of nonlinear functions composed in series. Their loss function is highly nonconvex, yet empirically even gradient descent minimization is sufficient to arrive at accurate and predictive models. It is hitherto unknown why deep neural networks are easily optimizable. We analyze the energy landscape of a spin glass model of deep neural networks using random matrix theory and algebraic geometry. We analytically show that the multilayered structure holds the key to optimizability: Fixing the number of parameters and increasing network depth, the number of stationary points in the loss function decreases, minima become more clustered in parameter space, and the trade-off between the depth and width of minima becomes less severe. Our analytical results are numerically verified through comparison with neural networks trained on a set of classical benchmark datasets. Our model uncovers generic design principles of machine learning models. 
  |  http://link.aps.org/abstract/PRL/v124/p108301  |  
------------------------------------------- 
10.1186/s12859-020-3393-1  |    Background:  Biomedical named-entity recognition (BioNER) is widely modeled with conditional random fields (CRF) by regarding it as a sequence labeling problem. The CRF-based methods yield structured outputs of labels by imposing connectivity between the labels. Recent studies for BioNER have reported state-of-the-art performance by combining deep learning-based models (e.g., bidirectional Long Short-Term Memory) and CRF. The deep learning-based models in the CRF-based methods are dedicated to estimating individual labels, whereas the relationships between connected labels are described as static numbers; thereby, it is not allowed to timely reflect the context in generating the most plausible label-label transitions for a given input sentence. Regardless, correctly segmenting entity mentions in biomedical texts is challenging because the biomedical terms are often descriptive and long compared with general terms. Therefore, limiting the label-label transitions as static numbers is a bottleneck in the performance improvement of BioNER. 
  Results:  We introduce DTranNER, a novel CRF-based framework incorporating a deep learning-based label-label transition model into BioNER. DTranNER uses two separate deep learning-based networks: Unary-Network and Pairwise-Network. The former is to model the input for determining individual labels, and the latter is to explore the context of the input for describing the label-label transitions. We performed experiments on five benchmark BioNER corpora. Compared with current state-of-the-art methods, DTranNER achieves the best F1-score of 84.56% beyond 84.40% on the BioCreative II gene mention (BC2GM) corpus, the best F1-score of 91.99% beyond 91.41% on the BioCreative IV chemical and drug (BC4CHEMD) corpus, the best F1-score of 94.16% beyond 93.44% on the chemical NER, the best F1-score of 87.22% beyond 86.56% on the disease NER of the BioCreative V chemical disease relation (BC5CDR) corpus, and a near-best F1-score of 88.62% on the NCBI-Disease corpus. 
  Conclusions:  Our results indicate that the incorporation of the deep learning-based label-label transition model provides distinctive contextual clues to enhance BioNER over the static transition model. We demonstrate that the proposed framework enables the dynamic transition model to adaptively explore the contextual relations between adjacent labels in a fine-grained way. We expect that our study can be a stepping stone for further prosperity of biomedical literature mining. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3393-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32046638/  |  
------------------------------------------- 
10.3802/jgo.2020.31.e65  |    |  https://www.ejgo.org/DOIx.php?id=10.3802/jgo.2020.31.e65  |  
------------------------------------------- 
10.1186/s12859-019-3190-x  |    Background:  Recently developed methods of protein contact prediction, a crucially important step for protein structure prediction, depend heavily on deep neural networks (DNNs) and multiple sequence alignments (MSAs) of target proteins. Protein sequences are accumulating to an increasing degree such that abundant sequences to construct an MSA of a target protein are readily obtainable. Nevertheless, many cases present different ends of the number of sequences that can be included in an MSA used for contact prediction. The abundant sequences might degrade prediction results, but opportunities remain for a limited number of sequences to construct an MSA. To resolve these persistent issues, we strove to develop a novel framework using DNNs in an end-to-end manner for contact prediction. 
  Results:  We developed neural network models to improve precision of both deep and shallow MSAs. Results show that higher prediction accuracy was achieved by assigning weights to sequences in a deep MSA. Moreover, for shallow MSAs, adding a few sequential features was useful to increase the prediction accuracy of long-range contacts in our model. Based on these models, we expanded our model to a multi-task model to achieve higher accuracy by incorporating predictions of secondary structures and solvent-accessible surface areas. Moreover, we demonstrated that ensemble averaging of our models can raise accuracy. Using past CASP target protein domains, we tested our models and demonstrated that our final model is superior to or equivalent to existing meta-predictors. 
  Conclusions:  The end-to-end learning framework we built can use information derived from either deep or shallow MSAs for contact prediction. Recently, an increasing number of protein sequences have become accessible, including metagenomic sequences, which might degrade contact prediction results. Under such circumstances, our model can provide a means to reduce noise automatically. According to results of tertiary structure prediction based on contacts and secondary structures predicted by our model, more accurate three-dimensional models of a target protein are obtainable than those from existing ECA methods, starting from its MSA. DeepECA is available from https://github.com/tomiilab/DeepECA. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3190-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31918654/  |  
------------------------------------------- 
10.1371/journal.pone.0226912  |   The target spraying effect of spray robots mainly depends on the control performance of the spraying arm during the processes of aiming and tracking. To further improve the robustness of the endpoint control and positioning accuracy of the spray arm, an improved potential field algorithm for the motion planning and control of the spray arm is proposed based on prophase research. The algorithm introduces a velocity potential field, visual field constraints and joint position limit constraints into the traditional artificial potential field method. The velocity potential field is used to ensure that the target state of the spraying arm is at the same velocity as the target crop (relative velocity) to achieve stable target tracking. The visual field constraints and joint position limit constraints are utilized to ensure the efficiency of the visual servo control and the movement of the spray arm. The algorithm can plan a feasible trajectory for the spraying arm in Cartesian space and image space, and use the speed controller to control the spraying arm movement along the trajectory for aiming and tracking. Simulation analysis shows that the algorithm can plan better motion trajectories than the servo controller based on image moments in previous studies. In addition, the experimental results show that the algorithm can effectively improve the robustness of targeting and tracking control for the spray robot. 
  |  http://dx.plos.org/10.1371/journal.pone.0226912  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31923217/  |  
------------------------------------------- 
10.1371/journal.pone.0227788  |   Determining intrinsic number of clusters in a multidimensional dataset is a commonly encountered problem in exploratory data analysis. Unsupervised clustering algorithms often rely on specification of cluster number as an input parameter. However, this is typically not known a priori. Many methods have been proposed to estimate cluster number, including statistical and information-theoretic approaches such as the gap statistic, but these methods are not always reliable when applied to non-normally distributed datasets containing outliers or noise. In this study, I propose a novel method called hierarchical linkage regression, which uses regression to estimate the intrinsic number of clusters in a multidimensional dataset. The method operates on the hypothesis that the organization of data into clusters can be inferred from the hierarchy generated by partitioning the dataset, and therefore does not directly depend on the specific values of the data or their distribution, but on their relative ranking within the partitioned set. Moreover, the technique does not require empirical data to train on, but can use synthetic data generated from random distributions to fit regression coefficients. The trained hierarchical linkage regression model is able to infer cluster number in test datasets of varying complexity and differing distributions, for image, text and numeric data, using the same regression model without retraining. The method performs favourably against other cluster number estimation techniques, and is also robust to parameter changes, as demonstrated by sensitivity analysis. The apparent robustness and generalizability of hierarchical linkage regression make it a promising tool for unsupervised exploratory data analysis and discovery. 
  |  http://dx.plos.org/10.1371/journal.pone.0227788  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31971953/  |  
------------------------------------------- 
10.1186/s12859-020-3341-0  |    Background:  Event extraction from the biomedical literature is one of the most actively researched areas in biomedical text mining and natural language processing. However, most approaches have focused on events within single sentence boundaries, and have thus paid much less attention to events spanning multiple sentences. The Bacteria-Biotope event (BB-event) subtask presented in BioNLP Shared Task 2016 is one such example; a significant amount of relations between bacteria and biotope span more than one sentence, but existing systems have treated them as false negatives because labeled data is not sufficiently large enough to model a complex reasoning process using supervised learning frameworks. 
  Results:  We present an unsupervised method for inferring cross-sentence events by propagating intra-sentence information to adjacent sentences using context trigger expressions that strongly signal the implicit presence of entities of interest. Such expressions can be collected from a large amount of unlabeled plain text based on simple syntactic constraints, helping to overcome the limitation of relying only on a small number of training examples available. The experimental results demonstrate that our unsupervised system extracts cross-sentence events quite well and outperforms all the state-of-the-art supervised systems when combined with existing methods for intra-sentence event extraction. Moreover, our system is also found effective at detecting long-distance intra-sentence events, compared favorably with existing high-dimensional models such as deep neural networks, without any supervised learning techniques. 
  Conclusions:  Our linguistically motivated inference model is shown to be effective at detecting implicit events that have not been covered by previous work, without relying on training data or curated knowledge bases. Moreover, it also helps to boost the performance of existing systems by allowing them to detect additional cross-sentence events. We believe that the proposed model offers an effective way to infer implicit information beyond sentence boundaries, especially when human-annotated data is not sufficient enough to train a robust supervised system. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3341-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31992184/  |  
------------------------------------------- 
10.1016/j.gene.2020.144517  |   Hairiness, which is a phenotypic trait common among land plants, primarily affects the stem, leaf, and floral organs. Plant hairiness is associated with complex functions. For example, glume hairiness in wheat is related to the resistance to biotic and abiotic stresses, and may also influence human health. In the present study, two pairs of near-isogenic lines (NILs) for glume hairiness, which were derived from a cross between a Tibetan semi-wild wheat accession (Triticum aestivum ssp. tibetanum Q1028) and a common wheat cultivar (T. aestivum 'Zhengmai 9023'), underwent a glume transcriptome analysis. We detected 27,935 novel genes, of which 18,027 were annotated. Additionally, 488 and 600 differentially expressed genes (DEGs) were detected in NIL1 and NIL2, respectively, with 37 DEGs detected in both NIL pairs. Moreover, 987 and 1584 single nucleotide polymorphisms (SNPs) were detected in NIL1 and NIL2, respectively, with 39 SNPs detected in both NIL pairs, of which most were located in the Hairy glume (Hg) gene region on chromosome arm 1AS. The annotation of the DEGs with gene ontology terms revealed that genes associated with hairiness in Arabidopsis and rice were similarly enriched. The possible functions of these genes related to glume hairiness were examined. The study results provide useful information for identifying candidate genes and the fine-mapping of Hg in the wheat genome. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0378-1119(20)30186-4  |  
------------------------------------------- 
10.1016/j.jenvman.2019.109867  |   Forests are important dynamic systems which are widely affected by fire worldwide. Due to the complexity and non-linearity of the forest fire problem, employing hybrid evolutionary algorithms is a logical task to achieve a reliable approximation of this environmental threat. Three fuzzy-metaheuristic ensembles, based on adaptive neuro-fuzzy inference systems (ANFIS) incorporated with genetic algorithm (GA), particle swarm optimization (PSO), and differential evolution (DE) evolutionary algorithms are used to produce the forest fire susceptibility map (FFSM) of a fire-prone region in Iran. A sensitivity analysis is also executed to evaluate the effectiveness of the proposed ensembles in terms of time and complexity. The results revealed that all models produce FFSMs with acceptable accuracy. However, the superiority of the GA-ANFIS was shown in both recognizing the pattern (AUROC<sub>train</sub> = 0.912 and Error = 0.1277) and predicting unseen fire events (AUROC<sub>test</sub> = 0.850 and Error = 0.1638). The optimized structures of the proposed GA-ANFIS and PSO-ANFIS ensembles could be good alternatives to traditional forest fire predictive models, and their FFSMs can be promisingly used for future planning and decision making in the proposed area. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0301-4797(19)31585-3  |  
------------------------------------------- 
10.1016/j.wneu.2019.11.105  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1878-8750(19)32945-6  |  
------------------------------------------- 
10.1016/j.amjsurg.2020.02.010  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9610(20)30078-7  |  
------------------------------------------- 
10.1056/NEJMp2000589  |    |  http://www.nejm.org/doi/full/10.1056/NEJMp2000589?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1371/journal.pcbi.1007452  |   We develop a method to learn a bio-inspired motion control policy using data collected from hawkmoths navigating in a virtual forest. A Markov Decision Process (MDP) framework is introduced to model the dynamics of moths and sparse logistic regression is used to learn control policy parameters from the data. The results show that moths do not favor detailed obstacle location information in navigation, but rely heavily on optical flow. Using the policy learned from the moth data as a starting point, we propose an actor-critic learning algorithm to refine policy parameters and obtain a policy that can be used by an autonomous aerial vehicle operating in a cluttered environment. Compared with the moths' policy, the policy we obtain integrates both obstacle location and optical flow. We compare the performance of these two policies in terms of their ability to navigate in artificial forest areas. While the optimized policy can adjust its parameters to outperform the moth's policy in each different terrain, the moth's policy exhibits a high level of robustness across terrains. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007452  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31917816/  |  
------------------------------------------- 
10.1371/journal.pone.0228105  |   The use of natural language data for animal population surveillance represents a valuable opportunity to gather information about potential disease outbreaks, emerging zoonotic diseases, or bioterrorism threats. In this study, we evaluate machine learning methods for conducting syndromic surveillance using free-text veterinary necropsy reports. We train a system to detect if a necropsy report from the Wisconsin Veterinary Diagnostic Laboratory contains evidence of gastrointestinal, respiratory, or urinary pathology. We evaluate the performance of several machine learning algorithms including deep learning with a long short-term memory network. Although no single algorithm was superior, random forest using feature vectors of TF-IDF statistics ranked among the top-performing models with F1 scores of 0.923 (gastrointestinal), 0.960 (respiratory), and 0.888 (urinary). This model was applied to over 33,000 necropsy reports and was used to describe temporal and spatial features of diseases within a 14-year period, exposing epidemiological trends and detecting a potential focus of gastrointestinal disease from a single submitting producer in the fall of 2016. 
  |  http://dx.plos.org/10.1371/journal.pone.0228105  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023271/  |  
------------------------------------------- 
10.6009/jjrt.2020_JSRT_76.1.16  |    Purpose:  X-ray pelvimetry is typically performed for the diagnosis of the cephalopelvic disproportion (CPD). The purpose of this study was to assess the utility of new computed tomography (CT) reconstruction "deep learning based reconstruction (DLR) " in ultra-low dose CT pelvimetry. 
  Method:  CT pelvimetry was performed 320-row CT. All CT images were reconstructed with and without DLR and transferred for workstation to processing martius and guthmann view. Radiologist and obstetrician-gynecologist subjectively ranked overall image quality of each CT image from the best to the worst. Exposure dose of the CT pelvimetry used a following calculated value, displayed CT dose index (CTDI) <sub>vol</sub> multiplied by measured value using the thimble chamber and pelvic phantom, and of the X-ray pelvimetry used Japan-Diagnositic Refernce Levels 2015 as a reference, were compared. 
  Result:  3D images obtained from CT pelvimetry with DLR showed accurate biparietal diameter and obstetric conjugate as compared to without DLR. Radiation dose of CT pelvimetry is 0.39 mGy, of X-ray pelvimetry is 1.18 mGy, respectively. Conculusion: Although the visualizing high contrast object, such as bone morphology, is likely to reduce exposure dose in CT examination generally, DLR enable to further dose reduction to keep image quality. 3D image processing from CT pelvimetry solves the problem of expansion rate in X-P pelvimetry and provide accurate measurements. Furthermore, CT pelvimetry can undergo more comfortable position for Pregnant Woman in Labor. 
  |  https://dx.doi.org/10.6009/jjrt.2020_JSRT_76.1.16  |  
------------------------------------------- 
10.1038/s41587-020-0418-2  |    |  https://dx.doi.org/10.1038/s41587-020-0418-2  |  
------------------------------------------- 
10.1371/journal.pone.0227677  |   Prosthetic vision is being applied to partially recover the retinal stimulation of visually impaired people. However, the phosphenic images produced by the implants have very limited information bandwidth due to the poor resolution and lack of color or contrast. The ability of object recognition and scene understanding in real environments is severely restricted for prosthetic users. Computer vision can play a key role to overcome the limitations and to optimize the visual information in the prosthetic vision, improving the amount of information that is presented. We present a new approach to build a schematic representation of indoor environments for simulated phosphene images. The proposed method combines a variety of convolutional neural networks for extracting and conveying relevant information about the scene such as structural informative edges of the environment and silhouettes of segmented objects. Experiments were conducted with normal sighted subjects with a Simulated Prosthetic Vision system. The results show good accuracy for object recognition and room identification tasks for indoor scenes using the proposed approach, compared to other image processing methods. 
  |  http://dx.plos.org/10.1371/journal.pone.0227677  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31995568/  |  
------------------------------------------- 
10.1016/S0140-6736(20)30318-4  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(20)30318-4  |  
------------------------------------------- 
10.1016/S0140-6736(19)32975-7  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)32975-7  |  
------------------------------------------- 
10.1016/j.aap.2019.105429  |   Recently, technologies for predicting traffic conflicts in real-time have been gaining momentum due to their proactive nature of application and the growing implementation of ADAS technology in intelligent vehicles. In ADAS, machine learning classifiers are utilised to predict potential traffic conflicts by analysing data from in-vehicle sensors. In most cases, a condition is classified as a traffic conflict when a safety surrogate (e.g. time-to-collision, TTC) crosses a pre-defined threshold. This approach, however, largely ignores other factors that influence traffic conflicts such as speed variance, traffic density, speed and weather conditions. Considering all these factors in detecting traffic conflicts is rather complex as it requires an integration and mining of heterodox data, the unavailability of traffic conflicts and conflict prediction models capable of extracting meaningful and accurate information in a timely manner. In addition, the model has to effectively handle large imbalanced data. To overcome these limitations, this paper presents a centralised digital architecture and employs a Deep Learning methodology to predict traffic conflicts. Highly disaggregated traffic data and in-vehicle sensors data from an instrumented vehicle are collected from a section of the UK M1 motorway to build the model. Traffic conflicts are identified by a Regional-Convolution Neural Network (R-CNN) model which detects lane markings and tracks vehicles from images captured by a single front-facing camera. This data is then integrated with traffic variables and calculated safety surrogate measures (SSMs) via a centralised digital architecture to develop a series of Deep Neural Network (DNN) models to predict these traffic conflicts. The results indicate that TTC, as expected, varies by speed, weather and traffic density and the best DNN model provides an accuracy of 94% making it reliable to employ in ADAS technology as proactive safety management strategies. Furthermore, by exchanging this traffic conflict awareness data, connected vehicles (CVs) can mitigate the risk of traffic collisions. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0001-4575(19)30973-X  |  
------------------------------------------- 
10.1038/d41586-020-00819-6  |    |  https://doi.org/10.1038/d41586-020-00819-6  |  
------------------------------------------- 
10.1038/s41591-019-0717-7  |    |  http://dx.doi.org/10.1038/s41591-019-0717-7  |  
------------------------------------------- 
10.1038/d41586-019-03951-0  |    |  https://doi.org/10.1038/d41586-019-03951-0  |  
------------------------------------------- 
10.3171/2020.1.FOCUS2069  |    |  https://thejns.org/doi/10.3171/2020.1.FOCUS2069  |  
------------------------------------------- 
10.1007/s00108-020-00762-8  |   Against the background of increasing antimicrobial resistance, antibiotic stewardship (ABS) is an important measure to counteract the spread of resistant pathogens and multidrug resistance. For Germany and Austria, a comprehensive S3 guideline is available, which was last updated in 2018. The control of antibiotic or anti-infective use in hospitals should be guided by specialized ABS teams. At the hospital level, ABS also includes a structured ongoing analysis of local antibiotic use and resistance data. Recommendations for locally adapted therapy regimens should be derived and implemented from this data analysis. ABS consists of regular ward rounds ("ABS visits"), during which members of the ABS team review the indication, dosage, route of administration and duration of antimicrobial therapy at the bedside. Here, the key challenge is to save antibiotics without compromising the individual patient. Digitalization and artificial intelligence offer new options for ABS, while the adaption of inpatient concepts to outpatient care is also important. 
  |  https://dx.doi.org/10.1007/s00108-020-00762-8  |  
------------------------------------------- 
10.1007/s00117-019-00624-x  |    Methodical issue:  Machine learning (ML) algorithms have an increasingly relevant role in radiology tackling tasks such as the automatic detection and segmentation of diagnosis-relevant markers, the quantification of progression and response, and their prediction in individual patients. 
  Standard radiological methods:  ML algorithms are relevant for all image acquisition techniques from computed tomography (CT) and magnetic resonance imaging (MRI) to ultrasound. However, different modalities result in different challenges with respect to standardization and variability. 
  Methodical innovations:  ML algorithms are increasingly able to analyze longitudinal data for the training of prediction models. This is relevant since it enables the use of comprehensive information for predicting individual progression and response, and the associated support of treatment decisions by ML models. 
  Performance:  The quality of detection and segmentation algorithms of lesions has reached an acceptable level in several areas. The accuracy of prediction models is still increasing, but is dependent on the availability of representative training data. 
  Achievements:  The development of ML algorithms in radiology is progressing although many solutions are still at a validation stage. It is accompanied by a parallel and increasingly interlinked development of basic methods and techniques which will gradually be put into practice in radiology. 
  Practical considerations:  Two factors will impact the relevance of ML in radiological practice: the thorough validation of algorithms and solutions, and the creation of representative diverse data for the training and validation in a realistic context. 
  |  https://dx.doi.org/10.1007/s00117-019-00624-x  |  
------------------------------------------- 
10.1038/d41586-020-00002-x  |    |  https://doi.org/10.1038/d41586-020-00002-x  |  
------------------------------------------- 
10.1097/RCT.0000000000000928  |   Deep learning (DL), part of a broader family of machine learning methods, is based on learning data representations rather than task-specific algorithms. Deep learning can be used to improve the image quality of clinical scans with image noise reduction. We review the ability of DL to reduce the image noise, present the advantages and disadvantages of computed tomography image reconstruction, and examine the potential value of new DL-based computed tomography image reconstruction. 
  |  http://dx.doi.org/10.1097/RCT.0000000000000928  |  
------------------------------------------- 
10.1038/s41596-019-0230-y  |   Exposure of lung tissues to cigarette smoke is a major cause of human disease and death worldwide. Unfortunately, adequate model systems that can reliably recapitulate disease biogenesis in vitro, including exposure of the human lung airway to fresh whole cigarette smoke (WCS) under physiological breathing airflow, are lacking. This protocol extension builds upon, and can be used with, our earlier protocol for microfabrication of human organs-on-chips. Here, we describe the engineering, assembly and operation of a microfluidically coupled, multi-compartment platform that bidirectionally 'breathes' WCS through microchannels of a human lung small airway microfluidic culture device, mimicking how lung cells may experience smoke in vivo. Several WCS-exposure systems have been developed, but they introduce smoke directly from above the cell cultures, rather than tangentially as naturally occurs in the lung due to lateral airflow. We detail the development of an organ chip-compatible microrespirator and a smoke machine to simulate breathing behavior and smoking topography parameters such as puff time, inter-puff interval and puffs per cigarette. Detailed design files, assembly instructions and control software are provided. This novel platform can be fabricated and assembled in days and can be used repeatedly. Moderate to advanced engineering and programming skills are required to successfully implement this protocol. When coupled with the small airway chip, this protocol can enable prediction of patient-specific biological responses in a matched-comparative manner. We also demonstrate how to adapt the protocol to expose living ciliated airway epithelial cells to smoke generated by electronic cigarettes (e-cigarettes) on-chip. 
  |  http://dx.doi.org/10.1038/s41596-019-0230-y  |  
------------------------------------------- 
10.1038/s41551-019-0504-2  |    |  None  |  
------------------------------------------- 
10.1016/S0140-6736(20)30061-1  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(20)30061-1  |  
------------------------------------------- 
10.1038/s41591-019-0726-6  |    |  http://dx.doi.org/10.1038/s41591-019-0726-6  |  
------------------------------------------- 
10.1371/journal.pone.0225695  |   Individuals with serious mental illness experience changes in their clinical states over time that are difficult to assess and that result in increased disease burden and care utilization. It is not known if features derived from speech can serve as a transdiagnostic marker of these clinical states. This study evaluates the feasibility of collecting speech samples from people with serious mental illness and explores the potential utility for tracking changes in clinical state over time. Patients (n = 47) were recruited from a community-based mental health clinic with diagnoses of bipolar disorder, major depressive disorder, schizophrenia or schizoaffective disorder. Patients used an interactive voice response system for at least 4 months to provide speech samples. Clinic providers (n = 13) reviewed responses and provided global assessment ratings. We computed features of speech and used machine learning to create models of outcome measures trained using either population data or an individual's own data over time. The system was feasible to use, recording 1101 phone calls and 117 hours of speech. Most (92%) of the patients agreed that it was easy to use. The individually-trained models demonstrated the highest correlation with provider ratings (rho = 0.78, p&lt;0.001). Population-level models demonstrated statistically significant correlations with provider global assessment ratings (rho = 0.44, p&lt;0.001), future provider ratings (rho = 0.33, p&lt;0.05), BASIS-24 summary score, depression sub score, and self-harm sub score (rho = 0.25,0.25, and 0.28 respectively; p&lt;0.05), and the SF-12 mental health sub score (rho = 0.25, p&lt;0.05), but not with other BASIS-24 or SF-12 sub scores. This study brings together longitudinal collection of objective behavioral markers along with a transdiagnostic, personalized approach for tracking of mental health clinical state in a community-based clinical setting. 
  |  http://dx.plos.org/10.1371/journal.pone.0225695  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31940347/  |  
------------------------------------------- 
10.1097/DCR.0000000000001519  |    Background:  High-resolution MRI is regarded as the best method to evaluate whether there is an involved circumferential resection margin in rectal cancer. 
  Objective:  We explored the application of the faster region-based convolutional neural network to identify positive circumferential resection margins in high-resolution MRI images. 
  Design:  This was a retrospective study. 
  Settings:  The study conducted at a single surgical unit of a public university hospital. 
  Patients:  We studied 240 patients with rectal cancer in the Affiliated Hospital of Qingdao University from July 2016 to August 2018, who were determined to have a positive circumferential resection margin and who had received a high-resolution MRI. All posttreatment cases were excluded from this study. 
  Main outcome measures:  The faster region-based convolutional neural network was trained by 12,258 transverse relaxation-weighted (T2-weighted imaging) images of pelvic high-resolution MRI to build an artificial intelligence platform and complete clinical tests. In this network, the proportion of positive and negative circumferential resection margin images was 1:2. In accordance with the test results of the validation group, the metrics of the receiver operating characteristic curves and the area under the curve were applied to compare the diagnostic results of the artificial intelligence platform with those of senior radiology experts. 
  Results:  In this artificial intelligence platform, the accuracy, sensitivity, and specificity of the circumferential resection margin status as determined were 0.932, 0.838, and 0.956. The area under the receiver operating characteristic curves was 0.953. The time required to automatically recognize an image was 0.2 seconds. 
  Limitations:  This is a single-center retrospective study with limited data volume and a highly selected patient cohort. 
  Conclusions:  In high-resolution MRI images of rectal cancer before treatment, the application of faster region-based convolutional neural network to segment the positive circumferential resection margin has high accuracy and feasibility. See Video Abstract at http://links.lww.com/DCR/B88. EVALUACIÓN DEL MARGEN DE RESECCIÓN CIRCUNFERENCIAL DEL CÁNCER RECTAL MEDIANTE EL USO DE UNA RED NEURONAL CONVOLUCIONAL MÁS RÁPIDA BASADA EN UNA REGIÓN EN IMÁGENES DE RESONANCIA MAGNÉTICA DE ALTA RESOLUCIÓN: La resonancia magnética de alta resolución se considera el mejor método para evaluar si existe un margen de resección circunferencial involucrado en el cáncer de recto.Se exploró la aplicación de la red neuronal convolucional más rápida basada en una región para identificar márgenes de resección circunferencial positivos en imágenes de resonancia magnética de alta resolución.Este fue un estudio retrospectivo realizado en una única unidad quirúrgica de un hospital universitario público.Estudiamos 240 pacientes con cáncer rectal en el Hospital Afiliado de la Universidad de Qingdao desde el 2 de julio de 2006 hasta el 2 de agosto de 2008, a los que se determinó que tenían un margen de resección circunferencial positivo y que habían recibido una resonancia magnética de alta resolución. Todos los casos posteriores al tratamiento fueron excluidos de este estudio.La red neuronal convolucional más rápida basada en una región recibió capacitación de 12,258 imágenes de RM pélvica de alta resolución con relajación transversal para construir una plataforma de inteligencia artificial y completar pruebas clínicas. En esta red, la proporción de imágenes con margen de resección circunferencial positivo y negativo fue 1: 2. De acuerdo con los resultados de las pruebas del grupo de validación, se aplicaron las métricas de las curvas de las características operativas del receptor y del área bajo la curva para comparar los resultados de diagnóstico de la plataforma de inteligencia artificial con los de expertos de radiología de alto nivel.En esta plataforma de inteligencia artificial, la precisión, sensibilidad y especificidad del estado del margen de resección circunferencial según lo determinado fueron 0.932, 0.838 y 0.956, respectivamente. El área bajo las curvas características de operación del receptor fue de 0.953. El tiempo requerido para reconocer automáticamente una imagen fue de 0.2 segundos.Este es un estudio retrospectivo de centro único con volumen de datos limitado y una cohorte de pacientes altamente seleccionada.En las imágenes de resonancia magnética de alta resolución de cáncer rectal antes del tratamiento, la aplicación de la red neuronal convolucional más rápida basada en una región, para segmentar el margen de resección circunferencial positivo tiene una alta precisión y factibilidad. Consulte Video Resumen en http://links.lww.com/DCR/B88. 
  |  http://dx.doi.org/10.1097/DCR.0000000000001519  |  
------------------------------------------- 
10.7518/hxkq.2020.01.016  |   With the development of industrial robot technology, robotics has entered the medical field, and the research and development of new robots for many medical applications have become a significant research direction in global robotics. Robots are widely used in various aspects of dentistry, such as prosthodontics, orthodontics, implants, endodontics, and oral surgery. This article mainly introduces the application of robots in stomatology from the above five aspects. 
 随着工业机器人技术的发展，机器人已进入医学领域，为众多医疗应用开发新型机器人，已成为全球机器人领域重要的研究方向。在口腔医学中，机器人已经应用于修复、正畸、种植、牙体牙髓以及口腔外科等领域中。本文主要从以上5个方面介绍了机器人在口腔医学中的应用进展。. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32037773/  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_6  |   This chapter proposes a method to detect metastatic liver cancer from X-ray CT images using a convolutional neural network (CNN). The proposed method generates various lesion images by the combination of three kinds of generation methods: (1) synthesis using Poisson Blending, (2) generation based on CT value distributions, and (3) generation using deep convolutional generative adversarial networks (DCGANs). The proposed method constructs two kinds of detectors by using synthetic (fake) lesion images generated by the methods as well as real ones. One of the detectors is a 2D CNN for detecting candidate regions in a CT image, and the other is a 3D CNN for validating the candidate regions. Experimental results showed that the proposed method gave 0.30 improvement from 0.65 to 0.95 in terms of the detection rate, and 0.70 improvement from 0.90 to 0.20 in terms of the number of false detections per case. From the results, we confirmed the effectiveness of the proposed method. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_6  |  
------------------------------------------- 
10.1007/s00115-019-00857-0  |   Imaging methods have become the main approach for identifying dysfunctional neuronal networks in schizophrenia. This review article presents recent results of disorders of neuronal networks at structural and functional levels and summarizes the current developments. Large multicenter analyses have further established patterns of regional brain alterations, while novel methods in magnetic resonance (MR) morphometry have contributed to differentiating early from delayed brain structural changes. The use of machine learning approaches has not only enabled the establishment of classification models using biological data for future differential diagnostic use, it has also facilitated multivariate models for outcome prediction following therapeutic interventions. Novel methods, such as BrainAGE, a surrogate marker of accelerated brain aging processes, have added to longitudinal studies to gain insights into the brain structural dynamics from early brain developmental alterations to progressive structural brain changes after disease onset. 
  |  https://dx.doi.org/10.1007/s00115-019-00857-0  |  
------------------------------------------- 
10.1371/journal.pone.0228065  |   Understanding the distribution of life's variety has driven naturalists and scientists for centuries, yet this has been constrained both by the available data and the models needed for their analysis. Here we compiled data for over 67,000 marine and terrestrial species and used artificial neural networks to model species richness with the state and variability of climate, productivity, and multiple other environmental variables. We find terrestrial diversity is better predicted by the available environmental drivers than is marine diversity, and that marine diversity can be predicted with a smaller set of variables. Ecological mechanisms such as geographic isolation and structural complexity appear to explain model residuals and also identify regions and processes that deserve further attention at the global scale. Improving estimates of the relationships between the patterns of global biodiversity, and the environmental mechanisms that support them, should help in efforts to mitigate the impacts of climate change and provide guidance for adapting to life in the Anthropocene. 
  |  http://dx.plos.org/10.1371/journal.pone.0228065  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32023269/  |  
------------------------------------------- 
10.1590/S1677-5538.IBJU.2020.02.08  |    |  None  |  
------------------------------------------- 
10.1371/journal.pone.0228579  |   Humans are entertained and emotionally captivated by a good story. Artworks, such as operas, theatre plays, movies, TV series, cartoons, etc., contain implicit stories, which are conveyed visually (e.g., through scenes) and audially (e.g., via music and speech). Story theorists have explored the structure of various artworks and identified forms and paradigms that are common to most well-written stories. Further, typical story structures have been formalized in different ways and used by professional screenwriters as guidelines. Currently, computers cannot yet identify such a latent narrative structure of a movie story. Therefore, in this work, we raise the novel challenge of understanding and formulating the movie story structure and introduce the first ever story-based labeled dataset-the Flintstones Scene Dataset (FSD). The dataset consists of 1, 569 scenes taken from a manual annotation of 60 episodes of a famous cartoon series, The Flintstones, by 105 distinct annotators. The various labels assigned to each scene by different annotators are summarized by a probability vector over 10 possible story elements representing the function of each scene in the advancement of the story, such as the Climax of Act One or the Midpoint. These elements are learned from guidelines for professional script-writing. The annotated dataset is used to investigate the effectiveness of various story-related features and multi-label classification algorithms for the task of predicting the probability distribution of scene labels. We use cosine similarity and KL divergence to measure the quality of predicted distributions. The best approaches demonstrated 0.81 average similarity and 0.67 KL divergence between the predicted label vectors and the ground truth vectors based on the manual annotations. These results demonstrate the ability of machine learning approaches to detect the narrative structure in movies, which could lead to the development of story-related video analytics tools, such as automatic video summarization and recommendation systems. 
  |  http://dx.plos.org/10.1371/journal.pone.0228579  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32045438/  |  
------------------------------------------- 
10.12116/j.issn.1004-5619.2020.01.018  |   Bone age assessment has always been one of the key issues and difficulties in forensic science. With the gradual development of machine learning in many industries, it has been widely introduced to imageology, genomics, oncology, pathology, surgery and other medical research fields in recent years. The reason why the above research fields can be closely combined with machine learning, is because the research subjects of the above branches of medicine belong to the computer vision category. Machine learning provides unique advantages for computer vision research and has made breakthroughs in medical image recognition. Based on the advantages of machine learning in image recognition, it was combined with bone age assessment research, in order to construct a recognition model suitable for forensic skeletal images. This paper reviews the research progress in bone age assessment made by scholars at home and abroad using machine learning technology in recent years. 
  题目:  机器学习在骨龄评估中的研究进展及展望. 
  摘要:  骨龄评估一直是法医学研究领域的重点及难点问题之一。近年来，随着机器学习在诸多行业内的逐渐兴起，其已被广泛引入影像学、基因组学、肿瘤学、病理学以及外科学等医学研究领域中。上述研究领域之所以能与机器学习紧密结合，主要是由于上述医学分支学科中的研究对象属于计算机视觉范畴，而机器学习对于计算机视觉研究有着得天独厚的优势，并在医学图像识别中取得突破性进展。基于机器学习在图像识别中的优势，将其与骨龄评估研究有机结合，旨在为构建适用于法医学骨骼影像图片的识别模型。基于此，本文将对近年来国内外学者运用机器学习技术在骨龄评估中的研究进展进行综述。. 
  关键词:  法医人类学；年龄测定，骨骼；机器学习；综述. 
  |  https://doi.org/10.12116/j.issn.1004-5619.2020.01.018  |  
------------------------------------------- 
10.1016/j.amjmed.2019.07.009  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(19)30587-X  |  
------------------------------------------- 
10.1016/j.ophtha.2019.07.014  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0161-6420(19)31795-6  |  
------------------------------------------- 
PMID:32048497  |   In recent years, minimally invasive surgeries have been undergoing a revolution with the development of robotic assisted surgery. Due to the anatomical region in which many operations are carried out, in small spaces through natural orifices in the body such as the mouth, the nose or the ear, as well as the esthetic discomfort caused by external scars in the head and neck, the use of robotic assisted surgery in the field of head and neck surgery is gaining momentum and is being implemented more and more in Israel and around the globe. Most of the operations currently performed in otolaryngology are carried out through the oral cavity or through distant and camouflaged skin cuts. In this article we will review the new applications and technologies in the field of surgical robotics in otolaryngology - head and neck surgeries, as well as summarize the experience of the Department of Otorhinolaryngology and Head and Neck Surgery at the Rabin Medical Center in robot assisted surgeries. 
  |  None  |  
------------------------------------------- 
10.1016/j.soin.2020.01.009  |   The e-track, a real challenge for caregivers. We have known for more than twenty years that in order to "defragment" healthcare plan, break down the barriers between the city and the hospital and improve coordination between healthcare professionals, a better flow and sharing of information is essential. We are now talking about a digital shift in the light of accelerating technical progress with the rapid adoption of smartphones and mobile applications, and now the themes of big data and artificial intelligence, which are very present in the media. 
  |  https://doi.org/10.1016/j.soin.2020.01.009  |  
------------------------------------------- 
10.1016/j.biortech.2020.122926  |   Vermicomposting is one of the best technologies for nutrient recovery from solid waste. This study aims to assess the efficiency of Artificial Neural Network (ANN) and Multiple Linear Regression (MLR) models in predicting nutrient recovery from solid waste under different vermicompost treatments. Seven chemical and biological indices were studied as input variables to predict total nitrogen (TN) and total phosphorus (TP) recovery. The developed ANN and MLR models were compared by statistical analysis including R-squared (R<sup>2</sup>), Adjusted-R<sup>2</sup>, Root Mean Square Error and Absolute Average Deviation. The results showed that vermicomposting increased TN and TP proportions in final products by 1.5 and 16 times. The ANN models provided better prediction for TN and TP with R<sup>2</sup> of 0.9983 and 0.9991 respectively, compared with MLR models with R<sup>2</sup> of 0.834 and 0.729. TN and C/N ratio were key factors for TP and TN prediction by ANN with percentages of 17.76 and 18.33. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-8524(20)30195-4  |  
------------------------------------------- 
10.1016/j.soin.2020.01.008  |   The shift in our healthcare system towards organisational models based on patient care management is one of the structural changes that have taken place in recent years. Digital technology represents a major lever to support this transformation, which has high stakes for improving the quality and efficiency of patient care. Positive regulation of the associated ethical issues can be achieved through the principle of a human guarantee of digital technology and artificial intelligence in health care, which is currently being recognised in the framework of the revision of the bioethics law. 
  |  https://doi.org/10.1016/j.soin.2020.01.008  |  
------------------------------------------- 
10.1038/s41585-020-0301-4  |    |  http://dx.doi.org/10.1038/s41585-020-0301-4  |  
------------------------------------------- 
10.1186/s12859-019-3332-1  |    Background:  Cell nuclei segmentation is a fundamental task in microscopy image analysis, based on which multiple biological related analysis can be performed. Although deep learning (DL) based techniques have achieved state-of-the-art performances in image segmentation tasks, these methods are usually complex and require support of powerful computing resources. In addition, it is impractical to allocate advanced computing resources to each dark- or bright-field microscopy, which is widely employed in vast clinical institutions, considering the cost of medical exams. Thus, it is essential to develop accurate DL based segmentation algorithms working with resources-constraint computing. 
  Results:  An enhanced, light-weighted U-Net (called U-Net+) with modified encoded branch is proposed to potentially work with low-resources computing. Through strictly controlled experiments, the average IOU and precision of U-Net+ predictions are confirmed to outperform other prevalent competing methods with 1.0% to 3.0% gain on the first stage test set of 2018 Kaggle Data Science Bowl cell nuclei segmentation contest with shorter inference time. 
  Conclusions:  Our results preliminarily demonstrate the potential of proposed U-Net+ in correctly spotting microscopy cell nuclei with resources-constraint computing. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3332-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31914944/  |  
------------------------------------------- 
10.1097/NNE.0000000000000685  |    |  http://dx.doi.org/10.1097/NNE.0000000000000685  |  
------------------------------------------- 
10.1038/s41571-020-0343-9  |    |  http://dx.doi.org/10.1038/s41571-020-0343-9  |  
------------------------------------------- 
PMID:31995325  |    |  None  |  
------------------------------------------- 
10.5014/ajot.2020.031849  |    Importance:  Finding strategies to enhance imitation skills in people with autism spectrum disorder (ASD) is of major clinical relevance. 
  Objective:  To evaluate whether contact with dogs may be a useful approach to elicit spontaneous imitation in people with ASD. 
  Design:  Participants completed a spontaneous imitation task under three experimental conditions: after a free-play interaction with a live dog, after a free-play interaction with a robotic dog, and after a waiting period that involved no stimuli. 
  Participants:  Ten children and 15 adults diagnosed with severe ASD. 
  Outcomes and measures:  Imitation ratio, imitation accuracy, and indicators of social motivation. 
  Results:  Children appeared more motivated and engaged more frequently in spontaneous imitation in the live dog condition than in the other conditions. No differences between conditions were found for adults for imitation or social motivation. However, correlations suggested a possible trend for adults in time spent engaging with the live dog before testing and in increased imitation frequency. 
  Conclusions and relevance:  The results are preliminary and do not indicate the utility of integrating (live) dogs into interventions aimed at promoting social motivation and enhancing imitation skills in people with ASD. However, they suggest that doing so holds promise. Larger scale studies are now needed. 
  What this article adds:  This research calls for occupational therapy practitioners' attention to the potential benefits that may derive from using dogs to promote spontaneous imitation, and increase imitation performance, in people with ASD, particularly children. 
  |  http://ajot.aota.org/article.aspx?doi=10.5014/ajot.2020.031849  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32078518/  |  
------------------------------------------- 
10.3760/cma.j.issn.0529-5815.2020.01.005  |   Digital intelligent hepatobiliary surgery has evolved over decades.It has experienced an evolution course from digital virtual human technology to the establishment of a quality-controlled and homogeneous three-dimensional visualization system for precision diagnosis and treatment of diseases, from three-dimensional visualization to the clinical transformation of digital intelligent technology and changes in the diagnosis and treatment model, from empirical diagnosis of diseases to the application of deep learning for the intelligent diagnosis and treatment of diseases, from empirical surgery to real-time multi-modal image guidance during surgery, and from the morphological diagnosis of tumors to accurate diagnosis from molecular imaging.During the whole process, only through continuous innovation in research, theory and technology can the "life" of digital intelligent surgery be endowed with new vitality.In the future, the definition of tumor boundary from the molecular and cellular levels and the early diagnosis and treatment of liver tumor through the functional visualization of key molecules will have significant clinical value for changing the prognosis of liver cancer.In addition, in order to realize intelligent navigation for hepatectomy and break through the technical bottleneck, it is of great clinical significance to develop an intelligent robot real-time navigation hepatectomy system with automatic navigation technology, machine learning intelligent planning technology and multimodal image fusion technology.This provides unprecedented opportunities and challenges for the development of digital intelligent hepatobiliary surgery. 
 数字智能化肝胆外科的发展经历了十几年的演进过程，从数字虚拟人技术到质控化、同质化三维可视化精准诊疗体系的建立；从三维可视化到数字智能化技术临床转化及诊疗模式的转换；实现了疾病的经验性诊断到深度学习智能化诊断与治疗和经验性手术到多模态影像实时手术导航的技术创新；从肿瘤的形态学诊断深入到分子影像学精准诊断的研究。在这个过程中只有不断地进行研究创新、理论创新和技术创新才能给数字智能化外科赋予新的生命力。未来，从关键分子功能可视化实现肝癌分子、细胞层面边界界定和早诊早治，对改变肝癌患者肝切除术后的预后具有重大的临床价值。此外，为了实现智能化肝切除手术导航，突破技术瓶颈，研发具有自动导航技术、机器学习智能规划技术和多模态影像融合技术的智能化机器人实时导航肝切除手术系统具有重要的临床意义，给数字智能化肝胆外科的发展带来了新的机遇和挑战。. 
  |  http://journal.yiigle.com/LinkIn.do?linkin_type=pubmed&DOI=10.3760/cma.j.issn.0529-5815.2020.01.005  |  
------------------------------------------- 
10.1016/j.foodchem.2019.126138  |   The fermentation products of edible fungi are rich in anthraquinones and have a variety of activities, including the antioxidant activity. Because of the large number of combinations, it is very difficult to obtain the optimal multi-strains co-fermentation to improve the yield of anthraquinone. In the present study, an intelligent model based on artificial neural networks (ANNs) using backpropagation (BP) and radial basis function (RBF) algorithms was developed and validated to predict the anthraquinone contents in 136 two fungi and 680 three fungi co-fermented products. After experimental validation of the anthraquinone contents, the mean absolute error and the mean bias error of the results from RBF ANN were lower than those from BP ANN. The results indicated that the anthraquinone contents in A. bisporus, C. comatus and H. erinaceus co-fermentation product was the highest (2.11%). Furthermore, this co-fermentation product showed strong antioxidant activity. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0308-8146(19)32290-3  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_9  |   This chapter focuses on modern deep learning techniques that are proposed for automatically recognizing and segmenting multiple organ regions on three-dimensional (3D) computed tomography (CT) images. CT images are widely used to visualize 3D anatomical structures composed of multiple organ regions inside the human body in clinical medicine. Automatic recognition and segmentation of multiple organs on CT images is a fundamental processing step of computer-aided diagnosis, surgery, and radiation therapy systems, which aim to achieve precision and personalized medicines. In this chapter, we introduce our recent works on addressing the issue of multiple organ segmentation on 3D CT images by using deep learning, a completely novel approach, instead of conventional segmentation methods originated from traditional digital image processing techniques. We evaluated and compared the segmentation performances of two different deep learning approaches based on 2D- and 3D deep convolutional neural networks (CNNs) without and with a pre-processing step. A conventional method based on a probabilistic atlas algorithm, which presented the best performance within the conventional approaches, was also adopted as a baseline for performance comparison. A dataset containing 240 CT scans of different portions of human bodies was used for training the CNNs and validating the segmentation performance of the learning results. A maximum number of 17 types of organ regions in each CT scan were segmented automatically and validated with the human annotations by using ratio of intersection over union (IoU) as the criterion. Our experimental results showed that the IoUs of the segmentation results had a mean value of 79% and 67% by averaging 17 types of organs that were segmented by the proposed 3D and 2D deep CNNs, respectively. All results using the deep learning approaches showed better accuracy and robustness than the conventional segmentation method that used the probabilistic atlas algorithm. The effectiveness and usefulness of deep learning approaches were demonstrated for multiple organ segmentation on 3D CT images. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_9  |  
------------------------------------------- 
10.17116/stomat20209901195  |   In the present study, the role of robot-assistance in dentistry and the main ways of its further development are analyzed. The basic structures of mechatronic devices and the features of their application in dentistry were reviewed. A retro-prospective analysis of the structures of robotic and robot-assisted systems used in dental practice was carried out. The development of robot-assisted and robotic systems should include the improvement of methods of intraoperative imaging, diagnostic tools, the improvement of surgical instruments, as well as the emergence of new robotic developments. 
 В представленном исследовании анализируются роль робот-ассистенции в стоматологии и основные пути ее дальнейшего развития. Рассмотрены основные конструкции механотронных устройств и особенности их применения в стоматологии. Проведен ретропроспективный анализ конструкций роботизированных и робот-ассистированных систем, применяемых в стоматологической практике. Развитие робот-ассистированных и роботизированных систем должно включать усовершенствование методов интраоперационной визуализации, средств диагностики, совершенствование хирургического инструментария, а также появление новых роботизированных разработок. 
  |  None  |  
------------------------------------------- 
10.1590/S1677-5538.IBJU.2020.02.11  |    |  None  |  
------------------------------------------- 
10.7507/1001-5515.201906053  |   Human motion recognition (HAR) is the technological base of intelligent medical treatment, sports training, video monitoring and many other fields, and it has been widely concerned by all walks of life. This paper summarized the progress and significance of HAR research, which includes two processes: action capture and action classification based on deep learning. Firstly, the paper introduced in detail three mainstream methods of action capture: video-based, depth camera-based and inertial sensor-based. The commonly used action data sets were also listed. Secondly, the realization of HAR based on deep learning was described in two aspects, including automatic feature extraction and multi-modal feature fusion. The realization of training monitoring and simulative training with HAR in orthopedic rehabilitation training was also introduced. Finally, it discussed precise motion capture and multi-modal feature fusion of HAR, as well as the key points and difficulties of HAR application in orthopedic rehabilitation training. This article summarized the above contents to quickly guide researchers to understand the current status of HAR research and its application in orthopedic rehabilitation training. 
 人体动作识别（HAR）是智慧医疗、体育训练、视频监控等众多领域的技术基础，受到社会各界的广泛关注。本文概述了 HAR 的研究进展及意义，将其归纳为动作捕捉和基于深度学习的动作分类两个过程。首先，详细介绍了基于视频、基于深度相机以及基于惯性传感器的三种主流动作捕捉方式，列举了常用的动作数据集。其次，从特征自动提取及多模态特征融合两方面来描述基于深度学习的 HAR，并介绍了正骨康复训练中如何通过 HAR 实现监督锻炼和模拟训练。最后,讨论了 HAR 的精准动作捕捉、多模态特征融合方法，以及在正骨康复训练应用中的重点和难点。本文通过总结以上内容旨在快速地引导研究人员了解 HAR 的研究现状及其在正骨康复训练中的应用。. 
  |  None  |  
------------------------------------------- 
10.1038/s41591-019-0725-7  |    |  http://dx.doi.org/10.1038/s41591-019-0725-7  |  
------------------------------------------- 
10.1038/d41586-020-00505-7  |    |  https://doi.org/10.1038/d41586-020-00505-7  |  
------------------------------------------- 
10.1038/s41467-019-14234-7  |   Bioinspired electronics are rapidly promoting advances in artificial intelligence. Emerging AI applications, e.g., autopilot and robotics, increasingly spur the development of power devices with new forms. Here, we present a strain-controlled power device that can directly modulate the output power responses to external strain at a rapid speed, as inspired by human reflex. By using the cantilever-structured AlGaN/AlN/GaN-based high electron mobility transistor, the device can control significant output power modulation (2.30-2.72 × 10<sup>3</sup> W cm<sup>-2</sup>) with weak mechanical stimuli (0-16 mN) at a gate bias of 1 V. We further demonstrate the acceleration-feedback-controlled power application, and prove that the output power can be effectively adjusted at real-time in response to acceleration changes, i.e., ▵P of 72.78-132.89 W cm<sup>-2</sup> at an acceleration of 1-5 G at a supply voltage of 15 V. Looking forward, the device will have great significance in a wide range of AI applications, including autopilot, robotics, and human-machine interfaces. 
  |  http://dx.doi.org/10.1038/s41467-019-14234-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31949147/  |  
------------------------------------------- 
10.17116/sudmed2020630119  |   The possibilities of Data Mining artificial intelligence technology, in particular the decision tree induction method, are presented to answer special questions during the forensic examination of criminal proceedings instituted against doctors in cases of inadequate medical care. Based on the results, justified by an automated mathematical program for intelligent database processing, specific forensic expert features of therapeutic and tactical defects in the provision of medical care are established. 
 Представлены возможности технологии искусственного интеллекта Data Mining, в частности метода индукции дерева решений, для ответа на специальные вопросы при проведении судебно-медицинской экспертизы уголовных дел, возбужденных против врачей в случаях ненадлежащего оказания медицинской помощи. На основании полученных результатов, обоснованных автоматизированной математической программой интеллектуальной обработки базы данных, установлены конкретные судебно-медицинские экспертные особенности лечебно-тактических дефектов при оказании медицинской помощи. 
  |  None  |  
------------------------------------------- 
10.1186/s12859-020-3368-2  |    Background:  Genome-wide association studies (GWAS) provide a powerful means to identify associations between genetic variants and phenotypes. However, GWAS techniques for detecting epistasis, the interactions between genetic variants associated with phenotypes, are still limited. We believe that developing an efficient and effective GWAS method to detect epistasis will be a key for discovering sophisticated pathogenesis, which is especially important for complex diseases such as Alzheimer's disease (AD). 
  Results:  In this regard, this study presents GenEpi, a computational package to uncover epistasis associated with phenotypes by the proposed machine learning approach. GenEpi identifies both within-gene and cross-gene epistasis through a two-stage modeling workflow. In both stages, GenEpi adopts two-element combinatorial encoding when producing features and constructs the prediction models by L1-regularized regression with stability selection. The simulated data showed that GenEpi outperforms other widely-used methods on detecting the ground-truth epistasis. As real data is concerned, this study uses AD as an example to reveal the capability of GenEpi in finding disease-related variants and variant interactions that show both biological meanings and predictive power. 
  Conclusions:  The results on simulation data and AD demonstrated that GenEpi has the ability to detect the epistasis associated with phenotypes effectively and efficiently. The released package can be generalized to largely facilitate the studies of many complex diseases in the near future. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3368-2  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32093643/  |  
------------------------------------------- 
10.1515/bmt-2018-0109  |   In recent times, the control of human-computer interface (HCI) systems is triggered by electrooculography (EOG) signals. Eye movements recognized based on the EOG signal pattern are utilized to govern the HCI system and do a specific job based on the type of eye movement. With the knowledge of various related examinations, this paper intends a novel model for eye movement recognition based on EOG signals by utilizing Grey Wolf Optimization (GWO) with neural network (NN). Here, the GWO is used to minimize the error function from the classifier. The performance of the proposed methodology was investigated by comparing the developed model with conventional methods. The results reveal the loftier performance of the adopted method with the error minimization analysis and recognition performance analysis in correspondence with varied performance measures such as accuracy, sensitivity, specificity, precision, false-positive rate (FPR), false-negative rate (FNR), negative predictive value (NPV), false discovery rate (FDR) and the F1 score. 
  |  https://www.degruyter.com/doi/10.1515/bmt-2018-0109  |  
------------------------------------------- 
10.1556/650.2020.HO2650  |    |  https://akjournals.com/doi/10.1556/650.2020.HO2650  |  
------------------------------------------- 
10.1016/S0140-6736(19)32553-X  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)32553-X  |  
------------------------------------------- 
10.1007/s00104-019-01043-3  |    Background:  The digitalization process is currently on everyone's lips and sweeping changes in the field of public health and especially in surgery are to be expected within the next few years. Besides general issues, such as electronic health records and medical information systems, artificial intelligence, robotics and model-based surgery will decisively impact on the daily routine. In order to provide the necessary knowledge base, to point out related risks and chances and also to define fields of action for surgery, the German Society of Surgery commissioned a position paper on digitalization. A first appraisal in form of an online survey is the subject of this article. 
  Methods:  This article is based on an online survey of the members of the German Society of Surgery and selected members of other related societies. The survey asked for the members' personal assessment concerning different aspects of the digitalization process and the respective state of knowledge as well as the impact on the field of surgery. 
  Results:  A total of 296 members contributed to this survey. According to their assessment, digitalization in surgery is currently associated with terms such as electronic health records and medical information systems but they also assume a relevant influence on their own activities and on the fields of interventional medicine and surgery. A relevant need for improvement of the current state of knowledge was highlighted, not only for general aspects of digitalization but also for surgically relevant issues in particular. The vast majority of interviewed members saw digitalization more as a chance for improvement than as a risk factor. 
  Conclusion:  According to the views of interviewed members of the German Society of Surgery the process of digital transformation will significantly impact the field of surgery. All those involved should feel responsible to contribute to and guide this process in order to maintain the surgically inherent requirements and to protect patient safety. The position paper on digitalization can serve as a basis and should define concrete recommendations for action. In the sense of an academic approach the new possibilities should be critically evaluated with respect to suitability and should be exclusively confined to applications that are beneficial to ourselves and to our patients. 
  |  https://dx.doi.org/10.1007/s00104-019-01043-3  |  
------------------------------------------- 
10.1093/neuonc/noz240  |    |  https://academic.oup.com/neuro-oncology/article-lookup/doi/10.1093/neuonc/noz240  |  
------------------------------------------- 
10.1038/d41586-020-00200-7  |    |  https://doi.org/10.1038/d41586-020-00200-7  |  
------------------------------------------- 
10.1016/j.ypmed.2019.105950  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0091-7435(19)30433-5  |  
------------------------------------------- 
10.1038/s41596-019-0289-5  |   Machine learning is a powerful tool for creating computational models relating brain function to behavior, and its use is becoming widespread in neuroscience. However, these models are complex and often hard to interpret, making it difficult to evaluate their neuroscientific validity and contribution to understanding the brain. For neuroimaging-based machine-learning models to be interpretable, they should (i) be comprehensible to humans, (ii) provide useful information about what mental or behavioral constructs are represented in particular brain pathways or regions, and (iii) demonstrate that they are based on relevant neurobiological signal, not artifacts or confounds. In this protocol, we introduce a unified framework that consists of model-, feature- and biology-level assessments to provide complementary results that support the understanding of how and why a model works. Although the framework can be applied to different types of models and data, this protocol provides practical tools and examples of selected analysis methods for a functional MRI dataset and multivariate pattern-based predictive models. A user of the protocol should be familiar with basic programming in MATLAB or Python. This protocol will help build more interpretable neuroimaging-based machine-learning models, contributing to the cumulative understanding of brain mechanisms and brain health. Although the analyses provided here constitute a limited set of tests and take a few hours to days to complete, depending on the size of data and available computational resources, we envision the process of annotating and interpreting models as an open-ended process, involving collaborative efforts across multiple studies and laboratories. 
  |  http://dx.doi.org/10.1038/s41596-019-0289-5  |  
------------------------------------------- 
10.1038/s41591-019-0651-8  |    |  http://dx.doi.org/10.1038/s41591-019-0651-8  |  
------------------------------------------- 
PMID:32267114  |   Nowadays, we are facing an overwhelming amount of public announcements concerning the rise of artificial intelligence (AI) in the world of medical imaging (including radiology, nuclear medicine and radiotherapy). While most of the applications are still limited to specific niches, there is a general trend to build real transversal platforms. Multiple industrial players, in collaboration with the clinicians in the field, are striving to build those platforms in order to offer plenty of use cases of AI for several purposes and needs (screening/detection, diagnosis and prediction). It is already undeniable that AI far exceeds human capabilities in terms of resolution, speed of image analysis and efficiency. Negative attitudes and skepticism from concerned professionals should be banned. Colla¬boration with data scientists and engineers for the large scale development and implementation should be pushed forward for the benefit of both patients and payers. 
 S’il y a bien un domaine où les annonces pleuvent en matière de développement de l’intelligence artificielle (IA), c’est le secteur de l’imagerie médicale au sens large du terme (regroupant la radiologie, la médecine nucléaire et la radiothérapie). Les applications, encore souvent uti¬lisées dans des niches précises, ont tendance à devenir beaucoup plus transversales. De multiples acteurs indus¬triels, en partenariat avec les utilisateurs, s’évertuent à construire de réelles plateformes qui offrent aux cliniciens une multitude d’applications utilisables pour combler plusieurs types de demandes et besoins (détection, diagnostic et prédiction). Il est indéniable que la capacité de l’IA dépasse largement nos capacités humaines en matière de résolution de l’image, de rapidité et d’efficience de lecture et d’analyse. Une attitude de négation ou de scepticisme de la part des professionnels du secteur n’est plus de mise. Ils doivent, sans attendre, collaborer avec les spécialistes data et les ingénieurs au développement à large échelle de l’IA en imagerie médicale et ce, au profit des patients et des payeurs. 
  |  https://www.rmlg.ulg.ac.be/aboel.php?num_id=3268&langue=EN  |  
------------------------------------------- 
10.1097/JU.0000000000000731  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000731  |  
------------------------------------------- 
10.1038/s41585-020-0294-z  |    |  http://dx.doi.org/10.1038/s41585-020-0294-z  |  
------------------------------------------- 
10.1038/s41571-019-0315-0  |    |  http://dx.doi.org/10.1038/s41571-019-0315-0  |  
------------------------------------------- 
10.1038/d41586-020-00160-y  |    |  https://doi.org/10.1038/d41586-020-00160-y  |  
------------------------------------------- 
10.1016/j.ecoenv.2019.110146  |   A quantitative structure-toxicity relationship (QSTR) model based on four descriptors was successfully developed for 1163 chemical toxicants against Tetrahymena pyriformis by applying general regression neural network (GRNN). The training set consisting of 600 organic compounds was used to train GRNN models that were evaluated with the test set of 563 compounds. For the optimal GRNN model, the training set possesses the coefficient of determination R<sup>2</sup> of 0.86 and root mean square (rms) error of 0.41, and the test set has R<sup>2</sup> of 0.80 and rms of 0.41. Investigated results indicate that the optimal GRNN model is accurate, although the GRNN model has only four descriptor and more samples in the test set. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0147-6513(19)31477-0  |  
------------------------------------------- 
10.1038/s41596-019-0286-8  |   Skilled forelimb behaviors are among the most important for studying motor learning in multiple species including humans. This protocol describes learned forelimb tasks for mice using a two-axis robotic manipulandum. Our device provides a highly compact adaptation of actuated planar two-axis arms that is simple and inexpensive to construct. This paradigm has been dominant for decades in primate motor neuroscience. Our device can generate arbitrary virtual movement tracks, arbitrary time-varying forces or arbitrary position- or velocity-dependent force patterns. We describe several example tasks permitted by our device, including linear movements, movement sequences and aiming movements. We provide the mechanical drawings and source code needed to assemble and control the device, and detail the procedure to train mice to use the device. Our software can be simply extended to allow users to program various customized movement assays. The device can be assembled in a few days, and the time to train mice on the tasks that we describe ranges from a few days to several weeks. Furthermore, the device is compatible with various neurophysiological techniques that require head fixation. 
  |  http://dx.doi.org/10.1038/s41596-019-0286-8  |  
------------------------------------------- 
10.6224/JN.202002_67(1).05  |   With Taiwan now an "aged society", home safety for older individuals has become a very important issue. The purpose of establishing early warning systems in homes and/or communities is to generate and disseminate meaningful warning information to medical institutions or rescue units in a timely manner so that they may take timely and appropriate action. The main purpose of this paper is to introduce the current application of information and communication technology (ICT, especially the Internet of Things and artificial intelligence) in early warning systems for home and community care. Two approaches to developing these systems are introduced: instant detection and prevention monitoring. Instant detection facilitates fall detection and personnel tracking, while the focus of prevention monitoring is on preventing falls and physiological status monitoring. The challenges faced by in incorporating ICT into these early monitoring systems are discussed as well. 
  Title:  居家社區照護早期警示系統之發展. 
 隨著高齡社會的來臨，長者的居家安全成為非常重要的議題。居家／社區的早期警示系統之目的在及時產生、傳播有意義的警告資訊，提供醫療院所或救助單位能迅速提供適當的照護行動。本文主要的目的在介紹目前資通訊科技在居家和社區早期警示系統上之應用，特別是物聯網及人工智慧。本文中將分成即時偵測及預防監控兩個層次來探討，即時偵測包含跌倒偵測和人員追蹤兩方面，而預防監控特別著重在預防跌倒和生理狀態監控兩方面。同時本文也探討現今發展資通訊科技在居家社區的早期監視系統所面臨的挑戰，期望透過本文的介紹說明能對早期監視系統的發展提供幫助。. 
  |  None  |  
------------------------------------------- 
10.1186/s12859-019-3311-6  |    Background:  Membrane transport proteins (transporters) play an essential role in every living cell by transporting hydrophilic molecules across the hydrophobic membranes. While the sequences of many membrane proteins are known, their structure and function is still not well characterized and understood, owing to the immense effort needed to characterize them. Therefore, there is a need for advanced computational techniques takes sequence information alone to distinguish membrane transporter proteins; this can then be used to direct new experiments and give a hint about the function of a protein. 
  Results:  This work proposes an ensemble classifier TooT-T that is trained to optimally combine the predictions from homology annotation transfer and machine-learning methods to determine the final prediction. Experimental results obtained by cross-validation and independent testing show that combining the two approaches is more beneficial than employing only one. 
  Conclusion:  The proposed model outperforms all of the state-of-the-art methods that rely on the protein sequence alone, with respect to accuracy and MCC. TooT-T achieved an overall accuracy of 90.07% and 92.22% and an MCC 0.80 and 0.82 with the training and independent datasets, respectively. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3311-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32321420/  |  
------------------------------------------- 
10.3390/ijerph17020498  |   Atrial Fibrillation (AF) is the most common cardiac arrhythmia found in clinical practice. It affects an estimated 33.5 million people, representing approximately 0.5% of the world's population. Electrocardiogram (ECG) is the main diagnostic criterion for AF. Recently, photoplethysmography (PPG) has emerged as a simple and portable alternative for AF detection. However, it is not completely clear which are the most important features of the PPG signal to perform this process. The objective of this paper is to determine which are the most relevant features for PPG signal analysis in the detection of AF. This study is divided into two stages: (a) a systematic review carried out following the Preferred Reporting Items for a Systematic Review and Meta-analysis of Diagnostic Test Accuracy Studies (PRISMA-DTA) statement in six databases, in order to identify the features of the PPG signal reported in the literature for the detection of AF, and (b) an experimental evaluation of them, using machine learning, in order to determine which have the greatest influence on the process of detecting AF. Forty-four features were found when analyzing the signal in the time, frequency, or time-frequency domains. From those 44 features, 27 were implemented, and through machine learning, it was found that only 11 are relevant in the detection process. An algorithm was developed for the detection of AF based on these 11 features, which obtained an optimal performance in terms of sensitivity (98.43%), specificity (99.52%), and accuracy (98.97%). 
  |  http://www.mdpi.com/resolver?pii=ijerph17020498  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31941071/  |  
------------------------------------------- 
10.1038/s41467-020-14578-5  |   Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an 'object manifold'. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with 'classification capacity', a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds' radius, dimensionality and inter-manifold correlations. 
  |  http://dx.doi.org/10.1038/s41467-020-14578-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32029727/  |  
------------------------------------------- 
10.1097/JU.0000000000000731.01  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000731.01  |  
------------------------------------------- 
10.1186/s12859-020-3364-6  |    Background:  In methylation analyses like epigenome-wide association studies, a high amount of biomarkers is tested for an association between the measured continuous outcome and different covariates. In the case of a continuous covariate like smoking pack years (SPY), a measure of lifetime exposure to tobacco toxins, a spike at zero can occur. Hence, all non-smokers are generating a peak at zero, while the smoking patients are distributed over the other SPY values. Additionally, the spike might also occur on the right side of the covariate distribution, if a category "heavy smoker" is designed. Here, we will focus on methylation data with a spike at the left or the right of the distribution of a continuous covariate. After the methylation data is generated, analysis is usually performed by preprocessing, quality control, and determination of differentially methylated sites, often performed in pipeline fashion. Hence, the data is processed in a string of methods, which are available in one software package. The pipelines can distinguish between categorical covariates, i.e. for group comparisons or continuous covariates, i.e. for linear regression. The differential methylation analysis is often done internally by a linear regression without checking its inherent assumptions. A spike in the continuous covariate is ignored and can cause biased results. 
  Results:  We have reanalysed five data sets, four freely available from ArrayExpress, including methylation data and smoking habits reported by smoking pack years. Therefore, we generated an algorithm to check for the occurrences of suspicious interactions between the values associated with the spike position and the non-spike positions of the covariate. Our algorithm helps to decide if a suspicious interaction can be found and further investigations should be carried out. This is mostly important, because the information on the differentially methylated sites will be used for post-hoc analyses like pathway analyses. 
  Conclusions:  We help to check for the validation of the linear regression assumptions in a methylation analysis pipeline. These assumptions should also be considered for machine learning approaches. In addition, we are able to detect outliers in the continuous covariate. Therefore, more statistical robust results should be produced in methylation analysis using our algorithm as a preprocessing step. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3364-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32000657/  |  
------------------------------------------- 
10.1097/JU.0000000000000720.01  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000720.01  |  
------------------------------------------- 
10.1016/j.watres.2020.115490  |   Continuous high frequency water quality monitoring is becoming a critical task to support water management. Despite the advancements in sensor technologies, certain variables cannot be easily and/or economically monitored in-situ and in real time. In these cases, surrogate measures can be used to make estimations by means of data-driven models. In this work, variables that are commonly measured in-situ are used as surrogates to estimate the concentrations of nutrients in a rural catchment and in an urban one, making use of machine learning models, specifically Random Forests. The results are compared with those of linear modelling using the same number of surrogates, obtaining a reduction in the Root Mean Squared Error (RMSE) of up to 60.1%. The profit from including up to seven surrogate sensors was computed, concluding that adding more than 4 and 5 sensors in each of the catchments respectively was not worthy in terms of error improvement. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0043-1354(20)30026-9  |  
------------------------------------------- 
10.1371/journal.pcbi.1007148  |   Machine learning algorithms are becoming increasingly popular for decoding psychological constructs based on neural data. However, as a step towards bridging the gap between theory-driven cognitive neuroscience and data-driven decoding approaches, there is a need for methods that allow to interpret trained decoding models. The present study demonstrates grouped model reliance as a model-agnostic permutation-based approach to this problem. Grouped model reliance indicates the extent to which a trained model relies on conceptually related groups of variables, such as frequency bands or regions of interest in electroencephalographic (EEG) data. As a case study to demonstrate the method, random forest and support vector machine models were trained on within-participant single-trial EEG data from a Sternberg working memory task. Participants were asked to memorize a sequence of digits (0-9), varying randomly in length between one, four and seven digits, where EEG recordings for working memory load estimation were taken from a 3-second retention interval. The present results confirm previous findings insofar as both random forest and support vector machine models relied on alpha-band activity in most subjects. However, as revealed by further analyses, patterns in frequency and topography varied considerably between individuals, pointing to more pronounced inter-individual differences than previously reported. 
  |  http://dx.plos.org/10.1371/journal.pcbi.1007148  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31905373/  |  
------------------------------------------- 
10.1186/s12859-020-3339-7  |    Background:  MicroRNAs (miRNAs) play important roles in a variety of biological processes by regulating gene expression at the post-transcriptional level. So, the discovery of new miRNAs has become a popular task in biological research. Since the experimental identification of miRNAs is time-consuming, many computational tools have been developed to identify miRNA precursor (pre-miRNA). Most of these computation methods are based on traditional machine learning methods and their performance depends heavily on the selected features which are usually determined by domain experts. To develop easily implemented methods with better performance, we investigated different deep learning architectures for the pre-miRNAs identification. 
  Results:  In this work, we applied convolution neural networks (CNN) and recurrent neural networks (RNN) to predict human pre-miRNAs. We combined the sequences with the predicted secondary structures of pre-miRNAs as input features of our models, avoiding the feature extraction and selection process by hand. The models were easily trained on the training dataset with low generalization error, and therefore had satisfactory performance on the test dataset. The prediction results on the same benchmark dataset showed that our models outperformed or were highly comparable to other state-of-the-art methods in this area. Furthermore, our CNN model trained on human dataset had high prediction accuracy on data from other species. 
  Conclusions:  Deep neural networks (DNN) could be utilized for the human pre-miRNAs detection with high performance. Complex features of RNA sequences could be automatically extracted by CNN and RNN, which were used for the pre-miRNAs prediction. Through proper regularization, our deep learning models, although trained on comparatively small dataset, had strong generalization ability. 
  |  https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3339-7  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31931701/  |  
------------------------------------------- 
10.1371/journal.pone.0226685  |   Measuring the diffusion of innovations from textual data sources besides patent data has not been studied extensively. However, early and accurate indicators of innovation and the recognition of trends in innovation are mandatory to successfully promote economic growth through technological progress via evidence-based policy making. In this study, we propose Paragraph Vector Topic Model (PVTM) and apply it to technology-related news articles to analyze innovation-related topics over time and gain insights regarding their diffusion process. PVTM represents documents in a semantic space, which has been shown to capture latent variables of the underlying documents, e.g., the latent topics. Clusters of documents in the semantic space can then be interpreted and transformed into meaningful topics by means of Gaussian mixture modeling. In using PVTM, we identify innovation-related topics from 170, 000 technology news articles published over a span of 20 years and gather insights about their diffusion state by measuring the topic importance in the corpus over time. Our results suggest that PVTM is a credible alternative to widely used topic models for the discovery of latent topics in (technology-related) news articles. An examination of three exemplary topics shows that innovation diffusion could be assessed using topic importance measures derived from PVTM. Thereby, we find that PVTM diffusion indicators for certain topics are Granger causal to Google Trend indices with matching search terms. 
  |  http://dx.plos.org/10.1371/journal.pone.0226685  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31967999/  |  
------------------------------------------- 
10.1038/s41591-020-0791-x  |   Electrocardiogram (ECG) acquisition is increasingly widespread in medical and commercial devices, necessitating the development of automated interpretation strategies. Recently, deep neural networks have been used to automatically analyze ECG tracings and outperform physicians in detecting certain rhythm irregularities<sup>1</sup>. However, deep learning classifiers are susceptible to adversarial examples, which are created from raw data to fool the classifier such that it assigns the example to the wrong class, but which are undetectable to the human eye<sup>2,3</sup>. Adversarial examples have also been created for medical-related tasks<sup>4,5</sup>. However, traditional attack methods to create adversarial examples do not extend directly to ECG signals, as such methods introduce square-wave artefacts that are not physiologically plausible. Here we develop a method to construct smoothed adversarial examples for ECG tracings that are invisible to human expert evaluation and show that a deep learning model for arrhythmia detection from single-lead ECG<sup>6</sup> is vulnerable to this type of attack. Moreover, we provide a general technique for collating and perturbing known adversarial examples to create multiple new ones. The susceptibility of deep learning ECG algorithms to adversarial misclassification implies that care should be taken when evaluating these models on ECGs that may have been altered, particularly when incentives for causing misclassification exist. 
  |  http://dx.doi.org/10.1038/s41591-020-0791-x  |  
------------------------------------------- 
10.1038/d41586-020-00768-0  |    |  https://doi.org/10.1038/d41586-020-00768-0  |  
------------------------------------------- 
10.1097/JU.0000000000000689.01  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000689.01  |  
------------------------------------------- 
10.1016/S0140-6736(20)30034-9  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(20)30034-9  |  
------------------------------------------- 
10.1007/978-3-030-30402-7_26  |   Food selectivity by children with autism spectrum disorder (ASD) is relatively high as compared to typical children and consequently puts them at risk of nutritional inadequacies. Thus, there is a need to educate children with ASD on food types and their benefits in a simple and interesting manner that will encourage food acceptance and enable a move toward healthy living. The use of technological intervention has proven to be an effective tool for educating children with ASD in maintaining attention and mastering new skills as compared to traditional methods. Some of the popularly used technologies are computer-based intervention and robotics which do not support ecological validity (i.e., mimicking natural scenario). Consideration of natural factors is essential for better learning outcomes and generalized skills which can easily be incorporated into reality-based technologies such as virtual reality, augmented reality, and mixed reality. These technologies provide evidence-based support for ecological validation of intervention and sustaining the attention of children with ASD. The main objective of this study is to review existing reality-based technology intervention for children with ASD and investigate the following: (1) commonly used reality-based technology, (2) types of intervention targeted with reality-based technology, and (3) what subjects' inclusion types are used in the reality-based interventions. These objective statements have guided our recommendation of reality-based technology that can support ecological validity of food intake intervention. 
  |  None  |  
------------------------------------------- 
10.1097/MD.0000000000019157  |    Introduction:  Peritoneal metastasis (PM) is a frequent condition in patients presenting with gastric cancer, especially in younger patients with advanced tumor stages. Computer tomography (CT) is the most common noninvasive modality for preoperative staging in gastric cancer. However, the challenges of limited CT soft tissue contrast result in poor CT depiction of small peritoneal tumors. The sensitivity for detecting PM remains low. About 16% of PM are undetected. Deep learning belongs to the category of artificial intelligence and has demonstrated amazing results in medical image analyses. So far, there has been no deep learning study based on CT images for the diagnosis of PM in gastric cancer. 
  We proposed a hypothesis:  CT images in the primary tumor region of gastric cancer had valuable information that could predict occult PM of gastric cancer, which could be extracted effectively through deep learning. 
  Objective:  To develop a deep learning model for accurate preoperative diagnosis of PM in gastric cancer. 
  Method:  All patients with gastric cancer were retrospectively enrolled. All patients were initially diagnosed as PM negative by CT and later confirmed as positive through surgery or laparoscopy. The dataset was randomly split into training cohort (70% of all patients) and testing cohort (30% of all patients). To develop deep convolutional neural network (DCNN) models with high generalizability, 5-fold cross-validation and model ensemble were utilized. The area under the receiver operating characteristic curve, sensitivity and specificity were used to evaluate DCNN models on the testing cohort. 
  Discussion:  This study will help us know whether deep learning can improve the performance of CT in diagnosing PM in gastric cancer. 
  |  http://dx.doi.org/10.1097/MD.0000000000019157  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32080093/  |  
------------------------------------------- 
PMID:32138489  |   The fourth industrial revolution refers to a fusion of technologies which blend the physical, digital and biological worlds. It can mitigate miseries of the teeming masses. However, India faces the unique challenge. It is both the creator of enormous amount of data required for this revolution as well as largest potential market for those innovations in the coming future. This article traces the challenges and draws parallels with the past experiences. The first transformation is expected to hit healthcare sector much sooner than projected earlier. It outlines the call to action required of our thought leaders today. 
  |  None  |  
------------------------------------------- 
10.1016/j.neuron.2019.12.002  |   Evolution is a blind fitting process by which organisms become adapted to their environment. Does the brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in artificial neural networks have exposed the power of optimizing millions of synaptic weights over millions of observations to operate robustly in real-world contexts. These models do not learn simple, human-interpretable rules or representations of the world; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Counterintuitively, similar to evolutionary processes, over-parameterized models can be simple and parsimonious, as they provide a versatile, robust solution for learning a diverse set of functions. This new family of direct-fit models present a radical challenge to many of the theoretical assumptions in psychology and neuroscience. At the same time, this shift in perspective establishes unexpected links with developmental and ecological psychology. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0896-6273(19)31044-X  |  
------------------------------------------- 
10.1164/rccm.201911-2123ED  |    |  http://www.atsjournals.org/doi/full/10.1164/rccm.201911-2123ED?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31747303/  |  
------------------------------------------- 
10.1097/JU.0000000000000632  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000632  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_10  |   The skin is the largest organ of our body. Skin disease abnormalities which occur within the skin layers are difficult to examine visually and often require biopsies to make a confirmation on a suspected condition. Such invasive methods are not well-accepted by children and women due to the possibility of scarring. Optical coherence tomography (OCT) is a non-invasive technique enabling in vivo examination of sub-surface skin tissue without the need for excision of tissue. However, one of the challenges in OCT imaging is the interpretation and analysis of OCT images. In this review, we discuss the various methodologies in skin layer segmentation and how it could potentially improve the management of skin diseases. We also present a review of works which use advanced machine learning techniques to achieve layers segmentation and detection of skin diseases. Lastly, current challenges in analysis and applications are also discussed. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_10  |  
------------------------------------------- 
10.1097/JU.0000000000000689.02  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000689.02  |  
------------------------------------------- 
10.1038/s41591-020-0824-5  |   The past decade has allowed the development of a multitude of digital tools. Now they can be used to remediate the COVID-19 outbreak. 
  |  http://dx.doi.org/10.1038/s41591-020-0824-5  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284618/  |  
------------------------------------------- 
PMID:32150710  |   It has been observed that the stratification of Autism Spectrum Disorders (ASD) generated by the current scales is not effective for the personalization of early treatments. The clinical evaluation of ASD requires its consideration as a continuum of deficits, and there is a need to identify biologically significant parameters (biomarkers) that have the power to automatically characterize each individual at different stages of neurological development. The emerging field of computational psychiatry (CP) attempts to meet the needs of precision diagnosis by developing powerful computational and mathematical techniques. A growing scientific activity proposes the use of implicit measures based on biosignals for the classification of ASD. Virtual reality (VR) technologies have demonstrated potential for ASD interventions, but most of the work has used virtual reality for the learning / objective of interventions. Very few studies have used biological signals for recording and detailed analysis of behavioral responses that can be used to monitor or produce changes over time. In this paper the concept of behavioral biomarkers based on VR or VRBB is introduced. VRBB will allow the classification of ASD using a paradigm of computational psychiatry based on implicit brain processes measured through psychophysiological signals and the behavior of subjects exposed to complex replicas of social conditions using virtual reality interfaces. 
 Se ha observado que la estratificación de trastornos del espectro autista (TEA) generada por las escalas actuales no es efectiva para la personalización de tratamientos tempranos. La evaluación clínica de TEA requiere su consideración como un continuo de déficits, y existe la necesidad de identificar parámetros biológicamente significativos (biomarcadores) que tengan el poder de caracterizar automáticamente a cada individuo en diferentes etapas del desarrollo neurológico. El incipiente campo de la psiquiatría computacional (CP) intenta satisfacer las necesidades de diagnóstico de precisión mediante el desarrollo de potentes técnicas computacionales y matemáticas. Una creciente actividad científica propone el uso de medidas implícitas basadas en bioseñales para la clasificación de ASD. Las tecnologías de realidad virtual (VR) han demostrado potencial para las intervenciones de TEA, pero la mayoría de los trabajos han utilizado la realidad virtual para el aprendizaje / objetivo de las intervenciones. Muy pocos estudios han utilizado señales biológicas para el registro y el análisis detallado de las respuestas conductuales que se pueden utilizar para monitorear o producir cambios a lo largo del tiempo. En el presente trabajo se introduce el concepto de biomarcadores conductuales basados en VR o VRBB. Los VRBB van a permitir la clasificación de TEA utilizando un paradigma de psiquiatría computacional basado en procesos cerebrales implícitos medidos a través de señales psicofisiológicas y el comportamiento de sujetos expuestos a complejas réplicas de condiciones sociales utilizando interfaces de realidad virtual. 
  |  http://www.medicinabuenosaires.com/PMID/32150710.pdf  |  
------------------------------------------- 
10.3760/cma.j.cn501120-20190403-00162  |   The early accurate diagnosis of burn depth is of great significance in determining the corresponding clinical intervention methods and judging the prognosis quality of burn patients. However, the current diagnostic method of burn depth still relies mainly on the empirical subjective judgment of clinicians, with low diagnostic accuracy. Especially for deep partial-thickness burn wounds, the error of early diagnosis is pretty big. In recent years, with the rapid development of artificial intelligence technology, deep learning algorithm combined with image analysis technology can better identify and analyze the information of medical images. This article reviews the research progress of artificial intelligence technology in the diagnosis of burn depth. 
 烧伤深度的早期精确诊断对决定相应的临床干预手段和判断烧伤患者预后质量具有重要意义。然而目前烧伤深度的诊断仍主要依靠临床医师的主观经验性判断，准确率低，尤其对于深Ⅱ度烧伤创面，早期诊断误差较大。近年来，人工智能技术发展迅速，深度学习算法结合图像分析技术能够更好地识别、分析医学图像信息。本文将对人工智能技术在烧伤深度诊断方面的研究进展进行综述。. 
  |  http://journal.yiigle.com/LinkIn.do?linkin_type=pubmed&DOI=10.3760/cma.j.cn501120-20190403-00162  |  
------------------------------------------- 
10.1148/radiol.2019192252  |    |  http://pubs.rsna.org/doi/10.1148/radiol.2019192252?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.acra.2019.07.031  |   The AUR Academic Radiology and Industry Leaders Roundtable was organized as an open discussion between academic leaders of top US academic radiology departments and industry leaders from top companies that provide equipment and services to radiology, including manufacturers, pharmaceutical companies, software developers and electronic medical record (EMR) providers. The format was that of a structured brainstorming session with pre-selected discussion topics. This roundtable was instrumental in widening perspectives and providing insights into the challenges and opportunities for our specialty, such as in the case of Artificial Intelligence (AI). 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30442-8  |  
------------------------------------------- 
10.3390/medicina56030141  |   The objective of this article is to discuss the inherent bias involved with artificial intelligence-based decision support systems for healthcare. In this article, the authors describe some relevant work published in this area. A proposed overview of solutions is also presented. The authors believe that the information presented in this article will enhance the readers' understanding of this inherent bias and add to the discussion on this topic. Finally, the authors discuss an overview of the need to implement transdisciplinary solutions that can be used to mitigate this bias. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32244930/  |  
------------------------------------------- 
10.2471/BLT.19.237289  |   Artificial intelligence holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also awareness of potential risks and harm that may be caused by unregulated developments of artificial intelligence. Guiding principles are being developed around the world to foster trustworthy development and application of artificial intelligence systems. These guidelines can support developers and governing authorities when making decisions about the use of artificial intelligence. The High-Level Expert Group on Artificial Intelligence set up by the European Commission launched the report <i>Ethical guidelines for trustworthy artificial intelligence</i> in2019. The report aims to contribute to reflections and the discussion on the ethics of artificial intelligence technologies also beyond the countries of the European Union (EU). In this paper, we use the global health sector as a case and argue that the EU's guidance leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally. We point to the urgency of shared globalized efforts to safeguard against the potential harms of artificial intelligence technologies in health care. 
 L'intelligence artificielle regorge de potentiel en matière d'interventions préventives et curatives précises, efficaces et bénéfiques. Mais par la même occasion, elle présente certains risques et peut s'avérer nocive si son développement n'est pas encadré par des règles. Partout dans le monde, des principes directeurs sont instaurés afin de promouvoir un niveau de fiabilité optimal dans l'évolution et l'application des systèmes basés sur l'intelligence artificielle. Ces principes peuvent aider les développeurs et les autorités gouvernementales à prendre des décisions relatives à l'intelligence artificielle. Le Groupe d'experts de haut niveau sur l'intelligence artificielle créé par la Commission européenne a récemment publié un rapport intitulé <i>Lignes directrices en matière d'éthique pour une IA digne de confiance</i>. Objectif de ce rapport : contribuer aux réflexions et discussions portant sur l'éthique des technologies fondées sur l'intelligence artificielle, y compris dans les pays n'appartenant pas à l'Union européenne (UE). Dans ce document, nous utilisons le secteur mondial de la santé comme exemple et estimons que les directives de l'UE accordent un pouvoir discrétionnaire trop important aux autorités locales et au contexte pour véritablement encourager la fiabilité de l'intelligence artificielle dans le monde. Nous insistons également sur l'urgence de mettre en place une protection globale commune contre les éventuels préjudices liés aux technologies d'intelligence artificielle dans le domaine des soins de santé. 
 La inteligencia artificial es muy prometedora en términos de intervenciones preventivas y curativas beneficiosas, precisas y eficaces. Al mismo tiempo, también hay conciencia de los posibles riesgos y daños que pueden causar los desarrollos no regulados de la inteligencia artificial. Se están elaborando principios fundamentales en todo el mundo para fomentar el desarrollo y la aplicación confiables de los sistemas de inteligencia artificial. Estas directrices pueden servir de apoyo a los desarrolladores y a las autoridades gobernantes en la toma de decisiones sobre el uso de la inteligencia artificial. El Grupo de Expertos de Alto Nivel sobre Inteligencia Artificial establecido por la Comisión Europea ha publicado recientemente el informe <i>Ethical guidelines for trustworthy artificial intelligence</i> (Directrices éticas para una inteligencia artificial confiable). El informe tiene por objeto contribuir a la reflexión y el debate sobre la ética de las tecnologías de inteligencia artificial incluso más allá de los países de la Unión Europea (UE). En este documento, se recurre al sector sanitario mundial como caso de referencia y se argumenta que las directrices de la UE conceden demasiado margen a la discreción local y contextualizada como para fomentar una inteligencia artificial confiable a nivel mundial. Se destaca la urgencia de compartir los esfuerzos internacionales para protegerse de los posibles daños de las tecnologías de inteligencia artificial en la atención sanitaria. 
 يجمل الذكاء الاصطناعي بين طياته وعوداً جمة فيما يتعلق بالتدخلات الوقائية والعلاجية المفيدة والدقيقة والفعالة. وفي الوقت ذاته، هناك أيضًا وعي بالمخاطر والأضرار المحتملة التي قد تحدث بسبب التطورات غير المنظمة للذكاء الاصطناعي. يتم حالياً تطوير مبادئ توجيهية حول العالم لتعزيز تنمية جديرة بالثقة، وتطبيق أنظمة الذكاء الاصطناعي. يمكن لهذه الإرشادات أن تدعم المطورين والسلطات الحاكمة عند اتخاذ القرارات بشأن استخدام الذكاء الاصطناعي. قامت مؤخراً مجموعة خبراء الذكاء الاصطناعي رفيعي المستوى، التي أسستها المفوضية الأوروبية، بإصدار تقرير بعنوان <i>المبادئ التوجيهية الأخلاقية للذكاء الاصطناعي الجدير بالثقة</i> . يهدف التقرير إلى المساهمة في الأفكار والمناقشة الخاصة بأخلاقيات تقنيات الذكاء الاصطناعي كذلك خارج دول الاتحاد الأوروبي (EU). وفي هذه الورقة، نحن نستخدم قطاع الصحة العالمي كحالة، ونؤمن بأن توجيهات الاتحاد الأوروبي تترك مجالاً كبيراً لتقدير محلي مقترن بالسياق لتعزيز ذكاءً اصطناعياً جدير بالثقة على مستوى العالم. كما نشير إلى الحاجة الملحة لبذل جهود عالمية مشتركة للوقاية من الأضرار المحتملة لتكنولوجيات الذكاء الاصطناعي في الرعاية الصحية. 
 人工智能在有益、精准和有效的预防与治疗干预方面具有广阔的前景。同时，人们也意识到人工智能的无度发展可能会引起潜在的风险和危害。全世界正在制定指导原则，以促进可信赖的人工智能系统的开发和应用。这些准则可以支持开发人员和管理机构制定有关人工智能用途的决策。由欧盟委员会成立的人工智能高级专家小组最近发布了<i>《可信赖的人工智能伦理准则》(Ethical guidelines for trustworthy artificial intelligence)</i> 报告。该报告旨在促进欧盟 (EU) 以外的国家也开展人工智能技术伦理方面的反思和讨论。在本文中，我们以全球卫生部门为例并且认为欧盟的准则还有很大的完善空间，可以根据当地的具体情况自行决定，在全球范围内促进可信赖的人工智能。我们指出必须立即在全球范围内共同努力，防范人工智能技术在医疗保健领域的潜在危害。. 
 Искусственный интеллект открывает большие перспективы для имеющих практическую значимость, точных и эффективных мероприятий по профилактике и лечению заболеваний. В то же самое время необходимо принимать во внимание потенциальные риски и вред, которые могут быть вызваны нерегулируемым развитием искусственного интеллекта. В настоящее время идет процесс глобальной разработки руководящих принципов, способствующих надежному и безопасному формированию и применению систем искусственного интеллекта. Данные рекомендации помогут разработчикам и руководящим органам в принятии решений относительно использования искусственного интеллекта. Созданная Европейской комиссией экспертная группа высокого уровня по искусственному интеллекту недавно выпустила отчет, озаглавленный <i>«Этические рекомендации для надежного и безопасного использования искусственного интеллекта»</i>. Цель отчета — содействие процессу анализа и обсуждения этической стороны применения технологий искусственного интеллекта за пределами Европейского союза (ЕС). В этой статье авторы рассматривают в качестве примера сектор общественного здравоохранения и доказывают, что рекомендации ЕС предоставляют слишком большую свободу действий на локальном уровне в контексте имеющихся условий, что препятствует обеспечению надежного использования искусственного интеллекта во всем мире. Авторы подчеркивают настоятельную необходимость совместных глобальных усилий по защите от потенциального вреда, который может быть нанесен сфере здравоохранения в результате использования технологий искусственного интеллекта. 
  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32284649/  |  
------------------------------------------- 
10.3390/diagnostics10040198  |   The novel coronavirus disease 19 (COVID-19) is rapidly spreading with a rising death toll and transmission rate reported in high income countries rather than in low income countries. The overburdened healthcare systems and poor disease surveillance systems in resource-limited settings may struggle to cope with this COVID-19 outbreak and this calls for a tailored strategic response for these settings. Here, we recommend a low cost blockchain and artificial intelligence-coupled self-testing and tracking systems for COVID-19 and other emerging infectious diseases. Prompt deployment and appropriate implementation of the proposed system have the potential to curb the transmissions of COVID-19 and the related mortalities, particularly in settings with poor access to laboratory infrastructure. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10040198  |  
------------------------------------------- 
10.1080/14787210.2020.1744275  |    |  http://www.tandfonline.com/doi/full/10.1080/14787210.2020.1744275  |  
------------------------------------------- 
10.1002/cac2.12012  |   The development of digital pathology and progression of state-of-the-art algorithms for computer vision have led to increasing interest in the use of artificial intelligence (AI), especially deep learning (DL)-based AI, in tumor pathology. The DL-based algorithms have been developed to conduct all kinds of work involved in tumor pathology, including tumor diagnosis, subtyping, grading, staging, and prognostic prediction, as well as the identification of pathological features, biomarkers and genetic changes. The applications of AI in pathology not only contribute to improve diagnostic accuracy and objectivity but also reduce the workload of pathologists and subsequently enable them to spend additional time on high-level decision-making tasks. In addition, AI is useful for pathologists to meet the requirements of precision oncology. However, there are still some challenges relating to the implementation of AI, including the issues of algorithm validation and interpretability, computing systems, the unbelieving attitude of pathologists, clinicians and patients, as well as regulators and reimbursements. Herein, we present an overview on how AI-based approaches could be integrated into the workflow of pathologists and discuss the challenges and perspectives of the implementation of AI in tumor pathology. 
  |  https://doi.org/10.1002/cac2.12012  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32277744/  |  
------------------------------------------- 
10.12116/j.issn.1004-5619.2020.01.016  |   Traditional forensic identification relies on forensic experts to manually extract information and provide identification opinions based on medicine, biology and other fields of knowledge combined with personal work experience, which is not only time-consuming and require great effort, but also affected by subjective factors that are difficult to overcome. In the era of big data, the booming development of artificial intelligence brings new ideas to forensic medicine. In recent years, forensic researchers at home and abroad have conducted many studies based on artificial intelligence technology, such as face recognition, age and gender identification, DNA analysis, postmortem interval estimation, injury and cause of death identification, showing the feasibility and advantages of using artificial intelligence technology to solve forensic identification problems. As a new means of technology that has adapted to the development of the times, artificial intelligence has brought new vitality to forensic medicine, but at the same time also some new challenges. How to deal with these challenges scientifically and form a new mode of 'artificial intelligence plus forensic medicine' with artificial intelligence and forensic medicine developing collaboratively is a new direction for the development of forensic medicine in the era of big data. 
  题目:  人工智能技术时代法医学科面临的新机遇与挑战. 
  摘要:  传统的法医学鉴定依赖于法医专家提取信息，运用医学、生物学等多领域知识结合工作经验作出鉴定意见，不仅耗时耗力，而且存在难以克服的主观因素。在大数据时代，人工智能的蓬勃发展为法医学带来了新思路。近些年，国内外法医学者们基于人工智能技术开展了人脸识别、年龄及性别鉴定、DNA分析、死亡时间推断、损伤以及死亡原因鉴定等多方面的研究，显示出运用人工智能技术解决法医学鉴定问题的可行性与优越性。人工智能技术作为一种适应时代发展的崭新技术手段，为法医学带来新活力的同时，也带来了新挑战。如何科学应对这些挑战，形成人工智能和法医学协同发展的“人工智能+法医学”新模式，是大数据时代下法医学发展的新方向。. 
  关键词:  法医学；人工智能；机器学习；深度学习；综述. 
  |  https://doi.org/10.12116/j.issn.1004-5619.2020.01.016  |  
------------------------------------------- 
10.1007/s00117-019-00630-z  |    Clinical/methodological problem:  In the reconstruction of three-dimensional image data, artifacts that interfere with the appraisal often occur as a result of trying to minimize the dose or due to missing data. Used iterative reconstruction methods are time-consuming and have disadvantages. 
  Standard radiological methods:  These problems are known to occur in computed tomography (CT), cone beam CT, interventional imaging, magnetic resonance imaging (MRI) and nuclear medicine imaging (PET and SPECT). 
  Methodological innovations:  Using techniques based on the use of artificial intelligence (AI) in data analysis and data supplementation, a number of problems can be solved up to a certain extent. 
  Performance:  The performance of the methods varies greatly. Since the generated image data usually look very good using the AI-based methods presented here while their results depend strongly on the study design, reliable comparable quantitative statements on the performance are not yet available in broad terms. 
  Evaluation:  In principle, the methods of image reconstruction based on AI algorithms offer many possibilities for improving and optimizing three-dimensional image datasets. However, the validity strongly depends on the design of the respective study in the structure of the individual procedure. It is therefore essential to have a suitable test prior to use in clinical practice. 
  Practical recommendations:  Before the widespread use of AI-based reconstruction methods can be recommended, it is necessary to establish meaningful test procedures that can characterize the actual performance and applicability in terms of information content and a meaningful study design during the learning phase of the algorithms. 
  |  https://dx.doi.org/10.1007/s00117-019-00630-z  |  
------------------------------------------- 
10.1016/S0140-6736(20)30294-4  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(20)30294-4  |  
------------------------------------------- 
10.1007/s00259-020-04678-1  |    |  https://dx.doi.org/10.1007/s00259-020-04678-1  |  
------------------------------------------- 
10.3389/fmed.2020.00100  |   Artificial intelligence (AI) has become a progressively prevalent Research Topic in medicine and is increasingly being applied to dermatology. There is a need to understand this technology's progress to help guide and shape the future for medical care providers and recipients. We reviewed the literature to evaluate the types of publications on the subject, the specific dermatological topics addressed by AI, and the most challenging barriers to its implementation. A substantial number of original articles and commentaries have been published to date and only few detailed reviews exist. Most AI applications focus on differentiating between benign and malignant skin lesions, however; others exist pertaining to ulcers, inflammatory skin diseases, allergen exposure, dermatopathology, and gene expression profiling. Applications commonly analyze and classify images, however, other tools such as risk assessment calculators are becoming increasingly available. Although many applications are technologically feasible, important implementation barriers have been identified including systematic biases, difficulty of standardization, interpretability, and acceptance by physicians and patients alike. This review provides insight into future research needs and possibilities. There is a strong need for clinical investigation in dermatology providing evidence of success overcoming the identified barriers. With these research goals in mind, an appropriate role for AI in dermatology may be achieved in not so distant future. 
  |  https://doi.org/10.3389/fmed.2020.00100  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32296706/  |  
------------------------------------------- 
10.1177/0194599819889686  |   Artificial intelligence (AI) is quickly expanding within the sphere of health care, offering the potential to enhance the efficiency of care delivery, diminish costs, and reduce diagnostic and therapeutic errors. As the field of otolaryngology also explores use of AI technology in patient care, a number of ethical questions warrant attention prior to widespread implementation of AI. This commentary poses many of these ethical questions for consideration by the otolaryngologist specifically, using the 4 pillars of medical ethics-autonomy, beneficence, nonmaleficence, and justice-as a framework and advocating both for the assistive role of AI in health care and for the shared decision-making, empathic approach to patient care. 
  |  http://journals.sagepub.com/doi/full/10.1177/0194599819889686?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.jpedsurg.2019.09.008  |   Exponential growth in computing power, data storage, and sensing technology has led to a world in which we can both capture and analyze incredible amounts of data. The evolution of machine learning has further advanced the ability of computers to develop insights from massive data sets that are beyond the capacity of human analysis. The convergence of computational power, data storage, connectivity, and Artificial Intelligence (AI) has led to health technologies that, to date, have focused on diagnostic areas such as radiology and pathology. The question remains how the digital revolution will translate in the realm of surgery. There are three main areas where the authors believe that AI could impact surgery in the near future: enhancement of training modalities, cognitive enhancement of the surgeon, and procedural automation. While the promise of Big Data, AI, and Automation is high, there have been unanticipated missteps in the use of such technologies that are worth considering as we evaluate how such technologies could/should be adopted in surgical practice. Surgeons must be prepared to adopt smarter training modalities, supervise the learning of machines that can enhance cognitive function, and ultimately oversee autonomous surgery without allowing for a decay in the surgeon's operating skills. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0022-3468(19)30654-2  |  
------------------------------------------- 
10.1016/j.acra.2019.08.019  |    |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30441-6  |  
------------------------------------------- 
10.1016/j.acra.2020.03.003  |   Coronavirus disease is an emerging infection caused by a novel coronavirus that is moving rapidly. High resolution computed tomography (CT) allows objective evaluation of the lung lesions, thus enabling us to better understand the pathogenesis of the disease. With serial CT examinations, the occurrence, development, and prognosis of the disease can be better understood. The imaging can be sorted into four phases: early phase, progressive phase, severe phase, and dissipative phase. The CT appearance of each phase and temporal progression of the imaging findings are demonstrated. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(20)30144-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32204987/  |  
------------------------------------------- 
10.1177/0846537120907507  |    |  http://journals.sagepub.com/doi/full/10.1177/0846537120907507?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1093/inthealth/ihaa007  |   Healthcare involves cyclic data processing to derive meaningful, actionable decisions. Rapid increases in clinical data have added to the occupational stress of healthcare workers, affecting their ability to provide quality and effective services. Health systems have to radically rethink strategies to ensure that staff are satisfied and actively supported in their jobs. Artificial intelligence (AI) has the potential to augment provider performance. This article reviews the available literature to identify AI opportunities that can potentially transform the role of healthcare providers. To leverage AI's full potential, policymakers, industry, healthcare providers and patients have to address a new set of challenges. Optimizing the benefits of AI will require a balanced approach that enhances accountability and transparency while facilitating innovation. 
  |  https://academic.oup.com/inthealth/article-lookup/doi/10.1093/inthealth/ihaa007  |  
------------------------------------------- 
10.1016/j.breast.2020.01.008  |   Communication is a core component of effective healthcare that impacts many patient and doctor outcomes, yet is complex and challenging to both analyse and teach. Human-based coding and audit systems are time-intensive and costly; thus, there is considerable interest in the application of artificial intelligence to this topic, through machine learning using both supervised and unsupervised learning algorithms. In this article we introduce health communication, its importance for patient and health professional outcomes, and the need for rigorous empirical data to support this field. We then discuss historical interaction coding systems and recent developments in applying artificial intelligence (AI) to automate such coding in the health setting. Finally, we discuss available evidence for the reliability and validity of AI coding, application of AI in training and audit of communication, as well as limitations and future directions in this field. In summary, recent advances in machine learning have allowed accurate textual transcription, and analysis of prosody, pauses, energy, intonation, emotion and communication style. Studies have established moderate to good reliability of machine learning algorithms, comparable with human coding (or better), and have identified some expected and unexpected associations between communication variables and patient satisfaction. Finally, application of artificial intelligence to communication skills training has been attempted, to provide audit and feedback, and through the use of avatars. This looks promising to provide confidential and easily accessible training, but may be best used as an adjunct to human-based training. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0960-9776(20)30009-6  |  
------------------------------------------- 
10.1111/iej.13265  |    Aim:  To verify the diagnostic performance of an artificial intelligence system based on the deep convolutional neural network method to detect periapical pathosis on cone-beam computed tomography (CBCT) images. 
  Methodology:  images of 153 periapical lesions obtained from 109 patients were included. The specific area of the jaw and teeth associated with the periapical lesions were then determined by a human observer. Lesion volumes were calculated using the manual segmentation methods using Fujifilm-Synapse 3D software (Fujifilm Medical Systems, Tokyo, Japan). The neural network was then used to determine (i) whether the lesion could be detected; (ii) if the lesion was detected, where it was localized (maxilla, mandible or specific tooth); and (iii) lesion volume. Manual segmentation and artificial intelligence (AI) (Diagnocat Inc., San Francisco, CA, USA) methods were compared using Wilcoxon signed rank test and Bland-Altman analysis. 
  Results:  The deep convolutional neural network system was successful in detecting teeth and numbering specific teeth. Only one tooth was incorrectly identified. The AI system was able to detect 142 of a total of 153 periapical lesions. The reliability of correctly detecting a periapical lesion was 92.8%. The deep convolutional neural network volumetric measurements of the lesions were similar to those with manual segmentation. There was no significant difference between the two measurement methods (P &gt; 0.05). 
  Conclusions:  Volume measurements performed by humans and by AI systems were comparable to each other. AI systems based on deep learning methods can be useful for detecting periapical pathosis on CBCT images for clinical application. 
  |  https://doi.org/10.1111/iej.13265  |  
------------------------------------------- 
10.1016/j.gie.2019.09.036  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(19)32303-X  |  
------------------------------------------- 
10.1161/CIRCIMAGING.120.010427  |    |  http://www.ahajournals.org/doi/full/10.1161/CIRCIMAGING.120.010427?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.4103/ijo.IJO_1780_19  |    |  http://www.ijo.in/article.asp?issn=0301-4738;year=2020;volume=68;issue=2;spage=405;epage=406;aulast=Ahuja  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31957738/  |  
------------------------------------------- 
10.1177/1932296820909900  |   Artificial intelligence (AI)-based algorithms are rapidly entering the health care field and have the potential to improve patient care. Our article focuses on the use of autonomous AI algorithms (ie, algorithms that can make clinical decisions without human oversight) in diagnostic imaging. In this article, we have used the example of diabetic retinopathy screening to highlight some important aspects to be considered by developers, policymakers, and end users when bringing autonomous AI algorithms into clinical practice. We have divided these aspects into (1) following the principles of safety, efficacy, and equity in all phases of development and implementation of the algorithm; (2) regulatory processes involving medical records, medical liability, and patient privacy; (3) cost and billing; and (4) the role of health care providers. 
  |  http://journals.sagepub.com/doi/full/10.1177/1932296820909900?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41582-019-0287-9  |    |  http://dx.doi.org/10.1038/s41582-019-0287-9  |  
------------------------------------------- 
10.3760/cma.j.issn.1671-0274.2020.01.006  |   The rapid development of computer technologies brings us great changes in daily life and work. Artificial intelligence is a branch of computer science, which is to allow computers to exercise activities that are normally confined to intelligent life. The broad sense of artificial intelligence includes machine learning and robots. This article mainly focuses on machine learning and related medical fields, and deep learning is an artificial neural network in machine learning. Convolutional neural network (CNN) is a type of deep neural network, that is developed on the basis of deep neural network, further imitating the structure of the visual cortex of the brain and the principle of visual activity. The current machine learning method used in medical big data analysis is mainly CNN. In the next few years, it is the developing trend that artificial intelligence as a conventional tool will enter the relevant departments of medical image interpretation. In addition, this article also shares the progress of the integration of artificial intelligence and biomedicine combined with actual cases, and mainly introduces the current status of CNN application research in pathological diagnosis, imaging diagnosis and endoscopic diagnosis for gastrointestinal diseases. 
 高速发展的计算机技术给日常生活及工作带来巨大变化。人工智能是计算机科学的一个分支，是让计算机去行使通常情况下具备智能生命才可能行使的活动，广义的人工智能涵盖机器学习和机器人等等，本文主要聚焦于机器学习与相关的医学领域，深度学习是机器学习中的人工神经网络，卷积神经网络（CNN）是深度神经网络的一种，是在深度神经网络基础上，进一步模仿大脑的视觉皮层构造和视觉活动原理而开发；目前在医疗大数据分析中应用的机器学习方式主要为CNN。在未来数年内，人工智能作为常规工具进入医学图像解读相关的科室是发展趋势。本文主要分享人工智能与生物医学的融合进展，并结合实际案例，重点介绍CNN在胃肠道疾病的病理诊断、影像学诊断及内镜诊断等方面的应用研究现状。. 
  |  http://journal.yiigle.com/LinkIn.do?linkin_type=pubmed&issn=1671-0274&year=2020&vol=23&issue=1&fpage=33  |  
------------------------------------------- 
10.1038/s41591-019-0649-2  |    |  http://dx.doi.org/10.1038/s41591-019-0649-2  |  
------------------------------------------- 
10.1136/bmj.m689  |    Objective:  To systematically examine the design, reporting standards, risk of bias, and claims of studies comparing the performance of diagnostic deep learning algorithms for medical imaging with that of expert clinicians. 
  Design:  Systematic review. 
  Data sources:  Medline, Embase, Cochrane Central Register of Controlled Trials, and the World Health Organization trial registry from 2010 to June 2019. 
  Eligibility criteria for selecting studies:  Randomised trial registrations and non-randomised studies comparing the performance of a deep learning algorithm in medical imaging with a contemporary group of one or more expert clinicians. Medical imaging has seen a growing interest in deep learning research. The main distinguishing feature of convolutional neural networks (CNNs) in deep learning is that when CNNs are fed with raw data, they develop their own representations needed for pattern recognition. The algorithm learns for itself the features of an image that are important for classification rather than being told by humans which features to use. The selected studies aimed to use medical imaging for predicting absolute risk of existing disease or classification into diagnostic groups (eg, disease or non-disease). For example, raw chest radiographs tagged with a label such as pneumothorax or no pneumothorax and the CNN learning which pixel patterns suggest pneumothorax. 
  Review methods:  Adherence to reporting standards was assessed by using CONSORT (consolidated standards of reporting trials) for randomised studies and TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) for non-randomised studies. Risk of bias was assessed by using the Cochrane risk of bias tool for randomised studies and PROBAST (prediction model risk of bias assessment tool) for non-randomised studies. 
  Results:  Only 10 records were found for deep learning randomised clinical trials, two of which have been published (with low risk of bias, except for lack of blinding, and high adherence to reporting standards) and eight are ongoing. Of 81 non-randomised clinical trials identified, only nine were prospective and just six were tested in a real world clinical setting. The median number of experts in the comparator group was only four (interquartile range 2-9). Full access to all datasets and code was severely limited (unavailable in 95% and 93% of studies, respectively). The overall risk of bias was high in 58 of 81 studies and adherence to reporting standards was suboptimal (&lt;50% adherence for 12 of 29 TRIPOD items). 61 of 81 studies stated in their abstract that performance of artificial intelligence was at least comparable to (or better than) that of clinicians. Only 31 of 81 studies (38%) stated that further prospective studies or trials were required. 
  Conclusions:  Few prospective deep learning studies and randomised trials exist in medical imaging. Most non-randomised trials are not prospective, are at high risk of bias, and deviate from existing reporting standards. Data and code availability are lacking in most studies, and human comparator groups are often small. Future studies should diminish risk of bias, enhance real world clinical relevance, improve reporting and transparency, and appropriately temper conclusions. 
  Study registration:  PROSPERO CRD42019123605. 
  |  http://www.bmj.com/cgi/pmidlookup?view=long&pmid=32213531  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32213531/  |  
------------------------------------------- 
10.1111/bjd.18933  |   The potential areas of application of artificial intelligence in dermatology are ever-increasing. With the wide availability of smartphones equipped with high-resolution cameras and impressive processing powers, harnessing these capabilities using machine learning (ML) could open new prospects in the management of dermatological disorders. Du-Harpur et al. have done a commendable job reviewing the utility of artificial intelligence in dermatology in an easily understandable manner by most dermatologists<sup>1</sup> . 
  |  https://doi.org/10.1111/bjd.18933  |  
------------------------------------------- 
10.14309/ajg.0000000000000476  |   Most colorectal polyps are diminutive, and malignant potential for these polyps is uncommon, especially for those in the rectosigmoid. However, many diminutive polyps are still being resected to determine whether these are adenomas or serrated/hyperplastic polyps. Resecting all the diminutive polyps is not cost-effective. Therefore, gastroenterologists have proposed optical diagnosis using image-enhanced endoscopy for polyp characterization. These technologies have achieved favorable outcomes, but are not widely available. Artificial intelligence has been used in clinical medicine to classify lesions. Here, artificial intelligence technology for the characterization of colorectal polyps is discussed in a decision-making context regarding diminutive colorectal polyps. 
  |  http://Insights.ovid.com/pubmed?pmid=31770118  |  
------------------------------------------- 
10.1093/gastro/goaa011  |   Radiomics uses computers to extract a large amount of information from different types of images, form various quantifiable features, and select relevant features using artificial-intelligence algorithms to build models, in order to predict the outcomes of clinical problems (such as diagnosis, treatment, prognosis, etc.). The study of liver diseases by radiomics will contribute to early diagnosis and treatment of liver diseases and improve survival and cure rates of liver diseases. This field is currently in the ascendant and may have great development in the future. Therefore, we summarize the progress of current research in this article and then point out the related deficiencies and the direction of future research. 
  |  https://academic.oup.com/gastro/article-lookup/doi/10.1093/gastro/goaa011  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32280468/  |  
------------------------------------------- 
10.1001/jama.2020.0370  |    |  https://jamanetwork.com/journals/jama/fullarticle/10.1001/jama.2020.0370  |  
------------------------------------------- 
10.1016/j.gie.2019.10.026  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(19)32395-8  |  
------------------------------------------- 
10.1016/j.suc.2020.01.001  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0039-6109(20)30001-3  |  
------------------------------------------- 
10.3290/j.qi.a43952  |   Artificial intelligence (AI) encompasses a broad spectrum of emerging technologies that continue to influence daily life. The evolution of AI makes the analysis of big data possible, which provides reliable information and improves the decision-making process. This article introduces the principles of AI and reviews the development of AI and how it is currently being used. AI technology has influenced the health care field because of the need for accurate diagnosis and superior patient care. In order to understand the trend of AI in dentistry, electronic searching was carried out, combined with approaching individual companies to obtain the details of AI-based services. The current applications of AI in clinical dentistry were introduced and summarized. In the future, the AI-based comprehensive care system is expected to establish high-quality patient care and innovative research and development, facilitating advanced decision support tools. The authors believe that an innovative inter-professional coordination among clinicians, researchers, and engineers will be the key to AI development in the field of dentistry. Despite the potential misinterpretations and the concern of patient privacy, AI will continue to connect with dentistry from a comprehensive perspective due to the need for precise treatment procedures and instant information exchange. Moreover, such developments will enable professionals to share health-related big data and deliver insights that improve patient care through hospitals, providers, researchers, and patients. 
  |  https://qi.quintessenz.de/index.php?doc=abstract&abstractID=43952/  |  
------------------------------------------- 
10.1259/dmfr.20190107  |    Objectives:  To investigate the current clinical applications and diagnostic performance of artificial intelligence (AI) in dental and maxillofacial radiology (DMFR). 
  Methods:  Studies using applications related to DMFR to develop or implement AI models were sought by searching five electronic databases and four selected core journals in the field of DMFR. The customized assessment criteria based on QUADAS-2 were adapted for quality analysis of the studies included. 
  Results:  The initial electronic search yielded 1862 titles, and 50 studies were eventually included. Most studies focused on AI applications for an automated localization of cephalometric landmarks, diagnosis of osteoporosis, classification/segmentation of maxillofacial cysts and/or tumors, and identification of periodontitis/periapical disease. The performance of AI models varies among different algorithms. 
  Conclusion:  The AI models proposed in the studies included exhibited wide clinical applications in DMFR. Nevertheless, it is still necessary to further verify the reliability and applicability of the AI models prior to transferring these models into clinical practice. 
  |  http://www.birpublications.org/doi/full/10.1259/dmfr.20190107?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.2217/nnm-2019-0366  |    |  http://www.futuremedicine.com/doi/full/10.2217/nnm-2019-0366?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.3881/j.issn.1000-503X.10961  |   As an important branch of artificial intelligence,the emerging medical artificial intelligence(MAI)is facing many ethical issues.MAI may offer the optimal diagnosis and treatment for patients but may also bring adverse effects on society and human beings.This article discusses the ethical problems caused by MAI and elucidates its development in a direction that meets ethical principles and requirements. 
  |  http://www.actacams.com/article/2020/1000-503X/1000-503X-42-1-128.pdf  |  
------------------------------------------- 
10.1097/NNA.0000000000000855  |   As systems evolve over time, their natural tendency is to become increasingly more complex. Studies in the field of complex systems have generated new perspectives on the application of management strategies in health systems. Much of this research appears as a natural extension of the cross-disciplinary field of systems theory. Since writing my 1st article for Managing Organizational Complexity in 2004, much has happened to further our understanding of complexity in healthcare systems. The growth of new computational methods in the fields of data science and data analytics has allowed scientists to identify signals or patterns in large complex data sets (big data) that in the past were seemingly hidden. Rather than relying on historical statistical methods to infer outcomes, these advanced methods combined with increased computer processing power allow machines to learn the structure of data and create artificial intelligence (AI). In our ongoing efforts to find solutions for complex healthcare problems, AI is becoming more and more an accepted method. The purpose of this edition of Managing Organizational Complexity is to define AI and machine learning, discuss the recent resurgence of AI, and then provide examples of how AI can provide value to healthcare with an emphasis on nursing. 
  |  http://dx.doi.org/10.1097/NNA.0000000000000855  |  
------------------------------------------- 
10.1038/s41569-019-0294-y  |    |  http://dx.doi.org/10.1038/s41569-019-0294-y  |  
------------------------------------------- 
10.1097/ICU.0000000000000656  |    Purpose of review:  The aim of this article is to review and discuss the history, current state, and future implications of promising biomedical offerings in the field of retina. 
  Recent findings:  The technologies discussed are some of the more recent promising biomedical developments within the field of retina. There is a US Food and Drug Administration-approved gene therapy product and artificial intelligence device for retina, with many other offerings in the pipeline. 
  Summary:  Signaling pathway therapies, genetic therapies, mitochondrial therapies, and artificial intelligence have shaped retina care as we know it and are poised to further impact the future of retina care. Retina specialists have the privilege and responsibility of shaping this future for the visual health of current and future generations. 
  |  http://dx.doi.org/10.1097/ICU.0000000000000656  |  
------------------------------------------- 
10.1038/s41467-019-14214-x  |   A bionic artificial device commonly integrates various distributed functional units to mimic the functions of biological sensory neural system, bringing intricate interconnections, complicated structure, and interference in signal transmission. Here we show an all-in-one bionic artificial nerve based on a separate electrical double-layers structure that integrates the functions of perception, recognition, and transmission. The bionic artificial nerve features flexibility, rapid response (&lt;21 ms), high robustness, excellent durability (&gt;10,000 tests), personalized cutability, and no energy consumption when no mechanical stimulation is being applied. The response signals are highly regionally differentiated for the mechanical stimulations, which enables the bionic artificial nerve to mimic the spatiotemporally dynamic logic of a biological neural network. Multifunctional touch interactions demonstrate the enormous potential of the bionic artificial nerve for human-machine hybrid perceptual enhancement. By incorporating the spatiotemporal resolution function and algorithmic analysis, we hope that bionic artificial nerves will promote further development of sophisticated neuroprosthetics and intelligent robotics. 
  |  http://dx.doi.org/10.1038/s41467-019-14214-x  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31937777/  |  
------------------------------------------- 
10.1001/jama.2020.4543  |    |  https://jamanetwork.com/journals/jama/fullarticle/10.1001/jama.2020.4543  |  
------------------------------------------- 
10.1038/s41586-019-1799-6  |   Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful<sup>1</sup>. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives<sup>2</sup>. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7% and 1.2% (USA and UK) in false positives and 9.4% and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening. 
  |  https://doi.org/10.1038/s41586-019-1799-6  |  
------------------------------------------- 
10.5946/ce.2020.054  |   Recently, significant improvements have been made in artificial intelligence. The artificial neural network was introduced in the 1950s. However, because of the low computing power and insufficient datasets available at that time, artificial neural networks suffered from overfitting and vanishing gradient problems for training deep networks. This concept has become more promising owing to the enhanced big data processing capability, improvement in computing power with parallel processing units, and new algorithms for deep neural networks, which are becoming increasingly successful and attracting interest in many domains, including computer vision, speech recognition, and natural language processing. Recent studies in this technology augur well for medical and healthcare applications, especially in endoscopic imaging. This paper provides perspectives on the history, development, applications, and challenges of deep-learning technology. 
  |  https://dx.doi.org/10.5946/ce.2020.054  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32252504/  |  
------------------------------------------- 
10.2174/1381612826666200331091156  |   With the continuous development of artificial intelligence (AI) technology, big datasupported AI technology with considerable computer and learning capacity has been applied in diagnosing different types of diseases. This study reviews the application of expert system, neural network, and deep learning used by AI technology in disease diagnosis. This paper also gives a glimpse of the intelligent diagnosis and treatment of digestive system diseases, respiratory system diseases, and osteoporosis by AI technology. 
  |  None  |  
------------------------------------------- 
10.3390/diagnostics10040231  |   Artificial intelligence (AI) is poised to change much about the way we practice radiology in the near future. The power of AI tools has the potential to offer substantial benefit to patients. Conversely, there are dangers inherent in the deployment of AI in radiology, if this is done without regard to possible ethical risks. Some ethical issues are obvious; others are less easily discerned, and less easily avoided. This paper explains some of the ethical difficulties of which we are presently aware, and some of the measures we may take to protect against misuse of AI. 
  |  http://www.mdpi.com/resolver?pii=diagnostics10040231  |  
------------------------------------------- 
10.24869/psyd.2020.25  |   Deep emotional traumas in societies overwhelmed by large-scale human disasters, like, global pandemic diseases, natural disasters, man-made tragedies, war conflicts, social crises, etc., can cause massive stress-related disorders. Motivated by the ongoing global coronavirus pandemic, the article provides an overview of scientific evidence regarding adverse impact of diverse human disasters on mental health in afflicted groups and societies. Following this broader context, psychosocial impact of COVID-19 as a specific global human disaster is presented, with an emphasis on disturbing mental health aspects of the ongoing pandemic. Limited resources of mental health services in a number of countries around the world are illustrated, which will be further stretched by the forthcoming increase in demand for mental health services due to the global COVID-19 pandemic. Mental health challenges are particularly important for the Republic of Croatia in the current situation, due to disturbing stress of the 2020 Zagreb earthquake and the high pre-pandemic prevalence of chronic Homeland-War-related posttraumatic stress disorders. Comprehensive approach based on digital psychiatry is proposed to address the lack of access to psychiatric services, which includes artificial intelligence, telepsychiatry and an array of new technologies, like internet-based computer-aided mental health tools and services. These tools and means should be utilized as an important part of the whole package of measures to mitigate negative mental health effects of the global coronavirus pandemic. Our scientific and engineering experiences in the design and development of digital tools and means in mitigation of stress-related disorders and assessment of stress resilience are presented. Croatian initiative on enhancement of interdisciplinary research of psychiatrists, psychologists and computer scientists on the national and EU level is important in addressing pressing mental health concerns related to the ongoing pandemic and similar human disasters. 
  |  http://www.psychiatria-danubina.com/UserDocsImages/pdf/dnb_vol32_no1/dnb_vol32_no1_25.pdf  |  
------------------------------------------- 
10.1016/j.amjmed.2019.08.017  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(19)30716-8  |  
------------------------------------------- 
10.1016/j.gie.2019.10.027  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0016-5107(19)32396-X  |  
------------------------------------------- 
10.3760/cma.j.issn.0376-2491.2020.03.001  |   人工智能（AI）和医学影像相结合是当今前沿的临床医学研究方向，AI凭借其强大的算力和先进的算法，在医学影像病变识别、智能诊断预测和临床疗效评估各个环节均发挥重要作用。中枢神经系统疾病因复杂的病理生理机制、精细的临床诊疗需求和高维的多模态影像数据，使得AI在该领域影像研究中的应用更具有挑战性。促进标准化数据采集和处理流程、优化AI算法算力、促进多学科融合是推动未来智能医学新模式转化进步的核心方向和目标。. 
  |  http://journal.yiigle.com/LinkIn.do?linkin_type=pubmed&DOI=10.3760/cma.j.issn.0376-2491.2020.03.001  |  
------------------------------------------- 
10.1017/S0022215119002536  |    Objective:  Deep learning using convolutional neural networks represents a form of artificial intelligence where computers recognise patterns and make predictions based upon provided datasets. This study aimed to determine if a convolutional neural network could be trained to differentiate the location of the anterior ethmoidal artery as either adhered to the skull base or within a bone 'mesentery' on sinus computed tomography scans. 
  Methods:  Coronal sinus computed tomography scans were reviewed by two otolaryngology residents for anterior ethmoidal artery location and used as data for the Google Inception-V3 convolutional neural network base. The classification layer of Inception-V3 was retrained in Python (programming language software) using a transfer learning method to interpret the computed tomography images. 
  Results:  A total of 675 images from 388 patients were used to train the convolutional neural network. A further 197 unique images were used to test the algorithm; this yielded a total accuracy of 82.7 per cent (95 per cent confidence interval = 77.7-87.8), kappa statistic of 0.62 and area under the curve of 0.86. 
  Conclusion:  Convolutional neural networks demonstrate promise in identifying clinically important structures in functional endoscopic sinus surgery, such as anterior ethmoidal artery location on pre-operative sinus computed tomography. 
  |  https://www.cambridge.org/core/product/identifier/S0022215119002536/type/journal_article  |  
------------------------------------------- 
10.4103/ijo.IJO_2175_19  |    |  http://www.ijo.in/article.asp?issn=0301-4738;year=2020;volume=68;issue=2;spage=396;epage=397;aulast=Madanagopalan  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31957736/  |  
------------------------------------------- 
10.1016/j.scitotenv.2019.135653  |   Wetlands are extraordinary ecosystems and important climate regulators that also contribute to reduce natural disaster risk. Unfortunately, wetlands are declining much faster than forests. The safeguarding of the wetlands also needs knowledge of the dynamics that control the water balance of these environments. Therefore, an accurate estimation of evapotranspiration in wetlands is an essential task. When adequate experimental data are available, some algorithms deriving from Artificial Intelligence research represent a promising alternative to the most common estimation techniques. In this study, starting from daily measurements of climatic variables such as net solar radiation, depth to water, wind speed, mean relative humidity, maximum temperature, minimum temperature, and mean temperature, using the Random Forest, Additive Regression of Decision Stump, Multilayer Perceptron and k-Nearest Neighbors algorithms, 24 estimation models, different in input variables, have been developed and compared. The data have been provided by USGS. They have been obtained from a measuring site in wetlands of Indian River County, Florida using the eddy-covariance technique. The accuracy of these models based on AI algorithms remains good even if the number of input variables is reduced from 7 to 3. Net solar radiation, mean temperature and mean relative humidity or wind speed measurements allow obtaining a sufficiently accurate estimation model. Random Forest and k-Nearest Neighbors provide slightly better performance than Additive Regression of Decision Stump and Multilayer Perceptron. The analyzed models show in most cases the lowest accuracy in the range 2-4 mm/day, while the highest accuracy is obtained in the ranges 0-2 mm/day and 6-8 mm/day, with the exception of the models based on the Additive Regression, which show similar levels of accuracy in the different considered sub-intervals. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(19)35648-7  |  
------------------------------------------- 
10.3389/fgene.2020.00309  |    |  https://doi.org/10.3389/fgene.2020.00309  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32328085/  |  
------------------------------------------- 
10.6009/jjrt.2020_JSRT_76.1.I  |    |  https://dx.doi.org/10.6009/jjrt.2020_JSRT_76.1.I  |  
------------------------------------------- 
10.2196/18873  |   Previous epidemic management research proves the importance of city-level information, but also highlights limited expertise in urban data applications during a pandemic outbreak. In this paper, we provide an overview of city-level information, in combination with analytical and operational capacity, that define urban intelligence for supporting response to disease outbreaks. We present five components (movement, facilities, people, information, and engagement) that have been previously investigated but remain siloed to successfully orchestrate an integrated pandemic response. Reflecting on the coronavirus disease (COVID-19) outbreak that was first identified in Wuhan, China, we discuss the opportunities, technical challenges, and foreseeable controversies for deploying urban intelligence during a pandemic. Finally, we emphasize the urgency of building urban intelligence through cross-disciplinary research and collaborative practice on a global scale. 
  |  https://publichealth.jmir.org/2020/2/e18873/  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32248145/  |  
------------------------------------------- 
10.1093/ndt/gfz226  |    |  https://academic.oup.com/ndt/article-lookup/doi/10.1093/ndt/gfz226  |  
------------------------------------------- 
10.3760/cma.j.issn.0376-2491.2020.06.003  |   人工智能决策系统能够帮助我们更好地制定临床方案，提高决策效率。成熟的系统需要有数据、核心算法以及好的界面支持。我们以中国临床肿瘤学会乳腺癌数据库为基础，基于国内核心算法，以指南为蓝本，创建了具有自主知识产权的人工智能决策系统，并利用Ⅰ~Ⅳ期临床研究，验证该系统与指南的符合率以及对不同级别医生的帮助程度。该系统的推广将进一步提高乳腺癌规范化诊疗，同时为其他智能决策系统的建立和应用提供经验。. 
  |  http://journal.yiigle.com/LinkIn.do?linkin_type=pubmed&DOI=10.3760/cma.j.issn.0376-2491.2020.06.003  |  
------------------------------------------- 
10.1097/MD.0000000000019114  |    Introduction:  Thoracic diseases include a variety of common human primary malignant tumors, among which lung cancer and esophageal cancer are among the top 10 in cancer incidence and mortality. Early diagnosis is an important part of cancer treatment, so artificial intelligence (AI) systems have been developed for the accurate and automated detection and diagnosis of thoracic tumors. However, the complicated AI structure and image processing made the diagnosis result of AI-based system unstable. The purpose of this study is to systematically review published evidence to explore the accuracy of AI systems in diagnosing thoracic cancers. 
  Methods and analysis:  We will conduct a systematic review and meta-analysis of the diagnostic accuracy of AI systems for the prediction of thoracic diseases. The primary objective is to assess the diagnostic accuracy of thoracic cancers, including assessing potential biases and calculating combined estimates of sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). The secondary objective is to evaluate the factors associated with different models, classifiers, and radiomics information. We will search databases such as PubMed/MEDLINE, Embase (via OVID), and the Cochrane Library. Two reviewers will independently screen titles and abstracts, perform full article reviews and extract study data. We will report study characteristics and assess methodological quality using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool. RevMan 5.3 and Meta-disc 1.4 software will be used for data synthesis. If pooling is appropriate, we will produce summary receiver operating characteristic (SROC) curves, summary operating points (pooled sensitivity and specificity), and 95% confidence intervals around the summary operating points. Methodological subgroup and sensitivity analyses will be performed to explore heterogeneity. 
  Prospero registration number:  CRD42019135247. 
  |  http://dx.doi.org/10.1097/MD.0000000000019114  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32049826/  |  
------------------------------------------- 
10.1016/j.dental.2020.03.021  |    Objectives:  This paper provides an overview of existing applications and concepts of robotic systems and artificial intelligence in dentistry. This review aims to provide the community with novel inputs and argues for an increased utilization of these recent technological developments, referred to as Dentronics, in order to advance dentistry. 
  Methods:  First, background on developments in robotics, artificial intelligence (AI) and machine learning (ML) are reviewed that may enable novel assistive applications in dentistry (Sec A). Second, a systematic technology review that evaluates existing state-of-the-art applications in AI, ML and robotics in the context of dentistry is presented (Sec B). 
  Results:  A systematic literature research in pubmed yielded in a total of 558 results. 41 studies related to ML, 53 studies related to AI and 49 original research papers on robotics application in dentistry were included. ML and AI have been applied in dental research to analyze large amounts of data to eventually support dental decision making, diagnosis, prognosis and treatment planning with the help of data-driven analysis algorithms based on machine learning. So far, only few robotic applications have made it to reality, mostly restricted to pilot use cases. 
  Significance:  The authors believe that dentistry can greatly benefit from the current rise of digital human-centered automation and be transformed towards a new robotic, ML and AI-enabled era. In the future, Dentronics will enhance reliability, reproducibility, accuracy and efficiency in dentistry through the democratized use of modern dental technologies, such as medical robot systems and specialized artificial intelligence. Dentronics will increase our understanding of disease pathogenesis, improve risk-assessment-strategies, diagnosis, disease prediction and finally lead to better treatment outcomes. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0109-5641(20)30076-2  |  
------------------------------------------- 
10.1136/bmj.l6927  |    |  http://www.bmj.com/cgi/pmidlookup?view=long&pmid=32198138  |  
------------------------------------------- 
10.1007/s10916-020-01562-1  |   The novel coronavirus (COVID-19) outbreak, which was identified in late 2019, requires special attention because of its future epidemics and possible global threats. Beside clinical procedures and treatments, since Artificial Intelligence (AI) promises a new paradigm for healthcare, several different AI tools that are built upon Machine Learning (ML) algorithms are employed for analyzing data and decision-making processes. This means that AI-driven tools help identify COVID-19 outbreaks as well as forecast their nature of spread across the globe. However, unlike other healthcare issues, for COVID-19, to detect COVID-19, AI-driven tools are expected to have active learning-based cross-population train/test models that employs multitudinal and multimodal data, which is the primary purpose of the paper. 
  |  https://dx.doi.org/10.1007/s10916-020-01562-1  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32189081/  |  
------------------------------------------- 
10.1097/ICU.0000000000000649  |    Purpose of review:  Current recommendations for glaucoma screening are decidedly neutral. No studies have yet documented improved long-term outcomes for individuals who undergo glaucoma screening versus those who do not. Given the long duration that would be required to detect a benefit, future studies that may answer this question definitively are unlikely. Nevertheless, advances in artificial intelligence and telemedicine will lead to more effective screening at lower cost. With these new technologies, additional research is needed to determine the costs and benefits of screening for glaucoma. 
  Recent findings:  Using optic disc photographs and/or optical coherence tomography, deep learning systems appear capable of diagnosing glaucoma more accurately than human graders. Eliminating the need for expert graders along with better technologies for remote imaging of the ocular fundus will allow for less expensive screening, which could enable screening of individuals with otherwise limited healthcare access. In India and China, where most glaucoma remains undiagnosed, glaucoma screening was recently found to be cost-effective. 
  Summary:  Recent advances in artificial intelligence and telemedicine have the potential to increase the accuracy, reduce the costs, and extend the reach of screening. Further research into implementing these technologies in glaucoma screening is required. 
  |  http://dx.doi.org/10.1097/ICU.0000000000000649  |  
------------------------------------------- 
10.1002/jsfa.10093  |    Background:  In this study, artificial intelligence models that identify sunn pest-damaged wheat grains (SDG) and healthy wheat grains (HWG) are presented. Svevo durum wheat cultivated in Konya province, Turkey is used for the process, with 150 HWG and 150 SDG being used for classification. Thanks to the constructed imaging setup, photos of the 300 wheat grains are obtained. Seventeen visual features of each wheat grain are extracted by image-processing techniques and evaluated in three different groups of dimension, texture and pattern as visual parameters. Artificial bee colony (ABC) optimization-based artificial neural network (ANN) and extreme learning machine (ELM) algorithms are implemented to classify the damaged wheat grains. 
  Results:  A correlation-based feature selection (CFS) technique is also utilized to find the most effective among the 17 features. In the classification process using five selected features, the mean absolute error (MAE) and root mean square error (RMSE) values for ABC-based ANN are calculated as 0.00174 and 0.00433 respectively. The proposed technique is integrated into graphical user interface (GUI) software to construct an effective detection system for practical use. 
  Conclusion:  The results indicate that, thanks to the modified ANN algorithm and implemented CFS algorithm, the detection accuracy of damaged wheat grains is considerably increased. © 2019 Society of Chemical Industry. 
  |  https://doi.org/10.1002/jsfa.10093  |  
------------------------------------------- 
10.1097/RTI.0000000000000512  |    |  http://dx.doi.org/10.1097/RTI.0000000000000512  |  
------------------------------------------- 
10.1249/JSR.0000000000000704  |   Digital transformation is becoming increasingly common in modern life and sports medicine, like many other medical disciplines, it is strongly influenced and impacted by this rapidly changing field. This review aims to give a brief overview of the potential that digital technologies can have for health care providers and patients in the clinical practice of sports medicine. We will focus on mobile applications, wearables, smart devices, intelligent machines, telemedicine, artificial intelligence, big data, system interoperability, virtual reality, augmented reality, exergaming, or social networks. While some technologies are already used in current medical practice, others still have undiscovered potential. Due to the diversity and ever changing nature of this field, we will briefly review multiple areas in an attempt to give readers some general exposure to the landscape instead of a thorough, deep review of one topic. Further research will be necessary to show how digitalization applications could best be used for patient treatments. 
  |  http://Insights.ovid.com/pubmed?pmid=32282462  |  
------------------------------------------- 
10.1177/1203475420923648  |    |  None  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_1  |   Deep learning is the state-of-the-art machine learning approach. The success of deep learning in many pattern recognition applications has brought excitement and high expectations that deep learning, or artificial intelligence (AI), can bring revolutionary changes in health care. Early studies of deep learning applied to lesion detection or classification have reported superior performance compared to those by conventional techniques or even better than radiologists in some tasks. The potential of applying deep-learning-based medical image analysis to computer-aided diagnosis (CAD), thus providing decision support to clinicians and improving the accuracy and efficiency of various diagnostic and treatment processes, has spurred new research and development efforts in CAD. Despite the optimism in this new era of machine learning, the development and implementation of CAD or AI tools in clinical practice face many challenges. In this chapter, we will discuss some of these issues and efforts needed to develop robust deep-learning-based CAD tools and integrate these tools into the clinical workflow, thereby advancing towards the goal of providing reliable intelligent aids for patient care. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_1  |  
------------------------------------------- 
10.1038/s41584-019-0361-0  |    |  http://dx.doi.org/10.1038/s41584-019-0361-0  |  
------------------------------------------- 
10.1259/bjr.20190580  |   Computer-aided diagnosis (CAD) has been a popular area of research and development in the past few decades. In CAD, machine learning methods and multidisciplinary knowledge and techniques are used to analyze the patient information and the results can be used to assist clinicians in their decision making process. CAD may analyze imaging information alone or in combination with other clinical data. It may provide the analyzed information directly to the clinician or correlate the analyzed results with the likelihood of certain diseases based on statistical modeling of the past cases in the population. CAD systems can be developed to provide decision support for many applications in the patient care processes, such as lesion detection, characterization, cancer staging, treatment planning and response assessment, recurrence and prognosis prediction. The new state-of-the-art machine learning technique, known as deep learning (DL), has revolutionized speech and text recognition as well as computer vision. The potential of major breakthrough by DL in medical image analysis and other CAD applications for patient care has brought about unprecedented excitement of applying CAD, or artificial intelligence (AI), to medicine in general and to radiology in particular. In this paper, we will provide an overview of the recent developments of CAD using DL in breast imaging and discuss some challenges and practical issues that may impact the advancement of artificial intelligence and its integration into clinical workflow. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190580?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.12116/j.issn.1004-5619.2020.01.017  |   The estimation of postmortem interval （PMI） is a core issue in forensic practice. A large amount of time-dependent data can be produced in the decomposition process of a body, however, such multidimensional data cannot be comprehensively and effectively analyzed and utilized by any existing conventional PMI estimation method. As a rapidly developing information technology, artificial intelligence （AI） has significant advantages in big data processing, due to it's comprehensiveness, efficiency and automation. Some scholars have already applied it to researches on the estimation of PMI, showing it's significant advantages in terms of accuracy and development prospect. This article reviews the significance, mode and progress of application of AI in PMI estimation and provides some suggestions and prospects for future study. 
  题目:  大数据与人工智能：死后间隔时间推断的新视野. 
  摘要:  死后间隔时间（postmortem interval，PMI）推断是法医学实践的核心问题之一。尸体腐败过程可产生大量时间依赖性参数，而现有的任一传统PMI推断方法均无法实现对此类多维度数据全面有效的分析和利用。作为近年来快速发展的信息技术，人工智能（artificial intelligence，AI）在大数据处理方面具有综合、高效、自动化的显著优势，部分学者已将其引入PMI推断研究，展现了其明显优于传统方法的准确性和发展前景。本文将对大数据及AI在PMI推断中应用的意义、模式及研究成果进行综述，并对后续研究提出建议与展望。. 
  关键词:  法医病理学；人工智能；死亡时间推断；综述. 
  |  https://doi.org/10.12116/j.issn.1004-5619.2020.01.017  |  
------------------------------------------- 
10.1038/s41575-019-0240-9  |   Hepatocellular carcinoma (HCC) is the most common form of primary adult liver cancer. After nearly a decade with sorafenib as the only approved treatment, multiple new agents have demonstrated efficacy in clinical trials, including the targeted therapies regorafenib, lenvatinib and cabozantinib, the anti-angiogenic antibody ramucirumab, and the immune checkpoint inhibitors nivolumab and pembrolizumab. Although these agents offer new promise to patients with HCC, the optimal choice and sequence of therapies remains unknown and without established biomarkers, and many patients do not respond to treatment. The advances and the decreasing costs of molecular measurement technologies enable profiling of HCC molecular features (such as genome, transcriptome, proteome and metabolome) at different levels, including bulk tissues, animal models and single cells. The release of such data sets to the public enhances the ability to search for information from these legacy studies and provides the opportunity to leverage them to understand HCC mechanisms, rationally develop new therapeutics and identify candidate biomarkers of treatment response. Here, we provide a comprehensive review of public data sets related to HCC and discuss how emerging artificial intelligence methods can be applied to identify new targets and drugs as well as to guide therapeutic choices for improved HCC treatment. 
  |  http://dx.doi.org/10.1038/s41575-019-0240-9  |  
------------------------------------------- 
10.1038/s41587-019-0387-5  |   A lack of tools to precisely control gene expression has limited our ability to evaluate relationships between expression levels and phenotypes. Here, we describe an approach to titrate expression of human genes using CRISPR interference and series of single-guide RNAs (sgRNAs) with systematically modulated activities. We used large-scale measurements across multiple cell models to characterize activities of sgRNAs containing mismatches to their target sites and derived rules governing mismatched sgRNA activity using deep learning. These rules enabled us to synthesize a compact sgRNA library to titrate expression of ~2,400 genes essential for robust cell growth and to construct an in silico sgRNA library spanning the human genome. Staging cells along a continuum of gene expression levels combined with single-cell RNA-seq readout revealed sharp transitions in cellular behaviors at gene-specific expression thresholds. Our work provides a general tool to control gene expression, with applications ranging from tuning biochemical pathways to identifying suppressors for diseases of dysregulated gene expression. 
  |  https://dx.doi.org/10.1038/s41587-019-0387-5  |  
------------------------------------------- 
10.1111/anae.15022  |    |  https://doi.org/10.1111/anae.15022  |  
------------------------------------------- 
10.1016/j.jenvman.2019.109871  |   Unplanned groundwater exploitation in coastal aquifers results in water decline and consequently triggers saltwater intrusion (SWI). This study formulates a novel modeling strategy based on GALDIT method using Artificial Intelligence (AI) models for mapping the vulnerability to SWI. This AI-based modeling strategy is a two-level learning process, where vulnerability to SWI at Level 1 can be predicted by such models as Artificial Neural Network (ANN), Sugeno Fuzzy Logic (SFL), and Neuro-Fuzzy (NF); and their outputs serve as the input to the model at Level 2, such as Support Vector Machine (SVM). This model is applied to Urmia aquifer, west coast of Lake Urmia, where both are currently declining. The construction of the above four models both at Levels 1 and 2 provide tools for mapping the SWI vulnerability of the study area. Model performances in the paper are studied using RMSE and R<sup>2</sup> metrics, where the models at Level 1 are found to be fit-for-purpose and the SVM at Level 2 is improved particularly with respect to the reduced scale of scatters in the results. Evaluating the result and groundwater samples by Piper diagram confirms the correspondence of SWI status with vulnerability index. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0301-4797(19)31589-0  |  
------------------------------------------- 
PMID:32191221  |   The aim of the study is to describe the effect of robot anxiety. The forming of the concept and the description of similar constructs, like technological anxiety and computer anxiety, is presented through a historical review. Given our current knowledge of this topic it seems that cognitive aspects are the key factors underlying robot anxiety. According to literature dissemination of knowledge appears as the best way to lower anxiety. Attempts to develop cognitive therapies are also mentioned. As presented a widely accepted instrument for the measurement of robot anxiety is yet to be developed. RAS and NARS are introduced as possible scales as they are often adapted to studies although both leave things to be desired. The "Uncanny Valley" is also discussed, as it can explain how the appearance of robots affect the approval they gain from people, thus emphasising the importance of the design of robots. 
  |  None  |  
------------------------------------------- 
10.1097/JU.0000000000000698.01  |    |  https://www.auajournals.org/doi/10.1097/JU.0000000000000698.01  |  
------------------------------------------- 
10.1038/s41467-019-13807-w  |   Finding new molecules with a desired biological activity is an extremely difficult task. In this context, artificial intelligence and generative models have been used for molecular de novo design and compound optimization. Herein, we report a generative model that bridges systems biology and molecular design, conditioning a generative adversarial network with transcriptomic data. By doing so, we can automatically design molecules that have a high probability to induce a desired transcriptomic profile. As long as the gene expression signature of the desired state is provided, this model is able to design active-like molecules for desired targets without any previous target annotation of the training compounds. Molecules designed by this model are more similar to active compounds than the ones identified by similarity of gene expression signatures. Overall, this method represents an alternative approach to bridge chemistry and biology in the long and difficult road of drug discovery. 
  |  http://dx.doi.org/10.1038/s41467-019-13807-w  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31900408/  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_8  |   Early detection of glaucoma is important to slow down progression of the disease and to prevent total vision loss. Retinal fundus photography is frequently obtained for various eye disease diagnosis and record and is a suitable screening exam for its simplicity and low cost. However, the number of ophthalmologists who are specialized in glaucoma diagnosis is limited. We have been studying automated schemes for detection of nerve fiber layer defects and analysis of optic disc deformation, two major signs of glaucoma, in assisting ophthalmologists' accurate and efficient diagnosis. In this chapter, our recent progress in computerized methods is discussed. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_8  |  
------------------------------------------- 
10.1016/j.urology.2019.07.056  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0090-4295(19)30874-X  |  
------------------------------------------- 
10.1097/ACO.0000000000000834  |    Purpose of review:  The availability of large datasets and computational power has prompted a revolution in Intensive Care. Data represent a great opportunity for clinical practice, benchmarking, and research. Machine learning algorithms can help predict events in a way the human brain can simply not process. This possibility comes with benefits and risks for the clinician, as finding associations does not mean proving causality. 
  Recent findings:  Current applications of Data Science still focus on data documentation and visualization, and on basic rules to identify critical lab values. Recently, algorithms have been put in place for prediction of outcomes such as length of stay, mortality, and development of complications. These results have begun being implemented for more efficient allocation of resources and in benchmarking processes, to allow identification of successful practices and margins for improvement. In parallel, machine learning models are increasingly being applied in research to expand medical knowledge. 
  Summary:  Data have always been part of the work of intensivists, but the current availability has not been completely exploited. The intensive care community has to embrace and guide the data science revolution in order to decline it in favor of patients' care. 
  |  http://dx.doi.org/10.1097/ACO.0000000000000834  |  
------------------------------------------- 
10.1016/j.neunet.2019.09.024  |   Machine learning is yielding unprecedented interest in research and industry, due to recent success in many applied contexts such as image classification and object recognition. However, the deployment of these systems requires huge computing capabilities, thus making them unsuitable for embedded systems. To deal with this limitation, many researchers are investigating brain-inspired computing, which would be a perfect alternative to the conventional Von Neumann architecture based computers (CPU/GPU) that meet the requirements for computing performance, but not for energy-efficiency. Therefore, neuromorphic hardware circuits that are adaptable for both parallel and distributed computations need to be designed. In this paper, we focus on Spiking Neural Networks (SNNs) with a comprehensive study of neural coding methods and hardware exploration. In this context, we propose a framework for neuromorphic hardware design space exploration, which allows to define a suitable architecture based on application-specific constraints and starting from a wide variety of possible architectural choices. For this framework, we have developed a behavioral level simulator for neuromorphic hardware architectural exploration named NAXT. Moreover, we propose modified versions of the standard Rate Coding technique to make trade-offs with the Time Coding paradigm, which is characterized by the low number of spikes propagating in the network. Thus, we are able to reduce the number of spikes while keeping the same neuron's model, which results in an SNN with fewer events to process. By doing so, we seek to reduce the amount of power consumed by the hardware. Furthermore, we present three neuromorphic hardware architectures in order to quantitatively study the implementation of SNNs. One of these architectures integrates a novel hybrid structure: a highly-parallel computation core for most solicited layers, and time-multiplexed computation units for deeper layers. These architectures are derived from a novel funnel-like Design Space Exploration framework for neuromorphic hardware. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(19)30304-1  |  
------------------------------------------- 
10.1097/ICU.0000000000000650  |    Purpose of review:  Pediatric retina is an exciting, but also challenging field, where patient age and cooperation can limit ease of diagnosis of a broad range of congenital and acquired diseases, inherited retinal degenerations are mostly untreatable and surgical outcomes can be quite different from those for adults. This review aims to highlight some recent advances and trends that are improving our ability to care for children with retinal conditions. 
  Recent findings:  Studies have demonstrated the feasibility of multimodal imaging even in nonsedated infants, with portable optical coherence tomography (OCT) and OCT angiography in particular offering structural insights into diverse pediatric retinal conditions. Encouraging long-term outcomes of subretinal voretigene neparvovec-rzyl injection for RPE65 mutation-associated Leber congenital amaurosis have inspired research on the optimization of subretinal gene delivery and gene therapy for other inherited retinal degenerations. In retinopathy of prematurity, machine learning and smartphone-based imaging can facilitate screening, and studies have highlighted favorable outcomes from intravitreal anti-vascular endothelial growth factor (anti-VEGF) injections. A nomogram for pediatric pars plana sclerotomy site placement may improve safety in complex surgeries. 
  Summary:  Multimodal imaging, gene therapy, machine learning and surgical innovation have been and will continue to be important to advances in pediatric retina. 
  |  http://dx.doi.org/10.1097/ICU.0000000000000650  |  
------------------------------------------- 
10.1016/j.bulcan.2020.01.008  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0007-4551(20)30011-4  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_11  |   Advancements in musculoskeletal analysis have been achieved by adopting deep learning technology in image recognition and analysis. Unlike musculoskeletal modeling based on computational anatomy, deep learning-based methods can obtain muscle information automatically. Through analysis of image features, both approaches can obtain muscle characteristics such as shape, volume, and area, and derive additional information by analyzing other image textures. In this chapter, we first discuss the necessity of musculoskeletal analysis and the required image processing technology. Then, the limitations of skeletal muscle recognition based on conventional handcrafted features are discussed, and developments in skeletal muscle recognition using machine learning and deep learning technology are described. Next, a technique for analyzing musculoskeletal systems using whole-body computed tomography (CT) images is shown. This study aims to achieve automatic recognition of skeletal muscles throughout the body and automatic classification of atrophic muscular disease using only image features, to demonstrate an application of whole-body musculoskeletal analysis driven by deep learning. Finally, we discuss future development of musculoskeletal analysis that effectively combines deep learning with handcrafted feature-based modeling techniques. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_11  |  
------------------------------------------- 
10.1080/17453674.2019.1711323  |   Artificial intelligence (AI) is a general term that implies the use of a computer to model intelligent behavior with minimal human intervention. AI, particularly deep learning, has recently made substantial strides in perception tasks allowing machines to better represent and interpret complex data. Deep learning is a subset of AI represented by the combination of artificial neuron layers. In the last years, deep learning has gained great momentum. In the field of orthopaedics and traumatology, some studies have been done using deep learning to detect fractures in radiographs. Deep learning studies to detect and classify fractures on computed tomography (CT) scans are even more limited. In this narrative review, we provide a brief overview of deep learning technology: we (1) describe the ways in which deep learning until now has been applied to fracture detection on radiographs and CT examinations; (2) discuss what value deep learning offers to this field; and finally (3) comment on future directions of this technology. 
  |  http://www.tandfonline.com/doi/full/10.1080/17453674.2019.1711323  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31928116/  |  
------------------------------------------- 
10.1038/s41591-019-0715-9  |   Intraoperative diagnosis is essential for providing safe and effective care during cancer surgery<sup>1</sup>. The existing workflow for intraoperative diagnosis based on hematoxylin and eosin staining of processed tissue is time, resource and labor intensive<sup>2,3</sup>. Moreover, interpretation of intraoperative histologic images is dependent on a contracting, unevenly distributed, pathology workforce<sup>4</sup>. In the present study, we report a parallel workflow that combines stimulated Raman histology (SRH)<sup>5-7</sup>, a label-free optical imaging method and deep convolutional neural networks (CNNs) to predict diagnosis at the bedside in near real-time in an automated fashion. Specifically, our CNNs, trained on over 2.5 million SRH images, predict brain tumor diagnosis in the operating room in under 150 s, an order of magnitude faster than conventional techniques (for example, 20-30 min)<sup>2</sup>. In a multicenter, prospective clinical trial (n = 278), we demonstrated that CNN-based diagnosis of SRH images was noninferior to pathologist-based interpretation of conventional histologic images (overall accuracy, 94.6% versus 93.9%). Our CNNs learned a hierarchy of recognizable histologic feature representations to classify the major histopathologic classes of brain tumors. In addition, we implemented a semantic segmentation method to identify tumor-infiltrated diagnostic regions within SRH images. These results demonstrate how intraoperative cancer diagnosis can be streamlined, creating a complementary pathway for tissue diagnosis that is independent of a traditional pathology laboratory. 
  |  http://dx.doi.org/10.1038/s41591-019-0715-9  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_7  |   At medical checkups or mass screenings, the fundus examination is effective for early detection of systemic hypertension, arteriosclerosis, diabetic retinopathy, etc. In most cases, ophthalmologists and physicians grade retinal images by the condition of the blood vessels, lesions. However, human observation does not provide quantitative results, thus blood vessel analysis is an important process in determining hypertension and arteriosclerosis, quantitatively. This chapter describes the latest automated blood vessel extraction using the deep convolution neural network (DCNN). Diabetic retinopathy is a common cardiovascular disease and a major factor in blindness. Therefore, early detection of diabetic retinopathy is very important to preventing blindness. A microaneurysm is an initial sign of diabetic retinopathy, and much research has been conducted for microaneurysm detection. This chapter also describes diabetic retinopathy detection and automated microaneurysm detection using the DCNN. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_7  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_4  |   For computer-aided diagnosis (CAD), detection, segmentation, and classification from medical imagery are three key components to efficiently assist physicians for accurate diagnosis. In this chapter, a completely integrated CAD system based on deep learning is presented to diagnose breast lesions from digital X-ray mammograms involving detection, segmentation, and classification. To automatically detect breast lesions from mammograms, a regional deep learning approach called You-Only-Look-Once (YOLO) is used. To segment breast lesions, full resolution convolutional network (FrCN), a novel segmentation model of deep network, is implemented and used. Finally, three conventional deep learning models including regular feedforward CNN, ResNet-50, and InceptionResNet-V2 are separately adopted and used to classify or recognize the detected and segmented breast lesion as either benign or malignant. To evaluate the integrated CAD system for detection, segmentation, and classification, the publicly available and annotated INbreast database is used over fivefold cross-validation tests. The evaluation results of the YOLO-based detection achieved detection accuracy of 97.27%, Matthews's correlation coefficient (MCC) of 93.93%, and F1-score of 98.02%. Moreover, the results of the breast lesion segmentation via FrCN achieved an overall accuracy of 92.97%, MCC of 85.93%, Dice (F1-score) of 92.69%, and Jaccard similarity coefficient of 86.37%. The detected and segmented breast lesions are classified via CNN, ResNet-50, and InceptionResNet-V2 achieving an average overall accuracies of 88.74%, 92.56%, and 95.32%, respectively. The performance evaluation results through all stages of detection, segmentation, and classification show that the integrated CAD system outperforms the latest conventional deep learning methodologies. We conclude that our CAD system could be used to assist radiologists over all stages of detection, segmentation, and classification for diagnosis of breast lesions. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_4  |  
------------------------------------------- 
10.1176/appi.psychotherapy.20200002  |    |  https://psychotherapy.psychiatryonline.org/doi/full/10.1176/appi.psychotherapy.20200002?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/S0140-6736(19)32987-3  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)32987-3  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_5  |   Lung cancer is the most common cancer among men and the third most common among women in the world. Many diagnostic techniques have been introduced to diagnose lung cancer. Positron emission tomography (PET)/computed tomography (CT) examination is an image diagnostic method that performs automatic detection and distinction of lung lesions. In addition, pathological examination by biopsy is performed for lesions that are suspected of being malignant, and appropriate treatment methods are applied according to the diagnosis results. Currently, lung cancer diagnosis is performed through coordination between respiratory, radiation, and pathological diagnosis experts, but there are some tasks, such as image diagnosis, that require a large amount of time and effort to complete. Therefore, we developed a decision support system using PET/CT and microscopic images at the time of image diagnosis, which leads to appropriate treatment. In this chapter, we introduce the proposed system using deep learning and radiomic techniques. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_5  |  
------------------------------------------- 
10.1126/science.abb9369  |    |  http://www.sciencemag.org/cgi/pmidlookup?view=long&pmid=32241926  |  
------------------------------------------- 
PMID:32239834  |   Recent progress in molecular engineering, digital imaging and artificial intelligence improve human modern medicine to levels never seen before. Digital pathology becomes the new standard of patient care in dermatology and personalized medicine. It is increasingly used for digital exchange of histological slides, personalized consultations, tumor boards, quantitative image analysis for research purposes and in education. Digital pathology allows automatization and quantification with greater consistency and accuracy than light microscopy. Personalized dermatology is focusing on tailoring therapy to the individual characteristics of each patient and allow to use genetic information in order to develop a treatment plan, uniquely suited to each patient, which in turn leads to improved quality of care and management of each individual. 
 L’ingénierie moléculaire, l’imagerie digitale et l’intelligence artificielle (IA) améliorent la médecine moderne à des niveaux jamais vus auparavant. La pathologie digitale (PD) est progressivement utilisée pour l’échange digital de lames histologiques produites en routine, les consultations personnalisées, les tumor boards, l’analyse quantitative d’images à des buts de recherche et dans l’éducation, et enfin l’archivage. La PD permet l’automatisation et la quantification avec plus de cohérence et de précision que la microscopie optique. La dermatologie personnalisée se concentre sur l’adaptation de la thérapie aux caractéristiques individuelles de chaque patient et permet d’utiliser les données génétiques afin de développer un plan de traitement individuellement adapté, en améliorant la qualité des soins et la prise en charge. 
  |  None  |  
------------------------------------------- 
10.7507/1001-5515.201904017  |   With the change of medical diagnosis and treatment mode, the quality of medical image directly affects the diagnosis and treatment of the disease for doctors. Therefore, realization of intelligent image quality control by computer will have a greater auxiliary effect on the radiographer's filming work. In this paper, the research methods and applications of image segmentation model and image classification model in the field of deep learning and traditional image processing algorithm applied to medical image quality evaluation are described. The results demonstrate that deep learning algorithm is more accurate and efficient than the traditional image processing algorithm in the effective training of medical image big data, which explains the broad application prospect of deep learning in the medical field. This paper developed a set of intelligent quality control system for auxiliary filming, and successfully applied it to the Radiology Department of West China Hospital and other city and county hospitals, which effectively verified the feasibility and stability of the quality control system. 
 随着医学诊断、治疗模式的改变，医学影像的质量直接影响着医生对病情的诊断和治疗。因此，通过计算机实现智能影像质控对放射科技师的拍片工作会有较大的辅助作用。本文拟就深度学习领域中的图像分割模型、图像分类模型结合传统图像处理算法应用于医学影像质量评价的研究方法及应用情况予以阐述。我们发现使用深度学习算法对医学影像大数据进行有效训练，提取出来的特征相比于单纯使用传统图像处理算法更加准确、高效，诠释了深度学习在医疗领域的广阔应用前景。本文开发出了一套辅助拍片智能质控系统，并成功应用到了华西医院和其他市、县级医院的放射科，有效验证了该质控系统的可行性与稳定性。. 
  |  None  |  
------------------------------------------- 
10.1007/s00117-019-00617-w  |    Clinical issue:  The reproducible and exhaustive extraction of information from radiological images is a central task in the practice of radiology. Dynamic developments in the fields of artificial intelligence (AI) and machine learning are introducing new methods for this task. Radiomics is one such method and offers new opportunities and challenges for the future of radiology. 
  Methodological innovations:  Radiomics describes the quantitative evaluation, interpretation, and clinical assessment of imaging markers in radiological data. Components of a radiomics analysis are data acquisition, data preprocessing, data management, segmentation of regions of interest, computation and selection of imaging markers, as well as the development of a radiomics model used for diagnosis and prognosis. This article explains these components and aims at providing an introduction to the field of radiomics while highlighting existing limitations. 
  Materials and methods:  This article is based on a selective literature search with the PubMed search engine. 
  Assessment:  Even though radiomics applications have yet to arrive in routine clinical practice, the quantification of radiological data in terms of radiomics is underway and will increase in the future. This holds the potential for lasting change in the discipline of radiology. Through the successful extraction and interpretation of all the information encoded in radiological images the next step in the direction of a more personalized, future-oriented form of medicine can be taken. 
  |  https://dx.doi.org/10.1007/s00117-019-00617-w  |  
------------------------------------------- 
10.1007/s00117-019-00616-x  |    Background:  The methods of machine learning and artificial intelligence are slowly but surely being introduced in everyday medical practice. In the future, they will support us in diagnosis and therapy and thus improve treatment for the benefit of the individual patient. It is therefore important to deal with this topic and to develop a basic understanding of it. 
  Objectives:  This article gives an overview of the exciting and dynamic field of machine learning and serves as an introduction to some methods primarily from the realm of supervised learning. In addition to definitions and simple examples, limitations are discussed. 
  Conclusions:  The basic principles behind the methods are simple. Nevertheless, due to their high dimensional nature, the factors influencing the results are often difficult or impossible to understand by humans. In order to build confidence in the new technologies and to guarantee their safe application, we need explainable algorithms and prospective effectiveness studies. 
  |  https://dx.doi.org/10.1007/s00117-019-00616-x  |  
------------------------------------------- 
10.1016/j.amjmed.2019.08.015  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(19)30711-9  |  
------------------------------------------- 
10.1126/science.aay8086  |    |  http://www.sciencemag.org/cgi/pmidlookup?view=long&pmid=32054749  |  
------------------------------------------- 
10.1056/NEJMp1912079  |    |  http://www.nejm.org/doi/full/10.1056/NEJMp1912079?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1038/s41587-019-0375-9  |    |  https://dx.doi.org/10.1038/s41587-019-0375-9  |  
------------------------------------------- 
10.5867/medwave.2020.03.7867  |    Introduction:  The evidence on COVID-19 is being produced at high speed, so it is challenging for decision-makers to keep up. It seems appropriate, then, to put into practice a novel approach able to provide the scientific community and other interested parties with quality evidence that is actionable, and rapidly and efficiently produced. 
  Methods and analysis:  We designed a protocol for multiple parallel systematic reviews and overviews of systematic reviews in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols (PRISMA-P). We will search for primary studies and systematic reviews that answer different questions related to COVID-19 using both a centralized repository (Epistemonikos database) and a manual search in MEDLINE/PubMed, EMBASE, and the Cochrane Central Register of Controlled Trials. We will also search for literature in several other sources. At least two researchers will independently undertake the selection of studies, data extraction, and assessment of the quality of the included studies. We will synthesize data for each question using meta-analysis, when possible, and we will prepare Summary of Findings tables according to the GRADE approach. All the evidence will be organized in an open platform (L·OVE - Living OVerview of Evidence) that will be continuously updated using artificial intelligence and a broad network of experts. 
  Ethics and dissemination:  No ethics approval is considered necessary. The results of these articles will be widely disseminated via peer-reviewed publications, social networks, and traditional media, and will be sent to relevant international organizations discussing this topic. 
  |  None  |  
------------------------------------------- 
10.1038/s41591-019-0703-0  |    |  http://dx.doi.org/10.1038/s41591-019-0703-0  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_2  |   Medical images have been widely used in clinics, providing visual representations of under-skin tissues in human body. By applying different imaging protocols, diverse modalities of medical images with unique characteristics of visualization can be produced. Considering the cost of scanning high-quality single modality images or homogeneous multiple modalities of images, medical image synthesis methods have been extensively explored for clinical applications. Among them, deep learning approaches, especially convolutional neural networks (CNNs) and generative adversarial networks (GANs), have rapidly become dominating for medical image synthesis in recent years. In this chapter, based on a general review of the medical image synthesis methods, we will focus on introducing typical CNNs and GANs models for medical image synthesis. Especially, we will elaborate our recent work about low-dose to high-dose PET image synthesis, and cross-modality MR image synthesis, using these models. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_2  |  
------------------------------------------- 
10.1371/journal.pone.0227494  |   This paper proposes an approach to accurately estimate the impedance value of a high impedance fault (HIF) and the distance from its fault location for a distribution system. Based on the three-phase voltage and current waveforms which are monitored through a single measurement in the network, several features are extracted using discrete wavelet transform (DWT). The extracted features are then fed into the optimized artificial neural network (ANN) to estimate the HIF impedance and its distance. The particle swarm optimization (PSO) technique is employed to optimize the parameters of the ANN to enhance the performance of fault impedance and distance estimations. Based on the simulation results, the proposed method records encouraging results compared to other methods of similar complexity for both HIF impedance values and estimated distances. 
  |  http://dx.plos.org/10.1371/journal.pone.0227494  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31999711/  |  
------------------------------------------- 
10.1016/j.amjmed.2019.10.012  |    |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(19)30954-4  |  
------------------------------------------- 
10.1038/s41467-020-14874-0  |   Phosphocreatine (PCr) plays a vital role in neuron and myocyte energy homeostasis. Currently, there are no routine diagnostic tests to noninvasively map PCr distribution with clinically relevant spatial resolution and scan time. Here, we demonstrate that artificial neural network-based chemical exchange saturation transfer (ANNCEST) can be used to rapidly quantify PCr concentration with robust immunity to commonly seen MRI interferences. High-quality PCr mapping of human skeletal muscle, as well as the information of exchange rate, magnetic field and radio-frequency transmission inhomogeneities, can be obtained within 1.5 min on a 3 T standard MRI scanner using ANNCEST. For further validation, we apply ANNCEST to measure the PCr concentrations in exercised skeletal muscle. The ANNCEST outcomes strongly correlate with those from <sup>31</sup>P magnetic resonance spectroscopy (R = 0.813, p &lt; 0.001, t test). These results suggest that ANNCEST has potential as a cost-effective and widely available method for measuring PCr and diagnosing related diseases. 
  |  http://dx.doi.org/10.1038/s41467-020-14874-0  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32102999/  |  
------------------------------------------- 
10.1038/s41467-019-13827-6  |   Neuromorphic computing based on spikes offers great potential in highly efficient computing paradigms. Recently, several hardware implementations of spiking neural networks based on traditional complementary metal-oxide semiconductor technology or memristors have been developed. However, an interface (called an afferent nerve in biology) with the environment, which converts the analog signal from sensors into spikes in spiking neural networks, is yet to be demonstrated. Here we propose and experimentally demonstrate an artificial spiking afferent nerve based on highly reliable NbO<sub>x</sub> Mott memristors for the first time. The spiking frequency of the afferent nerve is proportional to the stimuli intensity before encountering noxiously high stimuli, and then starts to reduce the spiking frequency at an inflection point. Using this afferent nerve, we further build a power-free spiking mechanoreceptor system with a passive piezoelectric device as the tactile sensor. The experimental results indicate that our afferent nerve is promising for constructing self-aware neurorobotics in the future. 
  |  http://dx.doi.org/10.1038/s41467-019-13827-6  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31896758/  |  
------------------------------------------- 
10.1038/s41587-019-0368-8  |   Detection of mosaic mutations that arise in normal development is challenging, as such mutations are typically present in only a minute fraction of cells and there is no clear matched control for removing germline variants and systematic artifacts. We present MosaicForecast, a machine-learning method that leverages read-based phasing and read-level features to accurately detect mosaic single-nucleotide variants and indels, achieving a multifold increase in specificity compared with existing algorithms. Using single-cell sequencing and targeted sequencing, we validated 80-90% of the mosaic single-nucleotide variants and 60-80% of indels detected in human brain whole-genome sequencing data. Our method should help elucidate the contribution of mosaic somatic mutations to the origin and development of disease. 
  |  https://dx.doi.org/10.1038/s41587-019-0368-8  |  
------------------------------------------- 
10.1254/fpj.19132  |   Animals can make appropriate decisions based on sensory information about the environment. Vision is one of the most critical ability for survival in dynamic situations in nature, particularly for mammalian species, such as primates, carnivores, and rodents. Although there is a huge computational cost involved in processing visual information, the brain can perform this task very rapidly using well-organized parallel and hierarchical neural circuits, enabling animals to rapidly sense the environment and, in turn, perform adaptive actions. Physiological, psychophysical, and clinical studies over hundreds of years have delineated the neural circuit mechanisms of the visual system. Artificial intelligence and robotics have also started making progress in this area. However, due to technical limitations, there are still many open questions that elude explanation in understanding the neural mechanism of visuomotor integration. Herein, we initially describe the anatomical structures of occipital cortices related to vision and then provide an overview of the physiological and clinical studies of the dorsal visual pathway related to spatial perception and prediction in non-human primate species. Finally, we introduce recent approaches in which rodents have been used as model species to elucidate the neural circuit mechanism of visually-guided behavior. Uncovering neural implementation of the association between visual-spatial perception and visuomotor function could provide key insights into the engineering of highly active robots and could also contribute to the development of novel therapeutic strategies addressing visual impairment and psychiatric/neurological disorders. 
  |  https://dx.doi.org/10.1254/fpj.19132  |  
------------------------------------------- 
10.1007/978-3-030-33128-3_3  |   Image-based computer-aided diagnosis (CAD) algorithms by the use of convolutional neural network (CNN) which do not require the image-feature extractor are powerful compared with conventional feature-based CAD algorithms which require the image-feature extractor for classification of lung abnormalities. Moreover, computer-aided detection and segmentation algorithms by the use of CNN are useful for analysis of lung abnormalities. Deep learning will improve the performance of CAD systems dramatically. Therefore, they will change the roles of radiologists in the near future. In this article, we introduce development and evaluation of such image-based CAD algorithms for various kinds of lung abnormalities such as lung nodules and diffuse lung diseases. 
  |  https://dx.doi.org/10.1007/978-3-030-33128-3_3  |  
------------------------------------------- 
10.1038/s41591-020-0789-4  |   Intensive-care clinicians are presented with large quantities of measurements from multiple monitoring systems. The limited ability of humans to process complex information hinders early recognition of patient deterioration, and high numbers of monitoring alarms lead to alarm fatigue. We used machine learning to develop an early-warning system that integrates measurements from multiple organ systems using a high-resolution database with 240 patient-years of data. It predicts 90% of circulatory-failure events in the test set, with 82% identified more than 2 h in advance, resulting in an area under the receiver operating characteristic curve of 0.94 and an area under the precision-recall curve of 0.63. On average, the system raises 0.05 alarms per patient and hour. The model was externally validated in an independent patient cohort. Our model provides early identification of patients at risk for circulatory failure with a much lower false-alarm rate than conventional threshold-based systems. 
  |  http://dx.doi.org/10.1038/s41591-020-0789-4  |  
------------------------------------------- 
10.1038/s41596-019-0251-6  |   DNA methylation data-based precision cancer diagnostics is emerging as the state of the art for molecular tumor classification. Standards for choosing statistical methods with regard to well-calibrated probability estimates for these typically highly multiclass classification tasks are still lacking. To support this choice, we evaluated well-established machine learning (ML) classifiers including random forests (RFs), elastic net (ELNET), support vector machines (SVMs) and boosted trees in combination with post-processing algorithms and developed ML workflows that allow for unbiased class probability (CP) estimation. Calibrators included ridge-penalized multinomial logistic regression (MR) and Platt scaling by fitting logistic regression (LR) and Firth's penalized LR. We compared these workflows on a recently published brain tumor 450k DNA methylation cohort of 2,801 samples with 91 diagnostic categories using a 5 × 5-fold nested cross-validation scheme and demonstrated their generalizability on external data from The Cancer Genome Atlas. ELNET was the top stand-alone classifier with the best calibration profiles. The best overall two-stage workflow was MR-calibrated SVM with linear kernels closely followed by ridge-calibrated tuned RF. For calibration, MR was the most effective regardless of the primary classifier. The protocols developed as a result of these comparisons provide valuable guidance on choosing ML workflows and their tuning to generate well-calibrated CP estimates for precision diagnostics using DNA methylation data. Computation times vary depending on the ML algorithm from &lt;15 min to 5 d using multi-core desktop PCs. Detailed scripts in the open-source R language are freely available on GitHub, targeting users with intermediate experience in bioinformatics and statistics and using R with Bioconductor extensions. 
  |  http://dx.doi.org/10.1038/s41596-019-0251-6  |  
------------------------------------------- 
10.1038/s41592-020-0779-y  |    |  http://dx.doi.org/10.1038/s41592-020-0779-y  |  
------------------------------------------- 
10.1134/S000629792001006X  |   Human genome contains ca. 20,000 protein-coding genes that could be translated into millions of unique protein species (proteoforms). Proteoforms coded by a single gene often have different functions, which implies different protein partners. By interacting with each other, proteoforms create a network reflecting the dynamics of cellular processes in an organism. Perturbations of protein-protein interactions change the network topology, which often triggers pathological processes. Studying proteoforms is a relatively new research area in proteomics, and this is why there are comparatively few experimental studies on the interaction of proteoforms. Bioinformatics tools can facilitate such studies by providing valuable complementary information to the experimental data and, in particular, expanding the possibilities of the studies of proteoform interactions. 
  |  https://dx.doi.org/10.1134/S000629792001006X  |  
------------------------------------------- 
10.1038/s41587-019-0376-8  |    |  https://dx.doi.org/10.1038/s41587-019-0376-8  |  
------------------------------------------- 
10.1038/d41586-020-00592-6  |    |  https://doi.org/10.1038/d41586-020-00592-6  |  
------------------------------------------- 
10.1097/MD.0000000000019022  |    Background:  Hutchinson-Gilford Progeria syndrome (HGPS) is a rare lethal premature and accelerated aging disease caused by mutations in the lamin A/C gene. Nevertheless, the mechanisms of cellular damage, senescence, and accelerated aging in HGPS are not fully understood. Therefore, we aimed to screen potential key genes, pathways, and therapeutic agents of HGPS by using bioinformatics methods in this study. 
  Methods:  The gene expression profile of GSE113648 and GSE41751 were retrieved from the gene expression omnibus database and analyzed to identify the differentially expressed genes (DEGs) between HGPS and normal controls. Then, gene ontology and the Kyoto encyclopedia of genes and genomes pathway enrichment analysis were carried out. To construct the protein-protein interaction (PPI) network, we used STRING and Cytoscape to make module analysis of these DEGs. Besides, the connectivity map (cMAP) tool was used as well to predict potential drugs. 
  Results:  As a result, 180 upregulated DEGs and 345 downregulated DEGs were identified, which were significantly enriched in pathways in cancer and PI3K-Akt signaling pathway. The top centrality hub genes fibroblast growth factor 2, decorin, matrix metallopeptidase2, and Fos proto-oncogene, AP-1 transcription factor subunit were screened out as the critical genes among the DEGs from the PPI network. Dexibuprofen and parthenolide were predicted to be the possible agents for the treatment of HGPS by cMAP analysis. 
  Conclusion:  This study identified key genes, signal pathways and therapeutic agents, which might help us improve our understanding of the mechanisms of HGPS and identify some new therapeutic agents for HGPS. 
  |  http://dx.doi.org/10.1097/MD.0000000000019022  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32049798/  |  
------------------------------------------- 
10.1097/ALN.0000000000002960  |   Artificial intelligence has been advancing in fields including anesthesiology. This scoping review of the intersection of artificial intelligence and anesthesia research identified and summarized six themes of applications of artificial intelligence in anesthesiology: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event and risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Based on papers identified in the review, several topics within artificial intelligence were described and summarized: (1) machine learning (including supervised, unsupervised, and reinforcement learning), (2) techniques in artificial intelligence (e.g., classical machine learning, neural networks and deep learning, Bayesian methods), and (3) major applied fields in artificial intelligence.The implications of artificial intelligence for the practicing anesthesiologist are discussed as are its limitations and the role of clinicians in further developing artificial intelligence for use in clinical care. Artificial intelligence has the potential to impact the practice of anesthesiology in aspects ranging from perioperative support to critical care delivery to outpatient pain management. 
  |  http://anesthesiology.pubs.asahq.org/article.aspx?doi=10.1097/ALN.0000000000002960  |  
------------------------------------------- 
10.1259/bjr.20190855  |   Advances in computing hardware and software platforms have led to the recent resurgence in artificial intelligence (AI) touching almost every aspect of our daily lives by its capability for automating complex tasks or providing superior predictive analytics. AI applications are currently spanning many diverse fields from economics to entertainment, to manufacturing, as well as medicine. Since modern AI's inception decades ago, practitioners in radiological sciences have been pioneering its development and implementation in medicine, particularly in areas related to diagnostic imaging and therapy. In this anniversary article, we embark on a journey to reflect on the learned lessons from past AI's chequered history. We further summarize the current status of AI in radiological sciences, highlighting, with examples, its impressive achievements and effect on re-shaping the practice of medical imaging and radiotherapy in the areas of computer-aided detection, diagnosis, prognosis, and decision support. Moving beyond the commercial hype of AI into reality, we discuss the current challenges to overcome, for AI to achieve its promised hope of providing better precision healthcare for each patient while reducing cost burden on their families and the society at large. 
  |  http://www.birpublications.org/doi/full/10.1259/bjr.20190855?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.neuron.2020.01.014  |   An international group of researchers met in November 2019 in Beijing to explore the intersection of neuroscience and AI. The aim was to offer a fertile ground for stimulating discussions and ideas, including issues such as policy making and the future of neuroscience and AI across the globe. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0896-6273(20)30040-4  |  
------------------------------------------- 
10.1136/bmj.m1326  |    |  http://www.bmj.com/cgi/pmidlookup?view=long&pmid=32245846  |  
------------------------------------------- 
10.1007/s00117-019-00613-0  |    Clinical/methodical issue:  In view of the diagnostic complexity and the large number of examinations, modern radiology is challenged to identify clinically significant prostate cancer (PCa) with high sensitivity and specificity. Meanwhile overdiagnosis and overtreatment of clinically nonsignificant carcinomas need to be avoided. 
  Standard radiological methods:  Increasingly, international guidelines recommend multiparametric magnetic resonance imaging (mpMRI) as first-line investigation in patients with suspected PCa. 
  Methodical innovations:  Image interpretation according to the PI-RADS criteria is limited by interobserver variability. Thus, rapid developments in the field of automated image analysis tools, including radiomics and artificial intelligence (AI; machine learning, deep learning), give hope for further improvement in patient care. 
  Performance:  AI focuses on the automated detection and classification of PCa, but it also attempts to stratify tumor aggressiveness according to the Gleason score. Recent studies present good to very good results in radiomics or AI-supported mpMRI diagnosis. Nevertheless, these systems are not widely used in clinical practice. 
  Achievements and practical recommendations:  In order to apply these innovative technologies, a growing awareness for the need of structured data acquisition, development of robust systems and an increased acceptance of AI as diagnostic support are needed. If AI overcomes these obstacles, it may play a key role in the quantitative and reproducible image-based diagnosis of ever-increasing prostate MRI examination volumes. 
  |  https://dx.doi.org/10.1007/s00117-019-00613-0  |  
------------------------------------------- 
10.1007/s00117-019-00621-0  |    Clinical/methodological issue:  Artificial intelligence (AI) is being increasingly used in the field of radiology. The aim of this review is to illustrate the developments expected in the next 5 to 10 years as well as possible advantages and risks. 
  Standard radiological methods:  Currently, all computed tomography (CT) images are reconstructed using programmed algorithms. Pathologies are detected by the radiologist with a high expenditure of time and evaluated using standardized procedures. 
  Methodological innovations:  AI can potentially provide a significant improvement to all these standard procedures in the future. CT reconstructions can be significantly enhanced using generative adversarial networks (GAN). Histology can be evaluated using radiomics or deep learning (DL)-based image analysis and the prognosis of the patient can be predicted highly individualized. 
  Performance:  The performance of the networks is strongly influenced by data quality and requires extensive validation. The ability and willingness of the manufacturers to integrate these into the existing RIS/PACS systems is also decisive. 
  Evaluation:  AI will have a large impact on the daily clinical work of radiologists. However, publications on the risks of the technology and on adequate validation are still lacking. In addition to opening new fields of application, further research regarding possible risks is warranted. 
  Practical recommendations:  In the next 5 to 10 years, AI will improve and facilitate work in clinical practice. The integration of the applications into the existing RIS/PACS systems is expected to take place via app stores and/or existing teleradiology networks. 
  |  https://dx.doi.org/10.1007/s00117-019-00621-0  |  
------------------------------------------- 
10.1038/s41581-019-0220-x  |    |  http://dx.doi.org/10.1038/s41581-019-0220-x  |  
------------------------------------------- 
10.1007/s00117-019-00615-y  |    Background:  Artificial intelligence (AI) is increasingly applied in the field of breast imaging. 
  Objectives:  What are the main areas where AI is applied in breast imaging and what AI and computer-aided diagnosis (CAD) systems are already available? 
  Materials and methods:  Basic literature and vendor-supplied information are screened for relevant information, which is then pooled, structured and discussed from the perspective of breast imaging. 
  Results:  Original CAD systems in mammography date almost 25 years back. They are much more widely applied in the United States than in Europe. The initial CAD systems exhibited limited diagnostic abilities and disproportionally high rates of false positive results. Since 2012, deep learning mechanisms have been applied and expand the application possibilities of AI. 
  Conclusion:  To date there is no algorithm that has beyond doubt been proven to outperform double reporting by two certified breast radiologists. AI could, however, in the foreseeable future, take over the following tasks: preselection of abnormal examinations to substantially reduce workload of the radiologists by either excluding normal findings from human review or by replacing the double reader in screening. Furthermore, the establishment of radio-patho-genomic correlations and their translation into clinical practice is hardly conceivable without AI. 
  |  https://dx.doi.org/10.1007/s00117-019-00615-y  |  
------------------------------------------- 
10.4274/tjo.galenos.2020.78989  |   Artificial intelligence is advancing rapidly and making its way into all areas of our lives. This review discusses developments and potential practices regarding the use of artificial intelligence in the field of ophthalmology, and the related topic of medical ethics. Various artificial intelligence applications related to the diagnosis of eye diseases were researched in books, journals, search engines, print and social media. Resources were cross-checked to verify the information. Artificial intelligence algorithms, some of which were approved by the US Food and Drug Administration, have been adopted in the field of ophthalmology, especially in diagnostic studies. Studies are being conducted that prove that artificial intelligence algorithms can be used in the field of ophthalmology, especially in diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity. Some of these algorithms have come to the approval stage. The current point in artificial intelligence studies shows that this technology has advanced considerably and shows promise for future work. It is believed that artificial intelligence applications will be effective in identifying patients with preventable vision loss and directing them to physicians, especially in developing countries where there are fewer trained professionals and physicians are difficult to reach. When we consider the possibility that some future artificial intelligence systems may be candidates for moral/ethical status, certain ethical issues arise. Questions about moral/ethical status are important in some areas of applied ethics. Although it is accepted that current intelligence systems do not have moral/ethical status, it has yet to be determined what the exact the characteristics that confer moral/ethical status are or will be. 
  |  https://doi.org/10.4274/tjo.galenos.2020.78989  |  https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/32167262/  |  
------------------------------------------- 
10.1007/s11938-020-00274-2  |    Purpose of review:  This review highlights the history, recent advances, and ongoing challenges of artificial intelligence (AI) technology in colonic polyp detection. 
  Recent findings:  Hand-crafted AI algorithms have recently given way to convolutional neural networks with the ability to detect polyps in real-time. The first randomized controlled trial comparing an AI system to standard colonoscopy found a 9% increase in adenoma detection rate, but the improvement was restricted to polyps smaller than 10 mm and the results need validation. As this field rapidly evolves, important issues to consider include standardization of outcomes, dataset availability, real-world applications, and regulatory approval. AI has shown great potential for improving colonic polyp detection while requiring minimal training for endoscopists. The question of when AI will enter endoscopic practice depends on whether the technology can be integrated into existing hardware and an assessment of its added value for patient care. 
  |  https://dx.doi.org/10.1007/s11938-020-00274-2  |  
------------------------------------------- 
10.1038/s41581-019-0243-3  |    |  http://dx.doi.org/10.1038/s41581-019-0243-3  |  
------------------------------------------- 
10.1152/physiolgenomics.00029.2020  |    |  http://journals.physiology.org/doi/full/10.1152/physiolgenomics.00029.2020?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed  |  
------------------------------------------- 
10.1016/j.amjmed.2020.03.033  |   Artificial intelligence is a fast-growing field and its applications to diabetes, a global pandemic, can reform the approach to diagnosis and management of this chronic condition.. Principles of machine learning have been utilized to build algorithms to support predictive models for the risk of developing diabetes or its consequent complications.. Digital therapeutics has proven to be an established intervention for lifestyle therapy in the management of diabetes. Patients are increasingly being empowered for self-management of diabetes and both patients and healthcare professionals are benefitting from clinical decision support. Artificial intelligence allows a continuous and burden-free remote monitoring of the patient's symptoms and biomarkers. Further, social media and online communities enhance patient engagement in diabetes care. Technical advances have helped to optimize resource utilization in diabetes. Together, these intelligent technical reforms have produced better glycemic control with reductions in fasting and postprandial glucose levels, glucose excursions, and glycosylated hemoglobin. Artificial intelligence will introduce a paradigm shift in diabetes care from conventional management strategies to building targeted data-driven precision care. 
  |  https://linkinghub.elsevier.com/retrieve/pii/S0002-9343(20)30339-9  |  
------------------------------------------- 
10.1007/s00117-019-00611-2  |    Clinical/methodical issue:  Artificial intelligence (AI) has the potential to improve diagnostic accuracy and management in patients with lung disease through automated detection, quantification, classification, and prediction of disease progression. 
  Standard radiological methods:  Owing to unspecific symptoms, few well-defined CT disease patterns, and varying prognosis, interstitial lungs disease represents a focus of AI-based research. 
  Methodical innovations:  Supervised and unsupervised machine learning can identify CT disease patterns using features which may allow the analysis of associations with specific diseases and outcomes. 
  Performance:  Machine learning on the one hand improves computer-aided detection of pulmonary nodules. On the other hand it enables further characterization of pulmonary nodules, which may improve resource effectiveness regarding lung cancer screening programs. 
  Achievements:  There are several challenges regarding AI-based CT data analysis. Besides the need for powerful algorithms, expert annotations and extensive training data sets that reflect physiologic and pathologic variability are required for effective machine learning. Comparability and reproducibility of AI research deserve consideration due to a lack of standardization in this emerging field. 
  Practical recommendations:  This review article presents the state of the art and the challenges concerning AI in lung imaging with special consideration of interstitial lung disease, and detection and consideration of pulmonary nodules. 
  |  https://dx.doi.org/10.1007/s00117-019-00611-2  |  
------------------------------------------- 
