{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608013001330", "dc:identifier": "doi:10.1016/j.neunet.2013.05.003", "eid": "1-s2.0-S0893608013001330", "prism:doi": "10.1016/j.neunet.2013.05.003", "pii": "S0893-6080(13)00133-0", "dc:title": "Training Lp norm multiple kernel learning in the primal ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "46", "prism:startingPage": "172", "prism:endingPage": "182", "prism:pageRange": "172-182", "dc:format": "application/json", "prism:coverDate": "2013-10-31", "prism:coverDisplayDate": "October 2013", "prism:copyright": "Copyright \u00a9 2013 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Liang, Zhizheng"}, {"@_fa": "true", "$": "Xia, Shixiong"}, {"@_fa": "true", "$": "Zhou, Yong"}, {"@_fa": "true", "$": "Zhang, Lei"}], "dc:description": "\n               Abstract\n               \n                  Some multiple kernel learning (MKL) models are usually solved by utilizing the alternating optimization method where one alternately solves SVMs in the dual and updates kernel weights. Since the dual and primal optimization can achieve the same aim, it is valuable in exploring how to perform Lp norm MKL in the primal. In this paper, we propose an Lp norm multiple kernel learning algorithm in the primal where we resort to the alternating optimization method: one cycle for solving SVMs in the primal by using the preconditioned conjugate gradient method and other cycle for learning the kernel weights. It is interesting to note that the kernel weights in our method can obtain analytical solutions. Most importantly, the proposed method is well suited for the manifold regularization framework in the primal since solving LapSVMs in the primal is much more effective than solving LapSVMs in the dual. In addition, we also carry out theoretical analysis for multiple kernel learning in the primal in terms of the empirical Rademacher complexity. It is found that optimizing the empirical Rademacher complexity may obtain a type of kernel weights. The experiments on some datasets are carried out to demonstrate the feasibility and effectiveness of the proposed method.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Multiple kernel learning"}, {"@_fa": "true", "$": "Manifold regularization"}, {"@_fa": "true", "$": "Primal optimization"}, {"@_fa": "true", "$": "Empirical Rademacher complexity"}, {"@_fa": "true", "$": "Data classification"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608013001330", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608013001330", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84879556490", "scopus-eid": "2-s2.0-84879556490", "pubmed-id": "23770740", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84879556490", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20130524", "$": "2013-05-24"}}}}}