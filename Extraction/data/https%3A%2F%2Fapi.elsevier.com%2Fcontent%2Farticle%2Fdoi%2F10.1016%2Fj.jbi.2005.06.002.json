{"scopus-eid": "2-s2.0-33644955795", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2005-07-26 2005-07-26 2010-10-06T18:12:05 1-s2.0-S1532046405000559 S1532-0464(05)00055-9 S1532046405000559 10.1016/j.jbi.2005.06.002 S300 S300.1 FULL-TEXT 1-s2.0-S1532046406X00291 2015-05-15T06:30:58.184067-04:00 0 0 20060401 20060430 2006 2005-07-26T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 39 39 2 2 Volume 39, Issue 2 9 184 195 184 195 200604 April 2006 2006-04-01 2006-04-30 2006 article fla Copyright \u00a9 2005 Elsevier Inc. All rights reserved. INTERPRETINGPROCEDURESDESCRIPTIVEGUIDELINES PELEG M 1 Introduction 1.1 Causes of errors during processing of descriptive information: results from cognitive studies 1.2 Working in a team helps in error detection: experiences from software engineering 1.3 Improving algorithm development processes: experiences from software engineering 2 Research questions 3 Methodology 3.1 Data collection 3.2 The ACP process of creating algorithms 3.3 Identifying modifications and classifying them 3.3.1 Errors vs. improvements 3.3.2 Important vs. unimportant modifications 3.3.3 Grouping errors according to their location in the narrative guideline 4 Results 5 Discussion 5.1 The different purposes of guidelines and clinical algorithms suggest differences in their content 5.2 Team-work is crucial for detecting errors 5.3 The need for an informatician on the team 5.3.1 Missing definitions of branching points and interaction among guidelines 5.3.2 Problem with negation and implication 5.3.3 Confusing AND with OR 5.3.4 Altering the control flow as a result of considering patient situations that were not addressed by the guideline 5.4 Overlearning may lead to errors 5.5 Confusing different situations may lead to errors 5.6 Implications to GLIF3 6 Conclusion 6.1 Limitations of the study Acknowledgments References FIELD 1990 M GUIDELINESFORCLINICALPRACTICEDIRECTIONSFORANEWPROGRAM ANZAI 1992 285 306 Y ADVANCEDMODELSCOGNITIONFORMEDICALTRAININGPRACTICE LEARNINGGRAPHREADINGSKILLSFORSOLVINGPROBLEMS ANZAI 1991 64 92 Y TOWARDAGENERALTHEORYEXPERTISEPROSPECTSLIMITS LEARNINGUSEREPRESENTATIONSFORPHYSICSEXPERTISE LEPROHON 1995 240 253 J PATEL 2001 147 168 V GILB 1993 T SOFTWAREINSPECTION FAGAN 1976 182 211 M WIEGERS 2002 K WILLIAMS 2000 19 25 L PARNAS 1987 259 265 D SHORTLIFFE 1998 97 123 E PATEL 1998 467 483 V BOXWALA 2004 147 161 A SNOW 2001 840 852 V KNUTH 1989 607 685 D SHIFFMAN 1997 382 393 R PATEL 1997 67 99 V HUMANMACHINECOGNITION COGNITIVEMODELSDIRECTIONALINFERENCEINEXPERTMEDICALREASONING PATEL 2005 2298 2538 V HANDBOOKTHINKINGREASONING THINKINGREASONINGINMEDICINE FIELD 1992 M GUIDELINESFORCLINICALPRACTICEDEVELOPMENTUSE TIERNEY 1995 316 322 W PELEGX2006X184 PELEGX2006X184X195 PELEGX2006X184XM PELEGX2006X184X195XM 2013-07-17T11:42:39Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(05)00055-9 S1532046405000559 1-s2.0-S1532046405000559 10.1016/j.jbi.2005.06.002 272371 2010-11-08T12:10:22.975898-05:00 2006-04-01 2006-04-30 1-s2.0-S1532046405000559-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/MAIN/application/pdf/3f6496a0c9f7b3429e02c3d7c064dec0/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/MAIN/application/pdf/3f6496a0c9f7b3429e02c3d7c064dec0/main.pdf main.pdf pdf true 224682 MAIN 12 1-s2.0-S1532046405000559-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/PREVIEW/image/png/fd0e17d8b3733a382abc4b7151b89c29/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/PREVIEW/image/png/fd0e17d8b3733a382abc4b7151b89c29/main_1.png main_1.png png 61655 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046405000559-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr1/DOWNSAMPLED/image/jpeg/0ae1099a72c266a6313ed70768071503/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr1/DOWNSAMPLED/image/jpeg/0ae1099a72c266a6313ed70768071503/gr1.jpg gr1 gr1.jpg jpg 18138 257 359 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr1/THUMBNAIL/image/gif/33dc82be8c9191d870ddd3f472fc56a3/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr1/THUMBNAIL/image/gif/33dc82be8c9191d870ddd3f472fc56a3/gr1.sml gr1 gr1.sml sml 2313 90 125 IMAGE-THUMBNAIL 1-s2.0-S1532046405000559-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr2/DOWNSAMPLED/image/jpeg/790653c2c22d5e36ea75c0f97f508e97/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr2/DOWNSAMPLED/image/jpeg/790653c2c22d5e36ea75c0f97f508e97/gr2.jpg gr2 gr2.jpg jpg 18314 326 348 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr2/THUMBNAIL/image/gif/6755f215d58a8ad4d80753077c938f57/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr2/THUMBNAIL/image/gif/6755f215d58a8ad4d80753077c938f57/gr2.sml gr2 gr2.sml sml 2289 94 100 IMAGE-THUMBNAIL 1-s2.0-S1532046405000559-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr5/DOWNSAMPLED/image/jpeg/090270555cb9f9b6aa41f8ccccf66972/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr5/DOWNSAMPLED/image/jpeg/090270555cb9f9b6aa41f8ccccf66972/gr5.jpg gr5 gr5.jpg jpg 27139 173 418 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr5/THUMBNAIL/image/gif/82d4ab5bd49ea39a249c483b9268659a/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr5/THUMBNAIL/image/gif/82d4ab5bd49ea39a249c483b9268659a/gr5.sml gr5 gr5.sml sml 2602 52 125 IMAGE-THUMBNAIL 1-s2.0-S1532046405000559-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr6/DOWNSAMPLED/image/jpeg/4f8e7b6d5adcb8f9a6222174bf7438b3/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr6/DOWNSAMPLED/image/jpeg/4f8e7b6d5adcb8f9a6222174bf7438b3/gr6.jpg gr6 gr6.jpg jpg 18212 229 374 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr6/THUMBNAIL/image/gif/51581f0b331ce44ff3e9e320297008e4/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr6/THUMBNAIL/image/gif/51581f0b331ce44ff3e9e320297008e4/gr6.sml gr6 gr6.sml sml 2387 76 125 IMAGE-THUMBNAIL 1-s2.0-S1532046405000559-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr3/DOWNSAMPLED/image/jpeg/3d1ff2fbc90b25d07320d0de657c9ff3/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr3/DOWNSAMPLED/image/jpeg/3d1ff2fbc90b25d07320d0de657c9ff3/gr3.jpg gr3 gr3.jpg jpg 16914 276 365 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr3/THUMBNAIL/image/gif/54a1ef94e9abdc99f45e6b3859af94b5/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr3/THUMBNAIL/image/gif/54a1ef94e9abdc99f45e6b3859af94b5/gr3.sml gr3 gr3.sml sml 2410 93 123 IMAGE-THUMBNAIL 1-s2.0-S1532046405000559-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr4/DOWNSAMPLED/image/jpeg/44d510b5e567b7ab45729aa27290595c/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr4/DOWNSAMPLED/image/jpeg/44d510b5e567b7ab45729aa27290595c/gr4.jpg gr4 gr4.jpg jpg 26893 251 527 IMAGE-DOWNSAMPLED 1-s2.0-S1532046405000559-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046405000559/gr4/THUMBNAIL/image/gif/00fc41564c61f346621dcfcf505dd585/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046405000559/gr4/THUMBNAIL/image/gif/00fc41564c61f346621dcfcf505dd585/gr4.sml gr4 gr4.sml sml 2188 59 125 IMAGE-THUMBNAIL YJBIN 1221 S1532-0464(05)00055-9 10.1016/j.jbi.2005.06.002 Elsevier Inc. Fig. 1 Total number of modifications made in different algorithm versions. There were seven versions: versions 1\u20136 were created a year before the \u201cNew Version\u201d was created. The column marked as \u201c1\u20136\u201d indicates the total number of modifications made during the first sessions of algorithm creation, which included the first six iterations. Fig. 2 Number of positive and negative modifications in the different algorithm versions. The version numbers are as explained in the legend in Fig. 1. Fig. 3 Number of positive and negative modifications made in the different algorithm versions, including unimportant modifications. The version numbers are as explained in the legend in Fig. 1. Fig. 4 The distribution of modification types made at each algorithm version. The version numbers are as explained in the legend in Fig. 2. Fig. 5 Distribution of types of statements in the algorithm according to their location in the narrative guideline. Categories I\u2013IV are explained in Section 3.3.3. The numbers by the pie slices indicate the number of statements belonging to a category and their relative percentages. Fig. 6 The percentage of modified statements, arranged by their location in the guideline, in the different algorithm versions. Interpreting procedures from descriptive guidelines Mor Peleg a \u204e morpeleg@mis.hevra.haifa.ac.il Lily A. Gutnik b Vincenza Snow c Vimla L. Patel b a Department of Management Information Systems, University of Haifa, Israel b Laboratory of Decision Making and Cognition, Department of Biomedical Informatics, Columbia University, NY, USA c American College of Physicians, Philadelphia, PA, USA \u204e Corresponding author. Fax: +972 4 828 8522. Abstract Errors in clinical practice guidelines may translate into errors in real-world clinical practice. The best way to eliminate these errors is to understand how they are generated, thus enabling the future development of methods to catch errors made in creating the guideline before publication. We examined the process by which a medical expert from the American College of Physicians (ACP) created clinical algorithms from narrative guidelines, as a case study. We studied this process by looking at intermediate versions produced during the algorithm creation. We identified and analyzed errors that were generated at each stage, categorized them using Knuth\u2019s classification scheme, and studied patterns of errors that were made over the set of algorithm versions that were created. We then assessed possible explanations for the sources of these errors and provided recommendations for reducing the number of errors, based on cognitive theory and on experience drawn from software engineering methodologies. Keywords Decision support Clinical practice guidelines Case study Computer-interpretable guidelines Knuth\u2019s classification scheme Software engineering Cognitive studies 1 Introduction Clinical guidelines are systematically developed statements to assist practitioner and patient\u2019s decisions about appropriate healthcare for specific clinical circumstances [1]. While the recommendations aim to be based on evidence, they are often not constructed in a way that reflects the flow of actual patient encounters. In order to solve this problem, clinical guidelines are sometimes portrayed as algorithms to guide physicians about the recommended steps of data gathering, decision making, and actions (i.e., process flow) during patient encounters. Clinical guidelines aim to eliminate clinician errors, reduce practice variation, and promote best medical practices. Yet, during the process of creating clinical algorithms out of the descriptive guideline text, errors are often introduced. By using cognitive psychological techniques to study the ways by which people process descriptive information to generate procedures, it is possible to focus on the reasons that cause people to err while they are processing descriptive information. Another domain, where processes for error detection have been developed, is software engineering. A short review of relevant results from these two domains is discussed below. 1.1 Causes of errors during processing of descriptive information: results from cognitive studies In previous work [2], Anzai and Patel examined the process by which people study scientific narrative text and use their knowledge to create mental model representations that would assist them in problem-solving tasks. These studies have shown that people learn how to solve a task by actually solving it (learning by doing). In fact, learning by doing is one of the dominant strategies of obtaining knowledge and skills. Learning by doing has also been studied in the context of nurturing a layperson in acquiring expertise by learning how to draw and use diagrams in the domain of physics [3]. In this study, it was shown that by representing a particular problem with a set of diagrams, physicists recognize the underlying structure of the problem and can organize computationally efficient inference procedures for solving it. It has been suggested that recognition, generation, and inference are three main components of the processes in medical problem solving and development of expertise. However, learning by doing has disadvantages, as people can also over learn, repeating errors that they have made throughout the process. An additional source of errors that may occur during problem solving was determined in research that investigated decision-making strategies used by nurses in telephone triage involving public emergency calls for medical help [4]. The researchers concluded that some of the errors that the triage nurses made occurred because they were confused by dealing with similar situations that should have been treated differently. Noticing the similarity and ignoring differences in pairs of situations, the nurses erroneously applied strategies that they used in one situation to another situation. Another study investigated the impact of algorithmic-based and text-based practice guidelines on clinical decision making by physicians at varying levels of expertise, with and without the use of the guidelines [5]. The narrative guideline and the algorithm served as reminder tools for the expert, whereas they were more useful for the non-experts as a means to fill in gaps in knowledge and as an aid in organizing their knowledge. This suggests that the content and presentation of guidelines should be adapted to the user\u2019s depth of knowledge. Guidelines and clinical algorithms are generally written by a team of experts in the medical area covered by the guideline. They aim to improve the quality of care and standardize care and are often targeted toward non-expert clinicians. Since experts approach the guideline with a highly organized knowledge base, as writers, they may inadvertently expect the readers to have the same knowledge, and be able to make the inferences required to fully comprehend the guideline. Therefore, they may not represent all of the guideline\u2019s knowledge explicitly. A non-expert may not be able to correctly make these inferences, leading to errors or frustration. This evidence led the authors to hypothesize that non-experts may use guidelines and algorithms inaccurately. This hypothesis was confirmed by the study, which showed that the non-experts seemed content to take things at face value, whereas the experts expressed a lot more concern in checking before diagnosing. 1.2 Working in a team helps in error detection: experiences from software engineering The software engineering community has investigated methods for reducing the rate of programming errors. These methods often involve team work for finding errors. One of the successful methods is inspection of software design and code [6]. When inspection is carried out systematically with well-defined roles for inspection participants, it can improve the quality of computer programs [7]. The team of participants include: (1) a moderator, who is a competent programmer but is not a technical expert on the program being inspected. The moderator schedules the inspection, moderates it, reports its results promptly, and schedules follow up on rework. (2) The designer of the program. (3) The coder of the program. (4) The tester, who is responsible for writing test cases and executing them. 1 For small-sized programs, the designer and coder may be the same person. It is best if the tester is a different individual, but if not, another individual fills the role of the tester during inspection. 1 Inspection sessions are planned for no longer than two hours because the error detection efficiency of most teams goes down after that period of time. The inspection session is highly structured. It begins in an overview in which the designer describes the design of the area to be examined and distributes design specification documents to the participants. The participants then study the documents, trying to understand the design, its intent and logic, and while doing so, they find errors. It has been shown that the rate of error detection increases if the participants first examine the ranked distribution of error types found by recent inspections and checklists of clues for finding errors (e.g., \u201care all constants defined?\u201d). This helps them focus on the most fruitful areas. Next, the coder explains how he will implement the design. The participants ask questions with the intention of finding errors. Questions are pursued until the error is recognized (solution of the error is not done using inspection). After the moderator writes and distributes his report, the designer or coder corrects the errors and a follow-up inspection session is scheduled. A less successful method, which is often used because it is easier to apply, is Walkthroughs. A walkthrough is an informal review, in which the programmer describes the code that he has written to some colleagues and solicits comments. The author acts as the moderator, and there is no defined procedure for carrying out the walkthrough session [8]. Another team-process that has been quantitatively shown to improve software quality and reduce time to market is pair programming. Pair programming involves two programmers working side by side at one computer on the same design, algorithm, code, or test [9]. Collaboration improves the problem-solving process. As a result, far less time is spent in the chaotic, time-consuming compile and test phases. Programmers who have been working in pairs have reported the ability to solve difficult tasks together very quickly, putting their collective knowledge to work. They also reported that working in a team helps them stay focused on their task. Pair programming improves the programmers\u2019 job satisfaction and overall confidence in their work. Arguing against pair programming, Parnas [10] has remarked: \u201cThere is no doubt that four eyes can see more than two. I doubt that the four eyes have to look at the code at the same time in the same place?.\u201d Parnas defined a process for reviewing software designs for the purpose of finding errors in the design and its representation, called Active Design Review [11]. The principle of this approach is that people use the design document, defend their own work, and cannot just watch passively. Instead of having one review process per project, teams conduct several types of reviews, each carried out with a different and appropriate group of reviewers, designed to find different types of design errors. Instead of conducting the reviews as discussions, questionnaires are prepared for the reviewers to answer about the design documentation. Responding to the questionnaire requires the reviewer to study the documentation carefully and to make assertions about design decisions. The issues raised by the reviewers as a result of answering the questionnaires are discussed in small meetings between the designers and each reviewer. Instead of paring programmers to produce code together, pairs, or small teams of software developers, are created, where each person creates a document and the other checks it. In the following examples of such pairs, each member of the pair checks the other\u2019s work. The pairs could be (a) a person who writes the requirement document and a person producing the corresponding design document; or (b) a person who writes the design document and another person who writes code (a designer and a coder) [10]. 1.3 Improving algorithm development processes: experiences from software engineering The Capability Maturity Model for Software [12] provides software organizations with guidance on how to gain control of their processes for developing and maintaining algorithms and software. This model organizes the process of maturing capabilities of the organization into five levels: (1) the initial level, in which the algorithm and software is developed in an ad hoc way, (2) the repeatable level, in which the necessary process discipline is in place to repeat earlier successes on projects with similar applications, (3) the defined level, in which the software process for both management and engineering activities is documented, standardized, and integrated into the organization, (4) the managed level, in which detailed measures of the software process and product quality are collected, and the (5) the optimizing level, in which the software development process is optimized. This model may also be relevant in improving the development of clinical guidelines with the aim of defining a replicable, productive, and efficient development process. 2 Research questions In this study, we have examined the process by which a medical expert from the ACP (VS) goes about creating clinical algorithms from narrative guidelines. We proposed three hypotheses relating to this process. 1. The iterative phases of the algorithm creation process in which the expert works alone are likely to result in improvement of the algorithm. When an expert creates a clinical algorithm based on her processing of a narrative guideline, she learns how to improve the algorithm by iterating through the process of algorithm creation (cf. [2] for the process of \u201clearning by doing\u201d). When she has new ideas for design modifications (e.g., alphabetizing treatment options that are not ranked), she carries them out throughout the process of algorithm creation. From one iteration to the next, fewer modifications are made until the expert is satisfied with her results. Reexamining the algorithm after a substantial period of time results in making further improvements. 2. The collaborative phases of algorithm creation results in improving the algorithm. Feedback from other professionals results in making more improvements than errors [13,14]. 3. The different purposes of guidelines and clinical algorithms suggest differences in their content. The evidence-based recommendations contained in the narrative guideline may not be sufficient to provide a complete procedure of care [15]. Therefore, a small number of algorithm statements would not be based on the narrative guideline, but on supplemental information not provided in the guideline. The percentage of changes made to different sections of the narrative guideline would be equal, since the narrative guideline had already been validated by experts and we assume that all of its parts are correct but may require changes to arrange the recommendations as a process of care that unfolds over time. However, the percentage of changes made to parts of the clinical algorithm that did not originate in the narrative would be smaller, because these statements were introduced by the expert as she created the algorithm with the procedural steps of patient care in mind. Therefore, she is less likely to change statements within the same session in which she introduced them. 3 Methodology In a previous study [15], we compared the final version of the algorithm to the narrative guideline from which it was developed, analyzing the types of modifications between them. By contrast, in this study we captured the versions of the algorithm that were created during its iterative creation process, rather than focusing on the final versions only. In this way, we could analyze the process and find reasons for modifications made between consecutive versions. Our combined expertise in cognitive psychology and computer science enabled us to produce different explanations for the modifications made, focusing on these different perspectives, and suggest recommendations for limiting errors in future processes of development of clinical guidelines, algorithms, and guideline implementations. These recommendations have implications for the conversion of narrative guidelines to computer-interpretable guidelines (CIGs), encoded in CIG formalisms such as GLIF3 [16], which allow automatic inference enabling decision support. In addition, studying the narrative guideline and algorithm development process will assist us in refining our authoring tools and it will assure robustness in GLIF3. 3.1 Data collection Two members of our research team (MP, VP) observed the ACP expert, as she created flowchart versions of clinical algorithms based on a narrative guideline that ACP had created previously. The guideline studied was Pharmacologic management of acute attacks of migraine and prevention of migraine headache [17]. We recorded the expert as she \u201cthought aloud\u201d about what she was doing, and we also captured any conversations with the investigators and other guideline creators. We kept all the drafts of the algorithms (six drafts of the first algorithm and two of the second), which showed progressive modifications in the algorithms. The Institutional Review Board reviewed and approved the study protocol. A year after the creation of the clinical algorithms, the ACP expert went over the last version of the algorithm set, which she had created earlier, and then generated a new version of it for the purpose of creating an html version that included hyperlinks. 3.2 The ACP process of creating algorithms After the ACP team has created a narrative guideline, it constructs a clinical algorithm and publishes it at www.acponline.org. The algorithm is first created by a medical expert, who reads the narrative guideline and creates versions of the algorithm in an iterative manner until she is satisfied with the results. Computer-based tools are not used during this process. When finished, she delivers a final copy to the director of scientific policy (DSP) at the ACP. This person examines the algorithm and compares it to the narrative guideline. Then, in a review session with the expert, the DSP suggests clarifications and modifications. The expert modifies the clinical algorithm and hands it over to a third member of the team, the flowchart designer, who uses software to generate flowcharts from the expert\u2019s hand-drawn algorithms. The expert then checks the flowcharts. 3.3 Identifying modifications and classifying them Our approach is to identify the types of errors introduced during the development process of narrative guidelines and CIGs. In addition to defining types of errors made, we are also interested in identifying their sources, and devising methods and tools for limiting them. We used a classification scheme proposed by Knuth [18] to classify modifications between narrative guideline text and the clinical algorithm produced from it. Knuth classified discrepancies between the requirements document for TeX and the resulting software. The first 12 of the 15 modification types that he suggested are applicable to the narrative guideline domain. The narrative guideline is analogous to the requirements document, whereas the clinical algorithm created from it is analogous to the software. We added specialization as a possible modification type. 1. Algorithm awry (A): incorrect or inadequate specification of process execution sequence. For example, not specifying the processes that take place in the system in their correct order is an algorithm awry. 2. Forgotten function or omission error (F): forgetting to specify a function, or omitting a part of the problem statement from the specification. For example, excluding one of the processes or objects that were described in the problem statement from the model is a forgotten function or omission error. 3. Language liability (L): misusing or misunderstanding the (specification) language. For example, using an action step instead of a decision step is a language liability error. 4. Mismatch between modules, or, in our case, between algorithms (M): a clinical algorithm that is specified using several linked sub-algorithms that result in inconsistent recommendations is a mismatch between modules. 5. Blunder or botch (B): thinking the right thing but writing it in a wrong way using correct syntax (e.g., confusing \u2018before\u2019 with \u2018after\u2019). 6. Trivial typo (T): a typographical error. 7. Cleanup for consistency or clarity (C): making modifications to make things more logical and/or easier to remember. 8. (a) Generalization or growth of ability (G): extending the original problem statement in order to make it more general or to increase its ability. (b) Specialization: making a definition narrower. 9. Interactive improvement (I): improving the specification for better response to user needs. 10. Promotion of portability (P): changing the organization or documentation of the specification. 11. Quest for quality (Q): changing the original problem statement (narrative) to increase the specification (algorithm) quality. 12. Surprising scenario (S): changing the original problem statement (narrative) due to scenarios that were not considered in the problem statement. 13. Efficiency enhancement (E): changing the process to make it more efficient. 14. Data structure debacle (D): not updating properly the representation of information to preserve appropriate invariants. 15. Reinforcement of robustness (R): making the specification (algorithm) handle erroneous input. 3.3.1 Errors vs. improvements In Knuth\u2019s classification, some of the modification types can be improvements with positive qualities (e.g., quest for quality) and some can be errors with negative qualities (e.g., blunder). Other modification types could include positive as well as negative qualities (e.g., generalization/specialization, which could include improving the algorithm where guideline eligibility was generalized from patients having a migraine attack to those experiencing headache or a specialization that is the result of an error, such as in the case of not listing all the possible criteria for being eligible for preventive treatment). In our analysis, it was important for us to distinguish between improvements and errors in order to see whether we could identify patterns of error occurrence during the iterative process of algorithm creation. 3.3.2 Important vs. unimportant modifications Not all of the modifications that the expert made during algorithm creation were equally important. We carried the analysis on the entire set of modifications and on the subset of modifications that we found to be important. Unimportant modifications were modifications that would not have affected the advice given by the algorithm (e.g., in addition to recommending efficacious drugs, specifying which drugs are not recommended) or modifications that were made in order to clarify terms that were probably already known to clinician users (e.g., giving examples of anti-emetics). We believe that in order to learn how to improve the algorithm creation process, resulting in more accurate and robust algorithms that are easy to computerize, the focus should be on modifications that are related to the important category. Taking the unimportant modifications into account may shift the results for the better, when many of the unimportant modifications are improvements, or for the worse, when many of the modifications are errors. In most cases, however, the results were not significantly affected by the exclusion of unimportant modifications. In this paper, we report the results of the important modifications. In cases were the results differed when including or excluding unimportant modifications, we report both of these results. 3.3.3 Grouping errors according to their location in the narrative guideline When looking at the different statements from the guideline narrative that made it to the clinical algorithm, we distinguished between statements that originated in the: (I) recommendations, (II) paragraph below the recommendations, (III) background material contained in the guideline, and (IV) information added that was not in the original guideline. We wanted to see whether the section on recommendations was informative enough to create most of the algorithm and whether most of the important modifications were made in particular parts of the narrative guideline. This may indicate where we should focus our attention when analyzing errors and their causes. 4 Results We recorded the modifications that the expert made between each consecutive algorithm versions. For each modification we recorded (1) whether the modification was positive or negative, (2) whether the modification was important or unimportant, (3) the location of the modification in the guideline narrative, and (4) the modification\u2019s category according to Knuth\u2019s classification. To analyze the data, we looked for patterns in the distribution of errors over various versions of algorithms. We also searched for possible explanations for errors made by the expert. We present the patterns that emerged from the data and developed theoretical accounts as a set of hypotheses, and describe the extent to which these hypotheses were confirmed. Hypothesis 1 The iterative process of algorithm creation, in which the expert works alone through the process of \u201clearning by doing,\u201d is likely to result in improvements in refining the algorithm The iterative process of algorithm creation contained the following stages. During the development of the first version, the expert created many modifications to the original narrative. Then she made modifications looking only at the previous algorithm versions, without consulting the narrative, until she did not find any errors. Next, she moved on to the next stage: continuously consulting the narrative guideline and making modifications until modifications were no longer needed. She then consulted the DSP and then made additional modifications (version 6) according to the DSP\u2019s comments. In examining the guideline again a year later, she found places where new modifications were required. She had forgotten the last algorithm that she had created, thus she examined this version with a critical eye and refined the algorithm. Fig. 1 describes the total number of modifications made in different algorithm versions. The results reflect the iterative development process. The expert created the first version based on the narrative, and created new algorithm versions by consulting only her previous versions until modifications were no longer necessary (version3). Consulting another source\u2014the narrative in version 4 and the DSP in version 6\u2014helped the expert make more relevant modifications. A year later (new version) she still made more modifications. The first hypothesis is examined in Fig. 2 , which gives the number of positive and negative modifications in the different algorithm versions. The hypothesis was confirmed in that modifications were made. However, contrary to the theories of \u201clearning by doing,\u201d what we did not expect was that more errors than positive modifications were generated when the clinical expert worked alone. Fig. 3 shows all the number of all modifications, including unimportant ones, which were made in the different algorithm versions. Examining all modifications, it appears that more improvements were made than errors. However, most of the improvements were unimportant and most of the errors were highly important. Fig. 4 shows the distribution of specific types of modification made during each algorithm version. The distribution of error types is not uniform. In the last version, more blunders (3) were made than in any previous version (one blunder was made in versions 4 and 6, and no blunders were made in versions 1, 2, 3, and 5). However, versions 1 through 6 were created during one session. Comparing the number of blunders made during that session (2 blunders) to the number of blunders made during the creation of the new version, which was done a year later (3 blunders), shows that an almost equal number of blunders were made in each session. Comparing the sum of modifications made in the first session to the number of modifications made during the creation of the new version, we found that no important interactive improvement modifications were generated in the last version as compared to three done in the first version. Correction modifications could have only been made in the new version because we considered cases where the expert transiently made a blunder during the first session, but caught it soon afterwards in the same session as artifacts that should not be considered. Similarly, Forgotten functions were introduced in the first version. If they were corrected later in versions 1 through 6 during the first session, then they would not have been counted as forgotten functions, but as artifacts that should not be considered. Hypothesis 2 Collaborative work is likely to result in much broader improvements to the algorithm As Fig. 2 shows, this hypothesis was confirmed. The clinical expert made more improvements only after collaboration with the director of scientific policy (DSP-version 6). As a result of this collaboration, only one error was generated, since collaboration provides error checks. Hypothesis 3 The different purposes of narrative guidelines and clinical algorithms suggest differences in their contents and organization. While most of the statements in the algorithm will be taken from the recommendations section of the guideline (I), from the paragraphs below each recommendation (II), or from the Background section (III), a small percentage of statements will be represented by statements not contained in the narrative guideline (IV). In addition, the percentage of modifications made to guideline statements that originated at the recommendation section (level I), the paragraph below it (level II), or in the background section of the guideline (level III) will be equal. The percentage of modifications made to algorithm statements that were not part of the narrative guideline (level IV) will be smaller than the percentage of modifications in the other levels. Fig. 5 shows the distribution of types of statements in the algorithm according to their location in the narrative guideline. The data in the figure show that the hypothesis was only partly confirmed. The clinical algorithm was different in its content than the narrative guideline. However, much of the information relevant for creating an algorithm was not found in the Recommendations section. Looking at sequential versions, we could not find a temporal pattern of the distribution of errors grouped by the origin of statements (results not shown). Fig. 6 shows the percentage of modified statements in the different algorithm versions, arranged by their location in the guideline. While a pattern of the distribution of modifications among the different levels did not exist, the last part of the hypothesis was confirmed. Once the medical expert added information to the algorithm that was not part of the narrative guideline (level IV), she did not make modifications to that information. Perhaps, this is due to the fact that she was not able to consult with the guideline or other medical experts to confirm that her additions were correct. 5 Discussion In interpreting the results, we tried to account for our hypotheses about patterns of errors. Looking at the individual errors that were made, and following the expert\u2019s explanation of her rationale for making the modifications, we tried to find the sources of errors and suggest ways to limit them. 5.1 The different purposes of guidelines and clinical algorithms suggest differences in their content While we hypothesized that most of the algorithm could be created from material found in the recommendation section and in the paragraphs below the recommendations, we found that much of the information that was relevant for creating an algorithm had its origin in the background section of the guideline (level III in Fig. 5). This suggests that the Recommendation section does not contain enough information for summarizing a process of care. We also found that a substantial part of the algorithm was not based on the guideline at all (IV). A possible explanation is that the narrative guideline aims to list recommendations that are based on scientific evidence (evidence-based guidelines), but this is not enough to generate a clinical algorithm that describes a process of care. This necessitates addition of information based on the expert\u2019s intuitive opinion. 5.2 Team-work is crucial for detecting errors While we expected the expert to make improvements, but not the errors, as she iteratively created the algorithm, we found that this was not the case (Fig. 2). Looking at the total number of modifications made by the end of the first session of algorithm creation process (total modifications versions 1\u20136), we found that more errors were made relative to positive modifications. Correlating the errors with the phases of work that the expert made while she was working alone versus errors she made while she was doing collaborative work, we saw that more errors than positive modifications were made when the clinical expert worked alone. Only upon following comments from the DSP, the clinical expert made more improvements than errors. As explained in Section 1, team work has been advocated as a means for detecting errors in software design and coding. Controlled experiments have shown that working in pairs reduces the number of errors that are generated [9]. The review process that the DSP and the clinical expert were involved in was similar to the inspection or design review used in the software engineering area, though it was less formally defined. In the ACP\u2019s review process, the expert gave the DSP the algorithms for review. After reviewing the algorithms and the original guideline, the DSP and the clinical expert met and the DSP asked for clarifications, pointed out terms that were not defined, and suggested a different design that combined the prevention and treatment algorithms. Perhaps the ACP\u2019s process could be improved by assimilating the process used during software inspection and review procedure. First, as in the inspection process, the review meeting may involve another clinician knowledgeable in medicine, who can act as a moderator, as well as a tester, such as an informatician, who can think of test cases for checking the completeness and correctness of the algorithm. Second, as in the design review process, using questionnaires that were prepared in advance, the expert can have one-on-one meetings with another clinician and informatician to review the algorithms that she had produced. Third, similar to the pair programming process, two clinicians could produce the algorithm together. It would be interesting to compare the number of errors produced in algorithms when using the standard process carried out by the ACP with the three alternative processes suggested here. 5.3 The need for an informatician on the team Despite the rigorous process of algorithm development, the informatician on our team (MP) still found places in the algorithm requiring modifications, which were not found by the ACP team. These modifications reflect a formal computer science perspective of algorithm creation, which is not clinical. The ACP team agreed with these modifications, which included the following. 5.3.1 Missing definitions of branching points and interaction among guidelines Guideline authors usually do not specify the interaction among related guidelines that are applicable to the same patient (in our case, the prevention of migraine and the treatment of acute migraines algorithms). The guideline noted that while a patient is on preventive therapy, care should be taken to avoid overuse of acute medication. However, the guideline did not clarify what overuse means. The expert first specified the interaction by introducing two alternative paths of the algorithm, (i.e., a branching point) so that a patient being treated by means of the prevention algorithm would not receive treatment according to the acute algorithm, thus he would not be treated for acute attacks at all. Later, the expert reversed her specification, by removing this branching point. This allowed her to make it possible for patients who are on the prevention algorithm to enter the acute algorithm. However, instead of restricting the use of acute medications, the specification gave acute treatment to patients in each of their acute attacks regardless of whether they were being treated in accordance to the prevention algorithm. The expert did not realize that neither of the specifications correctly addressed the situation of avoiding overuse of acute medications. Had a clear definition of overuse been given, the expert would have likely specified a branching point that decides when a patient being treated on the prevention algorithm should be routed to the acute algorithm. A computer scientist would have been suspicious of a situation where a branching point is removed. If there had been a problem with the course of action recommended to one of the branches, then that branch should have been fixed without removing the branching point. In fact, another branching point should have been added, which would divide the population of patients who are receiving treatment via the prevention algorithm and are experiencing an acute attack into those who should be treated for their acute attack and those who should not be treated. The expert wanted to allow a patient who should be treated for his acute attack to enter the acute algorithm, but she did not realize that by removing the branching point she also allowed patients who should not be treated for their acute attacks (thus avoiding overuse) to enter the acute algorithm. 5.3.2 Problem with negation and implication The guideline specified that if a patient is vomiting then he should be given non-oral medication (Vomiting implies non-oral). If we know that the patient is not vomiting, then what kind of medication should be given to him? The answer is any route of medication. We can see this by looking at the truth table for the implication statement \u201cvomiting implies non-oral.\u201d For the value of vomiting=false the implication is true when \u201cnon-oral\u201d is true or false. However, the expert specified that if the patient is not vomiting then only \u201coral\u201d medication should be given. Due to her confusion, she simply negated \u201cnon-oral.\u201d 5.3.3 Confusing AND with OR The articles, And and Or, are used informally in the English language. This creates a problem when defining terms and branch points in clinical algorithms. Ambiguity in term definitions that include and or or (inclusive or) needs to be resolved in order to support understandability of terms, reproducibility that preserves the meaning of terms, and the utility that is provided by the definitions [19]. In our study, the expert used \u201cAND/OR\u201d to replace the word \u201cwith.\u201d For example, in the sentence \u201ctwo attacks a month with disability lasting more than 3 days,\u201d the word with should be translated into \u201cAND.\u201d Formally, AND/OR equals OR. While OR makes a criterion more general, AND makes a criterion more specific. In order to resolve ambiguity, clinical experts should be made aware of this rule. 5.3.4 Altering the control flow as a result of considering patient situations that were not addressed by the guideline The guideline distinguishes between migraine-specific medications and non-steroidal anti-inflammatory drugs (NSAIDs). For patients who had taken NSAIDs with good response, the guideline recommends taking the same medication in later attacks. The symmetric situation, of patients who had taken migraine-specific medication with good response, was not considered. Computer scientists are trained to think about decision tables [20] that list the actions that should be taken for every possible variable assignment. They are taught to systematically list all decision variables and all their possible values and check that actions are defined for every possible combination of values for all of the variables that are important in making a decision. If the decision is on what medication to prescribe for an acute attack, and the variables are the type of medication used before and the type of response achieved, then the table should specify the course of action for all four combinations of these two binary data items. 5.4 Overlearning may lead to errors Overlearning refers to a process where we study material until we know it perfectly, and then continue to study it some more, until we master it. However, overlearning may some times lead to unnecessary modifications or even lead to errors. Errors of confidence are likely to be generated as one develops a familiarity with the problem [21]. We found several examples where the clinical expert exhibited this kind of behavior: \u2022 The expert added to the phrase \u201cNSAIDS\u201d the phrase \u201cwith proven efficacy\u201d because she remembered that she had made this modification before. Only, she did not notice that the addition was not necessary, as the list of NSAIDS contained only those with proven efficacy. \u2022 The expert made a modification to the algorithm for treatment of acute migraine attacks by combining a second decision criterion with a criterion that she had already seen in that algorithm. The two criteria should have been combined with AND, requiring that both of them be present (e.g., \u201ctwo or more attacks a month\u201d AND \u201cdisability lasting 3 days\u201d). However, the expert combined them with OR, mistakenly. The expert later found the same criterion in the prevention of migraine attacks, but there the two criteria were already combined correctly by the word \u201cwith\u201d that implied an AND relationship between the two criteria. Erroneously, she replicated the mistake that she had introduced in the treatment algorithm into the prevention algorithm. 5.5 Confusing different situations may lead to errors If we encounter a new situation and mistakenly identify it as being identical to a previously encountered situation, we may erroneously apply the technique that is appropriate for the original situation to the new situation. This is a problem of transfer, which is well documented in cognitive psychology literature [22]. We found several examples of this kind: \u2022 The expert remembered the comment made by the DSP about alphabetizing drug options when no ranking of options is given so as not to imply an order. However, she applied this technique to alphabetize a list of drugs that had been ordered by drug side-effects and availability. \u2022 The expert read the guideline text that discussed generally accepted indications for migraine prevention. One of the lines in that section referred to \u201ctwo or more attacks a month\u201d while two lines down in the same section, a reference was made to \u201ctwice a week.\u201d When the expert specified the first line, she mistakenly wrote \u201ctwo or more attacks a week,\u201d confusing month with week because of the two similar phrases found in the same guideline section. 5.6 Implications to GLIF3 The ACP team that develops clinical algorithms is already considering issues that are important for producing high-quality guidelines, as defined in the 1992 IOM report on the development of clinical guidelines [23]. These issues include: (1) validity of the algorithm, (2) reproducibility (i.e., the algorithm would always produce the same behavior for the same patient situations), (3) clinical applicability (i.e., eligibility criteria), (4) clinical flexibility\u2014(i.e., considering different patient scenarios that occur during a patient encounter), (5) a clear definition of clinical terms, decision points, and medical actions, (6) a clear definition of control flow, (7) logical and easy-to-follow modes of presentation, and (8) distinction among clinical decisions, actions, patient states, and entry and exit points of the algorithm. The ACP team uses a process of algorithm development that involves several people and several stages. This process includes face-to-face meetings and discussions of the algorithms that help spot errors or lack of clarity. Unfortunately, not all of the considerations that are important for automating guidelines are explicitly considered when the narrative guidelines are being developed. This situation creates difficulties when the guidelines are to be encoded in a CIG formalism, such as GLIF3 [16]. We suggest that guideline developers should be given instructions which explain the cognitive sources of errors as well as the software engineering methodologies of review and inspection to limit errors. Another recommendation is to provide guideline authors with tools for creating clinical algorithms, thus being able to create clinical algorithms and guidelines that would be more complete and consistent, allowing easier development into CIGs. When we created authoring tools for GLIF3, we had considered tools that would ease the generation of what we call \u201cthe conceptual algorithm specification\u201d [24]. This specification is created by clinical experts and does not formally specify decision criteria, but does specify the structure of the clinical algorithm and the patient data used to make clinical decisions. The GLIF3 authoring tool that we had developed [15] allows an author to create a graphical clinical algorithm, validate the algorithm for completeness and consistency, enforce definition of terms according to controlled terminologies, define rules for ranking alternative treatment options written in natural language, introduce links to support material, and specify other documentation attributes, such as the target audience of the guideline, and strength of evidence associated with each guideline step. At the ACP, the clinical expert does not use computerized tools to aid in the creation of the clinical algorithm. Once the algorithm is complete and validated, a flowchart designer creates a formal flowchart out of the boxes and arrows specification that the medical expert created. The GLIF3 authoring tools can be introduced at this stage of algorithm creation by the expert or at the last stage, as it is now done at the ACP. Although there are benefits to using the tools from the start, it may also be too time-consuming for the expert. In addition, some experts feel more comfortable in using paper and pencil to draft the algorithms. In these cases, the informatics-considerations and GLIF3 authoring tools should be introduced at a later stage of the algorithm creation. The flowchart designer should be a person trained in informatics, who has training in identifying logical errors or incompleteness of specification. She would use GLIF3 tools to formalize the algorithms created by the expert and approved by the DSP. She would be more involved in impacting the process of algorithm creation. This will involve more rounds of iterations of algorithm development, where the informatician and clinical expert would sit together while the informatician translates the algorithm into a more formal GLIF3 conceptual algorithm specification. We also believe that adding another person to the team will improve the quality of the algorithm created. The modifications that were made by the clinical expert based on the review of her work by the DSP resulted in many improvements and only one error, which could have been detected by an informatician. Other tools might be used to limit errors that result from forgetting to represent part of the narrative guideline or to copy part of the clinical algorithm or sidebars to the next version. A tool like GEM-Cutter [25] could be used to mark-up narrative guidelines using GEM elements. The tool could be used to view unmarked parts of the guideline, thus aiding in limiting omission errors. In this study, we used Knuth\u2019s classification scheme to categorize modifications between a narrative guideline and the final version of its clinical algorithm. Although the classification scheme was developed for modifications between requirements-documents and software products, we found it appropriate for categorizing modifications between narrative guidelines and clinical algorithms, as well as errors in CIG specifications. A different classification scheme was developed by Tierney et al. [26], who described the problems encountered while they encoded a heart failure guideline. They found that the guideline often lacked definitions of terms and branch points, did not focus on errors of commission, and did not account for comorbid conditions, concurrent drug therapy, or the timing of most interventions and follow-ups. Because our study did not examine implementation issues or the development of the narrative guideline prior to clinical algorithm generation, we only considered the first of these problem types, although the implementation of two other guidelines developed by the ACP team has been reported by Patel et al. [5]. 6 Conclusion We have studied the process of creating clinical algorithms by the ACP by looking at intermediate versions produced during the algorithm creation, rather than examining only the final products. We identified errors that were made at each stage, categorized them, and studied the patterns of errors. Analyzing patterns of errors as well as following individual errors, we found possible explanations for the sources of these errors. Based on our analysis, and on the methods used in the software engineering community for reducing errors in software code, we lean towards suggesting development of recommendations for authors who create clinical algorithm that go beyond the recommendations reported by Tierney et al. [26], as discussed in the previous section. These additional recommendations could include: (1) verifying that all relevant information is carried from the narrative guideline to all versions of the clinical algorithm, (2) providing all the information necessary to rank treatment options, and (3) considering different patient scenarios. Tierney\u2019s recommendations, augmented by our additional recommendations, can be stated as metrics for evaluation of the quality of encoded guidelines. These metrics can be used to progress from the repeatable to the defined level of the Capability Maturity Model, as discussed in Section 1. The GLIF3 authoring tool addresses the recommendations made by Tierney as well as the first three of our additional recommendations. We also lean towards recommending changes in the process for creating clinical algorithms in two main ways: (i) by introducing an informatician to the team who would review the guideline and (ii) by defining a more formal review/inspection process, as usually done in software development. Guideline developers could also benefit from some knowledge about the cognitive sources of errors. We believe that taking such measures may help reduce the number of errors introduced into the algorithms. Furthermore, it would maintain the rigorous quality determination and improvement in guideline algorithm creation, which are essential for the process of developing the guideline. Future studies will aim to inspect the generality of the results and examine the impact of the new process that we suggested. 6.1 Limitations of the study Cognition research looks at the psychological processes underlying performance, focusing on in-depth analysis of the perceptual and cognitive processes that lead to observable behavior. The focus is on understanding the knowledge structures and mental processes brought to bear during cognitive activity (e.g., problem solving and decision making). Although performance varies substantially from person to person, cognitive theories and methods allow us to capitalize on certain structural and processing regularities of the human information processing system, which give strength to generalizations, as in the case of the present study. Cognitive research strategies tend to vary as a function of theory development. The earlier stages typically necessitate detailed analyses of a few subjects, even as little as one subject, as researchers endeavor to characterize a new phenomenon. The findings from these detailed studies suggest phenomena that may exist, such as the nature of errors in the guideline creation process. However, subsequent research, guided by this initial detailed study, and using a larger representative sample of physicians, is necessary to test the generality of the findings. Acknowledgments This study was conducted while the first author was on leave at Columbia University\u2019s Department of Biomedical Informatics. We thank Dr. Christel Mottur-Pilson, the director of Scientific Policy at the American College of Physicians, Philadelphia, PA, for her help and her unconditional support of the study. References [1] M.J. Field K.N. Lohr Guidelines for clinical practice: directions for a new program 1990 Institute of Medicine, National Academy Press Washington, DC [2] Y. Anzai V.L. Patel Learning graph-reading skills for solving problems D.A. Evans V.L. Patel Advanced models of cognition for medical training and practice NATO ASI Series F: computer and systems sciences 1992 Springer-Verlag Heidelberg, Germany 285 306 [3] Y. Anzai Learning and use of representations for physics expertise K.A. Ericsson J. Smith Toward a general theory of expertise: prospects and limits 1991 MIT Press Cambridge, MA 64 92 [4] J. Leprohon V.L. Patel Decision-making strategies for telephone triage in emergency medical services Med. Decis. Making 15 3 1995 240 253 [5] V.L. Patel J.F. Arocha M. Diermeier J. How C. Mottur-Pilson Cognitive psychological studies of representation and use of clinical practice guidelines Int. J. Med. Inform. 63 3 2001 147 168 [6] T. Gilb D. Graham Software inspection 1993 Addison-Wesley Wokingham, UK [7] M.E. Fagan Design and code inspections to reduce errors in program development IBM Syst. J. 15 3 1976 182 211 [8] K.E. Wiegers Seven truths about peer reviews Cutter IT J. 15 7 2002 [9] L. Williams R.R. Kessler W. Cunningham R. Jeffries Strengthening the case for pair-programming IEEE Software 2000 19 25 [10] Parnas DL. XP Versus MTP: The Limits of Extreme Programming. Slides of a presentation given at: eXtreme Programing 2003, <www.xp2003.org/xp2002/talksinfo/parnas.pdf/>. [11] D.L. Parnas Active design reviews: principles and practice J. Syst. Software 7 1987 259 265 [12] Paulk MC, Curtis B, Chrissis MB, Weber C. Capability Maturity Model for Software, Version 1.1. In: Software Engineering Institute, CMU/SEI-93-TR-24, DTIC Number ADA263403; 1993, <http://www.sei.cmu.edu/publications/documents/93.reports/93.tr.024.html/>. [13] E.H. Shortliffe V.L. Patel J.J. Cimino O. Barnett R.A. Greenes A study of collaboration among medical informatics research laboratories Artif. Intell. Med. 12 2 1998 97 123 [14] V.L. Patel V.G. Allen J.F. Arocha E.H. Shortliffe Representing a clinical guideline in GLIF: individual and collaborative expertise J. Am. Med. Inform. Assoc. 5 5 1998 467 483 [15] Peleg M, Patel VL, Snow V, Tu S, Mottur-Pilson C, Shortliffe EH, et al. Support for guideline development through error classification and constraint checking. In: Proc AMIA Symp. 2002. p. 607\u201311. [16] A.A. Boxwala M. Peleg S. Tu O. Ogunyemi Q. Zeng D. Wang GLIF3: a representation format for sharable computer-interpretable clinical practice guidelines J. Biomed. Inform. 37 3 2004 147 161 [17] V. Snow K. Weiss E.M. Wall C. Mottur-Pilson Pharmacologic management of acute attacks of migraine and prevention of migraine headache (American Academy of Family Physicians, Medical Specialty Society; American College of Physicians, Medical Specialty Society) Ann. Intern. Med. 137 10 2001 840 852 [18] D.A. Knuth The errors of TEX Software\u2014Practice and Experience 19 7 1989 607 685 [19] Mendonca EA, Cimino JJ, Campbell KE, Spackman KA. Reproducibility of interpreting \u201cand\u201d and or in terminology systems. In: Proc AMIA Symp. 1998. p. 790\u20134. [20] R.N. Shiffman Representation of clinical practice guidelines in conventional and augmented decision tables J. Am. Med. Inform. Assoc. 4 5 1997 382 393 [21] V.L. Patel M. Ramoni Cognitive models of directional inference in expert medical reasoning K. Ford P. Feltovich R. Hoffman Human and machine cognition 1997 Lawrence Erlbaum Associates Hillsdale, NJ 67 99 [22] V.L. Patel J.F. Arocha J. Zhang Thinking and reasoning in medicine K.Ha. Morrison Handbook of thinking and reasoning 2005 Cambridge University Press Cambridge 2298 2538 [23] M.J. Field K.N. Lohr Guidelines for clinical practice: from development to use 1992 Institute of Medicine, National Academy Press Washington, DC [24] Greenes RA, Peleg M, Boxwala AA, Tu SW, Patel VL, Shortliffe EH. Sharable computer-based clinical practice guidelines: rationale, obstacles, approaches, and prospects. In: MedInfo. London, UK; 2001. p. 201\u20135. [25] Argawal A, Shiffman RN. Evaluation of guideline quality using GEM-Q. In: Medinfo. London, UK; 2001. p. 1097\u2013101. [26] W.M. Tierney J.M. Overhage B.Y. Takesue L.E. Harris M.D. Murray D.L. Varg Computerizing guidelines to improve care and patient outcomes: the example of heart failure J. Am. Med. Inform. Assoc. 2 5 1995 316 322", "scopus-id": "33644955795", "pubmed-id": "16099725", "coredata": {"eid": "1-s2.0-S1532046405000559", "dc:description": "Abstract Errors in clinical practice guidelines may translate into errors in real-world clinical practice. The best way to eliminate these errors is to understand how they are generated, thus enabling the future development of methods to catch errors made in creating the guideline before publication. We examined the process by which a medical expert from the American College of Physicians (ACP) created clinical algorithms from narrative guidelines, as a case study. We studied this process by looking at intermediate versions produced during the algorithm creation. We identified and analyzed errors that were generated at each stage, categorized them using Knuth\u2019s classification scheme, and studied patterns of errors that were made over the set of algorithm versions that were created. We then assessed possible explanations for the sources of these errors and provided recommendations for reducing the number of errors, based on cognitive theory and on experience drawn from software engineering methodologies.", "openArchiveArticle": "true", "prism:coverDate": "2006-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046405000559", "dc:creator": [{"@_fa": "true", "$": "Peleg, Mor"}, {"@_fa": "true", "$": "Gutnik, Lily A."}, {"@_fa": "true", "$": "Snow, Vincenza"}, {"@_fa": "true", "$": "Patel, Vimla L."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046405000559"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046405000559"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(05)00055-9", "prism:volume": "39", "prism:publisher": "Elsevier Inc.", "dc:title": "Interpreting procedures from descriptive guidelines", "prism:copyright": "Copyright \u00a9 2005 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Decision support"}, {"@_fa": "true", "$": "Clinical practice guidelines"}, {"@_fa": "true", "$": "Case study"}, {"@_fa": "true", "$": "Computer-interpretable guidelines"}, {"@_fa": "true", "$": "Software engineering"}, {"@_fa": "true", "$": "Cognitive studies"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "184-195", "prism:endingPage": "195", "prism:coverDisplayDate": "April 2006", "prism:doi": "10.1016/j.jbi.2005.06.002", "prism:startingPage": "184", "dc:identifier": "doi:10.1016/j.jbi.2005.06.002", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "standard", "@height": "257", "@width": "359", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "18138", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "90", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2313", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "326", "@width": "348", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "18314", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "94", "@width": "100", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2289", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "173", "@width": "418", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27139", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "52", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2602", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "229", "@width": "374", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "18212", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "76", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2387", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "276", "@width": "365", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "16914", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "123", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2410", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "251", "@width": "527", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "26893", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "59", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046405000559-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2188", "@ref": "gr4", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33644955795"}}