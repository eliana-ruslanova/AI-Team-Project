{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014002342", "dc:identifier": "doi:10.1016/j.neunet.2014.10.005", "eid": "1-s2.0-S0893608014002342", "prism:doi": "10.1016/j.neunet.2014.10.005", "pii": "S0893-6080(14)00234-2", "dc:title": "Towards an intelligent framework for multimodal affective data analysis ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "63", "prism:startingPage": "104", "prism:endingPage": "116", "prism:pageRange": "104-116", "dc:format": "application/json", "prism:coverDate": "2015-03-31", "prism:coverDisplayDate": "March 2015", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Poria, Soujanya"}, {"@_fa": "true", "$": "Cambria, Erik"}, {"@_fa": "true", "$": "Hussain, Amir"}, {"@_fa": "true", "$": "Huang, Guang-Bin"}], "dc:description": "\n               Abstract\n               \n                  An increasingly large amount of multimodal content is posted on social media websites such as YouTube and Facebook everyday. In order to cope with the growth of such so much multimodal data, there is an urgent need to develop an intelligent multi-modal analysis framework that can effectively extract information from multiple modalities. In this paper, we propose a novel multimodal information extraction agent, which infers and aggregates the semantic and affective information associated with user-generated multimodal data in contexts such as e-learning, e-health, automatic video content tagging and human\u2013computer interaction. In particular, the developed intelligent agent adopts an ensemble feature extraction approach by exploiting the joint use of tri-modal (text, audio and video) features to enhance the multimodal information extraction process. In preliminary experiments using the eNTERFACE dataset, our proposed multi-modal system is shown to achieve an accuracy of 87.95%, outperforming the best state-of-the-art system by more than 10%, or in relative terms, a 56% reduction in error rate.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Multimodal"}, {"@_fa": "true", "$": "Multimodal sentiment analysis"}, {"@_fa": "true", "$": "Facial expressions"}, {"@_fa": "true", "$": "Speech"}, {"@_fa": "true", "$": "Text"}, {"@_fa": "true", "$": "Emotion analysis"}, {"@_fa": "true", "$": "Affective computing"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014002342", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014002342", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84917739875", "scopus-eid": "2-s2.0-84917739875", "pubmed-id": "25523041", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84917739875", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20141106", "$": "2014-11-06"}}}}}