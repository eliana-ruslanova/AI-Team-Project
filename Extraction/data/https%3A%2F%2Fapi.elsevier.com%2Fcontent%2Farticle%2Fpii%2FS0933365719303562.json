{"scopus-eid": "2-s2.0-85073144666", "originalText": "serial JL 271219 291210 291682 291866 31 90 Artificial Intelligence in Medicine ARTIFICIALINTELLIGENCEINMEDICINE 2019-10-15 2019-10-15 2019-10-15 2019-10-15 2019-12-18T22:42:53 1-s2.0-S0933365719303562 S0933-3657(19)30356-2 S0933365719303562 10.1016/j.artmed.2019.101726 S300 S300.1 FULL-TEXT 1-s2.0-S0933365719X00093 2019-12-19T00:24:41.955987Z 0 0 20191101 20191130 2019 2019-10-15T10:26:29.901857Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table e-component body mmlmath affil appendices articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst primabst ref 0933-3657 09333657 UNLIMITED NIHGOLD true 101 101 C Volume 101 2 101726 101726 101726 201911 November 2019 2019-11-01 2019-11-30 2019 Research Articles article fla \u00a9 2019 The Authors. Published by Elsevier B.V. CLASSIFYINGCANCERPATHOLOGYREPORTSHIERARCHICALSELFATTENTIONNETWORKS GAO S 1 Introduction 2 Background 3 Materials and methods 3.1 Dataset details 3.1.1 Data cleaning 3.2 Hierarchical self-attention networks 3.2.1 Self-attention 3.2.2 Target-attention 3.2.3 Multihead attention 3.2.4 Hierarchical structure 3.2.5 Regularization 4 Results 4.1 Baselines, hyperparameters, and setup details 4.2 Evaluation metrics 4.3 Experimental results 5 Discussion 5.1 Error analysis 5.2 Visualizing document embeddings 5.3 Visualizing word importance 6 Conclusion Funding Authors\u2019 contribution Conflict of interest Appendix A Supplementary data References ALBERTI 2017 C SYNTAXNETMODELSFORCONLL2017SHAREDTASK ZHANG 2018 L HU 2017 M MULLENBACH 2018 1101 1111 J GEHRMANN 2018 S WARD 2017 E CARRELL 2014 749 758 D NGUYEN 2010 440 445 A WEEGAR 2015 73 78 R LEE 2018 29 J XIE 2017 F NGUYEN 2015 A AMIAANNUALSYMPOSIUMPROCEEDINGSVOL953 ASSESSINGUTILITYAUTOMATICCANCERREGISTRYNOTIFICATIONSDATAEXTRACTIONFREETEXTPATHOLOGYREPORTS CODEN 2009 937 949 A MARTINEZ 2011 1877 1882 D LI 2010 41 48 Y YALA 2017 203 211 A YIM 2016 455 W ZHENG 2015 S WANG 2018 34 49 Y PINTO 2016 16 A ZHANG 2015 649 657 X YOUNG 2017 T RECENTTRENDSINDEEPLEARNINGBASEDNATURALLANGUAGEPROCESSING CAMACHOCOLLADOS 2018 J WORDSENSEEMBEDDINGSASURVEYVECTORREPRESENTATIONSMEANING QIU 2018 244 251 J XIAO 2018 1419 1428 C HUGHES 2017 246 250 M CONNEAU 2017 1107 1116 A LE 2018 29 36 H JAGANNATHA 2016 473 482 A JAGANNATHA 2016 856 865 A PASCANU 2013 1310 1318 R YANG 2016 1480 1489 Z GAO 2018 321 330 S VASWANI 2017 5998 6008 A YU 2018 A GAO 2018 11 23 S SENEL 2018 1769 1779 L PENG 2015 2106 2111 H KIM 2014 1746 1751 Y MIKOLOV 2013 3111 3119 T KINGMA 2015 D DIETTERICH 1998 1895 1923 T GEHRING 2017 1243 1252 J DOSSANTOS 2015 626 634 C CLASSIFYINGRELATIONSBYRANKINGCONVOLUTIONALNEURALNETWORKS VANDERMAATEN 2008 2579 2605 L GAOX2019X101726 GAOX2019X101726XS Full 2019-09-23T14:10:27Z FundingBody National Institutes of Health http://creativecommons.org/licenses/by-nc-nd/4.0/ 2020-10-15T00:00:00.000Z 2020-10-15T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license. \u00a9 2019 The Authors. Published by Elsevier B.V. 2019-09-23T14:08:25.715Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/eoas NCI National Cancer Institute http://data.elsevier.com/vocabulary/SciValFunders/100000054 http://sws.geonames.org/6252001 item S0933-3657(19)30356-2 S0933365719303562 1-s2.0-S0933365719303562 10.1016/j.artmed.2019.101726 271219 2019-12-19T00:24:41.955987Z 2019-11-01 2019-11-30 UNLIMITED NIHGOLD 1-s2.0-S0933365719303562-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/MAIN/application/pdf/6f8fc948863f0a996d6df8eb4e1e4616/main.pdf main.pdf pdf true 1366645 MAIN 10 1-s2.0-S0933365719303562-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/PREVIEW/image/png/734490a911e2197756b92f7d93dd5f81/main_1.png main_1.png png 55253 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0933365719303562-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr1/THUMBNAIL/image/gif/2bdd436e5ceb911513a57f33dc336f01/gr1.sml gr1 gr1.sml sml 5112 163 135 IMAGE-THUMBNAIL 1-s2.0-S0933365719303562-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr2/THUMBNAIL/image/gif/1fd9fa953773ee6f1aa64e00a7ff4273/gr2.sml gr2 gr2.sml sml 5251 89 219 IMAGE-THUMBNAIL 1-s2.0-S0933365719303562-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr3/THUMBNAIL/image/gif/68605b1b8bdf32f626446fe6a434e9b4/gr3.sml gr3 gr3.sml sml 5807 164 215 IMAGE-THUMBNAIL 1-s2.0-S0933365719303562-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr4/THUMBNAIL/image/gif/c526c502f386f7a1d068e0e84725083f/gr4.sml gr4 gr4.sml sml 13132 130 219 IMAGE-THUMBNAIL 1-s2.0-S0933365719303562-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr5/THUMBNAIL/image/gif/6ee4abead01bf1495daeba6f4b1a70db/gr5.sml gr5 gr5.sml sml 6476 106 219 IMAGE-THUMBNAIL 1-s2.0-S0933365719303562-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr1/DOWNSAMPLED/image/jpeg/fe9872c36cfbb1aec6f73c558362b2fd/gr1.jpg gr1 gr1.jpg jpg 27359 410 339 IMAGE-DOWNSAMPLED 1-s2.0-S0933365719303562-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr2/DOWNSAMPLED/image/jpeg/21201507cc69de3c41171f8e02e22dcb/gr2.jpg gr2 gr2.jpg jpg 35536 275 678 IMAGE-DOWNSAMPLED 1-s2.0-S0933365719303562-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr3/DOWNSAMPLED/image/jpeg/feee87cc3d0142bfd90e9a95a5d0a894/gr3.jpg gr3 gr3.jpg jpg 10210 229 301 IMAGE-DOWNSAMPLED 1-s2.0-S0933365719303562-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr4/DOWNSAMPLED/image/jpeg/8890f952676de7ce89581a690c35f5f5/gr4.jpg gr4 gr4.jpg jpg 97156 403 678 IMAGE-DOWNSAMPLED 1-s2.0-S0933365719303562-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr5/DOWNSAMPLED/image/jpeg/6f988217dfc4b9d2e7dbd32149f87aea/gr5.jpg gr5 gr5.jpg jpg 51192 328 678 IMAGE-DOWNSAMPLED 1-s2.0-S0933365719303562-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr1/HIGHRES/image/jpeg/1433e009ea926c62aa768e15dc032b0e/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 190385 1813 1500 IMAGE-HIGH-RES 1-s2.0-S0933365719303562-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr2/HIGHRES/image/jpeg/527ddab26d8b49de2b1f9884e833eb4b/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 255788 1215 3000 IMAGE-HIGH-RES 1-s2.0-S0933365719303562-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr3/HIGHRES/image/jpeg/c65117ea8aadef23888531fc8c35895f/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 73477 1014 1333 IMAGE-HIGH-RES 1-s2.0-S0933365719303562-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr4/HIGHRES/image/jpeg/db97de44ac490c6e7d21d973c913adb1/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 881403 1785 3000 IMAGE-HIGH-RES 1-s2.0-S0933365719303562-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/gr5/HIGHRES/image/jpeg/03886856f37af55225aa27f7155cc374/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 417286 1452 3000 IMAGE-HIGH-RES 1-s2.0-S0933365719303562-mmc1.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/mmc1/MAIN/application/pdf/a52d217218e58e84726090300e2d2ee6/mmc1.pdf mmc1 mmc1.pdf pdf false 2293991 APPLICATION 1-s2.0-S0933365719303562-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/2fa8b37673c7745b22d3e148821a67a2/si1.gif si1 si1.gif gif 273 34 62 ALTIMG 1-s2.0-S0933365719303562-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/52109c90ddf4b2e4817872dc6ddbceda/si10.gif si10 si10.gif gif 225 38 39 ALTIMG 1-s2.0-S0933365719303562-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/13424d973e1ea27c48d97884fbdae0d1/si11.gif si11 si11.gif gif 262 34 61 ALTIMG 1-s2.0-S0933365719303562-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/c67dd2b36eee09d0793c8a710b5b6626/si12.gif si12 si12.gif gif 275 35 62 ALTIMG 1-s2.0-S0933365719303562-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/1fdeeb87030fdb0548a07592580238e9/si13.gif si13 si13.gif gif 1123 57 283 ALTIMG 1-s2.0-S0933365719303562-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/9c03beb7af79dc158fb1520fdc50b62a/si14.gif si14 si14.gif gif 1979 58 441 ALTIMG 1-s2.0-S0933365719303562-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/7854b493f6d83b30048f83009ff3c96a/si15.gif si15 si15.gif gif 1743 57 396 ALTIMG 1-s2.0-S0933365719303562-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/17e559f26bf1d6d161387aa1526ee9a2/si16.gif si16 si16.gif gif 603 38 159 ALTIMG 1-s2.0-S0933365719303562-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/5ae2ad634fd545a6cbe0f7af7a74a5c9/si17.gif si17 si17.gif gif 378 35 98 ALTIMG 1-s2.0-S0933365719303562-si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/5614103e5d15fab59f3a20fc806d233f/si18.gif si18 si18.gif gif 388 35 100 ALTIMG 1-s2.0-S0933365719303562-si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/b2c940ae99936ba647f07b63ed02e7a6/si19.gif si19 si19.gif gif 1552 60 378 ALTIMG 1-s2.0-S0933365719303562-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/76082e747d713a716e2384443fda7234/si2.gif si2 si2.gif gif 272 35 60 ALTIMG 1-s2.0-S0933365719303562-si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/69b48062253cc589f52c3949f7ace62a/si20.gif si20 si20.gif gif 1435 60 348 ALTIMG 1-s2.0-S0933365719303562-si21.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/a2e87ab445cf50f7fd30d119d431a165/si21.gif si21 si21.gif gif 3140 156 309 ALTIMG 1-s2.0-S0933365719303562-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/ca694536ad9a66afa8780eec31b20bc2/si3.gif si3 si3.gif gif 850 35 272 ALTIMG 1-s2.0-S0933365719303562-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/e04eb0a6912f1fd3da5fefe675135248/si4.gif si4 si4.gif gif 287 38 62 ALTIMG 1-s2.0-S0933365719303562-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/9a67fa3e631a985cf98de9f501bf1fc8/si5.gif si5 si5.gif gif 278 34 64 ALTIMG 1-s2.0-S0933365719303562-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/997a91505919d033df70d91d6fd33eab/si6.gif si6 si6.gif gif 269 34 61 ALTIMG 1-s2.0-S0933365719303562-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/83b4a5f7af9c821ba57da888f525e961/si7.gif si7 si7.gif gif 3038 139 315 ALTIMG 1-s2.0-S0933365719303562-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/98b2e44431974220617fdd80d47b64d6/si8.gif si8 si8.gif gif 204 30 40 ALTIMG 1-s2.0-S0933365719303562-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365719303562/STRIPIN/image/gif/a52a14232f111e5ac6f48323735a8ed6/si9.gif si9 si9.gif gif 173 30 34 ALTIMG ARTMED 101726 101726 S0933-3657(19)30356-2 10.1016/j.artmed.2019.101726 The Authors Fig. 1 Architecture for our hierarchical self-attention network (HiSAN). Fig. 1 Fig. 2 Types of attention mechanisms used in the HiSAN. Fig. 2 Fig. 3 Validation accuracy vs time on site classification task. Fig. 3 Fig. 4 The HiSAN encodes each document into a fixed-size vector representation (aka \u201cdocument embedding\u201d) that we can visualize in 2D using t-SNE. The left figure shows the document embeddings for all entries in the test set and right figure shows the document embeddings for all misclassified entries in the test set for the site task. Fig. 4 Fig. 5 A section of a pathology report on breast cancer. In the top figure, we can follow the attention weights across all the attention layers in the HiSAN to visualize the words used to make the final classification decision. In the bottom figure, we can examine the self-attention weights associated with a particular word (in this case, the word \u201cbreast\u201d) to visualize how the HiSAN finds relationships between individual words. We note that for both figures, we show the aggregate attention weights across all attention heads in the HiSAN. Fig. 5 Table 1 Top: test set accuracy and macro F-score (with 95% confidence intervals) on each classification task. Bottom: train and inference times (in seconds) for each classification method; train times for deep learning approaches are displayed as time per epoch\u202f\u00d7\u202fnumber of epochs to converge. Table 1 Site Laterality Behavior Histology Grade Acc Macro Acc Macro Acc Macro Acc Macro Acc Macro NB 66.81(66.35, 66.89) 11.74(11.65, 11.85) 80.48(80.23, 80.70) 32.50(32.40, 32.65) 91.11(90.96, 91.28) 26.68(26.44, 27.00) 46.28(46.09, 46.66) 1.54(1.48, 1.56) 51.48(50.98, 51.59) 26.78(26.49, 26.80) LR 88.14(88.09, 88.48) 56.62(56.21, 57.93) 88.43(88.35, 88.72) 43.26(42.97, 45.10) 96.12(95.94, 96.17) 82.07(78.07, 82.83) 72.83(72.64, 73.17) 24.43(24.00, 25.42) 66.59(66.52, 67.08) 60.72(59.35, 63.80) CNN 89.44(89.27, 89.63) 56.44(55.59, 57.10) 89.05(88.87, 89.24) 47.89(46.88, 48.87) 96.44(96.32, 96.55) 74.99(72.63, 77.28) 75.39(75.16, 75.66) 23.50(23.41, 24.87) 69.97(69.71, 70.26) 68.72(66.41, 70.76) HAN 89.85(89.67, 90.02) 61.19(60.38, 61.85) 89.02(88.83, 89.20) 47.69(46.59, 48.71) 96.54(96.43, 96.64) 80.31(77.79, 82.67) 75.68(75.44, 75.91) 26.82(26.80, 28.30) 71.10(70.85, 71.36) 74.03(72.79, 75.03) SAN 89.14(88.96, 89.32) 59.57(58.79, 60.93) 88.87(88.67, 89.05) 49.62(48.58, 50.57) 96.38(96.27, 96.49) 82.21(79.81, 84.31) 74.07(73.79, 74.32) 26.09(26.03, 27.50) 67.59(67.29, 67.86) 69.15(67.44, 70.53) HiSANsplit/10 tokens 90.17(89.99, 90.36) 62.70(61.74, 63.43) 89.34(89.16, 89.53) 48.83(48.83, 50.78) 96.70(96.59, 96.80) 82.94(80.41, 85.07) 76.15(75.91, 76.41) 29.86(29.69, 31.46) 71.33(71.05, 71.61) 74.05(72.71, 75.09) HiSAN split/lines 90.37(90.21, 90.54) 63.36(62.30, 64.07) 89.35(89.16, 89.53) 49.99(49.08, 51.01) 96.71(96.61, 96.82) 84.02(81.64, 86.07) 76.22(75.97, 76.45) 30.23(30.13, 31.84) 71.59(71.34, 71.86) 74.30(73.03, 75.33) Train time (s) Average predict time (s) for 64 docs Site Laterality Behavior Histology Grade NB 1.415E+1 3.29E+0 2.45E+0 7.94E+1 3.38E+0 4.61E\u22123 LR 2.01E+3 2.82E+2 1.56E+2 1.33E+4 4.32E+2 3.59E\u22123 CNN 1.06E+3\u202f\u00d7\u202f13 epochs 1.06E+3\u202f\u00d7\u202f8 epochs 1.05E+3\u202f\u00d7\u202f9 epochs 1.06E+3\u202f\u00d7\u202f14 epochs 1.06E+3\u202f\u00d7\u202f13 epochs 1.01E\u22121 HAN 5.11E+4\u202f\u00d7\u202f5 epochs 5.04E+4\u202f\u00d7\u202f4 epochs 5.00E+4\u202f\u00d7\u202f5 epochs 5.08E+4\u202f\u00d7\u202f7 epochs 5.05E+4\u202f\u00d7\u202f7 epochs 4.69E+0 SAN 1.28E+3\u202f\u00d7\u202f20 epochs 1.27E+3\u202f\u00d7\u202f11 epochs 1.27E+3\u202f\u00d7\u202f15 epochs 1.28E+3\u202f\u00d7\u202f19 epochs 1.28E+3\u202f\u00d7\u202f18 epochs 1.52E\u22121 HiSANsplit/10 tokens 2.19E+3\u202f\u00d7\u202f8 epochs 2.18E+3\u202f\u00d7\u202f5 epochs 2.17E+3\u202f\u00d7\u202f7 epochs 2.19E+3\u202f\u00d7\u202f10 epochs 2.18E+3\u202f\u00d7\u202f12 epochs 2.22E\u22121 HiSANsplit/lines 3.16E+3\u202f\u00d7\u202f9 epochs 3.14E+3\u202f\u00d7\u202f6 epochs 3.13E+3\u202f\u00d7\u202f7 epochs 3.17E+3\u202f\u00d7\u202f10 epochs 3.14E+3\u202f\u00d7\u202f10 epochs 4.52E\u22121 Table 2 Test set accuracy and macro F-score (with 95% confidence intervals) on the site and histology tasks using a reduced dataset in which cases from the 15 most prevalent cancer sites are removed. Table 2 Site Histology Acc Macro Acc Macro NB 42.02(40.35, 41.87) 15.19(14.61, 15.23) 32.21(31.92, 33.30) 0.74(0.69, 0.80) LR 72.24(70.94, 72.30) 57.82(55.68, 58.12) 65.69(64.73, 66.19) 25.26(23.73, 26.00) CNN 73.10(72.37, 73.72) 56.63(55.47, 57.61) 67.53(66.84, 68.25) 20.69(20.41, 22.37) HAN 72.81(72.12, 73.50) 57.30(56.20, 58.17) 68.90(68.23, 69.61) 22.93(22.63, 24.81) HiSANsplit/lines 75.16(74.49, 75.86) 62.52(61.19, 63.51) 70.41(69.70, 71.11) 27.71(27.31, 29.94) Classifying cancer pathology reports with hierarchical self-attention networks Shang Gao a \u204e gaos@ornl.gov John X. Qiu a Mohammed Alawad a Jacob D. Hinkle a Noah Schaefferkoetter a Hong-Jun Yoon a Blair Christian a Paul A. Fearn b Lynne Penberthy b Xiao-Cheng Wu c Linda Coyle d Georgia Tourassi a \u204e tourassig@ornl.gov Arvind Ramanathan a \u204e ramanathana@ornl.gov a Computational Sciences and Engineering Division, Health Data Sciences Institute, Oak Ridge National Laboratory, Oak Ridge, TN, USA Computational Sciences and Engineering Division, Health Data Sciences Institute, Oak Ridge National Laboratory Oak Ridge TN USA b Surveillance Informatics Branch, Division of Cancer Control and Population Sciences, National Cancer Institute, Bethesda, MD, USA Surveillance Informatics Branch, Division of Cancer Control and Population Sciences, National Cancer Institute Bethesda MD USA c Louisiana Tumor Registry, Louisiana State University Health Sciences Center School of Public Health, New Orleans, LA, USA Louisiana Tumor Registry, Louisiana State University Health Sciences Center School of Public Health New Orleans LA USA d Information Management Services Inc, Calverton, MD, USA Information Management Services Inc Calverton MD USA \u204e Corresponding authors. Highlights \u2022 HiSANs are a neural architecture designed for classifying cancer pathology reports. \u2022 HiSANs achieve better accuracy and macro F-score than existing classifiers. \u2022 HiSANs are an order of magnitude faster than the previous state-of-the-art, HANs. \u2022 HiSANs allow easy visualization of its decision-making process. Abstract We introduce a deep learning architecture, hierarchical self-attention networks (HiSANs), designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy, faster training, and clear interpretability. We evaluate performance on a corpus of 374,899 pathology reports obtained from the National Cancer Institute's (NCI) Surveillance, Epidemiology, and End Results (SEER) program. Each pathology report is associated with five clinical classification tasks \u2013 site, laterality, behavior, histology, and grade. We compare the performance of the HiSAN against other machine learning and deep learning approaches commonly used on medical text data \u2013 Naive Bayes, logistic regression, convolutional neural networks, and hierarchical attention networks (the previous state-of-the-art). We show that HiSANs are superior to other machine learning and deep learning text classifiers in both accuracy and macro F-score across all five classification tasks. Compared to the previous state-of-the-art, hierarchical attention networks, HiSANs not only are an order of magnitude faster to train, but also achieve about 1% better relative accuracy and 5% better relative macro F-score. Keywords Cancer pathology reports Clinical reports Deep learning Natural language processing Text classification 1 Introduction The National Cancer Institute's (NCI) Surveillance, Epidemiology, and End Results (SEER) program maintains repositories of detailed cancer pathology reports of the majority of cancer cases collected by its registries throughout the United States (https://seer.cancer.gov) \u2013 these reports represent a rich source of information on detailed cancer characteristics. However, extracting useful information such as cancer site or histology from pathology reports is usually done manually by specialists who are trained with specific knowledge for understanding the contents of the reports. This process is very costly and time-consuming, thereby limiting the ability of cancer registries to identify useful data elements from existing pathology reports and restricting time-sensitive applications such as precision medicine. Therefore, there is an immediate need to modernize the information extraction process across all cancer registries. Automated classification of cancer pathology reports is an active area of research that aims to utilize machine learning (ML) methods to identify key data elements from pathology reports, thereby reducing the burden on human annotators. Recently, deep learning approaches have been shown to be greatly effective for various natural language processing (NLP) tasks, such as parsing [1], sentiment analysis [2], and question answering [3]; as a result, many of these approaches have been adapted for medical NLP tasks, including clinical text classification [4] and information extraction [5]. Unfortunately, these deep learning architectures are generally designed for shorter text segments typically no longer than one paragraph. Cancer pathology reports present unique challenges including documents that are several pages long, long-distance linguistic dependencies over different report sections, and only a tiny fraction of the pathology report text being relevant to a specific classification task. Moreover, the variation in report structure and linguistic patterns across pathology labs present additional challenges. Therefore, many out-of-the-box deep learning architectures adapted from general NLP tasks are unable to efficiently tackle these challenges. In this work, we introduce a deep learning architecture, the hierarchical self-attention network (HiSAN), that is specifically designed to overcome many of the difficulties associated with cancer pathology reports. We test the effectiveness of the HiSAN against other state-of-the-art text classification methods on classifying five key cancer data elements \u2013 site, laterality, behavior, histology, and grade \u2013 from a dataset of approximately 375K cancer pathology reports. We show not only that the HiSAN achieves the best accuracy and macro F-score across all five tasks, but also that it runs more than an order of magnitude faster than similar hierarchical deep learning methods. Furthermore, we show how the HiSAN's unique architecture enables clear visualization of the specific keywords in each pathology report that identify each data element as well as visualization of the linguistic relationships between different words and text segments within a report. 2 Background Tracking cancer statistics across the population of the United States is an important priority for the Centers for Disease Control and Prevention (CDC), the North American Association of Central Cancer Registries (NAACCR), the American Cancer Society (ACS), and the National Cancer Institute (NCI) [6]. These statistics help inform cancer research, funding, and legislation, identify populations at risk for certain cancer types, and analyze the impact of early detection and treatment advances. NCI's SEER program maintains cancer registries across the US that collect, store, and manage cancer incidence and survival data covering approximately 34.6% of the US population (https://seer.cancer.gov/about/overview.html); detailed cancer characteristics such as anatomic site and histology are generally recorded in the form of cancer pathology reports. Because manually parsing cancer pathology reports is a time-intensive task requiring expert knowledge, automated tools for pathology report IR can significantly ease the burden on human annotators. Traditionally, most automated classification systems for pathology reports relied on rule-based approaches [7\u201313] or non-deep-learning ML approaches, such as Naive Bayes, logistic regression, Adaboost, support vector machines, and conditional random fields [14\u201319]. These approaches generally rely on either hand-engineered features or the appearance of specific n-gram word phrases to identify key data elements. These approaches have significant weaknesses \u2013 hand-engineered features can be brittle and fail when a document does not adhere to the pre-determined rules, while n-gram features are unable to account for any potentially important linguistic patterns that span beyond a short window of text [20,21]. In recent years, many automated NLP solutions have shifted towards the use of deep learning, as deep learning approaches have been shown to beat out traditional ML approaches across a wide range of NLP tasks [22]. One important benefit of deep learning approaches is their ability to learn their own salient features directly from text without the need for any human engineering; furthermore, unlike approaches that utilize n-gram features, many deep learning architectures can identify relevant linguistic patterns across long spans of text. Moreover, deep learning approaches generally represent text as word embedding vectors, which capture the semantic content of words in vector format [23]. Because word embedding representations can capture linguistic meaning and word similarity, deep learning approaches are better able to generalize when applied to documents with new words that have never been seen by the model. Recent studies have shown that deep learning approaches can outperform traditional ML and rule-based approaches in various clinical NLP tasks [24,25]. Convolutional neural networks (CNNs) are a popular type of deep learning architecture for NLP that have been effectively applied towards medical text analysis [24,26]; these utilize a sliding window that moves across the document and analyzes a set number of words at a time. Like traditional ML models that utilize n-gram based features, one noticeable weakness of CNNs is that they are unable to capture linguistic patterns beyond a small window, usually 3\u20135 words. Attempts have been made to mitigate this weakness by stacking multiple convolution layers to increase the receptive field [27]; however, a comparative study showed that these deeper networks are often unable to outperform their shallower counterparts [28]. Recurrent neural networks (RNNs) are another popular type of deep learning architecture for NLP and medical text analysis [29,30]. RNNs are designed to handle sequential data. They process a word sequence one word at a time, and at each word, the RNN accounts for information from all previous words in the sequence. Unlike CNNs, RNNs can therefore find linguistic patterns over very long text sequences. However, because the operations associated with each word in a text sequence depend on all previous words, RNNs are very slow compared to CNNs and are also difficult to parallelize [31]. The computational complexity of an RNN scales polynomially with the length of a given sequence; therefore, while RNN-based approaches may be tractable for shorter text documents such as Yelp reviews, they can be prohibitively slow for longer, multi-page text documents such as pathology reports. In our previous work, we analyzed the effectiveness of hierarchical attention networks (HANs) [32], an RNN-based architecture, on the effectiveness of identifying the cancer site and histological grade from pathology reports and found that HANs beat out CNNs and traditional ML approaches on a small set of \u223c1000 pathology reports [33]. However, we also found that HANs are very slow to train, requiring a processing time of nearly a second per report \u2013 training multiple epochs on a large corpus of pathology reports could mean several weeks of train time. Self-attention is a relatively new development in the deep learning community that has achieved state-of-the-art performance in various NLP tasks, including machine translation [34] and question answering [35]. Self-attention-based approaches utilize neural attention mechanisms (discussed in detail in the following section) that find linguistic relationships between different words within a text sequence. These approaches offer the best of both worlds \u2013 like RNNs, self-attention can find linguistic patterns across very long text sequences, and like CNNs, self-attention is fast to run. Another major benefit of attention-based architectures is that they are easy to interpret and visualize \u2013 the outputs from a neural attention mechanism directly tell a user which words the model is using to make a decision. In this work, we introduce the HiSAN, which utilizes a hierarchical architecture similar to the HAN but replaces the computationally expensive RNN layers with self-attention. We expand our previous experiments from two to five classification tasks and test on a much larger dataset of pathology reports. We show that HiSANs can not only achieve better accuracy and macro F-score than HANs but also train over 10 times faster. 3 Materials and methods 3.1 Dataset details Our full dataset consists of 374,899 cancer pathology reports obtained from the Louisiana tumor registry in May 2018 in accordance to the institutional review board protocol DOE000152. These reports cover all types of cancers among Louisiana residents from the years 2004 to 2018. Each report includes metadata, such as the date of the report and the unique patient ID, as well as the full clinical comments written by the pathologist analyzing the cancer tumor. Each pathology report includes a unique tumor ID identifying the specific tumor associated with that report; multiple reports may be associated with the same tumor ID. The Louisiana cancer registry provided us with the ground truth labels for five key data elements \u2013 site, laterality, behavior, histology, and grade \u2013 associated with each unique tumor ID. These were manually annotated by a human expert based off all pathology reports associated with that tumor ID as well as other available data for that specific tumor, including clinical notes. There are a total of 70 possible labels for site, 7 for laterality, 4 for behavior, 516 for histology, and 9 for grade; the label descriptions and the number of occurrences per label are available in our supplementary information (SI) section A, and more detailed information can be found in the SEER coding manual (https://seer.cancer.gov/tools/codingmanuals/). To simulate a real production environment in which a classifier trained on older existing reports must predict labels for new incoming reports, we split our dataset into train, validation, and test sets based off date. Because the same tumor ID may have multiple pathology reports associated with it over time, we designed our splitting to prevent reports from the same tumor ID being split between the train, validation, and test sets. Therefore, we first grouped all pathology reports by tumor ID; each pathology report belonging to the same tumor ID is assigned the date of the earliest report associated with that tumor ID \u2013 a report written in 2017 may be assigned a 2012 date if it belongs with a tumor ID with a report first written in 2012. We isolated all reports from 2016 and later into our test set; from the remaining reports from 2004 to 2015, we randomly selected 80% for our train set and used the other 20% for our validation set (ensuring reports from the same tumor ID were not split between both train and validation sets). This results in a train set of 236,519 reports, a validation set of 59,241 reports, and a test set of 78,856 reports. 3.1.1 Data cleaning Each raw pathology report was provided to us in XML format that included both metadata and text fields. For each report, we discarded the metadata fields, such as patient ID and registry ID, and retained all text fields, such as clinical history and formal diagnosis. We then lowercased all words, converted any unicode characters into their corresponding ascii characters, and removed any consecutive punctuation (e.g., multiple periods following one another were replaced by a single period). Any unique words appearing fewer than five times across the entire corpus were replaced with an \u201cunknown_word\u201d token. We applied several text modification and replacement steps to standardize the pathology reports. These include standardizing all clock-time references, which are used to identify cancer site in cancers such as breast cancer, into the format of a number 1\u201312 followed by the string \u201coclock\u201d. In addition, to reduce the vocabulary space, we converted all decimals into a \u201cdecimal\u201d word token and all integers larger than 100 into a \u201clarge_integer\u201d word token. Additional minor text modification and replacement steps are listed in SI section B. After cleaning, the average pathology report had a length of 633 tokens. Like the HAN, the HiSAN first breaks a long document down into smaller linguistic segments, such as individual sentences. Generally speaking, pathology reports do not naturally break down into sentences; instead, most pathology reports list relevant facts and information line by line. We therefore split each document into smaller segments based off the natural linebreaks occurring in each pathology reports. Unfortunately, not all pathology reports use line breaks to separate information \u2013 in some cases, facts are presented in long paragraphs of natural language or are separated by symbols such as \u2018#\u2019 or \u2018<\u2019. Consequently, after splitting pathology reports by linebreaks, if any line is longer than 50 words, we further split it based off a curated list of punctuation and symbols based off our observations of the characters used to itemize lists within our corpus; these symbols are provided in SI section B. After splitting into lines, the average pathology report had 70 lines with an average of 8.5 tokens per line. 3.2 Hierarchical self-attention networks The architecture of the HiSAN is similar to that of the hierarchical convolutional attention network (HCAN) [36], which is an architecture we previously developed for sentiment analysis and general text classification tasks. In our experiments, we found that several components used in the HCAN which improved performance on general text such as Yelp and Amazon reviews instead reduced performance when applied to clinical text classification \u2013 a detailed ablation study is available in SI section G. Therefore, we propose the new HiSAN architecture, which is better suited for classification of cancer pathology reports. The structure of our HiSAN is shown in Fig. 1 . Each component of the HiSAN is discussed in greater detail in the following subsections. 3.2.1 Self-attention A self-attention mechanism compares a sequence of embeddings against itself to find relationships between the entries in the sequence. Given a sequence of embeddings E \u2208 \u211d l \u00d7 d , where l is the length of the sequence and d is the embedding dimension, a basic self-attention mechanism generates a new sequence S \u2208 \u211d l \u00d7 d in which each entry s i is a weighted average of all entries e i in the original sequence. Intuitively speaking, each new entry s i should have captured within it the most pertinent information to that entry from all entries e i in the original sequence: (1) Self - Attention ( E ) = softmax ( EE \u22a4 ) E To improve upon this basic self-attention, rather than directly compare E against itself, we use functions to extract three different sets of features from E: (1) Q and (2) K, which are features that help find important relationships between entries in the sequence, and (3) V, which are the features that are used to generate the new output sequence. This allows for more expressive comparison between entries in a sequence. Certain features are highly useful when finding word relationships, such as identifying how biomedical terms correspond to each other in a pathology report, and these are captured in Q and K. On the other hand, certain features are more useful for the final classification task being targeted, and these are captured in V. We use three position-wise feedforward operations on the same input sequence E \u2208 \u211d l \u00d7 d to generate Q \u2208 \u211d l \u00d7 d , K \u2208 \u211d l \u00d7 d , and V \u2208 \u211d l \u00d7 d . Our position-wise feedforward operation is equivalent to a 1D convolution operation with a window size of one word: (2) Q = ELU ( Conv 1 D ( E , W q ) + b q ) K = ELU ( Conv 1 D ( E , W k ) + b k ) V = ELU ( Conv 1 D ( E , W v ) + b v ) Self - Attention ( Q , K , V ) = softmax QK T d V In the equations above, E is the original sequence of word/line embeddings and W q , W k , W v , b q , b k , and b v are the weights and biases associated with the 1D convolutions used to create each of the new Q, K, and V representations. We scale the dot product QK T by d because it has been found to improve performance when the dimension size of the embeddings d becomes very large [34]. Our implementation of self-attention is illustrated in the left two diagrams of Fig. 2 . 3.2.2 Target-attention For classification purposes, all sequences regardless of length must be represented as a fixed length vector so that it can be fed into the same softmax classification layer. Our target attention mechanism compares each entry in a sequence S \u2208 \u211d l \u00d7 d with a learned target vector T \u2208 \u211d 1 \u00d7 d that represents the information to look for given the current task. T is initialized randomly and then learned through backpropogation during the training progress; entries in S that are more similar to T will be assigned higher importance, and vice versa. The output of the target attention mechanism is a fixed vector O \u2208 \u211d 1 \u00d7 d which is a weighted average of the entries in S and captures information from the most critical entries based on the given task. Our target attention mechanism uses similar operations to that of our self-attention mechanism: (3) Target - Attention ( S ) = softmax TS \u22a4 d S Our implementation of target-attention is illustrated in the rightmost diagram of Fig. 2. 3.2.3 Multihead attention In both self-attention and target-attention, the attention operations calculate a set of attention weights used to compute a weighted average across the word embeddings; these attention weights are then applied across all dimensions d of the word embeddings. Vaswani et al. found that performance can be improved by using h parallel attention mechanisms, each which attends to a different portion of the embedding dimension d [34]. As a result, different portions of the embedding dimension can be combined using different weights \u2013 this captures more complex relations because different semantic concepts are often captured in different subportions of the embedding dimension d [37]. We apply the same principle in our implementations of self-attention and target attention: (4) MultiHead Self - Attention ( Q , K , V ) = [ head 1 , \u2026 , head h ] where head i = Self - Attention ( Q i , K i , V i ) (5) MultiHead Target - Attention ( S ) = [ head 1 , \u2026 , head h ] where head i = Target - Attention ( S i ) In the equations above, we split each of the Q, K, V, and S embeddings into h sub-embeddings such that { Q i , K i , V i , S i } \u2208 \u211d l \u00d7 d / h . Each set of sub-embeddings is then fed into its own self-attention or target attention function. The final output is the concatenation of the outputs head i from the individual attention functions, resulting in an output sequence S output \u2208 \u211d l \u00d7 d for multihead self-attention and O output \u2208 \u211d 1 \u00d7 d for multihead target-attention. A key benefit of this approach is that, unlike other techniques such as adding additional convolution filters, multihead attention increases model's flexibility without increasing the number of trainable model weights relative to regular attention. 3.2.4 Hierarchical structure Self-attention finds relationships between the entries in a sequence regardless of how far apart they are in that sequence. This means that no matter where two words appear in a document, self-attention can still identify the same relationship between those two words. To get the most out of this capability, we utilize a hierarchical structure similar to that used in our previous work with HANs [33] \u2013 for any given pathology report, we first segment the report into individual lines, then processes each line individually before processing the entire report. This forces the self-attention mechanism to first find local relationships between words in the same line and identify the most important words per line before trying to find global relationships between words across different lines. This allows the HiSAN to better focus on the words and word relationships that matter most to a given classification task. Our HiSAN uses two hierarchies. For each line in the pathology report, the \u201cword\u201d hierarchy constructs a line embedding representing the content of that line based off the most important word embeddings in that line. Then, the \u201cline\u201d hierarchy constructs a document embedding representing the content of the entire pathology report based off the most important line embeddings generated by the \u201cword\u201d hierarchy. This final document embedding is then used for classification. Each hierarchy in the HiSAN is composed of elementwise feedforward operations to create the Q, K, and V vectors used in self-attention, a multihead self-attention mechanism to find important relationships between the embeddings, and a multi-head target-attention mechanism that constructs the final output embedding representation. In our previous work on HANs, we found that a hierarchical structure gives a noticeable boost to classification performance on pathology reports. In our experiments, we test the effectiveness of the same hierarchical structure for our HiSAN. 3.2.5 Regularization Deep learning models are susceptible to overfitting; thus, we utilize dropout to regularize our network. We apply dropout in two ways. First, we dropout 10% of the word and line embeddings that are used in each hierarchy; this type of dropout has been shown to be effective in other NLP applications [38]. Second, we dropout 10% of the outputs from all self- and target-attention mechanisms (within each multihead attention) as follows: (6) Self - Attention ( Q , K , V ) = dropout softmax QK T d V (7) Target - Attention ( S ) = dropout softmax TS \u22a4 d S Dropping out random words in the normalized similarity matrix generated after the softmax function reduces overfitting by preventing the attention mechanisms from always learning the same relationships between different words, thereby forcing the attention mechanisms to explore new potential word relationships. We found that utilizing these two forms of dropout improved the performance of our HiSAN across all five classification tasks. 4 Results Our experiments consist of comparing the effectiveness of the HiSAN in classifying cancer pathology reports in five key data elements \u2013 site, laterality, behavior, histology, and grade \u2013 against other popular text classification methods. We compare the methods in accuracy, macro F-score, and speed. 4.1 Baselines, hyperparameters, and setup details We compare the performance of our HiSAN against both traditional ML and deep learning algorithms commonly used on medical text data as well as against other deep learning architectures that have been previously applied on medical text classification. The hyperparameters of all baseline models and the HiSAN were tuned on a separate twofold cross validation setup on the reports from 2004 to 2015, which includes the train and validation data but excludes the test data used to report our final results; this was done to expand the dataset size used for hyperparameter tuning in order to find more robust hyperparameters. For non-deep learning approaches, we compare against Naive Bayes (NB) and logistic regression (LR) using TF-IDF unigrams and bigrams as features; due to our large dataset of \u223c375K documents and the large feature space of \u223c780K unique unigrams and bigrams, other approaches such as support vector machines and random forests were prohibitively expensive in terms of memory and/or time. We tuned the hyperparameters for LR on our validation set \u2013 we use L1 regularization with a penalty strength of 1.0. For deep learning approaches, we compare against two popular text classification architectures that have previously been benchmarked on cancer pathology reports \u2013 a word-level convolutional neural network (CNN) and a hierarchical attention network (HAN). In our previous with work HANs [33], we found than RNNs without the hierarchical structure not only perform worse than HANs in accuracy and macro F-score for pathology report classification, but also take longer to train. Therefore, these have been excluded from our study. The word-level CNN is based off the architecture developed by Kim in 2014 for sentence classification [39]. In 2017, Qiu et al. showed that a similar architecture outperforms non-deep learning approaches when classifying site and grade from cancer pathology reports [24]. For our text CNN, we use the similar hyperparameters as those specified by Qiu \u2013 three parallel convolutional filters that process 3, 4, and 5 words at a time, each with 300 filters, followed by a maxpooling operation across the entire document. The HAN was originally developed by Yang et al. in 2016 for sentiment analysis [32]. Previously in 2018, we showed that the HAN beat out both the text CNN and non-deep learning approaches in identifying site and grade in cancer pathology reports [33] \u2013 we consider the HAN to be the current state-of-the-art in identifying key data elements in cancer pathology reports. For our HAN, we use the same hyperparameters as those used in our previous paper \u2013 each hierarchy is composed of a bi-directional GRU with 200 units and an attention mechanism with a hidden layer of 300 neurons. In addition to the CNN and HAN, we also compare against a non-hierarchical version of the HiSAN, which we call the self-attention network (SAN), to show that use of the hierarchical structure indeed improves performance. For both our SAN and HiSAN, we use a dimension size of 512 for d and 8 heads for h in our position-wise feedforward, multihead self-attention, and multihead target-attention operations for all tasks except behavior, which uses 256 for d. For our deep learning approaches, we generate word embeddings for each word in our vocabulary using the popular Word2Vec algorithm [40]. We train our Word2Vec embeddings on our entire corpus of pathology reports using a dimension size of 300, a window size of 5 words, and the continuous-bag-of-words algorithm. These word embeddings are set as learnable parameters during training so that each word embedding can be further tuned to maximize performance in our classification tasks. All non-deep learning approaches are implemented using the scikit-learn package in Python; these are each fitted on the train set and evaluated on the test set. All deep learning models are implemented using the Tensorflow package in Python; these are each trained on the train set using a batch size of 64 and the Adam optimizer [41] with learning rate 1E-4, beta1 0.9, and beta2 0.99. After every epoch, accuracy is evaluated on the validation set; training stops when validation accuracy fails to improve for 5 consecutive epochs. For each deep learning model, we save the model parameters following the epoch with the highest validation accuracy and use those parameters to evaluate on the test set. 4.2 Evaluation metrics Many of these classification tasks have extreme class imbalance \u2013 the pathology reports are distributed very unevenly between the possible classes. In particular, for histology, there are over 500 possible classes yet many of these histologies have fewer than 100 instances per class (see SI section A for histograms of instances per class). Thus, we evaluate the performance of each classification approach using two distinct metrics \u2013 overall accuracy and macro F-score. We note that in classification tasks such as ours in which each report is assigned to exactly one class, accuracy is the same as micro F-score. In each task, overall accuracy simply measures the percentage of reports in the test set classified correctly. This metric does not disproportionately penalize a classifier for performing poorly on the less populated classes. Macro F-score, on the other hand, is more heavily influenced by how well the classifier performs on the less prevalent classes. Intuitively, macro F-score measures classifier performance within each class, and then averages this performance across all classes; therefore, performing well in a rare class is equally important to performing well in a common class. Whereas accuracy measures how well a classifier performs overall, macro F-score better captures how well a classifier can identify cases that it does not see often, which is highly important in real-world settings. The mathematical implementation for macro F-score is described below: (8) Precision c = True Positives c True Positives c + False Positives c Recall c = True Positives c True Positives c + False Negatives c F 1 Score c = 2 \u00d7 Precision c \u00d7 Recall c Precision c + Recall c Macro F 1 Score = 1 n \u2211 c = i n F 1 Score c where n is the total number of possible classes within a given classification task and c is a specific class. For both accuracy and macro F-score, we establish 95% confidence intervals for each metric by bootstrapping samples from the test set \u2013 our procedure is described in SI section B. We use these confidence intervals to establish statistical significance between the difference in performance across the different approaches. We also measure training and inferences times for all classifiers \u2013 all models are timed on the same machine using four Intel Xeon E5-2695 v4 CPUs and one Tesla P100 GPU. 4.3 Experimental results The HiSAN achieves the highest accuracy and macro F-score across all five tasks, even compared to the HAN (Table 1 ). The performance of the HiSAN is especially noticeable for macro F-scores, which indicates that the HiSAN approach is much better at accurately classifying cases with very few examples. This is important for practical use because certain cancer types or histologies may be relatively rare but still need to be identified accurately. To further verify the statistical significance of the difference in performance between classifiers, we perform a McNemar's test [42] between the test set predictions of every pair of classifiers. Our results are shown in SI section D and confirm that the HiSAN's performance is significantly different than that of all other classifiers across all classification tasks. We show the F-score breakdown by class for each classifier on each classification task in SI section C. The HiSAN achieves the best F-score across the most classes, especially for the most prevalent classes; furthermore, when the HiSAN is not the top performer, it is still one of the top performers in the group. In the severely underrepresented classes (<1% of total samples), there is less consistency on which classifier performs best. We also note that some classes may be easier to identify than others \u2013 for example, in the site task, all classifiers are far better at identifying C60 (penis) than C48 (retroperitoneum and peritoneum) or C57 (other and unspecified female organs) even though they each have a similar number of training samples (0.13\u20130.18% of total samples). A manual examination revealed that the language for certain cancers is very distinct from all other cancers, making them easier to identify, while certain groups of cancers have significant language overlap, making them harder to distinguish; this is examined further in Sections 5.1 and 5.2. In Table 2 , we rerun our experiments on the site and histology tasks using a reduced dataset in which we remove all reports from the 15 most prevalent cancer sites. We keep the same train/validation/test splits as the original dataset; this results in 38,580 train samples, 9714 validation samples, and 11,600 test samples. Using this reduced dataset, the difference in performance between the HiSAN and the other classifiers is much clearer \u2013 this shows that when fewer samples are available for training, the HiSAN is better able to handle class imbalance. The HiSAN performs better than the SAN in both accuracy and macro F-score across all tasks, which suggests that utilizing a hierarchical structure that breaks down a long document into smaller chunks helps the model better locate text segments critical for classification, particularly in the more difficult tasks like histology and grade. This is consistent with our findings from our previous work with HANs \u2013 neural architectures specifically designed to process sequential data like RNNs and self-attention can locate important linguistic relationships over long text segments, but when these sequences get too long these architectures are unable to locate or retain critical information. By breaking down long documents like pathology reports into shorter segments like individual lines, we can take advantage of the benefits of RNNs or self-attention with far less information loss. We test two version of hierarchical splitting within the HiSAN, one which splits pathology reports into chunks of 10 tokens each, and one which splits pathology reports by line breaks (as described in our data cleaning section). We see that while both versions significantly outperform the SAN, splitting based on the line breaks performs slightly better \u2013 this makes sense because in most cases, the line breaks reflect how the original author organized information within the pathology report. For the remainder of this paper, references to the HiSAN will refer to the HiSAN that splits by lines. In terms of timing, the HiSAN takes roughly 2\u20133\u00d7 longer to train and 5x longer for inference compared to the CNN (Table 1 and Fig. 3 ). Relative to the HAN, the HiSAN completes training before the HAN completes a single epoch, and the HiSAN makes predictions over 10\u00d7 faster than the HAN. We note that our implementation of self-attention does not utilize positional embeddings. On its own, self-attention does not distinguish the order of words in an input sequence; therefore, many implementations of self-attention-based architectures utilize positional embeddings to capture information about word order [43,44]. However, we found that adding positional embeddings to the HiSAN did not consistently affect performance on our tasks (SI Section G). Furthermore, we found that adding additional architectural components from the HCAN [36] designed to capture more complex linguistic relationships reduce the performance of the HiSAN on pathology report classification (SI Section G). We expect that this is because the percentage of relevant words in cancer pathology reports is very low for our given tasks \u2013 only a few clinical concepts are useful for identifying a particular label. Therefore, compared to a task like sentiment analysis on short Yelp Reviews [36], word order and more complex linguistic features captured by additional architectural components such as parallel attention mechanisms are less important for our classification tasks. In our main set of experiments, we replace rare words (appearing fewer than five times in the corpus) with an \u201cunknown_word\u201d token. In SI Section H, we also show the performance of the HiSAN in which all possible words are included in the word embedding vocabulary matrix. Including all words gives small boost in performance in the histology task \u2013 we expect that this is because histology has the highest number of possible classes and highest class imbalance, so rare words may be useful for identifying rare classes. However, including all vocabulary decreases performance on all other tasks. 5 Discussion 5.1 Error analysis As described in our Dataset Details section, it is important to note that our ground truth labels are at the tumor-level rather than at the report-level \u2013 the label for each report is assigned based off all available data associated with the tumor ID associated with that report. Thus, it is possible that the tumor-level label may not be appropriate for a specific report if that report is for a secondary or metastatic site. To better understand how and why the HiSAN was misclassifying pathology reports, we manually analyzed 200 randomly selected pathology reports that were misclassified by the HiSAN. We focused on the site task because site is relatively easy for non-experts to identify compared to the other classification tasks. Based off our analysis of this subset, we identified four main categories of misclassified reports. In the first category, the report is misclassified because there does not appear to be any information associated with the ground truth site in the report; therefore, the ground truth site may be difficult or impossible to identify even by an expert. This is most likely because the report is an addendum or biopsy of a secondary or metastatic site. Out of 200 misclassified reports examined, 58 reports (29%) fell into this category. In the second category, the report mentions two or more (usually metastatic) sites. For these reports, the HiSAN may incorrectly predict a secondary site instead of the primary site. Out of 200 misclassified reports examined, 21 reports (10.5%) fell into this category. In the third category, the predicted site is close to ground truth site (i.e., within the same organ system or a neighboring organ), and the predicted site is often directly mentioned in the pathology report. For example, the ground truth site may be rectosigmoid junction and the HiSAN may predict colon, or the ground truth site may be cervix and the HiSAN may predict uterus. Out of 200 misclassified reports examined, 104 reports (52%) fell into this category. From these 104 reports, we attempted to manually predict the site without knowing the ground truth site or the HiSAN's predicted site. We predicted the ground truth site in 29% of the reports, agreed with the HiSAN's prediction in 60% of the reports, and chose a different incorrect site in the remaining 11% of the reports. The fourth and last category involves ill-defined sites (C76), unknown sites (C80), or a general catch-all site for a particular organ system (e.g., C57 unspecified female genital organs). Either the ground truth site is one of these ill-defined sites or the HiSAN mispredicts one of these ill-defined sites, despite the report mentioning specific organs or cancer sites. Out of 200 misclassified reports examined, 17 reports (8.5%) fell into this category. In addition, we generated and analyzed the confusion matrix from the HiSAN for each of the five tasks (available in SI section C). In the site task, most misclassifications were between sites associated with neighboring or highly related organs; four commonly confused groups of sites were (1) between C42 hematopoietic and reticuloendothelial systems, C44 skin, and C77 lymph nodes, (2) between C51 vulva, C52 vagina, C53 cervix, and C54 uterus, (3) between C64 kidney, C65 renal pelvis, C66 ureter, and C67 bladder, and (4) between C18 colon, C19 rectosigmoid junction, C20 rectum, and C21 anus. This is consistent with our manual analysis of the misclassified reports. Unsurprisingly, the classes with the lowest accuracy tended to be the classes with the least number of occurrences in the training set; this is similar to how overall accuracy on the site task dropped for all classifiers in our experiment in which we removed reports from the 15 most prevalent cancer sites. Our analysis shows that, for the site task, the misclassifications of the HiSAN (and other ML models) may be attributable to other factors, such as data mislabeling. To verify this, we tested the performance of all the trained models on a reduced test set of 9481 pathology reports. In this reduced test set, all reports belong to a Tumor ID that is associated with only a single pathology report, thereby eliminating any addendums and supplementary reports. The results of these experiments are available in SI section E. Compared to our results on the standard test set, we saw around 20\u201325% relative improvement in accuracy for the HiSAN across all tasks (measured by decrease in classification error) and around 4\u201324% relative improvement in macro F-score. Interestingly, the performance of NB, LR, CNN, and HAN slightly decreased or saw no significant improvement on the site, histology, and sometimes other tasks. We expect that this is because our reduced test set contains more minority classes in which these classifiers are less accurate (see SI Section C). Taken together, the benefit of using the HiSAN compared to the other classifiers is more apparent across all tasks on this reduced test set. 5.2 Visualizing document embeddings The penultimate layer of the HiSAN encodes each pathology report into a vector representation of fixed size \u2013 this \u201cdocument embedding\u201d represents the critical content within that report for a given classification task. In left side of Fig. 4 , we visualize the test set document embeddings on the site task colored by their ground truth cancer type, reduced into 2D-space via t-statistic stochastic neighborhood embedding (t-SNE) [45]. This allows us to better understand how the HiSAN encodes pathology reports associated with different types of cancers and also shows us the overlap of keywords in pathology reports associated with different types of cancers. We see that the HiSAN neatly separates the different cancer types into separate clusters. This type of visualization is useful for analyzing misclassified pathology reports. The right side of Fig. 4 shows only the document embeddings of the misclassified reports within the same test set. Using the same random sample of 200 misclassified reports from our error analysis, we examined the text of the report, the predicted and true labels, and the location of the document embedding in our visualization. We found that misclassified reports that are still located in a cluster belonging to the same organ system usually indicate that the HiSAN mispredicted a neighboring organ, while misclassified reports located in clusters associated with completely different organ systems usually indicate that the report mentions multiple sites or is about a secondary site rather than the primary site. Our visualization shows that within the site task, certain organ systems, such as digestive and urinary tract, tend to have higher rates of misclassification. Based off our manual examination, we found that there tends to be overlap in the language and terminology used for cancer sites belonging to certain organ systems (e.g., ureter cancer may also mention bladder), and therefore confusion is more likely. 5.3 Visualizing word importance A major benefit of the HiSAN is its transparency in how it makes decisions. Each hierarchy of the HiSAN is composed of attention mechanisms, which directly assign importance weights to the words/lines it is analyzing. We can therefore analyze the attention weights at both the self-attention and target-attention levels to see first how the HiSAN finds relationships between different words/lines and then how the HiSAN chooses which words/lines are most relevant to a given task. The top portion of Fig. 5 show an example pathology report annotated with the words that the HiSAN used to make its decision for the site task. We identified important words by following the trace of the attention weights across all the attention mechanisms in the HiSAN; we use the aggregate attention weights across all attention heads in the HiSAN. This can be a highly useful tool for human annotators when manually annotating a pathology report \u2013 because the HiSAN highlights the keywords in each report associated with the target classification task, the human can focus on those sections of the report to reduce the amount of time required to come to a decision. We also used these annotations to examine how and why the HiSAN fails in misclassified pathology reports, which were described above in our error analysis section. For example, if the HiSAN identifies keywords belonging to two entirely distinct cancer sites in the same report, it may suggest that metastasis is present or that the report is about a secondary site. The attention weights associated with the HiSAN's self-attention mechanism show how the HiSAN finds relationships between different words and lines. The bottom portion of Fig. 5 shows the self-attention weights associated with the word \u201cbreast\u201d within a particular line of a report; we use the aggregate self-attention weights across all attention heads in the HiSAN. We see that the HiSAN identifies that both \u201cductal carcinoma\u201d and \u201ccomedo necrosis\u201d are referring to \u201cbreast\u201d; a CNN-based model would have trouble capturing these relationships because CNNs can only identify relationships within a fixed window size, typically five words or less. This suggests that one reason the HiSAN outperforms CNNs is that it can capture more long distance linguistic relationships important for classification. Finally, the HiSAN utilizes multihead attention for all its attention mechanisms. We show in SI section F how different attention heads to pay attention to different relationships between words/lines. This is similar to ensembling with parallel networks, with each network analyzing a different portion of the embedding dimension of the words in a pathology report. The end result is a minor performance boost compared to using the same attention weights across the entire embedding dimension (see SI section F for a comparison of HiSAN performance using different numbers of attention heads). For annotation purposes, using the aggregate attention weights across all attention heads (as we did in Fig. 5) provides the most comprehensive visualization of all words and word relationships relevant to a given classification task, as it includes the different relationships captured within each individual attention head. 6 Conclusion In this paper, we presented the hierarchical self-attention network (HiSAN), a deep learning architecture designed for information retrieval from pathology reports. We demonstrated its superior performance relative to other traditional ML and deep learning text classification techniques across five classification tasks \u2013 site, laterality, behavior, histology, and grade \u2013 using a corpus of 374,899 pathology reports. The HiSAN achieved the best accuracy and macro-F score across all five tasks; its performance relative to the other classifiers was especially strong on the less prevalent classes. Furthermore, the HiSAN trains more than an order of magnitude faster than the previous state-of-the-art, HANs, and its attention-based architecture enables easy visualization of its decision-making process for any given pathology report. One finding of our study was that the HiSAN (and other classifiers) have more difficulty accurately classifying reports belonging to tumor IDs associated with multiple reports. This is because our ground truth labels were at the tumor level rather than report level. We found the performance of the HiSAN to improve significantly by limiting our test set to reports belonging to tumor IDs associated with only a single report. Therefore in practice, the performance of the HiSAN may be improved by restricting the set of pathology reports it is applied to (e.g., excluding metastatic at diagnosis and excluding reports that are dated long after diagnosis). For future work, we plan to examine methods that account for contextual information from all reports associated with the same unique tumor ID \u2013 for example, an additional report-level hierarchy can be used to capture report-level context and identify which reports are most relevant for identifying the ground truth labels for each tumor ID. Although our HiSAN achieves higher accuracy and macro F-score across all tasks compared to the previous state-of-the-art and all other baselines, there is still room for improvement on the difficult tasks, such as histology and grade, in order to bring it to a level in which it may be deployed in place of a human expert. For these more difficult tasks, uncertainty quantification methods may be utilized so that predictions with high confidence can be used without human review, thus reducing the burden on human annotators. We are currently exploring this aspect in partnership with NCI's SEER program. The code for the HiSAN is available online at github.com/iamshang1/Projects/tree/master/Papers/HiSAN. Funding This work has been supported in part by the Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program established by the U.S. Department of Energy (DOE) and the National Cancer Institute (NCI) of the National Institutes of Health. This work was performed under the auspices of the U.S. Department of Energy by Argonne National Laboratory under Contract DE-AC02-06-CH11357, Lawrence Livermore National Laboratory under Contract DEAC52-07NA27344, Los Alamos National Laboratory under Contract DE-AC5206NA25396, and Oak Ridge National Laboratory under Contract DE-AC05-00OR22725. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725. Authors\u2019 contribution S.G., A.R., and G.T. conceptualized the study and developed the experimental setup. S.G. designed and implemented the HiSAN and ran the experiments. J.Q., M.A., J.H., N.S., H.J.Y., and B.C. developed the text preprocessing and experimental pipeline. P.A.F, L.P., X.C.W, and L.C. oversaw and organized the preparation of the pathology reports. S.G. wrote the manuscript with support from A.R. and G.T. All authors read and approved the manuscript. Conflict of interest The authors have no competing interests to disclose. Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, at https://doi.org/10.1016/j.artmed.2019.101726. Appendix A Supplementary data The following are the supplementary data to this article: References [1] C. Alberti D. Andor I. Bogatyy M. Collins D. Gillick L. Kong Syntaxnet models for the conll 2017 shared task. 2017 arXiv:1703.04929 [2] L. Zhang S. Wang B. Liu Deep learning for sentiment analysis: a survey Wiley Interdisc Rev Data Min Knowl Discov 8 2018 [3] M. Hu Y. Peng X. Qiu Reinforced mnemonic reader for machine comprehension Proc IJCAI 2017 [4] J. Mullenbach S. Wiegreffe J. Duke J. Sun J. Eisenstein Explainable prediction of medical codes from clinical text NAACL HLT 2018: 16th annual conference of the north American chapter of the association for computational linguistics: human language technologies, vol. 1 2018 1101 1111 [5] S. Gehrmann F. Dernoncourt Y. Li E.T. Carlson J.T. Wu J. Welt Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives PLOS ONE 13 2 2018 [6] E. Ward B.A. Kohler C. Eheman A.G. Zauber R.N. Anderson A. Jemal Annual report to the nation on the status of cancer, 1975\u20132014, featuring survival J Natl Cancer Inst 109 9 2017 [7] D.S. Carrell S. Halgrim D.T. Tran D.S. Buist J. Chubak W.W. Chapman Using natural language processing to improve efficiency of manual chart abstraction in research: the case of breast cancer recurrence Am J Epidemiol 179 6 2014 749 758 [8] A.N. Nguyen M.J. Lawley D.P. Hansen R.V. Bowman B.E. Clarke E.E. Duhig Symbolic rule-based classification of lung cancer stages from free-text pathology reports J Am Med Inform Assoc 17 4 2010 440 445 [9] R. Weegar H. Dalianis Creating a rule based system for text mining of Norwegian breast cancer pathology reports Proc LOUHI 2015 73 78 [10] J. Lee H.-J. Song E. Yoon S.-B. Park S.-H. Park J.-W. Seo Automated extraction of biomarker information from pathology reports BMC Med Inform Decis Making 18 2018 29 [11] F. Xie J. Lee C.E. Munoz-Plaza E.E. Hahn W. Chen Application of text information extraction system for real-time cancer case identification in an integrated healthcare organization J Pathol Inform 8 2017 [12] A.N. Nguyen J. Moore J. O\u2019Dwyer S. Philpot Assessing the utility of automatic cancer registry notifications data extraction from free-text pathology reports AMIA annual symposium proceedings, vol. 953 American Medical Informatics Association 2015 [13] A. Coden G. Savova I. Sominsky M. Tanenblatt J. Masanz K. Schuler Automatically extracting cancer disease characteristics from pathology reports into a disease knowledge representation model J Biomed Inform 42 2009 937 949 [14] D. Martinez Y. Li Information extraction from pathology reports in a hospital setting Proc ACM int conf inf knowl manag 2011 1877 1882 ACM [15] Y. Li D. Martinez Information extraction of multiple categories from pathology reports Proc ALTA 2010 41 48 [16] A. Yala R. Barzilay L. Salama M. Griffin G. Sollender A. Bardia Using machine learning to parse breast pathology reports Breast Cancer Res Treat 161 2 2017 203 211 [17] W.-w. Yim T. Denman S.W. Kwan M. Yetisgen Tumor information extraction in radiology reports for hepatocellular carcinoma patients AMIA Summits Transl Sci Proc 2016 2016 455 [18] S. Zheng J.J. Lu C. Appin D. Brat F. Wang Support patient search on pathology reports with interactive online learning based data extraction J Pathol Inform 6 2015 [19] Y. Wang L. Wang M. Rastegar-Mojarad S. Moon F. Shen N. Afzal Clinical information extraction applications: a literature review J Biomed Inform 77 2018 34 49 [20] A.M. Pinto H.G. Oliveira A.O. Alves Comparing the performance of different NLP toolkits in formal and social media text Proc SLATE vol. 51 2016 16 [21] X. Zhang J.J. Zhao Y. LeCun Character-level convolutional networks for text classification Adv Neural Inf Process Syst 2015 649 657 [22] T. Young D. Hazarika S. Poria E. Cambria Recent trends in deep learning based natural language processing. 2017 arXiv:1708.02709 [23] J. Camacho-Collados M.T. Pilehvar From word to sense embeddings: a survey on vector representations of meaning. 2018 arXiv:1805.04032 [24] J.X. Qiu H.-J. Yoon P.A. Fearn G.D. Tourassi Deep learning for automated extraction of primary sites from cancer pathology reports IEEE J Biomed Health Inform 22 2018 244 251 [25] C. Xiao E. Choi J. Sun Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review J Am Med Inform Assoc 25 2018 1419 1428 [26] M. Hughes I. Li S. Kotoulas T. Suzumura Medical text classification using convolutional neural networks Stud Health Technol Inform 235 2017 246 250 [27] A. Conneau H. Schwenk L. Barrault Y. Lecun Very deep convolutional networks for text classification Proc ACL-EACL 2017 1107 1116 [28] H.T. Le C. Cerisara A. Denis Do convolutional networks need to be deep for text classification Proc conf AAAI artif intell 2018 29 36 [29] A.N. Jagannatha H. Yu Bidirectional RNN for medical event detection in electronic health records Proc HLT-NAACL 2016 473 482 [30] A.N. Jagannatha H. Yu Structured prediction models for RNN based sequence labeling in clinical text Proc conf empir methods nat lang process, vol. 2016 2016 856 865 [31] R. Pascanu T. Mikolov Y. Bengio On the difficulty of training recurrent neural networks Proc int conf mach learn 2013 1310 1318 [32] Z. Yang D. Yang C. Dyer X. He A. Smola E. Hovy Hierarchical attention networks for document classification Proc HLT-NAACL 2016 1480 1489 [33] S. Gao M.T. Young J.X. Qiu H.J. Yoon J.B. Christian P.A. Fearn Hierarchical attention networks for information extraction from cancer pathology reports J Am Med Inform Assoc 25 3 2018 321 330 [34] A. Vaswani N. Shazeer N. Parmar J. Uszkoreit L. Jones A.N. Gomez Attention is all you need Adv Neural Inf Process Syst 2017 5998 6008 [35] A.W. Yu D. Dohan M.-T. Luong R. Zhao K. Chen M. Norouzi Qanet: combining local convolution with global self-attention for reading comprehension Proc ICLR 2018 [36] S. Gao A. Ramanathan G.D. Tourassi Hierarchical convolutional attention networks for text classification Proc third workshop on representation learning for NLP 2018 11 23 [37] L.K. Senel I. Utlu V. Yucesoy A. Koc T. Cukur Semantic structure and interpretability of word embeddings IEEE Trans Audio Speech Lang Process 26 2018 1769 1779 [38] H. Peng L. Mou G. Li Y. Chen Y. Lu Z. Jin A comparative study on regularization strategies for embedding-based neural networks Proc conf empir methods nat lang process 2015 2106 2111 [39] Y. Kim Convolutional neural networks for sentence classification Proc conf empir methods nat lang process 2014 1746 1751 [40] T. Mikolov I. Sutskever K. Chen G.S. Corrado J. Dean Distributed representations of words and phrases and their compositionality Adv Neural Inf Process Syst 2013 3111 3119 [41] D.P. Kingma J.L. Ba Adam: a method for stochastic optimization Proc ICLR 2015 [42] T.G. Dietterich Approximate statistical tests for comparing supervised classification learning algorithms Neural Comput 10 1998 1895 1923 [43] J. Gehring M. Auli D. Grangier D. Yarats Y.N. Dauphin Convolutional sequence to sequence learning Proc int conf mach learn 2017 1243 1252 [44] C.N. dos Santos B. Xiang B. Zhou Classifying relations by ranking with convolutional neural networks 2015 626 634 [45] L. van der Maaten G.E. Hinton Visualizing data using t-SNE J Mach Learn Res 9 2008 2579 2605", "scopus-id": "85073144666", "pubmed-id": "31813492", "coredata": {"eid": "1-s2.0-S0933365719303562", "dc:description": "Abstract We introduce a deep learning architecture, hierarchical self-attention networks (HiSANs), designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy, faster training, and clear interpretability. We evaluate performance on a corpus of 374,899 pathology reports obtained from the National Cancer Institute's (NCI) Surveillance, Epidemiology, and End Results (SEER) program. Each pathology report is associated with five clinical classification tasks \u2013 site, laterality, behavior, histology, and grade. We compare the performance of the HiSAN against other machine learning and deep learning approaches commonly used on medical text data \u2013 Naive Bayes, logistic regression, convolutional neural networks, and hierarchical attention networks (the previous state-of-the-art). We show that HiSANs are superior to other machine learning and deep learning text classifiers in both accuracy and macro F-score across all five classification tasks. Compared to the previous state-of-the-art, hierarchical attention networks, HiSANs not only are an order of magnitude faster to train, but also achieve about 1% better relative accuracy and 5% better relative macro F-score.", "openArchiveArticle": "false", "prism:coverDate": "2019-11-30", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0933365719303562", "dc:creator": [{"@_fa": "true", "$": "Gao, Shang"}, {"@_fa": "true", "$": "Qiu, John X."}, {"@_fa": "true", "$": "Alawad, Mohammed"}, {"@_fa": "true", "$": "Hinkle, Jacob D."}, {"@_fa": "true", "$": "Schaefferkoetter, Noah"}, {"@_fa": "true", "$": "Yoon, Hong-Jun"}, {"@_fa": "true", "$": "Christian, Blair"}, {"@_fa": "true", "$": "Fearn, Paul A."}, {"@_fa": "true", "$": "Penberthy, Lynne"}, {"@_fa": "true", "$": "Wu, Xiao-Cheng"}, {"@_fa": "true", "$": "Coyle, Linda"}, {"@_fa": "true", "$": "Tourassi, Georgia"}, {"@_fa": "true", "$": "Ramanathan, Arvind"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0933365719303562"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0933365719303562"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0933-3657(19)30356-2", "prism:volume": "101", "articleNumber": "101726", "prism:publisher": "The Authors. Published by Elsevier B.V.", "dc:title": "Classifying cancer pathology reports with hierarchical self-attention networks", "prism:copyright": "\u00a9 2019 The Authors. Published by Elsevier B.V.", "openaccess": "1", "prism:issn": "09333657", "dcterms:subject": [{"@_fa": "true", "$": "Cancer pathology reports"}, {"@_fa": "true", "$": "Clinical reports"}, {"@_fa": "true", "$": "Deep learning"}, {"@_fa": "true", "$": "Natural language processing"}, {"@_fa": "true", "$": "Text classification"}], "openaccessArticle": "true", "prism:publicationName": "Artificial Intelligence in Medicine", "openaccessSponsorType": "FundingBody", "prism:pageRange": "101726", "prism:coverDisplayDate": "November 2019", "prism:doi": "10.1016/j.artmed.2019.101726", "prism:startingPage": "101726", "dc:identifier": "doi:10.1016/j.artmed.2019.101726", "openaccessSponsorName": "National Institutes of Health"}, "objects": {"object": [{"@category": "thumbnail", "@height": "163", "@width": "135", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5112", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "89", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5251", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "215", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5807", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "130", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13132", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "106", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6476", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "410", "@width": "339", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27359", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "275", "@width": "678", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "35536", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "229", "@width": "301", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "10210", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "403", "@width": "678", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "97156", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "328", "@width": "678", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "51192", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1813", "@width": "1500", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "190385", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1215", "@width": "3000", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "255788", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1014", "@width": "1333", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "73477", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1785", "@width": "3000", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "881403", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1452", "@width": "3000", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "417286", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-mmc1.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "APPLICATION", "@size": "2293991", "@ref": "mmc1", "@mimetype": "application/pdf"}, {"@category": "thumbnail", "@height": "34", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "273", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "39", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "225", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "34", "@width": "61", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "262", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "35", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "275", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "57", "@width": "283", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1123", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "58", "@width": "441", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1979", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "57", "@width": "396", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1743", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "159", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "603", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "35", "@width": "98", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "378", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "35", "@width": "100", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "388", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "60", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1552", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "35", "@width": "60", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "272", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "60", "@width": "348", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1435", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "156", "@width": "309", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si21.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "3140", "@ref": "si21", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "35", "@width": "272", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "850", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "287", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "34", "@width": "64", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "278", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "34", "@width": "61", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "269", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "139", "@width": "315", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "3038", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "40", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "204", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "34", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365719303562-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "173", "@ref": "si9", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85073144666"}}