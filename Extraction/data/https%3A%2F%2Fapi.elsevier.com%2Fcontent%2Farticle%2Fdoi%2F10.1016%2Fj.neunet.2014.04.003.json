{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014000872", "dc:identifier": "doi:10.1016/j.neunet.2014.04.003", "eid": "1-s2.0-S0893608014000872", "prism:doi": "10.1016/j.neunet.2014.04.003", "pii": "S0893-6080(14)00087-2", "dc:title": "Ideal regularization for learning kernels from labels ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "56", "prism:startingPage": "22", "prism:endingPage": "34", "prism:pageRange": "22-34", "dc:format": "application/json", "prism:coverDate": "2014-08-31", "prism:coverDisplayDate": "August 2014", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Pan, Binbin"}, {"@_fa": "true", "$": "Lai, Jianhuang"}, {"@_fa": "true", "$": "Shen, Lixin"}], "dc:description": "\n               Abstract\n               \n                  In this paper, we propose a new form of regularization that is able to utilize the label information of a data set for learning kernels. The proposed regularization, referred to as ideal regularization, is a linear function of the kernel matrix to be learned. The ideal regularization allows us to develop efficient algorithms to exploit labels. Three applications of the ideal regularization are considered. Firstly, we use the ideal regularization to incorporate the labels into a standard kernel, making the resulting kernel more appropriate for learning tasks. Next, we employ the ideal regularization to learn a data-dependent kernel matrix from an initial kernel matrix (which contains prior similarity information, geometric structures, and labels of the data). Finally, we incorporate the ideal regularization to some state-of-the-art kernel learning problems. With this regularization, these learning problems can be formulated as simpler ones which permit more efficient solvers. Empirical results show that the ideal regularization exploits the labels effectively and efficiently.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Kernel methods"}, {"@_fa": "true", "$": "Regularization"}, {"@_fa": "true", "$": "Labels"}, {"@_fa": "true", "$": "Ideal kernel"}, {"@_fa": "true", "$": "Semi-supervised learning"}, {"@_fa": "true", "$": "von Neumann divergence"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014000872", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014000872", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84900001495", "scopus-eid": "2-s2.0-84900001495", "pubmed-id": "24824969", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84900001495", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20140502", "$": "2014-05-02"}}}}}