{"scopus-eid": "2-s2.0-59349114281", "originalText": "serial JL 271122 291210 291684 291736 31 80 Vision Research VISIONRESEARCH 2008-12-23 2008-12-23 2011-01-20T15:00:00 1-s2.0-S0042698908005695 S0042-6989(08)00569-5 S0042698908005695 10.1016/j.visres.2008.11.006 S300 S300.1 FULL-TEXT 1-s2.0-S0042698909X00027 2015-05-14T02:56:28.155321-04:00 0 0 20090201 20090228 2009 2008-12-23T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 0042-6989 00426989 49 49 3 3 Volume 49, Issue 3 11 374 382 374 382 200902 February 2009 2009-02-01 2009-02-28 2009 article fla Copyright \u00a9 2008 Elsevier Ltd. All rights reserved. GENERATINGCUSTOMISEDEXPERIMENTALSTIMULIFORVISUALSEARCHUSINGGENETICALGORITHMSSHOWSEVIDENCEFORACONTINUUMSEARCHEFFICIENCY VERMA M 1 Introduction 2 Producing the saliency map 2.1 Feature extraction 2.2 Feature combination strategies 2.3 Assigning weights through the Hurst exponent 2.4 Comparative study 3 The Genetic Algorithm 4 Experiments 4.1 Experiment 1: block pop-out test 4.1.1 Participants and apparatus 4.1.2 Procedure 4.1.3 Results 4.2 Experiment 2: single target visual search 4.2.1 Procedure 4.2.2 Results 4.3 Discussion and conclusions Acknowledgements References ADELSON 1985 284 299 E ALLMAN 1985 407 430 J ANDERSON 1985 1147 1154 S BECK 1967 491 495 J BERGEN 1983 857 863 J BURT 1983 532 540 P DAUGMAN 1985 1160 1169 J DEVALOIS 1982 545 559 R DERRINGTON 1984 241 256 A DESIMONE 1995 193 222 R DUNCAN 1989 433 458 J FIELD 1987 2379 2394 D FIELD 1986 379 400 D FOSTER 1991 75 81 D GEISLER 1997 897 919 W HOLLAND 1975 J ADAPTATIONINNATURALARTIFICIALSYSTEMS HUANG 2005 101 111 L HUBEL 1962 106 154 D HURVICH 1957 384 404 L ITTI 2000 1489 1506 L ITTI 2001 161 169 L ITTI 1998 1254 1259 L JONES 1987 1233 1258 J JULESZ 1981 91 97 B KNIERIM 1992 961 980 J KOCH 1985 219 227 C MORAGLIA 1989 675 689 G NOTHDURFT 1991 1073 1078 H NOTHDURFT 1992 355 375 H NOTHDURFT 1999 15 34 H REICHARDT 1961 303 317 W SENSORYCOMMUNICATION AUTOCORRELATIONAPRINCIPLEFOREVALUATIONSENSORYINFORMATIONBYCENTRALNERVOUSSYSTEM SILLITO 1995 492 496 A TITCHENER 1908 E LECTURESELEMENTARYPSYCHOLOGYFEELINGATTENTION TREISMAN 1985 156 177 A TREISMAN 1988 201 237 A TREISMAN 1980 97 136 A TREISMAN 1988 15 48 A WEBSTER 1985 1124 1132 M WOLFE 1994 202 238 J WOLFE 1998 J ATTENTION VISUALSEARCH WOLFE 1999 111 125 J ZHAOPING 2006 911 933 L VERMAX2009X374 VERMAX2009X374X382 VERMAX2009X374XM VERMAX2009X374X382XM 2013-07-18T13:41:53Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S0042-6989(08)00569-5 S0042698908005695 1-s2.0-S0042698908005695 10.1016/j.visres.2008.11.006 271122 2011-02-03T13:36:38.503433-05:00 2009-02-01 2009-02-28 1-s2.0-S0042698908005695-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/MAIN/application/pdf/11a9a750994f169885ef99761e322043/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/MAIN/application/pdf/11a9a750994f169885ef99761e322043/main.pdf main.pdf pdf true 585307 MAIN 9 1-s2.0-S0042698908005695-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/PREVIEW/image/png/cc9bc5ede2adc49fb6201910e35b3003/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/PREVIEW/image/png/cc9bc5ede2adc49fb6201910e35b3003/main_1.png main_1.png png 58726 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0042698908005695-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/d655895ab678d05c0776c7d11211251a/si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/d655895ab678d05c0776c7d11211251a/si9.gif si9 si9.gif gif 212 19 11 ALTIMG 1-s2.0-S0042698908005695-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/1620213177b314f7084b89980e56674b/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/1620213177b314f7084b89980e56674b/si8.gif si8 si8.gif gif 517 32 89 ALTIMG 1-s2.0-S0042698908005695-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/7aabd78cc62a787eaf14530c23a1f1fb/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/7aabd78cc62a787eaf14530c23a1f1fb/si7.gif si7 si7.gif gif 856 47 108 ALTIMG 1-s2.0-S0042698908005695-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/e89959e1fa3c69e8a6d14a82c8275634/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/e89959e1fa3c69e8a6d14a82c8275634/si6.gif si6 si6.gif gif 1167 42 153 ALTIMG 1-s2.0-S0042698908005695-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/ab426d64057107536bf3af669ce0ad9d/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/ab426d64057107536bf3af669ce0ad9d/si5.gif si5 si5.gif gif 943 42 134 ALTIMG 1-s2.0-S0042698908005695-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/476f91253ccff96ce0f42ca53570fd72/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/476f91253ccff96ce0f42ca53570fd72/si4.gif si4 si4.gif gif 206 21 13 ALTIMG 1-s2.0-S0042698908005695-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/50ea70c4fb7149b6cb3057971289cec8/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/50ea70c4fb7149b6cb3057971289cec8/si3.gif si3 si3.gif gif 860 18 184 ALTIMG 1-s2.0-S0042698908005695-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/8e71b039f67fca339dea2f9f8d6151d4/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/8e71b039f67fca339dea2f9f8d6151d4/si2.gif si2 si2.gif gif 683 38 124 ALTIMG 1-s2.0-S0042698908005695-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/3cf020568db7fb303630e8dffa819b4b/si17.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/3cf020568db7fb303630e8dffa819b4b/si17.gif si17 si17.gif gif 628 20 114 ALTIMG 1-s2.0-S0042698908005695-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/7fc78ec5b35a4982f177e6dcd69f22d8/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/7fc78ec5b35a4982f177e6dcd69f22d8/si16.gif si16 si16.gif gif 553 20 86 ALTIMG 1-s2.0-S0042698908005695-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/d4c3ea3d2f0f004f71b01e0e2283eaa8/si15.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/d4c3ea3d2f0f004f71b01e0e2283eaa8/si15.gif si15 si15.gif gif 537 24 72 ALTIMG 1-s2.0-S0042698908005695-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/18e14660aa10a5337287567a09d76f12/si14.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/18e14660aa10a5337287567a09d76f12/si14.gif si14 si14.gif gif 1023 32 222 ALTIMG 1-s2.0-S0042698908005695-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/a00866456ba3a82e46cc3f6c88e52c7a/si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/a00866456ba3a82e46cc3f6c88e52c7a/si13.gif si13 si13.gif gif 1433 49 173 ALTIMG 1-s2.0-S0042698908005695-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/a38f4fcd3cf620b40133f876c4adc44e/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/a38f4fcd3cf620b40133f876c4adc44e/si12.gif si12 si12.gif gif 1091 42 156 ALTIMG 1-s2.0-S0042698908005695-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/c0cdc0476c6677bea15e0bfa1cdc3e91/si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/c0cdc0476c6677bea15e0bfa1cdc3e91/si11.gif si11 si11.gif gif 790 20 168 ALTIMG 1-s2.0-S0042698908005695-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/fd3260069c7d74d7ee3c59408e650403/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/fd3260069c7d74d7ee3c59408e650403/si10.gif si10 si10.gif gif 831 18 166 ALTIMG 1-s2.0-S0042698908005695-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/STRIPIN/image/gif/ada95b49033192a6367523959f2fa262/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/STRIPIN/image/gif/ada95b49033192a6367523959f2fa262/si1.gif si1 si1.gif gif 1480 19 319 ALTIMG 1-s2.0-S0042698908005695-fx1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx1/DOWNSAMPLED/image/gif/6031c3ba772f8df69cd83f7d2d163c1f/fx1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx1/DOWNSAMPLED/image/gif/6031c3ba772f8df69cd83f7d2d163c1f/fx1.gif fx1 fx1.gif gif 146 16 13 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx1/HIGHRES/image/gif/98a2aef51e21b86b0fa279882dec020f/fx1_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx1/HIGHRES/image/gif/98a2aef51e21b86b0fa279882dec020f/fx1_lrg.gif fx1 fx1.sml sml 668 161 128 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx1_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx1/HIGHRES/image/gif/98a2aef51e21b86b0fa279882dec020f/fx1_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx1/HIGHRES/image/gif/98a2aef51e21b86b0fa279882dec020f/fx1_lrg.gif fx1 fx1_lrg.gif gif 668 161 128 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx10/DOWNSAMPLED/image/gif/9c21ec5c1b08823fae8fb74835c418dd/fx10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx10/DOWNSAMPLED/image/gif/9c21ec5c1b08823fae8fb74835c418dd/fx10.gif fx10 fx10.gif gif 163 17 17 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx10/HIGHRES/image/gif/3521dbb55a7c0ece94b81600a91798d6/fx10_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx10/HIGHRES/image/gif/3521dbb55a7c0ece94b81600a91798d6/fx10_lrg.gif fx10 fx10.sml sml 874 167 169 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx10_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx10/HIGHRES/image/gif/3521dbb55a7c0ece94b81600a91798d6/fx10_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx10/HIGHRES/image/gif/3521dbb55a7c0ece94b81600a91798d6/fx10_lrg.gif fx10 fx10_lrg.gif gif 874 167 169 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx11/DOWNSAMPLED/image/gif/bb16a7b70624e24c1f11732c563d5826/fx11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx11/DOWNSAMPLED/image/gif/bb16a7b70624e24c1f11732c563d5826/fx11.gif fx11 fx11.gif gif 149 18 14 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx11.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx11/THUMBNAIL/image/gif/e68498886c519fd7f1b4dc99e91cd9aa/fx11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx11/THUMBNAIL/image/gif/e68498886c519fd7f1b4dc99e91cd9aa/fx11.sml fx11 fx11.sml sml 755 174 139 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx11_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx11/HIGHRES/image/gif/e68498886c519fd7f1b4dc99e91cd9aa/fx11_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx11/HIGHRES/image/gif/e68498886c519fd7f1b4dc99e91cd9aa/fx11_lrg.gif fx11 fx11_lrg.gif gif 755 174 139 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx12/DOWNSAMPLED/image/gif/5dd4fc27828cc97e293537c5ac55901c/fx12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx12/DOWNSAMPLED/image/gif/5dd4fc27828cc97e293537c5ac55901c/fx12.gif fx12 fx12.gif gif 161 18 15 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx12.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx12/HIGHRES/image/gif/55f3f11db7762ed0f67f418948b2dec9/fx12_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx12/HIGHRES/image/gif/55f3f11db7762ed0f67f418948b2dec9/fx12_lrg.gif fx12 fx12.sml sml 886 180 154 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx12_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx12/HIGHRES/image/gif/55f3f11db7762ed0f67f418948b2dec9/fx12_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx12/HIGHRES/image/gif/55f3f11db7762ed0f67f418948b2dec9/fx12_lrg.gif fx12 fx12_lrg.gif gif 886 180 154 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx2/DOWNSAMPLED/image/gif/6bbb389d9f55955792f3a44f423c537d/fx2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx2/DOWNSAMPLED/image/gif/6bbb389d9f55955792f3a44f423c537d/fx2.gif fx2 fx2.gif gif 145 18 16 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx2/HIGHRES/image/gif/0a0e638dd74769b208e29d168703c18d/fx2_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx2/HIGHRES/image/gif/0a0e638dd74769b208e29d168703c18d/fx2_lrg.gif fx2 fx2.sml sml 667 180 156 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx2_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx2/HIGHRES/image/gif/0a0e638dd74769b208e29d168703c18d/fx2_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx2/HIGHRES/image/gif/0a0e638dd74769b208e29d168703c18d/fx2_lrg.gif fx2 fx2_lrg.gif gif 667 180 156 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx3/DOWNSAMPLED/image/gif/dea289f7a686ec4ad21c3f730172c46d/fx3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx3/DOWNSAMPLED/image/gif/dea289f7a686ec4ad21c3f730172c46d/fx3.gif fx3 fx3.gif gif 144 17 13 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx3/HIGHRES/image/gif/07f5cc2ed76c54237b084d7a7c7671b8/fx3_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx3/HIGHRES/image/gif/07f5cc2ed76c54237b084d7a7c7671b8/fx3_lrg.gif fx3 fx3.sml sml 651 176 134 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx3_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx3/HIGHRES/image/gif/07f5cc2ed76c54237b084d7a7c7671b8/fx3_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx3/HIGHRES/image/gif/07f5cc2ed76c54237b084d7a7c7671b8/fx3_lrg.gif fx3 fx3_lrg.gif gif 651 176 134 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx4/DOWNSAMPLED/image/gif/51abdae8488fc417a1612f5de6e53eb7/fx4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx4/DOWNSAMPLED/image/gif/51abdae8488fc417a1612f5de6e53eb7/fx4.gif fx4 fx4.gif gif 152 16 14 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx4/HIGHRES/image/gif/a9fe8d41ed5fb8089c652f8cfed9da27/fx4_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx4/HIGHRES/image/gif/a9fe8d41ed5fb8089c652f8cfed9da27/fx4_lrg.gif fx4 fx4.sml sml 760 161 139 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx4_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx4/HIGHRES/image/gif/a9fe8d41ed5fb8089c652f8cfed9da27/fx4_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx4/HIGHRES/image/gif/a9fe8d41ed5fb8089c652f8cfed9da27/fx4_lrg.gif fx4 fx4_lrg.gif gif 760 161 139 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx5/DOWNSAMPLED/image/gif/aa0340c3683303ec3f9078c9211c0069/fx5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx5/DOWNSAMPLED/image/gif/aa0340c3683303ec3f9078c9211c0069/fx5.gif fx5 fx5.gif gif 156 15 13 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx5/HIGHRES/image/gif/7b65d3c30e2b2909283e7935c8ec8431/fx5_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx5/HIGHRES/image/gif/7b65d3c30e2b2909283e7935c8ec8431/fx5_lrg.gif fx5 fx5.sml sml 803 154 130 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx5_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx5/HIGHRES/image/gif/7b65d3c30e2b2909283e7935c8ec8431/fx5_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx5/HIGHRES/image/gif/7b65d3c30e2b2909283e7935c8ec8431/fx5_lrg.gif fx5 fx5_lrg.gif gif 803 154 130 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx6/DOWNSAMPLED/image/gif/a2c5d91c8fc9b0bac3f1383e33a0206c/fx6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx6/DOWNSAMPLED/image/gif/a2c5d91c8fc9b0bac3f1383e33a0206c/fx6.gif fx6 fx6.gif gif 182 18 13 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx6/HIGHRES/image/gif/2fc95369f372f59b24f6f9de157657fd/fx6_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx6/HIGHRES/image/gif/2fc95369f372f59b24f6f9de157657fd/fx6_lrg.gif fx6 fx6.sml sml 1180 180 130 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx6_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx6/HIGHRES/image/gif/2fc95369f372f59b24f6f9de157657fd/fx6_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx6/HIGHRES/image/gif/2fc95369f372f59b24f6f9de157657fd/fx6_lrg.gif fx6 fx6_lrg.gif gif 1180 180 130 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx7/DOWNSAMPLED/image/gif/8c00bb44c8262510cb5cc04ad4438f91/fx7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx7/DOWNSAMPLED/image/gif/8c00bb44c8262510cb5cc04ad4438f91/fx7.gif fx7 fx7.gif gif 143 17 15 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx7/HIGHRES/image/gif/ba7330f78ff28bb2cc81496cc0538cc0/fx7_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx7/HIGHRES/image/gif/ba7330f78ff28bb2cc81496cc0538cc0/fx7_lrg.gif fx7 fx7.sml sml 681 169 150 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx7_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx7/HIGHRES/image/gif/ba7330f78ff28bb2cc81496cc0538cc0/fx7_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx7/HIGHRES/image/gif/ba7330f78ff28bb2cc81496cc0538cc0/fx7_lrg.gif fx7 fx7_lrg.gif gif 681 169 150 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx8/DOWNSAMPLED/image/gif/b8aa7e09751a8cf9e588e9e0e394ef12/fx8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx8/DOWNSAMPLED/image/gif/b8aa7e09751a8cf9e588e9e0e394ef12/fx8.gif fx8 fx8.gif gif 149 17 14 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx8/HIGHRES/image/gif/c360f12ee783f14998d7debe96d02b51/fx8_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx8/HIGHRES/image/gif/c360f12ee783f14998d7debe96d02b51/fx8_lrg.gif fx8 fx8.sml sml 682 174 143 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx8_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx8/HIGHRES/image/gif/c360f12ee783f14998d7debe96d02b51/fx8_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx8/HIGHRES/image/gif/c360f12ee783f14998d7debe96d02b51/fx8_lrg.gif fx8 fx8_lrg.gif gif 682 174 143 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-fx9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx9/DOWNSAMPLED/image/gif/97a67b70c1767ab6c6782ba37d111eb5/fx9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx9/DOWNSAMPLED/image/gif/97a67b70c1767ab6c6782ba37d111eb5/fx9.gif fx9 fx9.gif gif 146 17 14 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-fx9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx9/HIGHRES/image/gif/5fc1a17c4f9598650534ee6307dafba7/fx9_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx9/HIGHRES/image/gif/5fc1a17c4f9598650534ee6307dafba7/fx9_lrg.gif fx9 fx9.sml sml 726 176 143 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-fx9_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/fx9/HIGHRES/image/gif/5fc1a17c4f9598650534ee6307dafba7/fx9_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/fx9/HIGHRES/image/gif/5fc1a17c4f9598650534ee6307dafba7/fx9_lrg.gif fx9 fx9_lrg.gif gif 726 176 143 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr2/DOWNSAMPLED/image/jpeg/47af671d657a27f078d092757f80a44d/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr2/DOWNSAMPLED/image/jpeg/47af671d657a27f078d092757f80a44d/gr2.jpg gr2 gr2.jpg jpg 93898 582 567 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr2/THUMBNAIL/image/gif/82327e56b348db4597c997e7e6993664/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr2/THUMBNAIL/image/gif/82327e56b348db4597c997e7e6993664/gr2.sml gr2 gr2.sml sml 4337 164 160 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr2/HIGHRES/image/jpeg/4f7e929616b07fe0a94586df94a06ad8/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr2/HIGHRES/image/jpeg/4f7e929616b07fe0a94586df94a06ad8/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 438474 2320 2261 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr3/DOWNSAMPLED/image/jpeg/022332d54cae9f5cb916cf1a2b011999/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr3/DOWNSAMPLED/image/jpeg/022332d54cae9f5cb916cf1a2b011999/gr3.jpg gr3 gr3.jpg jpg 8278 50 377 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr3/THUMBNAIL/image/gif/20d687879345772052a24794b6de3420/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr3/THUMBNAIL/image/gif/20d687879345772052a24794b6de3420/gr3.sml gr3 gr3.sml sml 1124 29 219 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr3/HIGHRES/image/jpeg/9094f68e529de1c3abe791314a17b55d/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr3/HIGHRES/image/jpeg/9094f68e529de1c3abe791314a17b55d/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 27338 179 1338 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr4/DOWNSAMPLED/image/jpeg/419d1329a845fe1b7444e6d9d1b3ad72/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr4/DOWNSAMPLED/image/jpeg/419d1329a845fe1b7444e6d9d1b3ad72/gr4.jpg gr4 gr4.jpg jpg 87354 355 378 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr4/THUMBNAIL/image/gif/de04a40f4c6746dd837c036c791ae1c9/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr4/THUMBNAIL/image/gif/de04a40f4c6746dd837c036c791ae1c9/gr4.sml gr4 gr4.sml sml 6173 164 174 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr4/HIGHRES/image/jpeg/a2d801789f3283c5ebfea6783efb2ae2/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr4/HIGHRES/image/jpeg/a2d801789f3283c5ebfea6783efb2ae2/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 384461 1259 1339 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr1/DOWNSAMPLED/image/jpeg/e130e9dd0aa47689405a8a8a576da52b/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr1/DOWNSAMPLED/image/jpeg/e130e9dd0aa47689405a8a8a576da52b/gr1.jpg gr1 gr1.jpg jpg 58613 533 377 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr1/THUMBNAIL/image/gif/a1d62d4dc63debc5a4c5bb138cd1c70b/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr1/THUMBNAIL/image/gif/a1d62d4dc63debc5a4c5bb138cd1c70b/gr1.sml gr1 gr1.sml sml 8624 164 116 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr1/HIGHRES/image/jpeg/a39b8e20ab6927fd2ee5cdf16b225381/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr1/HIGHRES/image/jpeg/a39b8e20ab6927fd2ee5cdf16b225381/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 339300 2123 1501 IMAGE-HIGH-RES 1-s2.0-S0042698908005695-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr5/DOWNSAMPLED/image/jpeg/ee31dac1ca65cfb5c4b5da5d7c154f80/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr5/DOWNSAMPLED/image/jpeg/ee31dac1ca65cfb5c4b5da5d7c154f80/gr5.jpg gr5 gr5.jpg jpg 47063 277 781 IMAGE-DOWNSAMPLED 1-s2.0-S0042698908005695-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr5/THUMBNAIL/image/gif/6b89ac1e7ac72ea0ccebd2956a6387cf/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr5/THUMBNAIL/image/gif/6b89ac1e7ac72ea0ccebd2956a6387cf/gr5.sml gr5 gr5.sml sml 5298 78 219 IMAGE-THUMBNAIL 1-s2.0-S0042698908005695-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698908005695/gr5/HIGHRES/image/jpeg/f3c7d3f439c4b0be224508f3661d62b8/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698908005695/gr5/HIGHRES/image/jpeg/f3c7d3f439c4b0be224508f3661d62b8/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 359845 1226 3458 IMAGE-HIGH-RES VR 5541 S0042-6989(08)00569-5 10.1016/j.visres.2008.11.006 Elsevier Ltd Fig. 1 The proposed framework for saliency detection. The feature maps for colour, luminance and orientation are combined to form the saliency map, which represents region salience using intensity values. Light pixels are associated with high saliency and dark pixels have low saliency. The saliency algorithm is utilised by the fitness function of a Genetic Algorithm to breed texton pair pop-out stimuli to order. Fig. 2 Comparison between the original iNVT implementation and our modified model. From left: The Input image, the original iNVT model output, and our enhanced model output. The rows present the minimum (a), maximum (b) and the median (c) difference cases from the Euclidean distance measures. Fig. 3 Examples of Biomorphs created. We have intentionally limited the recursive depth of the drawing algorithm so that the shapes are kept as simple as possible. Fig. 4 Target\u2013distracter images taken from the (a) 50th, (b) 33rd, (c) 15th, and the (d) 6th generations of the GA. The images have a resolution of 400\u00d7400 pixels, with the target region having four possible locations around the center of the image, (a) bottom-left, (b) bottom-right, (c) top-right and (d) top-left. Visual inspections show that as we move through the generations from (d) to (a), the model determined saliency increases the pop-out effect of the target region. Fig. 5 Results from both experiments. (a) Experiment 1: mean response times for generated stimuli from 20 subjects with predicted salience values for the corresponding images. A linear fit (R 2 =0.571) shows that as target predicted saliency rises from left to right on the graph, the mean response times decrease. (b) Experiment 2: results of the search task using six different target\u2013distracter pairings pulled from different sub ranges of the saliency continuum. The linear fitted slopes indicate that as the saliency of selected pairs decrease, there is an increase in target detection times as a function of distracter set-size. Error bars indicate 1 s.e.m. Table 1 Detection performance of the iNVT implementation and our model. Distances represent proximity to the ground truth and so a small mean distance is preferable. Algorithm Mean distance s.d. Range iNVT 124.143 43.678 190.306 (221.129\u201330.823) Our model 109.650 44.280 166.678 (216.620\u201349.942) Table 2 Error data (%) for Experiment 2, showing that errors are proportional to the inherent level of salience of the target\u2013distracter (T\u2013D) pairing as well as the set-size. T\u2013D Set-size Target-present Target-absent 1 3 5 10 15 36 1 3 5 10 15 36 vs. 0.9 1.3 1.2 1.3 0.9 1.2 0.9 0.2 0.4 1.3 1.9 0.2 vs. 1.6 1.3 1.0 1.4 1.4 1.8 1.7 0.7 1.1 0.3 1.4 0.4 vs. 1.2 1.7 2.0 2.4 0.7 3.3 0.3 1.3 1.8 1.0 0.2 1.3 vs. 0 1.9 3.4 1.1 2.3 4.7 0 0.6 0 1.6 0.8 0.3 vs. 1.6 3.1 2.3 7.4 10.1 11.4 1.1 0.4 1.0 0.3 0.4 0 vs. 0.6 4.3 10.3 8.4 10.3 12.2 1.0 0.6 0.6 2.5 0.1 1.4 Generating customised experimental stimuli for visual search using Genetic Algorithms shows evidence for a continuum of search efficiency Milan Verma \u204e milan@dcs.qmul.ac.uk Peter W. McOwan Queen Mary University of London, School of Electronic Engineering and Computer Science, London, UK \u204e Corresponding author. Abstract Here we present a new empirically validated computational paradigm that generates on demand stimuli with user-defined levels of salience for use in visual search tasks. Combining a Genetic Algorithm with a biologically motivated model for image saliency we are able to breed a range of customised texture elements for use in psychophysical experiments. We review these psychophysical studies, showing for the first time that an explicit and predictable continuum of search efficiency exists in human visual search. Keywords Psychophysics Pop-out Saliency Visual attention Genetic Algorithms 1 Introduction Julesz introduced the concept of textons (Julesz, 1981) as basic elements in pre-attentive visual perception, and this concept is frequently utilised to explain the phenomenon of spatial pop-out in images (Bergen & Julesz, 1983; Nothdurft, 1992). The visual search paradigm has been extensively used to psychophysically test bottom-up saliency and visual search models (Itti & Koch, 2000; Treisman & Gelade, 1980; Treisman, 1985, 1988; Wolfe, 1994, 1999; Zhaoping & Snowden, 2006). In a visual search task a target item may be defined either by a unique distinguishing feature (feature search), or by a combination of features (conjunctive search). In a conjunctive search task the distracter typically shares at least one feature with the target, and as a result, the target item is less easy to spot against the background of distracters and this usually results in a serial search for the target. Conversely, in a feature search task the target\u2013distracter disparity enables the target to \u2018pop-out\u2019 against the suppressed background (Beck, 1967; Foster & Ward, 1991; Nothdurft, 1991, 1992; Treisman & Gormican, 1988). In this case, the search slope of the task has a gradient of zero as additional distracters are added, implying that additional distracter items have no effect on the search time. This is taken to support the viewpoint that rapid pre-attentive processes can operate in parallel across the entire visual field (Treisman & Gelade, 1980). A number of researchers have opposed this parallel/serial dichotomy of visual search. Instead they suggest there exists a continuum of search efficiency (Desimone & Duncan, 1995; Duncan & Humphreys, 1989; Wolfe, 1998). This claim supports the idea of the primacy of visual saliency, the extent to which part of a scene contrasts from its surround (Duncan & Humphreys, 1989; Moraglia, 1989; Nothdurft, 1991, 1992), and this is a first principle in directing visual attention (Titchener, 1908). If the difference between target and distracter items is sufficiently large, the target pops out and if the ratio is small the search task degenerates to inefficient search, linearly increasing the search time as new distracters are added. Despite the theory of a parallel/search dichotomy being dismissed in several publications (notably Wolfe, 1998), there does not exist strong psychophysical evidence ruling it out once and for all. In this paper we address this important research question using an improved computational model for visual saliency detection and applying techniques from artificial intelligence to generate experimental stimuli. The methodology allows generation of synthetic textons pairs using a line drawing algorithm, in which spatial pop-out occurs with pre-selected, user-defined, levels of salience. This facilitates a new probe with which to examine the serial vs. parallel distinction. A Genetic Algorithm is used to optimise the search for texton pair images using the saliency model as the fitness function. Subsequent psychophysical tests verify the effectiveness of this process. The remainder of the paper is structured as follows. Section 2 outlines our saliency model, including details of the selection and combination of features. Section 3 describes the Genetic Algorithm and the process of building a stimuli set. Section 4 presents the psychophysical validation of the approach and finally, in Section 5 we discuss the results and reflect on their impact. 2 Producing the saliency map Fig. 1 illustrates the framework for the saliency model to be used. The focus of this section is to describe the procedure for computing the salience of a given image, with the details of the Genetic Algorithm to follow in the next section. In overview, for the given input image, colour, luminance and orientation features are computed across various scales and then integrated to form feature maps, which are combined to form the topographic saliency map (Koch & Ullman, 1985). The level of salience throughout the output image is represented through the pixel intensities, where in the example shown in Fig. 1 the tower and boat regions are calculated to carry high salience values. Much of this framework is analogous to the saliency model presented by Itti and Koch (2000), however in this section we detail our modifications to its operation: \u2022 logarithmic feature combination and assignment of coefficients depending on similarity across scales; \u2022 simple normalisation, rather than computationally intensive approaches, such as iterative normalisation (Itti & Koch, 2001); \u2022 quantification of saliency using a global measure based on similarities across scales; \u2022 performance enhancements by extracting orientation features in the Fourier domain. Results of tests against the Itti and Koch (2000) model will be presented to assess the performance of the modified model. Although the saliency model could be used for natural scenes or low-level images, we restrict its usage to the latter for this paper. 2.1 Feature extraction Feature pyramids are generated for colour, luminance and orientation features to enable computations on various resolution scales. Scale is simulated by using a Gaussian Pyramid (Burt & Adelson, 1983) for colour and luminance features and by varying the wavelength for orientation features. For a given input image I, the extracted salience S I is defined as (2.1) S I = combination ( C RG , BY , L mean ( I ) , O 0 , 45 , 90 , 135 ) where C, L and O are the feature maps for colour, luminance and orientation, respectively, which are produced by combining their corresponding pyramids. C has sub-feature pyramids for red\u2013green (RG) and blue\u2013yellow (BY) colour opponencies, L has no sub-features, and O has four sub-feature pyramids, one each for 0\u00b0, 45\u00b0, 90\u00b0 and 135\u00b0. The combination of these features will be covered in the next subsection. Here we detail how each of these features are acquired, starting with the luminance pyramid. First, using I the mean image is obtained (2.2) M I = ( r + g + b ) 3 The mean image is then convolved with a 6\u00d76 linearly separable Gaussian kernel and repetitively Gaussian subsampled by a factor of two to produce a pyramid with L scales (2.3) L = \u230a log e min ( W , H ) + 1 \u230b W and H are the respective width and height of input image I. At the finest scale (l =0), the image has a resolution equivalent to the input image and at the coarsest scale (l = L \u22121), the linear resolution of the image is 1 2 l th of the input image. Following the findings of Hurvich and Jameson (1957) there are two colour pyramids that are computed, one for red\u2013green opponency and another for blue\u2013yellow. Using the separated r, g, b channels of I, an RG image is produced (2.4) RG = ( r - g ) max ( r , g , b ) The RG image is then convolved and Gaussian subsampled in order to create a corresponding pyramid. The BY pyramid is computed in a similar manner, however the yellow component is formulated by obtaining the minimum value of the r and g values (Walther, 2006) (2.5) BY = ( b - min ( r , g ) ) max ( r , g , b ) Simple cells in the primary visual cortex were shown by (Hubel & Wiesel, 1962) to have receptive fields (RF) with antagonistic on and off regions. Due to their similar spatial response profiles these simple cells have been modelled as Gabor filters with relative success (Field & Tolhurst, 1986; Jones & Palmer, 1987). However, there are some drawbacks in their use. Firstly, they are not optimal if a reasonably broad and uniform coverage of the entire frequency spectrum is desired, since many Gabor filters will be needed. Secondly, measurements on mammalian visual systems indicate that there are simple cell responses that are symmetric on the log frequency scale (Anderson & Burr, 1985) but a Gabor filter has a Gaussian profile only on the linear frequency scale. Furthermore, the spatial frequency bandwidth of these simple cells range from about 0.5 octaves up to about 3 octaves (Daugman, 1985) however Gabor functions have a maximum of 1 octave (Adelson & Bergen, 1985). These problems are alleviated by using Log\u2013Gabor filters, which not only have Gaussian (symmetric) transfer functions when viewed in the logarithmic frequency scale but they also have extended tails at high frequencies enabling them to better encode natural images (Field, 1987). The parameters of the Log\u2013Gabor filters were set according to physiological findings (Daugman, 1985; Geisler & Albrecht, 1997; Webster & De Valois, 1985). The spatial frequency bandwidth is set to 1.5 octaves which has been considered the average bandwidth (De Valois, Albrecht, & Thorell, 1982). Also, De Valois et al. found that there are cells tuned to a variety of scales, greater than 4 octaves and containing at least 3 scales for each orientation. This supports our multi-scale design in which the radial axis is divided into six equal octave bands labelled [0,\u2026,5], and four orientations of 0\u00b0, 45\u00b0, 90\u00b0 and 135\u00b0 with respect to the vertical. The wavelength in each orientation is progressively halved from \u03b4 at scale 0 through to \u03b4/25 at scale 5 (see Eq. (2.3), given the size of the stimuli tested in this paper are 400\u00d7400 pixels), with the default initial wavelength set at 19.2 (see Section 4 for a justification). The orientation maps are computed by convolving each of the oriented Log\u2013Gabor filters with the mean image for each \u03b4 wavelength. We have found that as compared with the approach used for producing the colour and luminance pyramids, varying \u03b4 whilst maintaining the original image without subsampling is a more effective, though computationally intensive, approach for obtaining multi-scale results. Though formally equivalent to direct spatial convolution, to improve processing speed, which will prove critical in the synthesis methodology that follows, the convolution of these filters is performed in the Fourier domain using the Fast-Fourier Transform. The radial F r and the angular component F a are defined as (2.6) F r = e - log 2 r r 0 2 2 log 2 \u03c3 r r 0 2 (2.7) F a = e - ( a - \u03b8 0 ) 2 2 \u03c3 a 2 Therefore, the polar coordinates in the frequency domain of F r and F a given in Eqs. (2.6) and (2.7) can be defined by F(r, a)= F r \u00d7 F a . The center frequency is represented as r 0, \u03b8 0 is the filter orientation, \u03c3 r and \u03c3 a are the standard deviations of the respective radial and angular bandwidths in octaves, which are set to 2 octaves and \u03c0 4 , respectively. 2.2 Feature combination strategies It is now necessary to combine these raw processed features across sub-features at various scales to construct a saliency map. Ultimately there are two goals of the combination process, the first is to extract as much useful information as we can from the set of items to be combined and the second is to combine this information in a manner which represents their importance. These two issues are presented in detail before a common solution is proposed. Psychophysical studies (Allman, Miezin, & McGuinness, 1985; Knierim & Van Essen, 1992; Nothdurft, Gallant, & Van Essen, 1999; Sillito, Grieve, Jones, Cudeiro, & Davis, 1995) suggest that a stimulus image feature can excite a neuron in the primary visual cortex (V1) if its receptive field response is unique against a homogeneous background. Computationally, the behaviour of a center\u2013surround receptive field can be modelled by inter-scale subtraction between coarse and fine scales (Itti, Niebur, & Koch, 1998), assuming that the center (c) scales will tune to the foreground/target and the surround (s) scales will tune to the background/distractors of the stimulus image. The scales utilised for this operation are c \u2208 L and s = c + \u03b4 where \u03b4 \u2208{3,4}. In practice this is trivial when computing colour and luminance differences, however for orientation features the emphasis on the selection of the correct center and surround scales is greater. For orientation, tuning filter parameters for detecting the foreground and background is a challenging task since the content of the stimuli may be rich in detail and contain a variety of image features. Unfortunately, shortfalls in the discrimination of the foreground and background will lead to an inaccurate center\u2013surround subtraction. To remove this dependency on the selection of scales and circumvent this problem our solution follows the assertion that regions of interest in certain stimuli will occur as strong localised peaks in feature maps (Hu, Xie, Ma, Chia, & Deepu, 2004). Saliency algorithms tend to detect a plethora of features, which require a selection process to combine those that are useful. A number of earlier models have used ad hoc weighted linear combination strategies, some using learning or optimisation to tune the weights of feature maps. Others have employed competition strategies (i.e. Winner-Takes-All: Itti et al., 1998; Koch & Ullman, 1985; Koene & Zhaoping, 2007) despite this potentially leading to features being completely disregarded. In their model Hu et al. (2004) propose a dynamic weighting strategy in which the density of salient points in relation to the rest of the image determines the overall saliency. For each salient point they use a convex hull, join together neighbouring salient points and calculate the area of the resultant polygon. Polygons for each salient point are computed and summed to determine the overall \u2018spatial compactness\u2019 of the image. The density of each polygon is then considered, following the assumption that a region is highly salient if the salient points within that region are close together in relation to the rest of the image. The weighting coefficients are approximated according to the spatial compactness and the saliency density. There is a high dependence on obtaining the correct convex hull, since an incorrect region estimation (such as approximating a region which should actually be two or more regions) leads to a poor weighting score. Along with other such region-based approaches (Li, Ma, & Zhang, 2003), the parameters must be tuned to obtain reasonable results. To overcome these problems, in the following section we propose a dynamic weighting strategy to score a given feature map by evaluating the global spatial relationships of the features. 2.3 Assigning weights through the Hurst exponent Identifying peaks rather than using inter-scale subtraction enable us to replicate the behaviour of center\u2013surround receptive fields far more effectively for multi-scale orientation features. Furthermore, the same approach can be used to infer weighting coefficients for the maps to be combined. In our model this functionality has been achieved by employing the Hurst Exponent (Blok, 2000). This operation measures the cross-correlation of signal data, analysing the spatial correspondence of filter responses at various scales (see Reichardt (1961) for a biological justification). Through this operation a value can be recovered allowing quantitative comparison between the filter responses so providing a score that can be used to obtain weights for the maps across scales and sub-features to generate feature maps. Given a map f the signal variance in two orthogonal directions (x and y) is evaluated. Decomposing these variance values into sub groups of extent \u03c4, membership of which is based on similarities in neighbouring averages, generates new associated variance values at increasingly reduced scales. These values are generated so they have the property that they successively halve the two characteristic standard deviations of the original map. This decomposition continues until a pre-defined level of resolution is reached, which is set above the Nyquist frequency at 4 pixels to avoid aliasing. We then further calculate the variance \u03c3 \u03c4 of the values at each level and evaluate the quotient (2.8) H = log 2 ( \u03c4 \u00b7 \u03c3 \u03c4 ) / log 2 \u03c4 This is mathematically equivalent to a linear regression analysis on a log\u2013log plot of \u03c3 \u03c4 against \u03c4 to estimate the Hurst exponent H as the best fit slope, where 0< H <1. If H \u21921 this implies there is a strong dependence across the scales indicating that the feature map contains strong localised signal responses. If the Hurst Exponent estimation function is denoted as \u03be(x), the map score is (2.9) V mx , y = \u03be ( \u03c3 m x ) + \u03be ( \u03c3 m y ) where m is the feature map to be scored. The total number of maps considered for combination, M, is feature dependent. For example, when combining features across scales M = L (Eq. (2.3)), when combining colour sub-features M =2, when combining orientation sub-features M =4 and when combining overall feature maps M =3. Having scored the maps from 0 to M \u22121, the maps are then selected in descending order based on overall score and are weighted according to the following coefficient: (2.10) W \u03b2 = e 1 ( M - \u03b2 ) \u00b7 M - 1 log 2 M where \u03b2 is the position of the given map in the sorted scores list. This weighted combination of maps makes use of processes that can be envisaged to take place on biological substrates, such as nearest neighbour comparisons, multi resolution topographic maps, logarithmic responses and ratio evaluation. Maps are normalised (see Fig. 1) using standard normalisation techniques and the feature maps are fused together to form the saliency map. 2.4 Comparative study Although its usage is not constrained to basic image features, for this paper we use the saliency model to evaluate only low-level images. To test its effectiveness, we compared the regions of interest defined by the Itti and Koch (2000) model implementation which is part of the iLab Neuromorphic Vision (iNVT) C++ toolkit (http://ilab.usc.edu/toolkit/), with our modified version of the model. We used 500 synthesised images similar to those in Fig. 4 and therefore the ground truth of the target patch was known for each image. Details of image construction are given in Section 3. Images were computed using a 1.7GHz Intel\u00ae Pentium\u00ae M Laptop. Using the iNVT implementation, each 400\u00d7400 image took on average 20.3s to complete, compared with 18.2s using our model. To score the detected regions of interest, the Euclidean distance (Eq. (2.12)) was calculated from the midpoint of the ground truth (g x , g y ) to the midpoints of each detected region(i =1,\u2026, N). We reward larger ellipses near the ground truth by factoring in their mass and account for false-positives by taking the average of distances from the ground truth (2.11) \u2211 i = 1 N d i \u00d7 e i , w \u00d7 e i , h 4 \u00d7 \u03c0 \u2211 i = 1 N e i , w \u00d7 e i , h 4 \u00d7 \u03c0 given that d is the Euclidean distance defined as (2.12) d i = ( e i , x - g x ) 2 + ( e i , y - g y ) 2 and e i,w and e i,h are the respective width and height values of the ellipse e i . Table 1 shows the overall detection performance. We can observe that despite having marginally more variability in predictions, the average distance from the ground truth is much smaller for our model. The iNVT implementation can be more precise, however also falsely detects the target further away than our model. In fact, a qualitative evaluation of the results revealed that the iNVT implementation would frequently falsely detect the target. This was offset by strong detections of weak pop-out stimuli as compared with our model. Overall, we suspect that despite these drawbacks, due to the nature of its center\u2013surround multi-scale compuations the iNVT implementation may strongly detect a wider range of stimuli. However, we propose that the strengths and weaknesses of our model to detect strong and weak pop-out stimuli could make it a better estimator of human performance. Furthermore, since our model is able to strongly identify the textons that constitute the block of targets, it could be a more accurate precursor to an object recognition algorithm. Fig. 2 presents some detection results from the two algorithms. 3 The Genetic Algorithm A saliency model could be tested using stimuli that have been manually chosen, however since the choice would be premeditated potentially useful stimuli could be absent from any human designed test set. A Genetic Algorithm (GA) provides an automated optimisation mechanism of searching the diverse range of textons and accurately returning a texton pairing with an inherent chosen level of salience. Even when collecting a range of stimuli, as we will show below, organising salience values into bins enable a uniform spread of results. Our modified saliency model is biologically plausible, however any visual attention model which can quantify a global measure of salience could be adopted for synthesising psychophysical stimuli using our approach. A Genetic Algorithm is a computer program which mimics the evolutionary processes of natural selection and genetics (Holland, 1975). It serves as a meta heuristic optimised search technique where candidate solutions to the problem are represented as a population of chromosomes. The fitness of the candidate population is calculated, and the best solutions progress to the subsequent populations via the application of genetic operators, such as mutation and crossover. Over time the populations will evolve to comprise the fittest individuals, which are the problem solutions with the highest fitness values. In our framework, population fitness is quantified through the saliency computation detailed earlier (the colour channel is not used to generate the stimuli in this study). If the model is valid, high values of model saliency should then correspond to high-levels of visual saliency when observed. The experimental stimuli we seek to evolve here comprise of a series of black and white Biomorph textons as shown in Fig. 3. Biomorphs not only have simple production rules but they also provide a rich diversity of instantiations. We develop these Biomorphs on a 4\u00d74 lattice of cells, where each 4\u00d74 set of cells contain either target or distracter textons (see Fig. 4 for examples). Each 400\u00d7400 pixel image has two Biomorph patterns, with each Biomorph chromosome having two 39-bit parts defining their left and right halves. Each Biomorph half is encoded with an initial 8-bits defining magnitude in eight possible directions and the last 31-bits defining the directions chosen during each recursive step of the drawing algorithm. The initial population is set at 12 chromosomes, elitism is set at 0.2 and the mutation rate at 0.04. Each subsequent generation evolves with the elite chromosomes along with randomly chosen chromosomes at random one-point cross-over positions. Fig. 4 shows examples of the synthesised stimuli at various generations corresponding to changing levels of target\u2013distracter saliency as measured by our saliency model (Section 2). To obtain a collection of stimuli covering a similar range of saliency levels, we divided the search space into a series of m bins where m \u2208{0,\u2026, M \u22121} and M = N images binSize , for example to obtain 400 images with each bin containing 10 images, N images =400 and binSize =10. Thus a given group m would have images with fitness values ranging from m \u00b7 ( maxFitness M ) through to m + 1 \u00b7 ( maxFitness M ) . Using our chromosome representation, a bit switch could potentially translate to a drastic change in appearance. This, along with each half of the Biomorph having its own 78-bit representation, means that even without mutation or crossover, we can obtain a reasonably variant set of Biomorphs across generations. The impact of a bit switch could be reduced, which would make the GA more efficient but would reduce the variety of textons produced. 4 Experiments Two psychophysical visual search experiments were conducted to investigate the search performance of the framework for a variety of synthesised stimuli. While it could be argued that any saliency model could be applied to the following experimental approach, our pilot studies showed that our modified model provides excellent target detection results for low-level stimuli. In Experiment 1 we psychophysically evaluate observer search performance on images that have been generated to pre-defined target\u2013distracter salience levels. In Experiment 2, we examine the search performance for custom generated target\u2013distracter pairings as a function of distracter density. 4.1 Experiment 1: block pop-out test 4.1.1 Participants and apparatus Twenty observers participated in the experiment of which 9 were female and 11 were male. The ages of the participants ranged from 19 to 25 (m =21.3, s.d.=9.5). Half of the participants were naive about the issues being investigated. All subjects had normal or corrected-to-normal visual acuity. The experiment was conducted using a Dell Dimension\u00ae 8300 PC with an ATI Radeon\u00ae 9700 PRO graphics adapter. Stimulus images were presented on a 19-inch standard CRT monitor (Sony GDM-F500) with a spatial resolution of 1280\u00d71024 pixels and a temporal resolution of 160Hz. The participants viewed the presented stimuli at a viewing distance of 57cm. The dimensions of the active display area were 33.9\u00d727.1cm and assuming 96DPI, the size of each stimulus image was 10.6\u00d710.6cm (400\u00d7400 pixels), so each image subtended 10.6\u00b0 of visual angle. The parameters of the saliency model were matched to those in the experimental set up. For colour and luminance features no changes were necessary but for orientation features we set the wavelength appropriately. Assuming 16 cycles/degree, the optimal wavelength would be 2.4 pixels (200/5.3\u00b0/16), however since we are considering multiple scales this was set as the median with the maximum wavelength being 19.2 pixels. A chin rest was used to constrain head movements. Subjects responded to stimuli using a Cambridge Research Systems (CRS) response box (CB6) and eye movements were monitored using a CRS 50Hz video eye tracker coupled with the Matlab Video Eyetracker\u00ae Toolbox. 4.1.2 Procedure Before beginning each trial, the camera was calibrated by having the viewer fixate on various known locations across the screen. Each trial began with a central bull\u2019s eye fixation circle (0.26\u00b0 wide) being shown on the screen for 2000ms. This was followed by a 300ms blank interval, after which a stimulus image was presented for 1000ms randomly consisting of either a target-present or target-absent visual search array. The presentation of the fixation circle and the blank screen was repeated between each trial. The stimuli consisted of a block of textons (the target) amongst a background of distracters. Observers were instructed that they would see a series of texture items of which a target 4\u00d74 patch varied in appearance from its background. They were told that this image would be displayed for 2s or until they responded to acknowledge the existence or absence of the patch by pressing one of the two valid buttons. One button was used to indicate the target was present and the other to indicate it was absent and the time between the stimulus display and the subject response was recorded as the response time. The subjects were not told where the target would appear (one of the four possible inner quadrants, see Fig. 4) however they were informed that not all images would have a target. Each trial lasted a maximum of 3300ms. The stimuli presented to each observer were a randomisation of the ranked list of 400 saliency values generated by our GA, along with an equal number of null trails consisting of randomly produced Biomorphs. 4.1.3 Results Null and incorrect trials were removed from the analysis. Eye tracker data was used to regulate the experiment; images where at least one subject was not focusing within a 5mm radius of the initial fixation circle were excluded from the analysis (3.5% or 14 images). Fig. 5 a presents the results of the experiment, showing the mean response time of predicted stimuli. There exists a continuum of search performance, each slope reflecting the relative ranking of the target\u2013distracter pair values as selected from the saliency model range. The figure shows that response times have strong negative correlation to levels of salience. To test this interpretation, a Spearman\u2019s rho was conducted, which confirmed this as a main effect, r =\u22120.72, n =386, p <0.01, with high-levels of salience associated with lower response times. Of the 386 target-present images, 72 contained at least one error in detecting the target. The false-negative error rate, the percentage of observers missing the target for a given target-present image, revealed a negative correlation with saliency values, r =\u22120.56, n =72, p <0.01. The same effect was found for false-positive error rates (for 10.1% or 39 images) for images that were erroneously reported to contain a target, r =\u22120.28, n =39, p <0.01. These effects reaffirm that a target is more difficult to search for when salience is low and easier when salience is high. 4.2 Experiment 2: single target visual search 4.2.1 Procedure For single target visual search, the same experimental setup was used as with Experiment 1, except for the changes mentioned here. The stimuli consisted of a 6\u00d76 lattice of Biomorphs textons with a single target amongst a series of distracters. The location of the target was in one of four possible positions: (2,2), (2,5), (5,2) or (5,5). Once the target item was assigned to one of the four candidate cells, the remaining distracter textons were randomly allocated to the vacant cells in the lattice. The target plus distracter set-sizes were either 1, 3, 5, 10, 15 or 36. Each target\u2013distracter pairing was chosen using a ranked list of fitness (saliency) values for the constituent texton images. The full saliency range generated from our model was first divided into six equal sub ranges from which target\u2013distracter pairs were then randomly selected. This allows target\u2013distracter pairs to coverage a broad range of saliency differentials. The use of six sub ranges here is arbitrary, but serves as a convenient quantisation of the saliency range, though finer levels of differential discrimination are possible with this method. In total there were 288 trials per experimental participant; for each of the six target\u2013distracter pairings there were 24 trials for each of the target-present and target-absent conditions. For the target-present condition, the six target\u2013distracter set-sizes were repeated for each of the four possible locations of the single target. For the target-absent condition, depending on the target\u2013distracter pairing, all the textons were of the corresponding distracter type. Each stimulus image was presented in a random order and the subjects were instructed to acknowledge the presence or absence of a target using the response box. 4.2.2 Results As with Experiment 1, the eye tracker was used to filter the results, leading to the exclusion of 1.4% of trials from the experiment along with response latencies exceeding \u00b1 2 s.d. from the mean for each subject (1.5%). Participants were not told where the targets could possibly appear and so the post-experiment interviews and the eye-tracking results revealed that the entire lattice was scanned throughout the experiment. Fig. 5b presents target-present and target-absent mean response times for the experiment as a function of the set-size. Table 2 presents the corresponding false-negative (target-absent) and the false-positive (target-absent) error rates averaged across participants. Search for amongst was very efficient (2.20 ms/item), followed by amongst (34.89 ms/item), amongst (48.27ms/item), amongst (80.29ms/item), amongst (113.9ms/item), with the most inefficient search being amongst (237.4ms/item). Mean response were analysed with a 6\u00d72\u00d76 repeated measures analysis of variance (ANOVA) with target\u2013distracter pairing, target presence and set-size as within-subject factors. Greenhouse\u2013Geisser corrections of the p values are reported in cases where the assumption of sphericity was violated. As expected, there was a main effect of target presence, F(1,19)=545.183, p <0.001; response times were higher in target-absent trails than in target-present trials F(2,36)=151.38, p <0.001. There were also significant main effects of the target\u2013distracter pairing, F(1,45)=305.28, p <0.001, and set-size, F(2,29)=772.43, p <0.001. The analysis showed a significant display set-size \u00d7 target\u2013distracter pairing interaction, F(25,475)=527.92, p <0.001; the effect of the pairings was greater with a larger set-size. There was also a target presence \u00d7 target\u2013distracter pairing interaction, F(3,48)=370.2, p <0.001; the target being present or not had an influence on the effect of target\u2013distracter pairing. Finally, the three-way interaction between target presence, target\u2013distracter pairing, and set-size was significant, F(25,475)=62.56, p <0.001. Other effects in the ANOVA were nonsignificant. 4.3 Discussion and conclusions The methodology we have presented here allows the generation of target\u2013distracter texton stimuli with pre-set levels of saliency. Experiments 1 and 2 indicate that the methodology is psychophysically valid, in that our model can generate experimental stimuli to create a prescribed ranking of observer validated visual saliency. The experiments presented not only validate the mathematical model but provide useful evidence to inform the debate over the mechanisms supporting low-level visual search. The response times from the experiments present a wide ranging, and using our stimuli predictable, continuum of search efficiency, some target\u2013distracter pairs are more efficient to search for than others. Indeed, the graph in Fig. 5b indicates the possibility that arbitrary set-size vs. response time curves could possibly be generated by the creation and selection of suitable texton target\u2013distracter pairs. These results provide strong new evidence against a simple serial vs. parallel dichotomy in visual search. Furthermore, the characteristics of the slopes from Experiment 2 concur with the findings of Wolfe (1998), that target-present slopes are roughly half of target-absent slopes, following the self-terminating search theory. The interaction effect observed between target\u2013distracter pairing and set-sizes, reveal an interesting finding consistent with related research. The pairings are more influential for high set-sizes, suggesting that search becomes more efficient as more features distinguish the target from the non-targets (Duncan & Humphreys, 1989). Post-experimental interviews and quantitative analysis of response times reveal that \u2018difficulty\u2019 of searching tends to be related to the complexity and density of the stimuli, effectively limited by the amount of visual information that can be processed simultaneously. This idea of limited capacity processing is at some level incorporated in most contemporary theories (Duncan & Humphreys, 1989; Huang & Pashler, 2005; Treisman & Gelade, 1980). There are a number of areas which have not been addressed in this paper and are proposed for future work. Though our modified saliency model works well with a single target, we have yet to fully examine the performance of our methodology with multiple targets in the scene. Also, as can be observed from Fig. 5b, for low-saliency pairings search performance is varied for set-sizes 10 and 15. A possible explanation for this could be the effect of gaps (vacant cells) between textons in the lattice (Reddy & VanRullen, 2007), the modelling of which could provide more accurate results. A natural extension to the work presented in this paper would be to investigate the role of additional features, particularly colour, in a visual search task. Other approaches could be adopted for obtaining image features, such as the DKL colour model (Derrington, Krauskopf, & Lennie, 1984) and additional attributes to model could include spatial distribution, density, size and eccentricity. Furthermore, the modified saliency model reported in this paper has only been systematically tested on low-level stimuli we have generated. The emphasis of future work could be to validate the changes made to the Itti and Koch (2000) model for a wider range of low-level stimuli and images of natural scenes. In this paper we present a new methodology for texton stimulus generation. The textons previously presented in research papers to date have typically been manually created using letters of the alphabet or other simple geometric figures. This methodological constraint could potentially hamper the explanatory scope for work in this field or could introduce confounding high-level elements such as letter or shape recognition. Even statistical meta analysis work, for example by Wolfe (1998), have had to make use of these constrained texton type data sets. Here we argue that the continuum of complexity that Biomorph based texton generation affords allows a range of abstract and previously unknown forms, which are generated by the computer rather than by the researcher. Our process, employing artificial intelligence techniques to assist in the generation of novel experimental stimuli could be applied in a range of fields. For example, the methodology can provide a useful test bed for the empirical validation of mathematical models of psychological processes. To test whether a putative mathematical model is in fact an accurate predictor of subject performance, search optimisation techniques (where the mathematical model is utilised as the fitness function), can be used to create a novel set of experimental stimuli with well defined expectations of subject performance. This synthetic stimulus set can then be experimentally measured, and the fit of empirical results to predicted results evaluated. Acknowledgements We wish to thank Professor Alan Johnston for comments on early drafts of this paper. The work has been supported by the EPSRC. References Adelson and Bergen, 1985 E.H. Adelson J.R. Bergen Spatiotemporal energy models for the perception of motion Journal Optical Society of America, A 2 1985 284 299 Allman et al., 1985 J. Allman F. Miezin E. McGuinness Stimulus specific responses from beyond the classical receptive field: Neurophysiological mechanisms for local-global comparisons in visual neurons Annual Review Neuroscience 8 1985 407 430 Anderson and Burr, 1985 S.J. Anderson D.C. Burr Spatial and temporal selectivity of the human motion detection system Vision Research 25 8 1985 1147 1154 Beck, 1967 J. Beck Perceptual grouping produced by line figures Perception and Psychophysics 2 1967 491 495 Bergen and Julesz, 1983 J.R. Bergen B. Julesz Rapid discrimination of visual patterns IEEE Transactions on Systems, Man, & Cybernetics 13 1983 857 863 Blok, 2000 Blok, H. J. (2000). On the nature of the stock market: Simulations and experiments. Ph.D. Thesis, University of British Columbia (pp. 193\u2013207). Burt and Adelson, 1983 P.J. Burt E.H. Adelson The Laplacian pyramid as a compact image code IEEE Transactions on Communication 31 4 1983 532 540 Daugman, 1985 J.G. Daugman Uncertainty relations for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters Journal Optical Society of America, A 2 1985 1160 1169 De Valois et al., 1982 R.L. De Valois D.G. Albrecht L.G. Thorell Spatial frequency selectivity of cells in macaque visual cortex Vision Research 22 1982 545 559 Derrington et al., 1984 A.M. Derrington J. Krauskopf P. Lennie Chromatic mechanisms in lateral geniculate nucleus of macaque Journal of Physiology (London) 357 1984 241 256 Desimone and Duncan, 1995 R. Desimone J. Duncan Neural mechanisms of selective visual attention Annual Review of Neuroscience 18 1995 193 222 Duncan and Humphreys, 1989 J. Duncan G.W. Humphreys Visual search and stimulus similarity Psychological Review 96 1989 433 458 Field, 1987 D.J. Field Relations between the statistics of natural images and the response properties of cortical cells Journal Optical Society of America, A 4 1987 2379 2394 Field and Tolhurst, 1986 D.J. Field D.J. Tolhurst The structure and symmetry of simple-cell receptive-field profiles in the cat\u2019s visual cortex Proceedings of the Royal Society of London, B 228 1986 379 400 Foster and Ward, 1991 D.H. Foster P.A. Ward Asymmetries in oriented-line detection indicate two orthogonal filters in early vision Proceedings of the Royal Society of London, B 243 1991 75 81 Geisler and Albrecht, 1997 W.S. Geisler D.G. Albrecht Visual cortex neurons in monkeys and cats: Detection, discrimination, and identification Visual Neuroscience 14 1997 897 919 Holland, 1975 J.H. Holland Adaptation in natural and artificial systems 1975 University of Michigan Press Ann Arbor, MI Hu et al., 2004 Hu, Y. Q., Xie, X., Ma, W. Y., Chia, L. T., & Deepu, R. (2004). Salient region detection using weighted feature maps based on the human visual attention model. In The fifth pacific-rim conference on multimedia (Vol. 3(5), pp. 1363\u20131368). Huang and Pashler, 2005 L. Huang H. Pashler Attention capacity and task difficulty in visual search Cognition 94 B 2005 101 111 Hubel and Wiesel, 1962 D.H. Hubel T.N. Wiesel Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex Journal of Physiology 160 1962 106 154 Hurvich and Jameson, 1957 L.M. Hurvich D. Jameson An opponent-process theory of color vision Psychological Review 64 6 1957 384 404 Itti and Koch, 2000 L. Itti C. Koch A saliency-based search mechanism for overt and covert shifts of visual attention Vision Research 40 10\u201312 2000 1489 1506 Itti and Koch, 2001 L. Itti C. Koch Feature combination strategies for saliency-based visual attention Systems Journal of Electronic Imaging 10 2001 161 169 Itti et al., 1998 L. Itti E. Niebur C. Koch A model of saliency-based fast visual attention for rapid scene analysis IEEE Transactions on Pattern Analysis and Machine Intelligence 20 11 1998 1254 1259 Jones and Palmer, 1987 J.P. Jones L.A. Palmer An evaluation of the two-dimensional Gabor filter models of simple receptive fields in cat striate cortex Journal of Neurophysiology 58 1987 1233 1258 Julesz, 1981 B. Julesz Textons, the elements of texture perception, and their interactions Nature 290 1981 91 97 Knierim and Van Essen, 1992 J.J. Knierim D.C. Van Essen Neuronal responses to static texture patterns in area V1 of the alert macaque monkey Journal Neurophysiology 67 4 1992 961 980 Koch and Ullman, 1985 C. Koch S. Ullman Shifts in selective visual-attention towards the underlying neural circuitry Human Neurobiology 4 1985 219 227 Koene and Zhaoping, 2007 Koene, A. R., & Zhaoping, L. (2007). Feature-specific interactions in salience from combined feature contrasts: Evidence for a bottom-up saliency map in V1. Journal of Vision, 7(7): 6, 11\u201314. Li et al., 2003 Li, Y., Ma, Y., & Zhang, H. (2003). Salient region detection and tracking in video. In ICME 2003: Proceedings of ieee international conference on multimedia and expo. Moraglia, 1989 G. Moraglia Visual search: Spatial frequency and orientation Perceptual and Motor Skills 69 1989 675 689 Nothdurft, 1991 H.C. Nothdurft Texture segmentation and pop-out from orientation contrast Vision Research 31 1991 1073 1078 Nothdurft, 1992 H.C. Nothdurft Feature analysis and the role of similarity in preattentive vision Perception & Psychophysics 52 1992 355 375 Nothdurft et al., 1999 H.C. Nothdurft J.L. Gallant D.C. Van Essen Response modulation by texture surround in primate area V1: Correlates of \u201cpopout\u201d under anesthesia Visual Neuroscience 16 1999 15 34 Reddy and VanRullen, 2007 Reddy, L., & VanRullen, R. (2007). Spacing affects some but not all visual searches: Implications for theories of attention and crowding. Journal of Vision, 7(2): 6, 1\u201317. Reichardt, 1961 W. Reichardt Autocorrelation, a principle for the evaluation of sensory information by the central nervous system W.A. Rosenblith Sensory communication 1961 Wiley New York 303 317 Sillito et al., 1995 A.M. Sillito K.L. Grieve H.E. Jones J. Cudeiro J. Davis Visual cortical mechanisms detecting focal orientation discontinuities Nature 378 1995 492 496 Titchener, 1908 E.B. Titchener Lectures on the elementary psychology of feeling and attention 1908 The MacMillan Company New York Treisman, 1985 A. Treisman Preattentive processing in vision Computer Vision, Graphics and Image Processing 31 1985 156 177 Treisman, 1988 A. Treisman Features and objects: The fourteenth bartlett memorial lecture Quarterly Journal of Experimental Psychology, A 2 40 1988 201 237 Treisman and Gelade, 1980 A. Treisman G. Gelade A feature-integration theory of attention Cognitive Psychology 12 1980 97 136 Treisman and Gormican, 1988 A. Treisman S. Gormican Feature analysis in early vision: Evidence from search asymmetries Psychological Review 95 1988 15 48 Walther, 2006 Walther, D. (2006). Appendix A.2. Color opponencies for bottom-up attention. Interactions of visual attention and object recognition: Computational modeling, algorithms, and psychophysics. Ph.D. Thesis, California Institute of Technology (pp. 98\u201399). Webster and De Valois, 1985 M.A. Webster R.L. De Valois Relationship between spatial-frequency and orientation tuning of striate-cortex cells Journal Optical Society of America 2 7 1985 1124 1132 Wolfe, 1994 J.M. Wolfe Guided search 2.0: a revised model of visual search Psychonomic Bulletin & Review 1 1994 202 238 Wolfe, 1998 J.M. Wolfe Visual search H. Pashler Attention 1998 University College London Press London Wolfe, 1999 J.M. Wolfe The psychophysical evidence for a binding problem in human vision Neuron 24 11\u201317 1999 111 125 Zhaoping and Snowden, 2006 L. Zhaoping R.J. Snowden A theory of a saliency map in primary visual cortex (V1) tested by psychophysics of colour\u2013orientation interference in texture segmentation Visual Cognition 14 4\u20138 2006 911 933", "scopus-id": "59349114281", "pubmed-id": "19059427", "coredata": {"eid": "1-s2.0-S0042698908005695", "dc:description": "Abstract Here we present a new empirically validated computational paradigm that generates on demand stimuli with user-defined levels of salience for use in visual search tasks. Combining a Genetic Algorithm with a biologically motivated model for image saliency we are able to breed a range of customised texture elements for use in psychophysical experiments. We review these psychophysical studies, showing for the first time that an explicit and predictable continuum of search efficiency exists in human visual search.", "openArchiveArticle": "true", "prism:coverDate": "2009-02-28", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0042698908005695", "dc:creator": [{"@_fa": "true", "$": "Verma, Milan"}, {"@_fa": "true", "$": "McOwan, Peter W."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0042698908005695"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0042698908005695"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0042-6989(08)00569-5", "prism:volume": "49", "prism:publisher": "Elsevier Ltd.", "dc:title": "Generating customised experimental stimuli for visual search using Genetic Algorithms shows evidence for a continuum of search efficiency", "prism:copyright": "Copyright \u00a9 2008 Elsevier Ltd. All rights reserved.", "openaccess": "1", "prism:issn": "00426989", "prism:issueIdentifier": "3", "dcterms:subject": [{"@_fa": "true", "$": "Psychophysics"}, {"@_fa": "true", "$": "Pop-out"}, {"@_fa": "true", "$": "Saliency"}, {"@_fa": "true", "$": "Visual attention"}, {"@_fa": "true", "$": "Genetic Algorithms"}], "openaccessArticle": "true", "prism:publicationName": "Vision Research", "prism:number": "3", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "374-382", "prism:endingPage": "382", "prism:coverDisplayDate": "February 2009", "prism:doi": "10.1016/j.visres.2008.11.006", "prism:startingPage": "374", "dc:identifier": "doi:10.1016/j.visres.2008.11.006", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "19", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "212", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "32", "@width": "89", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "517", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "47", "@width": "108", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "856", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "153", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1167", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "134", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "943", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "206", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "184", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "860", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "124", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "683", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "114", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "628", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "86", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "553", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "24", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "537", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "32", "@width": "222", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1023", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "49", "@width": "173", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1433", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "156", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1091", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "168", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "790", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "166", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "831", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "319", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1480", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "16", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "146", "@ref": "fx1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "161", "@width": "128", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "668", "@ref": "fx1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "161", "@width": "128", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx1_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "668", "@ref": "fx1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "17", "@width": "17", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "163", "@ref": "fx10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "167", "@width": "169", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "874", "@ref": "fx10", "@mimetype": "image/gif"}, {"@category": "high", "@height": "167", "@width": "169", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx10_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "874", "@ref": "fx10", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "18", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "149", "@ref": "fx11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "174", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx11.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "755", "@ref": "fx11", "@mimetype": "image/gif"}, {"@category": "high", "@height": "174", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx11_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "755", "@ref": "fx11", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "18", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "161", "@ref": "fx12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "180", "@width": "154", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx12.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "886", "@ref": "fx12", "@mimetype": "image/gif"}, {"@category": "high", "@height": "180", "@width": "154", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx12_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "886", "@ref": "fx12", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "18", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "145", "@ref": "fx2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "180", "@width": "156", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "667", "@ref": "fx2", "@mimetype": "image/gif"}, {"@category": "high", "@height": "180", "@width": "156", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx2_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "667", "@ref": "fx2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "17", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "144", "@ref": "fx3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "176", "@width": "134", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "651", "@ref": "fx3", "@mimetype": "image/gif"}, {"@category": "high", "@height": "176", "@width": "134", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx3_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "651", "@ref": "fx3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "16", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "152", "@ref": "fx4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "161", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "760", "@ref": "fx4", "@mimetype": "image/gif"}, {"@category": "high", "@height": "161", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx4_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "760", "@ref": "fx4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "15", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "156", "@ref": "fx5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "154", "@width": "130", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "803", "@ref": "fx5", "@mimetype": "image/gif"}, {"@category": "high", "@height": "154", "@width": "130", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx5_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "803", "@ref": "fx5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "18", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "182", "@ref": "fx6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "180", "@width": "130", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1180", "@ref": "fx6", "@mimetype": "image/gif"}, {"@category": "high", "@height": "180", "@width": "130", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx6_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "1180", "@ref": "fx6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "17", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "143", "@ref": "fx7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "169", "@width": "150", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "681", "@ref": "fx7", "@mimetype": "image/gif"}, {"@category": "high", "@height": "169", "@width": "150", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx7_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "681", "@ref": "fx7", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "17", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "149", "@ref": "fx8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "174", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "682", "@ref": "fx8", "@mimetype": "image/gif"}, {"@category": "high", "@height": "174", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx8_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "682", "@ref": "fx8", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "17", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "146", "@ref": "fx9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "176", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "726", "@ref": "fx9", "@mimetype": "image/gif"}, {"@category": "high", "@height": "176", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-fx9_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "726", "@ref": "fx9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "582", "@width": "567", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "93898", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "160", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4337", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "high", "@height": "2320", "@width": "2261", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "438474", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "50", "@width": "377", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "8278", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "29", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1124", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "high", "@height": "179", "@width": "1338", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "27338", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "355", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "87354", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "174", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6173", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1259", "@width": "1339", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "384461", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "533", "@width": "377", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "58613", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "116", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8624", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "2123", "@width": "1501", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "339300", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "277", "@width": "781", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "47063", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "78", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5298", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1226", "@width": "3458", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698908005695-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "359845", "@ref": "gr5", "@mimetype": "image/jpeg"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/59349114281"}}