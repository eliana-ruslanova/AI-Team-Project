{"scopus-eid": "2-s2.0-84885432509", "originalText": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2013-08-17 2013-08-17 2014-08-29T04:37:22 1-s2.0-S0169260713002800 S0169-2607(13)00280-0 S0169260713002800 10.1016/j.cmpb.2013.08.004 S300 S300.2 FULL-TEXT 1-s2.0-S0169260713X00124 2015-05-14T05:25:39.534626-04:00 0 0 20131201 20131231 2013 2013-08-17T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccessdate sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 0169-2607 01692607 DELAY 2014-10-14 BZG true 112 112 3 3 Volume 112, Issue 3 12 441 454 441 454 201312 December 2013 2013-12-01 2013-12-31 2013 Section I: Methodology article fla Copyright \u00a9 2013 Elsevier Ireland Ltd. IMPROVEDMETHODEARLYDIAGNOSISSMOKINGINDUCEDRESPIRATORYCHANGESUSINGMACHINELEARNINGALGORITHMS AMARAL J 1 Introduction 2 Methods 2.1 Data sets 2.2 Forced oscillation measurements and parameters 2.3 The studied classifiers 2.4 Performance evaluation 2.5 Feature selection 2.6 Design of the experiments 3 Results 3.1 Forced oscillation parameters 3.2 Performance of the studied classifiers 3.2.1 Experiment 1\u2014The use of original FOT parameters without feature selection 3.2.2 Experiment 2\u2014Feature selection on original FOT parameters 3.2.3 Experiment 3\u2014Feature selection on cross products of FOT parameters 3.3 Pursuit for the best features and classifier parameters 4 Discussion 5 Conclusions 6 Future plans References MATHERS 2006 e442 C ENRIGHT 2000 645 652 P KAMINSKY 2001 205 209 A POLKEY 2004 718 719 M CROXTON 2002 838 844 T OOSTVEEN 2003 1026 1041 E BATES 2011 1233 1272 J KACZKA 2011 337 359 D MACLEOD 2001 505 516 D NAVAJAS 2001 555 562 D FARIA 2009 22 A DIMANGO 2006 399 410 A FARIA 2010 1295 1304 A SILVA 2011 2085 2091 K FARIA 2012 A BIOMEDICALENGINEERINGTECHNICALAPPLICATIONSINMEDICINE FORCEDOSCILLATIONTECHNIQUEINDETECTIONSMOKINGINDUCEDRESPIRATORYCHANGES AMARAL 2012 183 193 J AMERICANTHORACICSOCIETYEUROPEANRESPIRATORYSOCIETY 2005 319 338 MELO 2000 2867 2872 P LORINO 1997 150 155 A PESLIN 1981 93 115 R YING 1990 1186 1192 Y KUNCHEVA 2004 L COMBININGPATTERNCLASSIFIERSMETHODSALGORITHMS WEBB 2002 A STATISTICALPATTERNRECOGNITION HAYKIN 1994 S NEURALNETWORKSACOMPREHENSIVEFOUNDATION VAPNIK 2000 V NATURESTATISTICALLEARNINGTHEORY MCCULLAGH 1989 P GENERALIZEDLINEARMODELS ZHANG 2000 451 462 G PEDREIRA 2009 284 290 C GOLDBAUM 2002 162 169 M WITTEN 2005 I DATAMININGPRACTICALMACHINELEARNINGTOOLSTECHNIQUES DIETTERICH 1998 1895 1923 D FAWCETT 2006 861 874 T KOHAVI 1995 1137 1145 R PROCEEDINGS14THINTERNATIONALJOINTCONFERENCEARTIFICIALINTELLIGENCE ASTUDYCROSSVALIDATIONBOOTSTRAPFORACCURACYESTIMATIONMODELSELECTION REFAEILZADEH 2009 P CROSSVALIDATIONENCYCLOPEDIADATABASESYSTEMS DEMSAR 2006 1 30 J GUYON 2003 1157 1182 I HANLEY 1982 29 36 J OBUCHOWSKI 2005 364 372 N PINTEA 2009 49 66 S METZ 1978 283 298 C BRADLEY 1997 1145 1159 P LING 2003 519 524 C PROCEEDINGS18THINTERNATIONALCONFERENCEARTIFICIALINTELLIGENCE AUCASTATISTICALLYCONSISTENTMOREDISCRIMINATINGMEASUREACCURACY HUANG 2005 299 310 J HEIJDEN 2004 F CLASSIFICATIONPARAMETERESTIMATIONSTATEESTIMATIONENGINEERINGAPPROACHUSINGMATLAB GOLDBERG 1989 D GENETICALGORITHMSINSEARCHOPTIMIZATIONMACHINELEARNING MICHALEWICZ 1992 Z GENETICALGORITHMSDATASTRUCTURESEVOLUTIONPROGRAMS LACERDA 2003 111 122 E ILSEOK 2004 1424 1437 O LESSMANN 2006 3063 3069 S PROCEEDINGSINTERNATIONALJOINTCONFERENCENEURALNETWORKS GENETICALGORITHMSFORSUPPORTVECTORMACHINEMODELSELECTION FENG 2007 111 120 T VAFAIE 2010 200 203 H PROCEEDINGSFOURTHINTERNATIONALCONFERENCETOOLSARTIFICIALINTELLIGENCE GENETICALGORITHMSATOOLFORFEATURESELECTIONINMACHINELEARNING ANBARASI 2010 5370 5376 M DELONG 1988 837 845 E SWETS 1988 1285 1293 J AMARALX2013X441 AMARALX2013X441X454 AMARALX2013X441XJ AMARALX2013X441X454XJ Full 2014-10-14T09:16:45Z FundingPartnerOpenArchive Brazilian Government http://www.elsevier.com/open-access/userlicense/1.0/ item S0169-2607(13)00280-0 S0169260713002800 1-s2.0-S0169260713002800 10.1016/j.cmpb.2013.08.004 271322 2014-08-29T02:32:35.030293-04:00 2013-12-01 2013-12-31 DELAY 2014-10-14 BZG 1-s2.0-S0169260713002800-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/MAIN/application/pdf/e70397617d3c061b2a4aebbcb3307a92/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/MAIN/application/pdf/e70397617d3c061b2a4aebbcb3307a92/main.pdf main.pdf pdf true 2289869 MAIN 14 1-s2.0-S0169260713002800-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/PREVIEW/image/png/2c540b5f57e8696f8725b21d69c327ec/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/PREVIEW/image/png/2c540b5f57e8696f8725b21d69c327ec/main_1.png main_1.png png 43804 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0169260713002800-gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr6/HIGHRES/image/jpeg/2716b7c5635cc4bccfad65a512c981a3/gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr6/HIGHRES/image/jpeg/2716b7c5635cc4bccfad65a512c981a3/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 602419 1062 1943 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr5/HIGHRES/image/jpeg/2af02a4b531ddb32c2d7e28954e561fa/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr5/HIGHRES/image/jpeg/2af02a4b531ddb32c2d7e28954e561fa/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 533284 1064 1950 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr4/HIGHRES/image/jpeg/ae31682d58333eebcec46e885cd30b94/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr4/HIGHRES/image/jpeg/ae31682d58333eebcec46e885cd30b94/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 487580 2243 1499 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr3/HIGHRES/image/jpeg/50788d79dc5fe665ee2940fa0e1fe618/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr3/HIGHRES/image/jpeg/50788d79dc5fe665ee2940fa0e1fe618/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 621207 1100 1947 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr2/HIGHRES/image/jpeg/8cfe03ea68cd3dc24e0ca65dc6b67b1d/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr2/HIGHRES/image/jpeg/8cfe03ea68cd3dc24e0ca65dc6b67b1d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 1264376 4208 2621 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr1/HIGHRES/image/jpeg/e778b28ec5ad99acf4d2f962a1e4522d/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr1/HIGHRES/image/jpeg/e778b28ec5ad99acf4d2f962a1e4522d/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 43367 125 2120 IMAGE-HIGH-RES 1-s2.0-S0169260713002800-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr6/DOWNSAMPLED/image/jpeg/08ab4b8212ea222dd906170f2525b319/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr6/DOWNSAMPLED/image/jpeg/08ab4b8212ea222dd906170f2525b319/gr6.jpg gr6 gr6.jpg jpg 69114 300 548 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr5/DOWNSAMPLED/image/jpeg/f03e8d2b87e9d0e65c7cde8a0dcf4c81/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr5/DOWNSAMPLED/image/jpeg/f03e8d2b87e9d0e65c7cde8a0dcf4c81/gr5.jpg gr5 gr5.jpg jpg 66978 300 550 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr4/DOWNSAMPLED/image/jpeg/1c616db0fa6ac2283a5dc0d11b8d091b/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr4/DOWNSAMPLED/image/jpeg/1c616db0fa6ac2283a5dc0d11b8d091b/gr4.jpg gr4 gr4.jpg jpg 42953 506 338 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr3/DOWNSAMPLED/image/jpeg/6c22f78558f63bd02b6574d79a3bb169/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr3/DOWNSAMPLED/image/jpeg/6c22f78558f63bd02b6574d79a3bb169/gr3.jpg gr3 gr3.jpg jpg 70154 311 550 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr2/DOWNSAMPLED/image/jpeg/0d3ed6e38cdd3ed8c559e79adef93be2/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr2/DOWNSAMPLED/image/jpeg/0d3ed6e38cdd3ed8c559e79adef93be2/gr2.jpg gr2 gr2.jpg jpg 128779 950 592 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr1/DOWNSAMPLED/image/jpeg/b6ad44458da6c3963c17e505647458cd/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr1/DOWNSAMPLED/image/jpeg/b6ad44458da6c3963c17e505647458cd/gr1.jpg gr1 gr1.jpg jpg 10316 35 598 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713002800-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr6/THUMBNAIL/image/gif/9dab3e5660533925727115a7fc2e4164/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr6/THUMBNAIL/image/gif/9dab3e5660533925727115a7fc2e4164/gr6.sml gr6 gr6.sml sml 8342 120 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713002800-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr5/THUMBNAIL/image/gif/f1c44dbdbb62c09d02373a1a003e730b/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr5/THUMBNAIL/image/gif/f1c44dbdbb62c09d02373a1a003e730b/gr5.sml gr5 gr5.sml sml 7602 119 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713002800-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr4/THUMBNAIL/image/gif/7a9733d41786e33e6109041670decff0/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr4/THUMBNAIL/image/gif/7a9733d41786e33e6109041670decff0/gr4.sml gr4 gr4.sml sml 4067 163 109 IMAGE-THUMBNAIL 1-s2.0-S0169260713002800-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr3/THUMBNAIL/image/gif/ada63d493851796321623e99efaf8075/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr3/THUMBNAIL/image/gif/ada63d493851796321623e99efaf8075/gr3.sml gr3 gr3.sml sml 8069 124 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713002800-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr2/THUMBNAIL/image/gif/924013632ae6f8e0810a2074bf2f987f/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr2/THUMBNAIL/image/gif/924013632ae6f8e0810a2074bf2f987f/gr2.sml gr2 gr2.sml sml 4328 164 102 IMAGE-THUMBNAIL 1-s2.0-S0169260713002800-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713002800/gr1/THUMBNAIL/image/gif/785c896d4b1c65d49c45c2991032566a/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713002800/gr1/THUMBNAIL/image/gif/785c896d4b1c65d49c45c2991032566a/gr1.sml gr1 gr1.sml sml 676 13 219 IMAGE-THUMBNAIL COMM 3640 S0169-2607(13)00280-0 10.1016/j.cmpb.2013.08.004 Elsevier Ireland Ltd Fig. 1 Chromosome used for searching for best features on the original FOT parameters and SVM parameters. Fig. 2 Respiratory resistance curves (A) and box plot representation (B) as a function of frequency in normal volunteers. Reactance as a function of frequency (C) and box plot representation (D) in the normal group. Similar description for resistance (E), (F) and reactance (G), (H) in the smoking group. The top and the bottom of the box plot represent the 25th- to 75th-percentile values, while the circle represents the mean value and the bar across the box represents the 50th-percentile value. The whiskers outside the box represent the 5th- to 95th-percentile values. Fig. 3 ROC curves for the first experiment. BFP: Best FOT parameter (obtained without use of classifiers); LOGLC: logistic linear classifier; 1-NN: k nearest neighbor (k =1); ANN: artificial neural networks; SVM: support vectors machines. Fig. 4 Comparative analysis of the sensitivities obtained at specificities levels of 75% (A) and 90% (B). Fig. 5 ROC curves for the second experiment. Fig. 6 ROC curves for the third experiment. Table 1 Forced oscillation parameters of the studied groups. Control (n =28) Smokers (n =28) p-value R 0 (cmH2O/L/s) 2.40\u00b10.78 2.97\u00b10.75 0.003 R m (cmH2O/L/s) 2.45\u00b10.68 2.93\u00b10.62 0.009 S (cmH2O/L/s2) 5.42\u00b115.79 \u22124.18\u00b125.67 ns f r (Hz) 10.89\u00b11.72 14.06\u00b14.20 0.005 X m (cmH2O/L/s) 0.55\u00b10.27 0.24\u00b10.41 0.002 C dyn,rs (L/cmH2O) 0.021\u00b10.004 0.017\u00b10.005 0.002 Z rs4Hz (cmH2O/L/s) 3.13\u00b10.76 4.09\u00b11.07 0.0004 R 0: respiratory resistance extrapolated at 0Hz. R m: mean respiratory resistance. S: slope of the linear relationship of resistance versus frequency. f r: resonance frequency. X m: mean respiratory reactance. Crs,dyn: respiratory system dynamic compliance. Z rs4Hz: absolute value of respiratory impedance in 4Hz. ns: non-significant. Values are presented as mean\u00b1SD. Table 2 Results of the experiment 1. The 95% confidence interval is shown in parenthesis bellow each performance metric. The AUC standard error is also shown in parenthesis. Se (%) Sp (%) AUC BFP 70.2 (60.5\u201380) 77.4 (68.4\u201386.3) 0.77 (0.04) (0.69\u20130.84) LOGLC 71.4 (61.8\u201381.1) 70.2 (60.5\u201380.0) 0.78 (0.04) (0.71\u20130.85) 1-NN 77.4 (68.4\u201386.3) 84.5 (76.8\u201392.3) 0.89 (0.03) (0.83\u20130.94) ANN 77.4 (68.4\u201386.3) 73.8 (64.4\u201383.2) 0.79 (0.03) (0.72\u20130.86) SVM 77.4 (68.4\u201386.3) 86.9 (79.7\u201394.1) 0.87 (0.03) (0.81\u20130.92) BFP: best FOT parameter (obtained without use of classifiers). LOGLC: logistic linear classifier. 1-NN: k nearest neighbor (k =1). ANN: artificial neural networks. SVM: support vectors machines. Se: sensitivity; Sp: specificity. AUC: area under the ROC curve. Table 3 Comparison of AUCs\u2014experiment 1. LOGLC KNN ANN SVM BFP 0.015\u00b10.030 0.121\u00b10.039 ** 0.025\u00b10.045 0.103\u00b10.039 ** LOGLC \u2013 0.105\u00b10.037 ** 0.009\u00b10.039 0.087\u00b10.032 ** KNN \u2013 \u2013 \u22120.096\u00b10.035 ** \u22120.018\u00b10.028 ANN \u2013 \u2013 \u2013 0.078\u00b10.034 * BFP: best FOT parameter (obtained without the use of classifiers). LOGLC: logistic linear classifier. 1-NN: k nearest neighbor (k =1). ANN: artificial neural networks. SVM: support vectors machines. * p <0.05. ** p <0.01. Table 4 Results of the experiment 2. Se (%) Sp (%) AUC BFP 70.2 (60.5\u201380) 77.4 (68.4\u201386.3) 0.77(0.04) (0.69\u20130.84) LOGLC 71.4 (61.8\u201381.1) 72.6 (63.1\u201382.2) 0.79(0.03) (0.72\u20130.86) KNN 82.1 (74.0\u201390.3) 79.8 (71.2\u201388.4) 0.87(0.02) (0.82\u20130.93) ANN 72.6 (63.1\u201382.2) 76.2 (67.1\u201385.3) 0.82(0.03) (0.76\u20130.87) SVM 78.6 (69.8\u201387.3) 85.7 (78.2\u201393.2) 0.86(0.03) (0.81\u20130.92) Table 5 comparison of AUCs\u2014experiment 2. LOGLC KNN ANN SVM BFP 0.027\u00b10.027 0.107\u00b10.041 ** 0.043\u00b10.044 0.100\u00b10.040 * LOGLC \u2013 0.080\u00b10.038 * 0.016\u00b10.040 0.073\u00b10.034 * KNN \u2013 \u2013 \u22120.065\u00b10.034 \u22120.007\u00b10.026 ANN \u2013 \u2013 \u2013 0.057\u00b10.028 BFP: best FOT parameter (obtained without use of classifiers). LOGLC: logistic linear classifier. 1-NN: k nearest neighbor (k =1). ANN: artificial neural networks. SVM: support vectors machines. * p <0.05. ** p <0.01. Table 6 Results of the experiment 3. Se (%) Sp (%) AUC Z rs4Hz 70.2 (60.5\u221280) 77.4 (68.4\u221286.3) 0.77(0.04) (0.69\u22120.84) LOGLC 76.2 (67.7\u221285.3) 76.2 (67.1\u221285.3) 0.82(0.03) (0.76\u22120.88) KNN 84.5 (76.8\u221292.3) 85.7 (78.2\u221293.2) 0.91(0.02) (0.86\u22120.95) ANN 78.6 (69.8\u221287.3) 78.6 (69.8\u221287.3) 0.82(0.03) (0.75\u22120.88) SVM 85.7 (78.2\u201393.2) 84.3 (76.8\u221292.3) 0.91(0.02) (0.86\u22120.95) Table 7 Comparison of AUCs\u2014experiment 3. LOGLC KNN ANN SVM BFP 0.058\u00b10.033 0.143\u00b10.036 ** 0.054\u00b10.039 0.142\u00b10.037 ** LOGLC \u2013 0.085\u00b10.034 * 0.004\u00b10.033 0.084\u00b10.036 * KNN \u2013 \u2013 \u20130.089\u00b10.035 * 0.001\u00b10.0196 ANN \u2013 \u2013 \u2013 0.88\u00b10.038 * BFP: best FOT parameter (obtained without use of classifiers). LOGLC: logistic linear classifier. 1-NN: k nearest neighbor (k =1). ANN: artificial neural networks. SVM: support vectors machines. * p <0.05. ** p <0.01. Table 8 Genetic algorithm parameters. Parameter Value Population size 100 Number of generations 50 Crossover rate 0.80 Mutation rate 0.01 Fitness function AUC Table 9 Best parameters and selected features for experiment 1. Selected features Classifiers Parameter Value AUC (f r), (X m), (R 0), (S), (R m), (C rs,dyn), (Z rs4Hz) LOGLC None None 0.78 KNN Number of nearest neighbor 1 0.89 ANN Number of hidden nodes 9 0.79 SVM Regularization parameter(C) 2.38 0.87 Radius (r) 0.30 Table 10 Best parameters and selected features for experiment 2. Selected features Classifiers Parameter Value AUC (X m), (R 0), (S), (R m), (Z rs4Hz) LOGLC None None 0.79 (f r), (X m), (R 0), (R m), (Z rs4Hz) KNN Number of nearest neighbor 1 0.87 (f r), (X m), (R 0), (R m), (Z rs4Hz) ANN Number of hidden nodes 8 0.82 (f r), (X m), (R 0), (S),(R m), (Z rs4Hz) SVM Regularization parameter (C) 13.89 0.86 Radius (r) 0.24 Table 11 Best parameters and selected features for experiment 3. Selected features Classifiers Parameter Value AUC (f r \u00d7 R 0), (f r \u00d7 S), (f r \u00d7 R m), (f r \u00d7 C rs,dyn), (f r \u00d7 Z rs4Hz), (X m \u00d7 R m), (X m \u00d7 C rs,dyn), (R 0 \u00d7 R m), (R m \u00d7 Z rs4Hz), (C rs,dyn \u00d7 C rs,dyn) LOGLC None None 0.82 (f r \u00d7 f r), (f r \u00d7 X m), (f r \u00d7 R m), (f r \u00d7 C rs,dyn), (X m \u00d7 R 0), (X m \u00d7 R m), (R 0 \u00d7 C rs,dyn), (R m \u00d7 R m), (R m \u00d7 C rs,dyn), (C rs,dyn \u00d7 Z rs4Hz) KNN Number of nearest neighbor 1 0.91 (f r \u00d7 f r), (f r \u00d7 X m), (f r \u00d7 R m), (f r \u00d7 C rs,dyn), (X m \u00d7 R 0), (X m \u00d7 R m), (R 0 \u00d7 C rs,dyn), (R m \u00d7 R m), (R m \u00d7 C rs,dyn), (C rs,dyn \u00d7 Z rs4Hz) ANN Number of hidden nodes 2 0.82 (f r \u00d7 f r), (X m \u00d7 X m), (X m \u00d7 R 0), (X m \u00d7 R m), (X m \u00d7 Z rs4Hz), (R m \u00d7 R m), (R m \u00d7 C rs,dyn) SVM Regularization parameter (C) 3.07 0.91 Radius (r) 0.13 Table 12 Comparison of AUCs\u2014best results of all experiments. NN1 SVM1 NN2 NN3 SVM3 Z rs4Hz 0.121\u00b10.039 * 0.103\u00b10.039 * 0.107\u00b10.041 * 0.143\u00b10.036 ** 0.142\u00b10.037 ** NN1 \u2013 0.018\u00b10.028 0.014\u00b10.021 0.022\u00b10.022 0.020\u00b10.026 SVM1 \u2013 \u2013 0.004\u00b10.033 0.040\u00b10.031 0.039\u00b10.032 NN2 \u2013 \u2013 \u2013 0.0357\u00b10.020 0.035\u00b10.026 NN3 \u2013 \u2013 \u2013 \u2013 0.001\u00b10.020 BFP: best FOT parameter (obtained without use of classifiers). NN1: k nearest neighbor (k =1)\u2014experiment 1. SVM1: support vectors machines\u2014experiment 1. NN2: k nearest neighbor (k =1)\u2014experiment 2. NN3: k nearest neighbor (k =1)\u2014experiment 3. SVM3: support vectors machines\u2014experiment 3. * p <0.01. ** p <0.001. An improved method of early diagnosis of smoking-induced respiratory changes using machine learning algorithms Jorge L.M. Amaral a Agnaldo J. Lopes b Jos\u00e9 M. Jansen b Alvaro C.D. Faria c Pedro L. Melo c \u204e plopes@uerj.br plopeslib@gmail.com a Department of Electronics and Telecommunications Engineering, State University of Rio de Janeiro, Rio de Janeiro, Brazil Department of Electronics and Telecommunications Engineering, State University of Rio de Janeiro Rio de Janeiro Brazil b Pulmonary Function Laboratory, Pedro Ernesto University Hospital, State University of Rio de Janeiro, Rio de Janeiro, Brazil Pulmonary Function Laboratory, Pedro Ernesto University Hospital, State University of Rio de Janeiro Rio de Janeiro Brazil c Biomedical Instrumentation Laboratory, Institute of Biology Roberto Alcantara Gomes and Laboratory of Clinical and Experimental Research in Vascular Biology (BioVasc) State University of Rio de Janeiro, Rio de Janeiro, Brazil Biomedical Instrumentation Laboratory, Institute of Biology Roberto Alcantara Gomes and Laboratory of Clinical and Experimental Research in Vascular Biology (BioVasc) State University of Rio de Janeiro Rio de Janeiro Brazil \u204e Corresponding author. Tel.: +55 21 23340705. Abstract The purpose of this study was to develop an automatic classifier to increase the accuracy of the forced oscillation technique (FOT) for diagnosing early respiratory abnormalities in smoking patients. The data consisted of FOT parameters obtained from 56 volunteers, 28 healthy and 28 smokers with low tobacco consumption. Many supervised learning techniques were investigated, including logistic linear classifiers, k nearest neighbor (KNN), neural networks and support vector machines (SVM). To evaluate performance, the ROC curve of the most accurate parameter was established as baseline. To determine the best input features and classifier parameters, we used genetic algorithms and a 10-fold cross-validation using the average area under the ROC curve (AUC). In the first experiment, the original FOT parameters were used as input. We observed a significant improvement in accuracy (KNN=0.89 and SVM=0.87) compared with the baseline (0.77). The second experiment performed a feature selection on the original FOT parameters. This selection did not cause any significant improvement in accuracy, but it was useful in identifying more adequate FOT parameters. In the third experiment, we performed a feature selection on the cross products of the FOT parameters. This selection resulted in a further increase in AUC (KNN=SVM=0.91), which allows for high diagnostic accuracy. In conclusion, machine learning classifiers can help identify early smoking-induced respiratory alterations. The use of FOT cross products and the search for the best features and classifier parameters can markedly improve the performance of machine learning classifiers. Keywords Clinical decision support Early diagnosis Artificial intelligence Forced oscillation technique Smoking Chronic obstructive pulmonary disease 1 Introduction Mortality caused by smoking is one of the few chronic diseases where further increases in prevalence are predicted in the coming decades. According to World Health Organization estimates, tobacco-attributable deaths will rise from 5.4 million in 2005 to 6.4 million in 2015 and 8.3 million in 2030. The number of projected deaths for 2030 ranges from 7.4 to 9.7 million [1]. One of the main causes of this adverse scenario is that the diagnosis of smoking-induced respiratory changes is usually made only in late stages, when respiratory function is already impaired. There is general agreement in the literature that it is necessary to develop new accurate and non-invasive tests of lung function [2\u20134]. In the case of smoking, the National Heart Lung and Blood Institute recently recommended that research into new technologies to improve non-invasive testing of lung function in this disease should be a priority [5]. The experimental method used to implement system identification in the analysis of respiratory mechanics constitutes what is termed the forced oscillation technique (FOT). The FOT is based on the application of a pressure signal during spontaneous breathing, and the resulting pressure and flow changes are analysed to calculate respiratory impedance [6\u201310]. This method is currently the state-of-the-art for the assessment of lung function [7] and has been used for research purposes for many years [3,6\u20139]. The method is simple and requires only passive cooperation, with no forced expiratory manoeuvres. Despite the obvious advantages of the FOT in terms of its non-invasiveness and lack of dependence on patient cooperation, the FOT has not become a standard methodology for the routine assessment of lung function. In the context of a diagnostic framework, although obtaining respiratory impedance values is easy, the interpretation of resistance and reactance curves and the derived parameters measured by the FOT requires training and experience, and it is a difficult task for the untrained pulmonologist. In recent years, strong evidence has emerged regarding the convenience of FOT measurements in several contexts [6\u20138]. The efficacy of many FOT measurements has been demonstrated in terms of the achievement of various clinical goals [6\u20138]. In particular, recent studies from our group provided evidence that measurements of respiratory impedance using the FOT may contribute in the diagnosis of chronic obstructive pulmonary disease (COPD) [11\u201315]. COPD is a disease associated with an abnormal inflammatory response of the lungs to noxious particles or gases, particularly cigarette smoke, the primary risk factor for COPD [16]. To simplify the diagnostic use of the FOT, our group recently developed clinical decision support systems based on machine learning (ML) algorithms that were able to help diagnose COPD using FOT measurements [17,18]. In addition to the development of an automatic classifier, the use of ML algorithms also contributed to improve the diagnostic accuracy of COPD [18]. However, these studies were limited to the diagnosis of late stages of COPD, when medical and social costs are already high [16]. Previous studies have shown that the FOT may contribute to the detection of early respiratory changes due to smoking [11]. These initial studies, however, presented limited diagnostic accuracy and did not include the development of dedicated clinical decision support systems. Based on these promising results and limitations, we hypothesized that the use of ML algorithms associated with FOT measurements would help in the early detection of the effects of smoking on the respiratory system. Such a system would detect early smoking-induced respiratory changes while these pathologic changes are still potentially reversible, which is of utmost importance in the prevention of COPD [16]. There has been no research dedicated to this problem to date. The aims of this study were (1) to evaluate the performance of several ML algorithms to develop an automatic classifier to help diagnose early smoking-induced respiratory changes using forced oscillation measurements; (2) to increase the accuracy of the FOT in detecting early respiratory abnormalities in smoking patients; and (3) to identify the best configuration for the diagnosis of respiratory changes in smoking patients. The remainder of this article is organized as follows. The healthy and smoking patient groups we examined are characterized in Section 2, along with a description of the measurement protocol. This section also presents the evaluated classifiers and describes the methods used for performance evaluation, classifier comparison, feature selection and experimental design. Section 3 presents the results, and Section 4 discusses the results with respect to the search for the best classifier and parameters for detecting early respiratory abnormalities. Finally, Section 5 summarizes the main outcomes of this investigation and proposes future steps in this research topic. 2 Methods 2.1 Data sets We utilized a group of FOT measurements that differed slightly from a previous study regarding the diagnosis of early smoking abnormalities [11]. This study included healthy control subjects with normal spirometry [19] who had never smoked, as well as smoking subjects without COPD [16] and history of asthma [20], who had no cardiovascular, gastrointestinal, renal or neurological symptoms. This group of measurements allowed us to identify the best baseline parameter for accuracy comparisons and evaluate the effects of optimized classifiers in diagnostic accuracy. In the present work, we performed experiments with two data sets. The first dataset consisted of 7 possible input features (FOT parameters) from 168 measurements acquired from 56 volunteers. We found no differences between the biometrical characteristics of healthy individuals and smokers (healthy, n =28, age: 33.1\u00b18.2, weight: 66.1\u00b111.8kg, height: 167.4\u00b18.2cm; smokers, n =28, age: 35.1\u00b19.7 years, weight: 66.1\u00b110.7kg, height: 166.9\u00b17.9cm) [11]. The second data set consisted of 28 input features representing the cross products of the original FOT parameters of the same 168 measurements. 2.2 Forced oscillation measurements and parameters The system used for respiratory impedance analysis was developed in our laboratory and described in detail previously [21]. Measurements were conducted in conformity with the recommendations issued by a task force of the European Respiratory Society [6]. Briefly, the instrument allowed evaluation of total respiratory input impedance (Z rs), which was estimated from signals coming from a pressure transducer (P) and pneumotachograph (V\u2032) placed close to the individual's mouth (Z rs = P/V\u2032). The oscillations applied to the respiratory system were produced by a loudspeaker and directed to the individual's mouth while the individual breathes voluntarily. The forced pseudorandom noise used in this study was composed of 16 harmonics (4\u201332Hz) of the fundamental (2Hz). The peak-to-peak amplitude of this excitation signal was 2cmH2O. During the measurements, the subjects used a nose clip and supported their cheeks and sub-mandible tissues with their hands to reduce the shunt effect [6,7]. Pressure and flow signals were sampled at 1024Hz for 16s. A fast Fourier transform algorithm was applied to adjacent and interpolated 4s data blocks. Impedance data corresponding to coherence values higher than 0.9 were retained for analysis and calculated by averaging three 16s manoeuvres. Linear regression analysis was performed for the real part of the impedance from 4 to 16Hz, which extrapolated the respiratory resistance at 0Hz (R 0) and the slope (S) of the linear relationship of resistance versus frequency. These parameters are related to the total resistance and homogeneity of the respiratory system, respectively [12,22,23]. Mean resistance (R m), primarily sensitive to airway calibre [9], was also calculated. The imaginary part of the impedance was characterized by the mean reactance (X m) and the resonance frequency (f r), which are associated with ventilation homogeneity [24]. The respiratory system dynamic compliance (C rs,dyn) and the absolute value of respiratory impedance in 4Hz (Z rs4Hz) were also evaluated. Z rs4Hz represents the total mechanical load of the respiratory system [6,9], and it is associated with the work required to move air in the respiratory system. 2.3 The studied classifiers In this particular study, the following classification algorithms were evaluated: \u2022 Logistic linear classifier [25,26] \u2022 k nearest neighbor [25] \u2022 Neural networks [27] \u2022 Support vectors machines [28] These algorithms were chosen because they represent a broad variety of classifier algorithms from Lippmann's list of types of classifiers [25]. These algorithms will be briefly described. A complete description of the algorithms can be found in the references. The logistic linear classifier is obtained by logistic regression. The classifier is a member of the family of methods called generalized linear models (\u201cGLM\u201d) [29]. Such models include a linear portion followed by a link function. The linear function of the predictor variables is calculated, and the result of this calculation is passed through the link function. For logistic regression, the linear result is run through a logistic (sigmoid) function. The parameters of the linear function can be determined by maximising the likelihood criterion using a logistic (sigmoid) function. Although it is structurally simple, logistic regression is used extensively in numerous disciplines, including the medical and social science fields. Logistic regression is quick to fit, and the discovered model is easy to implement and quick to recall. It frequently achieves better performance than competing, more complex techniques. The k nearest neighbor (KNN) algorithm is one of the simplest and most elegant classification methods in pattern recognition [25]. KNN is a type of instance-based learning, or lazy learning, which means that in the learning stage, it simply stores a set of labelled instances (training set). When a new query has to be classified, the algorithm finds k numbers of training instances closest to the query point, using a similarity function usually based on Euclidean distance. The classification is performed using majority vote among the classification of the k objects. If k =1, then the object is simply assigned to the class of its nearest neighbor. An artificial neural network (ANN) is a massive parallel system [27] composed of many neurons (simple processing elements) with a function that is determined by the network architecture, synaptic weights (connection strengths) and the processing performed at the neurons. Neural networks are capable of acquiring knowledge through a learning process and storing it in the synaptic weights. One of the most successful neural network architectures is the multilayer perceptron (MLP). MLP has been successfully applied to a variety of pattern recognition problems in industry, business, science [30] and medical diagnosis [30,31]. One of the most important features of a neural network is the ability to generalize what it has learned from the training procedure. This feature allows the network to address noise in the input data and provide correct outputs to new data patterns, i.e., data that were not used to train the network. Support vector machines (SVM) are learning systems based on statistical learning theory [28] and have been successfully used in a variety of classification and regression problems. For a two-class classification problem, the basic form SVM is a linear classifier that performs classification by constructing a hyperplane that optimally separates the classes. The optimal hyperplane is the one that provides the maximal margin. (The margin is defined as the distance from a training sample and the hyperplane). It can be proven that this particular solution has the highest generalization ability. This formulation can be generalized by applying a non-linear mapping of the training set. The data are transformed into a new feature high-dimensional space where the classes are more easily separable and an optimal hyperplane can be found. The radial basis function Kernel is frequently used to accomplish this non-linear mapping and is frequently the first non-linear mapping to consider. Although the decision surface (hyperplane) is linear in the high dimensional space, when it is observed in the original low-dimensional feature space, it is no longer linear, indicating that SVM can also be applied to data that is not linearly separable [32]. 2.4 Performance evaluation The main goal of performance evaluation is to choose the best classifier model and estimate its performance on future examples (termed \u201cgeneralization\u201d) [33,34]. To obtain a performance evaluation, one first has to choose the performance function based on the specific domain of the application. Some of the more commonly used measures are accuracy, sensitivity, specificity, true positive rate, false positive rate, recall, precision and the area under the receiver operating characteristic (ROC) curve (AUC) [35]. In this work, we chose sensitivity (Se), specificity (Sp) and area under the curve (AUC) for the ROC curves because they are often used in medical diagnosis and allow comparison of our results to other recent studies performed by our group [11,13]. After choosing the performance function, one has to define the evaluation structure to estimate performance of the learned model from available data. We want to estimate the performance of an algorithm in unobserved examples to determine the generalization capability of the algorithm. This performance evaluation can be conducted using either hold-out or k-fold cross-validation procedures [33]. We did not use hold-out because it is necessary to split the available datasets into training and test datasets. Hold-out is trained with a training data set, and the performance of the trained classifier is evaluated in the test data set to estimate the generalization accuracy. A drawback with hold-out is that different hold-out sets (different splits) result in different results. Additionally, depending on the size of available data, one can underestimate of the generalization capability [36]. We chose to use k-fold cross validation because it allows a better use of the available dataset. The dataset is partitioned into k equal (or approximately equal) data subsets or folds [37]. For each fold in turn, use that folder for testing and the remaining k \u22121 folders are used for training a classifier. The performance of each learning algorithm on each fold can be tracked. Upon completion, k samples of the performance metric are available and different methodologies, such as averaging, can be used to obtain an aggregate measure of classification accuracy from these samples. It is also possible to use these samples in a statistical hypothesis test to compare two or more machine learning algorithms. The hypothesis test is another key element when one would like to compare two or more machine learning algorithms. In the hypothesis test, we want to test if there is no difference in the performance of two classifiers (null hypothesis) under a certain confidence level (usually 95%). For comparing within one data set, one can use Student's t-test (t-test) or one of its variations [33]. Dietrich [34] notes that the use of a t-test has a risk of Type I errors, i.e., determining a difference where none exists, and suggests the use of 5\u00d72 cross-validation or McNemar's test. For multiple data sets from different domains, Demsar [38] recommends Wilcoxon's signed ranks test, Friedman tests and post hoc tests. The hypothesis test used McNemar's test, following the recommendations of Dietrich [34], and Wilcoxon's signed ranks test on the AUCs of the test folds, as suggested by Demsar [38]. 2.5 Feature selection As a part of the design process of the classifier system, it is customary to perform an input feature selection step. The purpose of this step is to obtain the smallest set of relevant and informative features that can yield satisfactory performance [39]. Other motivations to perform feature selection are related to general data reduction to reduce storage space, increase algorithm speed, gain knowledge on the process that generates the data and allow data visualization (2D or 3D) [39]. Feature selection is also important when one is dealing with a large number of inputs. This case requires estimating a large number of model parameters, which can be difficult in datasets of limited size [31]. We can essentially divide feature selection methods into three groups: filters, wrappers and embedded methods [39]. Filter methods provide a ranked order of the features using a relevant index, such as correlation coefficients or classical statistical tests. Wrappers normally apply an efficient search strategy to find the best features based on the machine learning algorithm performance, such as the classification accuracy. Embedded methods perform feature selection in the process of training and are usually specific to some given learning machines, such as decision trees [39]. We determined the best features for classification using a wrapper strategy. The applied search strategy looked for the feature set that maximized the area under the ROC curve (AUC). The AUC was used as a performance metric because it is often used in medical diagnosis [40\u201343] and provides a better metric than accuracy to compare classifiers [44\u201346]. The AUC allows the use of the probability estimations or \u201cconfidence\u201d of the class prediction provided by the classifiers. This information is completely lost when one uses accuracy, because it does not consider the probability of the prediction; as long as the class with the largest probability estimation is the same as the target, it is regarded as correct [46]. Bradley [44] has compare popular machine learning algorithms using AUC and found it presents some desired properties such as: better sensitivity in ANOVA tests, it is independent of the decision threshold and it invariant to a priori class probability distributions. Huang and Ling [46] have established a formal criteria for comparing two different measures for learning algorithms and they have shown theoretically and empirically that AUCs, in general, are much better measure than the accuracy. Two different search strategies were used as a result of using two different input sets. For the input of the original FOT parameters, we applied an exhaustive search instead of using suboptimal search strategies, such as forward and backward [47], because the number of parameters was small. When the input set was the cross products of the FOT parameters, the search was conducted using genetic algorithms [48,49]. Genetic algorithms provide an adaptive searching mechanism inspired by Darwin's principle of reproduction and survival of the fittest. The individuals (solutions) in a population are represented by chromosomes, and each individual is associated with a fitness value (problem evaluation). The chromosomes are subjected to an evolutionary process that takes several cycles (generations). The basic operations are selection, reproduction, crossover and mutation. Parent selection yields a higher probability of reproduction to the fittest individuals. During crossover some reproduced individuals cross and exchange their genetic characteristics. Mutations may occur in a small percentage and cause a random variation in the genetic material, thus contributing to introduce variety in the population. The evolutionary process guides the genetic algorithm through more promising regions in the search space. Some of the advantages of using genetic algorithms are that it is a global search technique, it can be applied to optimize ill-structured problems, and it does not require a precise mathematical formulation for the problem. Genetic algorithms are robust, applicable to many problems and efficient in that either a sub-optimal or optimal solution may be found within reasonable time and computational effort. 2.6 Design of the experiments We conducted our study with three experiments. In the first experiment, we did not perform any feature selection. We used the dataset of the original FOT parameters. The four classifiers (LOGLC, KNN, ANN and SVM) were implemented with a pattern recognition toolbox (prtools) for Matlab [50]. The LOGLC did not have parameters. In the KNN, k was set 1, so we have the one nearest neighbor classifier (1-NN). In the ANN classifier, the parameter to be search is the number of neurons in the hidden layer. The SVM with a radial basis function kernel had two parameters, the regularization parameter C and the standard deviation of the radial basis function r. The search for the best parameters was performed with a 10-fold cross-validation using the average area under the ROC curve (AUC) in the test folds as a performance index. The strategy used to avoid over-fitting is based on the use of the cross-validation, the choice of classifier complexity and in the training procedures. The logistic linear classifier is a simple linear model, so it is less prone to over-fitting. The artificial neural network is trained with early stopping which a standard training procedure to improve the generalization and thus to avoid over-fitting [27]. In this method, the training procedure is stopped when the performance on validation set (which is different from the training and the test sets) does not improve anymore. In this work, the validation set is an artificially generated set of 1000 samples per class, based on k-nearest neighbor interpolation on the training set. The support vector machine (SVM) training procedure uses regularization. This technique helps to prevent over-fitting by penalizing model complexity. In the SVM, the parameter C controls the amount of regularization. If C is small, then there is a small penalty to increase the margin, hence improving the generalization, at the cost of a few misclassified training points. If C is big, the penalty to increase the margin is higher, so there will be less increase in the generalization. So, in order to maintain a good generalization capability, the search for the appropriate value of C is restricted to a small interval. The k nearest neighbor classifier, with the number of neighbors set to be equal 1, does not have any mechanism in the training procedure to avoid over-fitting. In this case and also for all other classifiers, the use of the k-fold-cross-validation will help to provide an estimate of the generalization error (or other performance measure), which is obtained by averaging the performance measure in the k test folds. It means that the reported result is the average performance of k classifiers trained and tested with k different partitions of the available dataset. This procedure helps to mitigate the over-fitting because it prevents the report of optimistic result obtained from a specific division of the dataset in train and test sets. The ROC curve for Z rs4Hz was used to compare the performance of the classifiers because in a previous study [11], it was considered the best FOT parameter for detecting the early effects of smoking. In the second experiment, we performed a search for a smaller set of the original FOT that would result in better performance. This feature selection in the dataset of original FOT parameters was performed using the wrapper strategy. We searched for the set of input features that would maximize the average AUC. For the LOGLC and 1-NN classifiers, we performed an exhaustive search. For the ANN, we did not perform the feature selection through a search because it takes a long time to build a classifier due to the training procedures. Instead, we used the features selected by others classifiers and only performed the search for the best number of hidden neurons. For the SVM classifier, the search was conducted using genetic algorithms (GA). The use of GA for model and feature selection has been successfully reported in several applications [51\u201356]. In our application, the pursuit for the best parameters (C, r) was conducted together with feature selection. The success of searches using genetic algorithms depends on how the solution is coded in the chromosome and on the fitness function chosen for evaluation of the solution [49]. Because the purpose of the search was to find the best features and parameters for the classifier, the chromosome was divided into two parts: features and parameters. In the former part, each gene indicated if one should use a particular feature or not. If the gene value was equal to 1, the particular feature should be selected; if the gene value was 0, the feature was not selected. The latter chromosome part represented the values of the classifier parameters. Fig. 1 depicts an example of a chromosome for searching for the best features and SVM parameters. Each of the first seven genes represents one of the original FOT parameters. The last two represent the regularization parameter (C) and the standard deviation of the radial basis function (r). The fitness function calculated the average AUC in the test folds of a 10-fold cross-validation. In the third experiment, we performed a search for a smaller set of the cross products of original FOT parameters that would result in better performance. The second dataset, which consisted of 28 input features representing the cross products of the original FOT parameters, was used. In this case, an exhaustive search would be too costly because we would have to perform the 10-fold cross-validation for the (228 \u22122) possible feature input sets. Therefore, in this experiment, the searches for the best features and classifier parameters were performed using Genetic Algorithms. In the three experiments, the comparisons between classifiers were made using McNemar's and Wilcoxon's tests implemented in Matlab 7.4.0 using the Statistics Toolbox 6.0. In addition, the AUCs obtained during the experiments were compared using MedCalc 8.2 (Medicalc Software, Mariakerke, Belgium) with the methodology suggested in Delong et al. [57]. 3 Results 3.1 Forced oscillation parameters Fig. 2 shows the respiratory resistance and reactance as a function of frequency in normal and smoking groups. The smoking patients presented highly significant increases in R 0, R m and f r (p =0.003, p =0.009 and p =0.005, respectively, Table 1 ). S did not show any significant changes, whereas significant decreases were observed in X m (p =0.002) and C rs,dyn (p =0.002). Z rs4Hz was higher in smokers (p =0.0004). 3.2 Performance of the studied classifiers 3.2.1 Experiment 1\u2014The use of original FOT parameters without feature selection Fig. 3 depicts a ROC curve for the best FOT parameter (BFP) and the average ROC curve for each studied classifier. Table 2 shows the sensitivity (Se), specificity (Sp) and area under the curve (AUC) for the ROC curves in Fig. 3. In this table, the optimal Se and Sp points were chosen to balance the highest values of these parameters. Table 3 shows comparisons among the AUCs (difference between AUCs) obtained with the BFP and each studied classifier. For an additional analysis of the ROC, Fig. 4A resumes the Se observed at Sp of 75% (representing a moderate specificity). The 90% specificity level was also described (Fig. 4B) because it theoretically forces the cases presumed to be the most difficult into the disease group by allowing only 10% false positives [38]. McNemar's test, when applied to all pairs of classifiers, indicated that there was a statistically significant difference between LOGLC and 1-NN and LOGLC and SVM. Wilcoxon's test showed statistically significant differences between LOGLC and 1-NN and LOGLC and SVM. We also observed differences between 1-NN and ANN. 3.2.2 Experiment 2\u2014Feature selection on original FOT parameters Fig. 5 depicts the reference ROC curve for the BFP and the average ROC curve for each classifier, and Table 4 describes the associated parameters. Table 5 shows comparisons among the AUCs. Fig. 4 resumes the sensitivities at 75% and 90% specificity for Z rs4Hz, LOGLC, 1-NN, ANN and SVM. McNemar's test revealed a statistically significant difference between LOGLC and 1-NN. The Wilcoxon's test showed difference between LOGLC and 1-NN, LOGLC and SVM. 3.2.3 Experiment 3\u2014Feature selection on cross products of FOT parameters Fig. 6 depicts the baseline ROC curve for the BFP and the average ROC curve for each studied classifier. Table 6 shows the derived parameters, while Table 7 shows comparisons among the AUCs obtained in this experiment. Sensitivities at 75% and 90% specificity for the BFP, LOGLC, 1-NN, ANN and SVM are described in Fig. 4. McNemar's test applied to all pairs of classifiers indicated that there was a statistically significant difference between LOGLC and 1-NN and LOGLC and SVM. Wilcoxon's test showed differences between ANN and SVM and ANN and 1-NN. 3.3 Pursuit for the best features and classifier parameters Table 8 shows the parameters applied in the genetic algorithm to search for the optimal features and parameters. Tables 9\u201311 show the best parameters, the selected features for each classifier and their respective AUCs. Table 12 describes the comparisons of the AUCs obtained with the best results in all of the performed experiments. 4 Discussion Because of the high social and medical costs associated with smoking-induced diseases, early identification and treatment of these patients is important to avoid severe and expensive stages of these diseases [2]. A recent consensus recommended that all smokers, including those who may be at risk for or already have COPD, should be offered the most intensive smoking cessation intervention feasible [16]. This consensus also recommended that patients should be identified as early in the course of the disease as possible, contributing to prevent smoking uptake and maximize cessation [16]. In this study, we designed and evaluated several ML algorithms to develop an automatic classifier to help diagnose early smoking-induced respiratory changes using forced oscillation measurements. We have demonstrated for the first time that such a clinical decision support system may increase the accuracy of the FOT in detecting these early respiratory abnormalities. Notably, the use of feature selection and cross products together with 1-NN and SVM classifiers allowed us to obtain an accurate clinical diagnosis while the smoking-induced pathologic changes are still potentially reversible. The clinical decision support system was developed using a group of smokers with a mean tobacco consumption of 11.2\u00b17.3 pack-years, characterising a sample of smokers with early respiratory changes but only small and non-significant reductions in their spirometric parameters [11]. Their smoking habits resulted in changes in oscillatory mechanics that were consistent with the involved pathophysiology [16]. In accordance with previous results [11\u201313,15,17,18], we observed a significant increase in resistive parameters and a reduction in reactive parameters (Table 1). The first experiment showed that all designed classifiers present better performance than the BFP used as baseline (AUC=0.77) (Figs. 3 and 4, Tables 2 and 3). Considering the comparisons of the sensitivities at 75% and 90% specificity (Fig. 4), it is worth mention that the sensitivities of 1-NN and SVM were higher than that obtained by the BFP. Table 2 shows that 1-NN demonstrated higher AUC (0.89), followed by SVM (0.87). According to the literature, ROC curves with AUCs between 0.50 and 0.70 indicate low diagnostic accuracy, AUCs between 0.70 and 0.90 indicate moderate accuracy, and AUCs between 0.90 and 1.00 indicate high accuracy [58]. Our results indicate that both 1-NN and SVM present AUCs close to the high accuracy range. Table 3 shows statistical differences in the ROC curves of the BFP and the classifiers 1-NN and SVM. It is worth to mention that these statistical differences were in agreement with the McNemar's test and Wilcoxon's test. Compared with the first experiment, the feature selection performed in the second experiment (Figs. 4 and 5, Tables 4 and 5) did not show any significant improvement in the AUC. Regarding the sensitivities, we observed an improvement in only the sensitivity for ANN (Fig. 4A). At 90% specificity, we observed an increase of sensitivity only for SVM (Fig. 4B). Although the feature selection did not help increase the overall performance, it helped identify the most useful FOT parameters. One may argue, in spite of the arguments presented in Section 2.5 to support the use of AUC in the feature selection, another performance measure would have better results. In order to verify this claim, a comparison of the results in the feature selection using AUC and accuracy was made in the experiment 2 for the KNN classifier (k =1). The experiment 2 was chosen, because an exhaustive search is performed, so the difference in the performance could not be caused by the search procedure or due to the choice of classifier parameters (the only parameter k was set to be equal 1). Using AUC for feature selection, the selected FOT parameters were: f r, X m, R 0, R m, Z rs4Hz Using Accuracy for feature selection, the selected FOT parameters were: f r, R 0, R m, C rs,dyn, Z rs4Hz The procedures have chosen a large number of common FOT parameters (f r, R 0, R m, Z rs4Hz) and in both cases the AUC=0.87 and no significant statistical differences were found. Therefore, at least for this experiment, there is no evidence that AUC is a worse choice than accuracy to select features. Table 5 indicates statistical differences in the ROC curves of the BFP and the classifiers 1-NN and SVM, which provide additional support to the results observed in the McNemar's test and Wilcoxon's test. In the third experiment, the feature selection performed on the cross products of the FOT parameters improved the accuracy of all classifiers (Figs. 4 and 6, Tables 6 and 7). In addition, this experiment also helped identify the most useful pairs of FOT parameters. The AUCs for 1-NN and SVM (0.91) were in the high accuracy diagnostic range [58]. In this experiment, all of the studied classifiers presented higher sensitivity values than the BFP (Fig. 4A). At 75% specificity, representing moderate specificity, 1-NN and SVM presented much higher sensitivities (88% and 92%) than the BFP (70%) (Fig. 4B). Once again, there are statistical differences in the ROC curves of the BFP and the classifiers 1-NN and SVM (Table 7), and also an endorsement of the statistical differences found in the McNemar's test and Wilcoxon's test. Tables 9\u201311 show the selected features for each of the studied classifiers. In a recent study [11], Z rs4Hz was the best parameter for detecting early effects of smoking, followed by R 0 and C rs,dyn. The parameters f r and X m were also considered useful. This finding is consistent with the results found in experiments 2 and 3. In experiment 2 (Table 10), the feature selection for 1-NN chose four (Z rs4Hz, R 0, f r, and X m) of the five cited parameters. The feature selection for SVM also chose four (Z rs4Hz, R 0, f r, and X m) of these five parameters. In the third experiment (Table 11), the selected cross products were also consistent with the results identified in [11]. For the LOGLC, all the selected products had at least one of the most useful values. For 1-NN, this situation occurred in nine of ten selected products, and for the SVM, it occurred in six of seven selected products. Table 12 compares the best results of each experiment with the baseline provided by the best FOT parameter (Z rs4Hz). In all experiments, the best classifiers presented better results than the best FOT parameter. There were not statistical differences between the best classifiers. However, it is worth to mention that the last experiment presented the best p-value (p <0.001) in comparison with the BFP. The FOT has a long history in the investigation of smoking-induced respiratory diseases [15]. Although its present use is predominantly in the research domain, the FOT will likely soon enter routine clinical use. One of the main factors that limit its clinical use is that the interpretation of the derived parameters measured by the FOT requires training and experience. By contrast, as noted recently by the GOLD consensus [16], diagnostic simplicity is a key feature for the busy non-specialist clinician. The present work provides evidence that ML algorithms may simplify the use of FOT. Therefore, ML algorithms associated with FOT measurements could be a simple tool in screening early respiratory changes induced by smoking. If this hypothesis is confirmed in a wider number of subjects, this approach may offer the ability to show abnormalities in a phase in which pathological changes are still potentially reversible, helping to prevent the development of COPD. 5 Conclusions We designed and evaluated several classifier systems to develop a clinical decision support system to assist the diagnosis of early respiratory abnormalities in smoking patients. The use of the cross products of the FOT indexes and the search for best features and classifier parameters introduced a significant improvement in the diagnostic accuracy. 1-NN and SVM classifiers were the most robust classifiers, reaching values that allow accurate clinical diagnosis, identifying early respiratory changes with approximately 85% sensitivity and specificity. In addition, the developed system may also be helpful for simplifying the use of the FOT in the routine assessment of lung function. 6 Future plans Future plans include (1) to add to the classification system the ability of identifying the level of airflow obstruction in COPD (mild, moderate, severe or very severe); (2) to contribute to the diagnosis of airway obstruction in asthma; and (3) to improve the understanding and management of COPD and its exacerbations by integrating ML algorithms and home monitoring using FOT and telemedicine services. References [1] C.D. Mathers D. Loncar Projections of global mortality and burden of disease from 2002 to 2030 PLoS Med. 3 11 2006 e442 [2] P.L. Enright R.M. Crapo Controversies in the use of spirometry for early recognition and diagnosis of chronic obstructive pulmonary disease in cigarette smokers Clin. Chest Med. 1 4 2000 645 652 [3] A.D. Kaminsky C.G. Irvin New insights from lung function Curr. Opin. Allergy Clin. Immunol. 1 2001 205 209 [4] M.I. Polkey R. Farr\u00e9 A.T. Dinh-Xuan Respiratory monitoring: revisiting classical physiological principles with new tools Eur. Respir. J. 24 2004 718 719 [5] T.L. Croxton G.G. Weinmann R.M. Senior J.R. Hoidal Future research directions in chronic obstructive pulmonary disease Am. J. Respir. Crit. Care Med. 165 2002 838 844 [6] E. Oostveen D. MacLeod H. Lorino R. Farr\u00e9 Z. Hantos K. Desager F. Marchal The forced oscillation technique in clinical practice: methodology, recommendations and future developments Eur. Respir. J. 22 2003 1026 1041 [7] J.H.T. Bates C.G. Irvin R. Farr\u00e9 Z. Hantos Oscillation mechanics of the respiratory system Compre. Physiol. 1 2011 1233 1272 [8] D.W. Kaczka R.L. Dellac\u00e1 Oscillation mechanics of the respiratory system: applications to lung disease Crit. Rev. Biomed. Eng. 39 4 2011 337 359 [9] D. Macleod M. Birch Respiratory input impedance measurements: forced oscillation methods Med. Biol. Eng. Comput. 39 2001 505 516 [10] D. Navajas R. Farr\u00e9 Forced oscillation technique: from theory to clinical applications Monaldi Arch. Chest Dis. 6 6 2001 555 562 [11] A.C.D. Faria A.J. Lopes J.M. Jansen P.L. Lopes Evaluating the forced oscillation technique in the detection of early smoking-induced respiratory changes Bio. Med. Eng. Online 8 2009 22 [12] A.M.G.T. Di Mango A.J. Lopes J.M. Jansen P.L. Melo Changes in respiratory mechanics with increasing degrees of airway obstruction in COPD: detection by the forced oscillation technique Respir. Med. 100 3 2006 399 410 [13] A.C.D. Faria A.A. Costa A.J. Lopes J.M. Jansen P.L. Melo Forced oscillation technique in the detection of smoking-induced respiratory alterations: diagnostic accuracy and comparison with spirometry Clinics 65 12 2010 1295 1304 [14] K.K.D. Silva A.J. Lopes J.M. Jansen P.L. Melo Within total inspiratory and expiratory impedance in patients with severe chronic obstructive pulmonary disease Clinics 66 12 2011 2085 2091 [15] A.C.D. Faria K.K. Dames da Silva G.M. Costa A.J. Lopes P.L. Melo Forced oscillation technique in the detection of smoking-induced respiratory changes R. Hudak M. Penhaker J. Majernik Biomedical Engineering\u2014Technical Applications in Medicine 2012 InTech Croatia Chapter 13 [16] GOLD\u2014Global initiative for chronic obstructive lung disease, Global Strategy for the Diagnosis, Management and Prevention of Chronic Obstructive Pulmonary Disease In: www.goldcopd.org, Available from: \u3008http://www.goldcopd.org/uploads/users/files/GOLD_Report_2011_Feb21.pdf\u3009, 2001. [17] J.L.M. Amaral, A.C.D. Faria, A.J. Lopes, J.M. Jansen, P.L. Melo, Automatic identification of chronic obstructive pulmonary disease based on forced oscillation measurements and artificial neural networks, in: 32nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Buenos Aires, Argentina, 2010. [18] J.L.M Amaral A.C.D. Faria A.J. Lopes J.M. Jansen P.L. Melo Machine learning algorithms and forced oscillation measurements applied to the automatic identification of chronic obstructive pulmonary disease Comput. Methods Programs Biomed. 105 3 2012 183 193 [19] American Thoracic Society/European Respiratory Society Task force: standardization of lung function testing Eur. Respir. J. 26 2005 319 338 [20] WHO\u2014World Health Organization: GINA\u2013Global Initiative for Asthma. 2006. Available from: \u3008http://www.ginasthma.org\u3009. [21] P.L. Melo M.M. Werneck A. Giannella-Neto A new impedance spectrometer for scientific and clinical studies of the respiratory system Rev. Sci. Instrum. 71 2000 2867 2872 [22] A.M. Lorino F. Zerah A. Mariette A. Harf H. Lorino Respiratory resistive impedance in obstructive patients: linear regression analysis vs viscoelastic modeling Eur. Respir. J. 10 1997 150 155 [23] R. Peslin B. Hannhart J. Pino Mechanical impedance of the chest in smokers and non-smokers Bull. Eur. Physiopathol. Respir. 17 1981 93 115 [24] Y. Ying R. Peslin C. Duvivier C. Gallina J. Felicio da Silva Respiratory input and transfer mechanical impedances in patients with chronic obstructive pulmonary disease Eur. Respir. J. 3 1990 1186 1192 [25] L.I. Kuncheva Combining Pattern Classifiers: Methods and Algorithms 2004 Wiley-Interscience New Jersey [26] A.R.R. Webb Statistical Pattern Recognition second ed. 2002 John Wiley & Sons, Ltd New Jersey [27] S. Haykin Neural Networks a Comprehensive Foundation 1994 Macmillan College Publishing Company Englewood Cliffs [28] V.N. Vapnik The Nature of Statistical Learning Theory second ed. 2000 Springer New York [29] P. McCullagh J.A. Nelder Generalized Linear Models second ed. 1989 Chapman and Hall/CRC London [30] G.P. Zhang Neural networks for classification: a survey IEEE Trans. Syst. Man Cybern. Part C: Appl. Rev. 30 2000 451 462 [31] C.E. Pedreira L. Macrini M.G. Land E.S. Costa New decision support tool for treatment intensity choice in childhood acute lymphoblastic leukemia IEEE Trans. Inf. Technol. Biomed. 13 2009 284 290 [32] M.H. Goldbaum P.A. Sample K. Chan J. Williams T.W. Lee E. Blumenthal C.A. Girkin L.M. Zangwill C. Bowd T. Sejnowski R.N. Weinreb Comparing machine learning classifiers for diagnosing glaucoma from standard automated perimetry Invest. Ophthalmol. Visual Sci. 43 1 2002 162 169 [33] I.H. Witten E. Frank Data Mining Practical Machine Learning Tools and Techniques second ed. 2005 Morgan Kaufmann San Francisco [34] D.T. Dietterich Approximate statistical tests for comparing supervised classification learning algorithms Neural Comput. 10 1998 1895 1923 [35] T. Fawcett An introduction to ROC analysis Pattern Recognit. Lett. 7 8 2006 861 874 [36] R. Kohavi A study of cross-validation and bootstrap for accuracy estimation and model selection Proceedings of the 14th International Joint Conference on Artificial Intelligence 1995 1137 1145 [37] P. Refaeilzadeh L. Tang H. Liu Cross Validation, Encyclopedia of Database Systems 2009 Springer New York [38] J. Demsar Statistical comparisons of classifiers over multiple data sets J. Mach. Learn. Res. 7 2006 1 30 [39] I. Guyon A. Elisseeff An introduction to variable and feature selection J. Mach. Learn. Res. 3 2003 1157 1182 [40] J.A. Hanley B.J. McNeil The meaning and use of the area under a receiver operating characteristic (ROC) curve Radiology 143 1982 29 36 [41] N. Obuchowski ROC analysis Am. J. Roentgenol. 184 2 2005 364 372 [42] S. Pintea R. Moldovan The receiver-operating characteristic (ROC) analysis: fundamentals and applications in clinical psychology J. Cogn. Behav. Psychother. 9 1 2009 49 66 [43] C.E. Metz Basic principles of ROC analysis Semin. Nucl. Med. 8 4 1978 283 298 [44] P. Bradley The use of the area under the ROC curve in evaluation of machine learning algorithms Pattern Recognit. 30 7 1997 1145 1159 [45] C.X. Ling J. Huang H. Zhang AUC: a statistically consistent and more discriminating measure than accuracy Proceedings of 18th International Conference on Artificial Intelligence 2003 519 524 [46] J. Huang C.X. Ling Using AUC and accuracy in evaluating learning algorithms IEEE Trans. Knowl. Data Eng. 17 3 2005 299 310 [47] F. Heijden R. Duin R. Ridder D.M.J. Tax Classification, Parameter Estimation and State Estimation: An Engineering Approach Using MATLAB 2004 John Wiley & Sons Ltd New Jersey [48] D.E. Goldberg Genetic Algorithms in Search, Optimization, and Machine Learning 1989 Addison-Wesley Indiana [49] Z. Michalewicz Genetic Algorithms+Data Structures=Evolution Programs 1992 Springer Verlag Berlin [50] R.P.W. Duin, P., Juszczak, P. Paclik., E. Pekalska, D. de Ridder, D.M.J., Tax, S., Verzakov, PRTools4.1, A Matlab Toolbox for Pattern Recognition, Delft University of Technology, Holland, 2007. [51] E.G.M. Lacerda A.C.P.L.F. Carvalho T.B. Ludermir Model selection via genetic algorithms for RBF networks J. Intell. Fuzzy Syst. 13 2 2003 111 122 [52] O. Il-Seok J.S. Lee B.R. Moon Hybrid genetic algorithms for feature selection IEEE Trans. Pattern Anal. Mach. Intell. 26 11 2004 1424 1437 [53] S. Lessmann R. Stahlbock S.F. Crone Genetic algorithms for support vector machine model selection Proceedings International Joint Conference on Neural Networks 2006 3063 3069 [54] T. Feng F. Xuezheng Y. Zhang A.G. Bourgeois A genetic algorithm-based method for feature subset selection Soft Comput. 12 2 2007 111 120 [55] H. Vafaie K. De Jong Genetic algorithms as a tool for feature selection in machine learning Proceedings of Fourth International Conference on Tools with Artificial Intelligence 2010 200 203 [56] M. Anbarasi E. Anupriya N.CH.S.N. Iyengar Enhanced prediction of heart disease with feature subset selection using genetic algorithm Int. J. Eng. Sci. Technol. 2 10 2010 5370 5376 [57] E.R. DeLong D.M. DeLong D.L. Clarke-Pearson Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach Biometrics 44 1988 837 845 [58] J.A. Swets Measuring the accuracy of diagnostic systems Science 240 1988 1285 1293", "scopus-id": "84885432509", "pubmed-id": "24001924", "coredata": {"eid": "1-s2.0-S0169260713002800", "dc:description": "Abstract The purpose of this study was to develop an automatic classifier to increase the accuracy of the forced oscillation technique (FOT) for diagnosing early respiratory abnormalities in smoking patients. The data consisted of FOT parameters obtained from 56 volunteers, 28 healthy and 28 smokers with low tobacco consumption. Many supervised learning techniques were investigated, including logistic linear classifiers, k nearest neighbor (KNN), neural networks and support vector machines (SVM). To evaluate performance, the ROC curve of the most accurate parameter was established as baseline. To determine the best input features and classifier parameters, we used genetic algorithms and a 10-fold cross-validation using the average area under the ROC curve (AUC). In the first experiment, the original FOT parameters were used as input. We observed a significant improvement in accuracy (KNN=0.89 and SVM=0.87) compared with the baseline (0.77). The second experiment performed a feature selection on the original FOT parameters. This selection did not cause any significant improvement in accuracy, but it was useful in identifying more adequate FOT parameters. In the third experiment, we performed a feature selection on the cross products of the FOT parameters. This selection resulted in a further increase in AUC (KNN=SVM=0.91), which allows for high diagnostic accuracy. In conclusion, machine learning classifiers can help identify early smoking-induced respiratory alterations. The use of FOT cross products and the search for the best features and classifier parameters can markedly improve the performance of machine learning classifiers.", "openArchiveArticle": "false", "prism:coverDate": "2013-12-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0169260713002800", "dc:creator": [{"@_fa": "true", "$": "Amaral, Jorge L.M."}, {"@_fa": "true", "$": "Lopes, Agnaldo J."}, {"@_fa": "true", "$": "Jansen, Jos\u00e9 M."}, {"@_fa": "true", "$": "Faria, Alvaro C.D."}, {"@_fa": "true", "$": "Melo, Pedro L."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0169260713002800"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0169260713002800"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0169-2607(13)00280-0", "prism:volume": "112", "prism:publisher": "Elsevier Ireland Ltd.", "dc:title": "An improved method of early diagnosis of smoking-induced respiratory changes using machine learning algorithms", "prism:copyright": "Copyright \u00a9 2013 Elsevier Ireland Ltd.", "openaccess": "1", "prism:issn": "01692607", "prism:issueIdentifier": "3", "dcterms:subject": [{"@_fa": "true", "$": "Clinical decision support"}, {"@_fa": "true", "$": "Early diagnosis"}, {"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Forced oscillation technique"}, {"@_fa": "true", "$": "Smoking"}, {"@_fa": "true", "$": "Chronic obstructive pulmonary disease"}], "openaccessArticle": "true", "prism:publicationName": "Computer Methods and Programs in Biomedicine", "prism:number": "3", "openaccessSponsorType": "FundingPartnerOpenArchive", "prism:pageRange": "441-454", "prism:endingPage": "454", "prism:coverDisplayDate": "December 2013", "prism:doi": "10.1016/j.cmpb.2013.08.004", "prism:startingPage": "441", "dc:identifier": "doi:10.1016/j.cmpb.2013.08.004", "openaccessSponsorName": "Brazilian Government"}, "objects": {"object": [{"@category": "high", "@height": "1062", "@width": "1943", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "602419", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1064", "@width": "1950", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "533284", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2243", "@width": "1499", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "487580", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1100", "@width": "1947", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "621207", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "4208", "@width": "2621", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "1264376", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "125", "@width": "2120", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "43367", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "300", "@width": "548", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "69114", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "300", "@width": "550", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "66978", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "506", "@width": "338", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "42953", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "311", "@width": "550", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "70154", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "950", "@width": "592", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "128779", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "35", "@width": "598", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "10316", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "120", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8342", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "119", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7602", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "109", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4067", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "124", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8069", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "102", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4328", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713002800-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "676", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84885432509"}}