{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608005001292", "dc:identifier": "doi:10.1016/j.neunet.2005.06.025", "eid": "1-s2.0-S0893608005001292", "prism:doi": "10.1016/j.neunet.2005.06.025", "pii": "S0893-6080(05)00129-2", "dc:title": "Handling missing values in support vector machine classifiers ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "18", "prism:issueIdentifier": "5-6", "prism:startingPage": "684", "prism:endingPage": "692", "prism:pageRange": "684-692", "prism:number": "5-6", "dc:format": "application/json", "prism:coverDate": "2005-08-31", "prism:coverDisplayDate": "July\u2013August 2005", "prism:copyright": "Copyright \u00a9 2005 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "IJCNN 2005", "dc:creator": [{"@_fa": "true", "$": "Pelckmans, K."}, {"@_fa": "true", "$": "De Brabanter, J."}, {"@_fa": "true", "$": "Suykens, J.A.K."}, {"@_fa": "true", "$": "De Moor, B."}], "dc:description": "\n               Abstract\n               \n                  This paper discusses the task of learning a classifier from observed data containing missing values amongst the inputs which are missing completely at random\n                        1\n                     \n                     \n                        1\n                        An abbreviated version of some portions of this article appeared in (Pelckmans et al., 2005a) as part of the IJCNN 2005 proceedings, published under the IEEE copyright.\n                     . A non-parametric perspective is adopted by defining a modified risk taking into account the uncertainty of the predicted outputs when missing values are involved. It is shown that this approach generalizes the approach of mean imputation in the linear case and the resulting kernel machine reduces to the standard Support Vector Machine (SVM) when no input values are missing. Furthermore, the method is extended to the multivariate case of fitting additive models using componentwise kernel machines, and an efficient implementation is based on the Least Squares Support Vector Machine (LS-SVM) classifier formulation.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608005001292", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608005001292", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "27744579949", "scopus-eid": "2-s2.0-27744579949", "pubmed-id": "16111866", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/27744579949", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20050818", "$": "2005-08-18"}}}}}