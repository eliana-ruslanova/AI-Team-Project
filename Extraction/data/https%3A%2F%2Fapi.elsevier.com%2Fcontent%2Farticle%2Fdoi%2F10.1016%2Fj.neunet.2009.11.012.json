{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608009002974", "dc:identifier": "doi:10.1016/j.neunet.2009.11.012", "eid": "1-s2.0-S0893608009002974", "prism:doi": "10.1016/j.neunet.2009.11.012", "pii": "S0893-6080(09)00297-4", "dc:title": "On the sparseness of 1-norm support vector machines ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "23", "prism:issueIdentifier": "3", "prism:startingPage": "373", "prism:endingPage": "385", "prism:pageRange": "373-385", "prism:number": "3", "dc:format": "application/json", "prism:coverDate": "2010-04-30", "prism:coverDisplayDate": "April 2010", "prism:copyright": "Copyright \u00a9 2009 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Zhang, Li"}, {"@_fa": "true", "$": "Zhou, Weida"}], "dc:description": "\n               Abstract\n               \n                  There is some empirical evidence available showing that 1-norm Support Vector Machines (1-norm SVMs) have good sparseness; however, both how good sparseness 1-norm SVMs can reach and whether they have a sparser representation than that of standard SVMs are not clear. In this paper we take into account the sparseness of 1-norm SVMs. Two upper bounds on the number of nonzero coefficients in the decision function of 1-norm SVMs are presented. First, the number of nonzero coefficients in 1-norm SVMs is at most equal to the number of only the exact support vectors lying on the +1 and \u22121 discriminating surfaces, while that in standard SVMs is equal to the number of support vectors, which implies that 1-norm SVMs have better sparseness than that of standard SVMs. Second, the number of nonzero coefficients is at most equal to the rank of the sample matrix. A brief review of the geometry of linear programming and the primal steepest edge pricing simplex method are given, which allows us to provide the proof of the two upper bounds and evaluate their tightness by experiments. Experimental results on toy data sets and the UCI data sets illustrate our analysis.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Sparse learning"}, {"@_fa": "true", "$": "Sparseness"}, {"@_fa": "true", "$": "1-norm SVM"}, {"@_fa": "true", "$": "Standard SVM"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608009002974", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608009002974", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "76849107689", "scopus-eid": "2-s2.0-76849107689", "pubmed-id": "20018477", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/76849107689", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20091203", "$": "2009-12-03"}}}}}