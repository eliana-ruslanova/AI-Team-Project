{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014002457", "dc:identifier": "doi:10.1016/j.neunet.2014.11.003", "eid": "1-s2.0-S0893608014002457", "prism:doi": "10.1016/j.neunet.2014.11.003", "pii": "S0893-6080(14)00245-7", "dc:title": "Approximate kernel competitive learning ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "63", "prism:startingPage": "117", "prism:endingPage": "132", "prism:pageRange": "117-132", "dc:format": "application/json", "prism:coverDate": "2015-03-31", "prism:coverDisplayDate": "March 2015", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Wu, Jian-Sheng"}, {"@_fa": "true", "$": "Zheng, Wei-Shi"}, {"@_fa": "true", "$": "Lai, Jian-Huang"}], "dc:description": "\n               Abstract\n               \n                  Kernel competitive learning has been successfully used to achieve robust clustering. However, kernel competitive learning (KCL) is not scalable for large scale data processing, because (1) it has to calculate and store the full kernel matrix that is too large to be calculated and kept in the memory and (2) it cannot be computed in parallel. In this paper we develop a framework of approximate kernel competitive learning for processing large scale dataset. The proposed framework consists of two parts. First, it derives an approximate kernel competitive learning (AKCL), which learns kernel competitive learning in a subspace via sampling. We provide solid theoretical analysis on why the proposed approximation modelling would work for kernel competitive learning, and furthermore, we show that the computational complexity of AKCL is largely reduced. Second, we propose a pseudo-parallelled approximate kernel competitive learning (PAKCL) based on a set-based kernel competitive learning strategy, which overcomes the obstacle of using parallel programming in kernel competitive learning and significantly accelerates the approximate kernel competitive learning for large scale clustering. The empirical evaluation on publicly available datasets shows that the proposed AKCL and PAKCL can perform comparably as KCL, with a large reduction on computational cost. Also, the proposed methods achieve more effective clustering performance in terms of clustering precision against related approximate clustering approaches.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Kernel clustering"}, {"@_fa": "true", "$": "Kernel competitive learning"}, {"@_fa": "true", "$": "Large scale computation"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014002457", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014002457", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84919951779", "scopus-eid": "2-s2.0-84919951779", "pubmed-id": "25528318", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84919951779", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20141127", "$": "2014-11-27"}}}}}