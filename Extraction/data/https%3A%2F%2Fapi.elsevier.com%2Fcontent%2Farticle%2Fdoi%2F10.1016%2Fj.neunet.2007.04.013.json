{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608007000354", "dc:identifier": "doi:10.1016/j.neunet.2007.04.013", "eid": "1-s2.0-S0893608007000354", "prism:doi": "10.1016/j.neunet.2007.04.013", "pii": "S0893-6080(07)00035-4", "dc:title": "Learning grammatical structure with Echo State Networks ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2007 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "20", "prism:issueIdentifier": "3", "prism:startingPage": "424", "prism:endingPage": "432", "prism:pageRange": "424-432", "prism:number": "3", "dc:format": "application/json", "prism:coverDate": "2007-04-30", "prism:coverDisplayDate": "April 2007", "prism:copyright": "Copyright \u00a9 2007 Published by Elsevier Ltd.", "prism:publisher": "Published by Elsevier Ltd.", "prism:issueName": "Echo State Networks and Liquid State Machines", "dc:creator": [{"@_fa": "true", "$": "Tong, Matthew H."}, {"@_fa": "true", "$": "Bickett, Adam D."}, {"@_fa": "true", "$": "Christiansen, Eric M."}, {"@_fa": "true", "$": "Cottrell, Garrison W."}], "dc:description": "\n               Abstract\n               \n                  Echo State Networks (ESNs) have been shown to be effective for a number of tasks, including motor control, dynamic time series prediction, and memorizing musical sequences. However, their performance on natural language tasks has been largely unexplored until now. Simple Recurrent Networks (SRNs) have a long history in language modeling and show a striking similarity in architecture to ESNs. A comparison of SRNs and ESNs on a natural language task is therefore a natural choice for experimentation. Elman applies SRNs to a standard task in statistical NLP: predicting the next word in a corpus, given the previous words. Using a simple context-free grammar and an SRN with backpropagation through time (BPTT), Elman showed that the network was able to learn internal representations that were sensitive to linguistic processes that were useful for the prediction task. Here, using ESNs, we show that training such internal representations is unnecessary to achieve levels of performance comparable to SRNs. We also compare the processing capabilities of ESNs to bigrams and trigrams. Due to some unexpected regularities of Elman\u2019s grammar, these statistical techniques are capable of maintaining dependencies over greater distances than might be initially expected. However, we show that the memory of ESNs in this word-prediction task, although noisy, extends significantly beyond that of bigrams and trigrams, enabling ESNs to make good predictions of verb agreement at distances over which these methods operate at chance. Overall, our results indicate a surprising ability of ESNs to learn a grammar, suggesting that they form useful internal representations without learning them.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Echo state networks"}, {"@_fa": "true", "$": "Simple recurrent networks"}, {"@_fa": "true", "$": "Grammar learning"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608007000354", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608007000354", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "34249874927", "scopus-eid": "2-s2.0-34249874927", "pubmed-id": "17556116", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/34249874927", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20070503", "$": "2007-05-03"}}}}}