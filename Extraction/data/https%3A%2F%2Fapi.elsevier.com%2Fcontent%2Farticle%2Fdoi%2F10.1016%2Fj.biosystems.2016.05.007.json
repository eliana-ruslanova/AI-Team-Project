{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0303264716300703", "dc:identifier": "doi:10.1016/j.biosystems.2016.05.007", "eid": "1-s2.0-S0303264716300703", "prism:doi": "10.1016/j.biosystems.2016.05.007", "pii": "S0303-2647(16)30070-3", "dc:title": "Robotic action acquisition with cognitive biases in coarse-grained state space ", "prism:publicationName": "Biosystems", "prism:aggregationType": "Journal", "prism:issn": "03032647", "prism:volume": "145", "prism:startingPage": "41", "prism:endingPage": "52", "prism:pageRange": "41-52", "dc:format": "application/json", "prism:coverDate": "2016-07-31", "prism:coverDisplayDate": "July 2016", "prism:copyright": "\u00a9 2016 Elsevier Ireland Ltd. All rights reserved.", "prism:publisher": "Elsevier Ireland Ltd.", "dc:creator": [{"@_fa": "true", "$": "Uragami, Daisuke"}, {"@_fa": "true", "$": "Kohno, Yu"}, {"@_fa": "true", "$": "Takahashi, Tatsuji"}], "dc:description": "\n               Abstract\n               \n                  Some of the authors have previously proposed a cognitively inspired reinforcement learning architecture (LS-Q) that mimics cognitive biases in humans. LS-Q adaptively learns under uniform, coarse-grained state division and performs well without parameter tuning in a giant-swing robot task. However, these results were shown only in simulations. In this study, we test the validity of the LS-Q implemented in a robot in a real environment. In addition, we analyze the learning process to elucidate the mechanism by which the LS-Q adaptively learns under the partially observable environment. We argue that the LS-Q may be a versatile reinforcement learning architecture, which is, despite its simplicity, easily applicable and does not require well-prepared settings.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Loosely symmetric model"}, {"@_fa": "true", "$": "Q-learning"}, {"@_fa": "true", "$": "Acrobot"}, {"@_fa": "true", "$": "Giant-swing robot"}, {"@_fa": "true", "$": "Partially observable markov decision process"}, {"@_fa": "true", "$": "Biologically inspired cognitive architecture"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0303264716300703", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0303264716300703", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84973440991", "scopus-eid": "2-s2.0-84973440991", "pubmed-id": "27195484", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84973440991", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20160516", "$": "2016-05-16"}}}}}