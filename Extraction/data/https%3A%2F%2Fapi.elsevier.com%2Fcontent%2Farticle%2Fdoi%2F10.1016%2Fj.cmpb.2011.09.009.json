{"scopus-eid": "2-s2.0-84856228303", "originalText": "serial JL 271322 291210 291791 291871 291901 31 90 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2011-10-21 2011-10-21 2014-09-30T23:42:41 1-s2.0-S0169260711002562 S0169-2607(11)00256-2 S0169260711002562 10.1016/j.cmpb.2011.09.009 S300 S300.3 FULL-TEXT 1-s2.0-S0169260712X00038 2015-05-14T05:25:39.534626-04:00 0 0 20120301 20120331 2012 2011-10-21T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccessdate sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor primabst ref 0169-2607 01692607 DELAY 2013-01-20 BZG false 105 105 3 3 Volume 105, Issue 3 2 183 193 183 193 201203 March 2012 2012-03-01 2012-03-31 2012 Section I: Methodology article fla Copyright \u00a9 2011 Elsevier Ireland Ltd. MACHINELEARNINGALGORITHMSFORCEDOSCILLATIONMEASUREMENTSAPPLIEDAUTOMATICIDENTIFICATIONCHRONICOBSTRUCTIVEPULMONARYDISEASE AMARAL J 1 Introduction 2 Background 3 Design considerations 3.1 Classification system 3.2 The studied classifiers 3.3 Feature selection 3.4 Performance evaluation 4 Methods 4.1 Subjects and spirometry 4.2 Forced oscillation technique 4.3 Feature selection 4.4 Search for the best classifier parameters 5 Results 5.1 Characteristics of the subjects 5.2 Feature selection 5.3 Performance of the studied classifiers using different feature selection methods 5.3.1 Experiment 1\u2014use of all features (FOT parameters) 5.3.2 Experiment 2\u2014forward selection search 5.3.3 Experiment 3\u2014forward floating selection 5.3.4 Experiment 4\u2014analysis of correlation coefficients 5.3.5 Experiment 5\u2014correlation coefficients 5.4 Search for the best classifier parameters 5.5 Performance of the KNN classifiers using different feature selection methods 6 Discussion 7 Conclusions 8 Future plans Conflict of interest Acknowledgements References ENRIGHT 2000 645 652 P LJUNG 1987 L SYSTEMIDENTIFICATIONTHEORYFORUSER DUBOIS 1956 587 594 A FARIA 2009 8 22 A ORR 1994 101 109 A BRIGHT 1998 1885 1891 P LEON 1994 548 553 M RASANEN 1998 433 439 J PERCHIAZZI 2001 1817 1824 G UNCU 2010 241 250 U LOPES 2011 1015 1019 A DIMANGO 2006 399 410 A IONESCU 2010 78 85 C BARUA 2004 3848 3851 M CONFPROCIEEEENGMEDBIOLSOC CLASSIFICATIONPULMONARYDISEASESBASEDIMPULSEOSCILLOMETRICMEASUREMENTSLUNGFUNCTIONUSINGNEURALNETWORKS BARUA 2005 327 331 M CONFPROCIEEEENGMEDBIOLSOC CLASSIFICATIONIMPULSEOSCILLOMETRICPATTERNSLUNGFUNCTIONINASTHMATICCHILDRENUSINGARTIFICIALNEURALNETWORKS MACLEOD 2001 505 516 D HELLINCKX 2001 564 570 J AMARAL 2010 J 32NDANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYBUENOSAIRESARGENTINA AUTOMATICIDENTIFICATIONCHRONICOBSTRUCTIVEPULMONARYDISEASEBASEDFORCEDOSCILLATIONMEASUREMENTSARTIFICIALNEURALNETWORKS KUNCHEVA 2004 L COMBININGPATTERNCLASSIFIERSMETHODSALGORITHMS DUDA 2000 R PATTERNCLASSIFICATION TAN 2006 P INTRODUCTIONDATAMINING WITTEN 2005 I DATAMININGPRACTICALMACHINELEARNINGTOOLSTECHNIQUES HAYKIN 1994 S NEURALNETWORKSACOMPREHENSIVEFOUNDATION VAPNIK 2000 V NATURESTATISTICALLEARNINGTHEORY ZHANG 2000 451 462 G PEDREIRA 2009 284 290 C GOLDBAUM 2002 162 169 M GUYON 2003 1157 1182 I DIETTERICH 1998 1895 1923 D FAWCETT 2006 861 874 T REFAEILZADEH 2009 P CROSSVALIDATIONENCYCLOPEDIADATABASESYSTEMS DEMSAR 2006 1 30 J CAVALCANTI 2006 2207 2219 J MELO 2000 2867 2872 P MELO 2000 102 108 P FARIA 2009 93 104 A DUIN 2007 R PRTOOLS41AMATLABTOOLBOXFORPATTERNRECOGNITION MARTINEZ 2005 W EXPLORATORYDATAANALYSISMATLAB CONOVER 1999 W PRACTICALNONPARAMETRICSTATISTICS DELANY 2005 359 378 S HANLEY 1982 29 36 J SWETS 1983 266 267 J SWETS 1988 1285 1293 J GOLPE 1999 932 937 R CROXTON 2002 838 844 T AMARALX2012X183 AMARALX2012X183X193 AMARALX2012X183XJ AMARALX2012X183X193XJ Full 2013-07-22T23:09:17Z FundingPartnerOpenArchive Brazilian Government http://www.elsevier.com/open-access/userlicense/1.0/ item S0169-2607(11)00256-2 S0169260711002562 1-s2.0-S0169260711002562 10.1016/j.cmpb.2011.09.009 271322 2014-10-02T02:08:11.564929-04:00 2012-03-01 2012-03-31 DELAY 2013-01-20 BZG 1-s2.0-S0169260711002562-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/MAIN/application/pdf/60a837c137c6179f2b5c51df3a71bef7/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/MAIN/application/pdf/60a837c137c6179f2b5c51df3a71bef7/main.pdf main.pdf pdf true 791952 MAIN 11 1-s2.0-S0169260711002562-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/PREVIEW/image/png/9b53aaef27c8202336869cc28dfd54c2/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/PREVIEW/image/png/9b53aaef27c8202336869cc28dfd54c2/main_1.png main_1.png png 49079 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0169260711002562-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/STRIPIN/image/gif/6e56e72f43a84cb30d7a08aa818137ba/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/STRIPIN/image/gif/6e56e72f43a84cb30d7a08aa818137ba/si1.gif si1 si1.gif gif 1849 45 305 ALTIMG 1-s2.0-S0169260711002562-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr5/HIGHRES/image/jpeg/8284d24429dd21adb3d837a3de4881b7/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr5/HIGHRES/image/jpeg/8284d24429dd21adb3d837a3de4881b7/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 203300 1388 1663 IMAGE-HIGH-RES 1-s2.0-S0169260711002562-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr4/HIGHRES/image/jpeg/1b9aa7d426cd4f7cfb978208058a1197/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr4/HIGHRES/image/jpeg/1b9aa7d426cd4f7cfb978208058a1197/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 216738 1385 1663 IMAGE-HIGH-RES 1-s2.0-S0169260711002562-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr3/HIGHRES/image/jpeg/460742a1050648068b7ace82c25373e2/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr3/HIGHRES/image/jpeg/460742a1050648068b7ace82c25373e2/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 211177 1389 1663 IMAGE-HIGH-RES 1-s2.0-S0169260711002562-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr2/HIGHRES/image/jpeg/92d7be01a2dd8620cc7ee96d56594e15/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr2/HIGHRES/image/jpeg/92d7be01a2dd8620cc7ee96d56594e15/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 227583 1391 1663 IMAGE-HIGH-RES 1-s2.0-S0169260711002562-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr1/HIGHRES/image/jpeg/9581d20389eeee304568abb36c4b61a5/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr1/HIGHRES/image/jpeg/9581d20389eeee304568abb36c4b61a5/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 225115 1234 1663 IMAGE-HIGH-RES 1-s2.0-S0169260711002562-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr5/DOWNSAMPLED/image/jpeg/55d487db7ed8a4fa157ec5c8f43f335b/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr5/DOWNSAMPLED/image/jpeg/55d487db7ed8a4fa157ec5c8f43f335b/gr5.jpg gr5 gr5.jpg jpg 27555 313 375 IMAGE-DOWNSAMPLED 1-s2.0-S0169260711002562-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr4/DOWNSAMPLED/image/jpeg/cdb1ba93d68186a0586660d9bd251706/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr4/DOWNSAMPLED/image/jpeg/cdb1ba93d68186a0586660d9bd251706/gr4.jpg gr4 gr4.jpg jpg 29256 312 375 IMAGE-DOWNSAMPLED 1-s2.0-S0169260711002562-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr3/DOWNSAMPLED/image/jpeg/70ce97bb917746f7173e97c24f7f5608/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr3/DOWNSAMPLED/image/jpeg/70ce97bb917746f7173e97c24f7f5608/gr3.jpg gr3 gr3.jpg jpg 28921 313 375 IMAGE-DOWNSAMPLED 1-s2.0-S0169260711002562-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr2/DOWNSAMPLED/image/jpeg/64e576756ce60daeb9c963a51882c8da/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr2/DOWNSAMPLED/image/jpeg/64e576756ce60daeb9c963a51882c8da/gr2.jpg gr2 gr2.jpg jpg 29935 314 375 IMAGE-DOWNSAMPLED 1-s2.0-S0169260711002562-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr1/DOWNSAMPLED/image/jpeg/f7ef61695de09d597803ec3439cb8707/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr1/DOWNSAMPLED/image/jpeg/f7ef61695de09d597803ec3439cb8707/gr1.jpg gr1 gr1.jpg jpg 28049 278 375 IMAGE-DOWNSAMPLED 1-s2.0-S0169260711002562-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr5/THUMBNAIL/image/gif/507afbe9383f95a414580c9209732e72/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr5/THUMBNAIL/image/gif/507afbe9383f95a414580c9209732e72/gr5.sml gr5 gr5.sml sml 4810 164 196 IMAGE-THUMBNAIL 1-s2.0-S0169260711002562-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr4/THUMBNAIL/image/gif/3b4ae6e5a2ce427c54b1f847a3defd09/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr4/THUMBNAIL/image/gif/3b4ae6e5a2ce427c54b1f847a3defd09/gr4.sml gr4 gr4.sml sml 5105 164 197 IMAGE-THUMBNAIL 1-s2.0-S0169260711002562-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr3/THUMBNAIL/image/gif/cc1d9fb8f28825c64f1f4cefd7e7cf9d/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr3/THUMBNAIL/image/gif/cc1d9fb8f28825c64f1f4cefd7e7cf9d/gr3.sml gr3 gr3.sml sml 4820 164 196 IMAGE-THUMBNAIL 1-s2.0-S0169260711002562-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr2/THUMBNAIL/image/gif/28c48bca4d2de27b8c1cc740a55049fa/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr2/THUMBNAIL/image/gif/28c48bca4d2de27b8c1cc740a55049fa/gr2.sml gr2 gr2.sml sml 5232 164 196 IMAGE-THUMBNAIL 1-s2.0-S0169260711002562-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260711002562/gr1/THUMBNAIL/image/gif/c8eb5af13705212a589995aefd6ed83d/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260711002562/gr1/THUMBNAIL/image/gif/c8eb5af13705212a589995aefd6ed83d/gr1.sml gr1 gr1.sml sml 5694 163 219 IMAGE-THUMBNAIL COMM 3283 S0169-2607(11)00256-2 10.1016/j.cmpb.2011.09.009 Elsevier Ireland Ltd Fig. 1 Average ROC curve for experiment 1. Fig. 2 Average ROC curve for experiment 2. Fig. 3 Average ROC curve for experiment 3. Fig. 4 Average ROC curve for experiment 4. Fig. 5 Average ROC curve for experiment 5. Table 1 Biometric and spirometric characteristics of the studied groups. CG COPD p Age (years) 55.2\u00b116.7 61.4\u00b19.7 ns (0.38) Weight (kg) 65.4\u00b111.8 66.0\u00b18.4 ns (0.84) Height (cm) 162.2\u00b18.9 163.5\u00b17.9 ns (0.58) FEV1 (L) 2.8\u00b10.9 1.4\u00b10.7 <0.0001 FEV1 (% pred) 107.1\u00b120.3 57.0\u00b127.7 <0.0001 FEF/FVC (%) 100.3\u00b132.3 28.5\u00b118.3 <0.0001 FEV1/FVC (%) 87.9\u00b110.0 55.0\u00b116.7 <0.0001 Table 2 selected features using different strategies. Search strategy Selected features Forward f r, X m, R 0, C rs,dyn, |Z rs| Backward f r, X m, R 0, |Z rs| Forward floating f r, X m, R 0, |Z rs| Table 3 Results of the experiment 1. Classifier Acc Se Sp AUC LBNC 0.89\u00b10.07 0.78\u00b10.14 1.00\u00b10.00 0.95\u00b10.05 KNN 0.97\u00b10.04 0.96\u00b10.09 0.99\u00b10.04 1.00\u00b10.00 DTREE 0.90\u00b10.05 0.90\u00b10.08 0.91\u00b10.07 0.95\u00b10.04 ANN 0.93\u00b10.06 0.89\u00b10.12 0.96\u00b10.06 0.97\u00b10.05 SVM 0.96\u00b10.05 0.97\u00b10.05 0.94\u00b10.05 1.00\u00b10.01 LBNC, Linear Bayes Normal Classifier; KNN, K nearest neighbor; DTREE, decision trees; ANN, artificial neural networks; SVM, support vector machines; Acc, accuracy; Se, sensitivity; Sp, specificity; AUC, area under the ROC curve. Bold indicates the best values of accuracy, sensitivity, specificity and AUC. Table 4 Results of the experiment 2. Classifier Acc Se Sp AUC LBNC 0.90\u00b10.07 0.80\u00b10.13 1.00 \u00b1 0.00 0.96\u00b10.05 KNN 0.95 \u00b1 0.04 0.93\u00b10.09 0.97\u00b10.05 1.00 \u00b1 0.00 DTREE 0.89\u00b10.07 0.88\u00b10.13 0.91\u00b10.09 0.95\u00b10.04 ANN 0.94\u00b10.05 0.92\u00b10.09 0.96\u00b10.07 0.96\u00b10.06 SVM 0.95 \u00b1 0.05 0.95 \u00b1 0.09 0.95\u00b10.07 0.98\u00b10.03 Bold indicates the best values of accuracy, sensitivity, specificity and AUC. Table 5 Results of the experiment 3. Classifier Acc Se Sp AUC LBNC 0.89\u00b10.07 0.78\u00b10.14 1.00 \u00b1 0.00 0.96\u00b10.04 KNN 0.95 \u00b1 0.04 0.92\u00b10.07 0.97\u00b10.06 0.99 \u00b1 0.03 DTREE 0.89\u00b10.07 0.89\u00b10.11 0.89\u00b10.09 0.95\u00b10.05 ANN 0.91\u00b10.08 0.87\u00b10.09 0.95\u00b10.10 0.97\u00b10.06 SVM 0.95 \u00b1 0.05 0.94 \u00b1 0.07 0.95\u00b10.09 0.96\u00b10.05 Bold indicates the best values of accuracy, sensitivity, specificity and AUC. Table 6 Results of the experiment 4. Classifier Acc Se Sp AUC LBNC 0.90\u00b10.07 0.80\u00b10.13 1.00 \u00b1 0.00 0.96\u00b10.05 KNN 0.95 \u00b1 0.09 0.93 \u00b1 0.10 0.97\u00b10.09 0.99 \u00b1 0.03 DTREE 0.91\u00b10.08 0.89\u00b10.11 0.93\u00b10.11 0.96\u00b10.05 ANN 0.91\u00b10.05 0.89\u00b10.09 0.93\u00b10.09 0.95\u00b10.05 SVM 0.93\u00b10.11 0.91\u00b10.11 0.94\u00b10.14 0.96\u00b10.07 Bold indicates the best values of accuracy, sensitivity, specificity and AUC. Table 7 Experiment 5 results. Classifier Acc Se Sp AUC LBNC 0.90\u00b10.07 0.80\u00b10.13 1.00 \u00b1 0.00 0.97\u00b10.05 KNN 0.93 \u00b1 0.05 0.91 \u00b1 0.09 0.96\u00b10.07 0.99 \u00b1 0.01 DTREE 0.90\u00b10.07 0.91 \u00b1 0.09 0.89\u00b10.15 0.95\u00b10.05 ANN 0.93 \u00b1 0.05 0.91 \u00b1 0.09 0.94\u00b10.07 0.97\u00b10.04 SVM 0.91\u00b10.07 0.82\u00b10.14 1.00\u00b10.00 0.97\u00b10.04 Bold indicates the best values of accuracy, sensitivity, specificity and AUC. Table 8 Selected parameters for different selected features. Selected features Classifiers Parameter Value Average accuracy All features DTREE Splitting criterion Purity 0.89 Pruning type None ANN Number of hidden nodes 8 0.95 SVM Regularization parameter(C) 8 0.96 Radius (r) 0.707 f r, X m, R 0, C rs,dyn, |Z rs| DTREE Splitting criterion Purity 0.90 Pruning type None ANN Number of hidden nodes 7 0.92 SVM Regularization parameter(C) 4 0.95 Radius (r) 0.5 Table 9 selected parameters for different selected features. Selected features Classifiers Parameter Value Average accuracy f r, X m, R 0, |Z rs| DTREE Splitting criterion Purity 0.89 Pruning type None ANN Number of hidden nodes 3 0.90 SVM Regularization parameter(C) 22.627 0.94 Radius (r) 0.42 f r, R 0, C rs,dyn DTREE Splitting criterion Purity 0.92 Pruning type None ANN Number of hidden nodes 3 0.92 SVM Regularization Parameter(C) 8 0.95 Radius (r) 0.5 R 0, C rs,dyn DTREE Splitting criterion Purity 0.89 Pruning type None ANN Number of hidden nodes 3 0.91 SVM Regularization parameter(C) 0.25 0.91 Radius (r) 1 Table 10 Comparisons of the results achieved by KNN in all of the experiments. Experiment Acc Se Sp AUC All Features 0.97\u00b10.04 0.96\u00b10.09 0.99\u00b10.04 1.00\u00b10.00 f r, X m, R 0, C rs,dyn, |Z rs| 0.95\u00b10.04 0.93\u00b10.09 0.97\u00b10.05 1.00\u00b10.00 f r, X m, R 0, |Z rs| 0.95\u00b10.04 0.92\u00b10.07 0.97\u00b10.06 0.99\u00b10.03 f r, R 0, C rs,dyn 0.95\u00b10.09 0.93\u00b10.10 0.97\u00b10.09 0.99\u00b10.03 R 0, C rs,dyn 0.93\u00b10.05 0.91\u00b10.09 0.96\u00b10.07 0.99\u00b10.01 Machine learning algorithms and forced oscillation measurements applied to the automatic identification of chronic obstructive pulmonary disease Jorge L.M. Amaral a Agnaldo J. Lopes b Jos\u00e9 M. Jansen b Alvaro C.D. Faria c Pedro L. Melo c \u204e plopes@uerj.br plopeslib@gmail.com a Department of Electronics and Telecommunications Engineering, State University of Rio de Janeiro, Rio de Janeiro, Brazil b Pulmonary Function Laboratory, Pedro Ernesto University Hospital, State University of Rio de Janeiro, Rio de Janeiro, Brazil c Biomedical Instrumentation Laboratory, Institute of Biology Roberto Alcantara Gomes and Laboratory of Clinical and Experimental Research in Vascular Biology (BioVasc), State University of Rio de Janeiro, Rio de Janeiro, Brazil \u204e Corresponding author. Abstract The purpose of this study is to develop a clinical decision support system based on machine learning (ML) algorithms to help the diagnostic of chronic obstructive pulmonary disease (COPD) using forced oscillation (FO) measurements. To this end, the performances of classification algorithms based on Linear Bayes Normal Classifier, K nearest neighbor (KNN), decision trees, artificial neural networks (ANN) and support vector machines (SVM) were compared in order to the search for the best classifier. Four feature selection methods were also used in order to identify a reduced set of the most relevant parameters. The available dataset consists of 7 possible input features (FO parameters) of 150 measurements made in 50 volunteers (COPD, n =25; healthy, n =25). The performance of the classifiers and reduced data sets were evaluated by the determination of sensitivity (Se), specificity (Sp) and area under the ROC curve (AUC). Among the studied classifiers, KNN, SVM and ANN classifiers were the most adequate, reaching values that allow a very accurate clinical diagnosis (Se>87%, Sp>94%, and AUC>0.95). The use of the analysis of correlation as a ranking index of the FOT parameters, allowed us to simplify the analysis of the FOT parameters, while still maintaining a high degree of accuracy. In conclusion, the results of this study indicate that the proposed classifiers may contribute to easy the diagnostic of COPD by using forced oscillation measurements. Keywords Clinical decision support Artificial intelligence Classification Forced oscillation technique Respiratory system Chronic obstructive pulmonary disease 1 Introduction Chronic obstructive pulmonary disease (COPD) is a major cause of chronic morbidity and mortality throughout the world [1]. According to WHO estimates, 80 million people have moderate to severe COPD. More than 3 million people died of COPD in 2005, which corresponds to 5% of all deaths globally [2]. The chronic airflow limitation characteristic of COPD is caused by a mixture of small airway disease (obstructive bronchiolitis) and parenchymal destruction (emphysema) [1]. There is an agreement in the literature that new measurement technologies that are able to detect COPD in early stages would contribute to decreasing medical and economic burdens [3]. Submitting a physical system to forced oscillations is a very general approach to the investigation of its structure and/or properties [4]. Its application to respiratory mechanics was first proposed by DuBois et al. [5]. This method, known as forced oscillation technique (FOT), consists of applying small sinusoidal pressure variations to stimulate the respiratory system at frequencies higher than the normal breathing frequency and measuring the flow response. This method characterizes the respiratory impedance and its two components, respiratory system resistance (R rs) and reactance (X rs). The method is simple and requires only passive co-operation and no forced expiratory maneuvers. Recently, this technique has been successfully applied in the detection of early respiratory changes in smokers [6]. Although obtaining respiratory impedance values is easy, the resulting values are difficult to understand by clinicians as they are based on an electrical equivalent circuit model of the respiratory system. In the context of a diagnosis framework, the interpretation of resistance and reactance curves, as well as the derived parameters measured by the FOT, requires training and experience, and is difficult task for the untrained pulmonologist. Methods based on machine learning (ML) have been widely used to develop classifiers. These systems can extract information from different classes of signals after having been trained to perform this specific task by learning from examples. In respiratory mechanics, ML proved to be useful as a pattern recognition method to optimize alarms of anesthesia breathing circuits [7], detection of upper airway obstruction [8], esophageal intubation [9], assessment of lung injury [10], static compliance in animal models [11] and the evaluation of spirometric exams [12]. Recently, a severity classification for idiopathic pulmonary fibrosis by using fuzzy logic was proposed [13]. 2 Background Previous works [14,15] have compared groups of controls and COPD patients observing clear modifications in FOT parameters. However, categorization of pulmonary diseases by looking at the plotted curves of respiratory impedance or derived parameters can prove a difficult task for the untrained pulmonologist. This raises the question: an ML based approach to the analysis of FOT data can provide an efficient method to recognize COPD? In fact, only two recent conference papers have addressed this question [16,17]. In the work of Baru\u00e1 et al. [16], an artificial neural network (ANN) was used to recognize and classify the diseases of the central and peripheral airways. The authors used IOS measurements and a feedforward ANN that was trained by the backpropagation algorithm. After supervised training, the classifier produced a 98.47% and 61.53% correct classification rate when the same data and a new set of unseen data were used, respectively. It was pointed out that the proposed classifier could be further improved with the inclusion of more training samples combined with fuzzy logic decision rules. In a latter work of the same group [17], a classifier based on ANN was capable of distinguishing between relatively constricted and nonconstricted airway conditions in asthmatic children. The performance of the classifier was evaluated by two methods: (1) using all of the patterns during training as well as in the feed-forward stage and (2) using only 60% of the data set during training and with the remaining 40% as unseen patterns. The classification accuracies obtained were 95.01% and 98.61%, respectively. The authors concluded that ANNs can successfully be trained with the impulse oscillation system (IOS) data, enabling them to generalize the IOS parameter relationships to classify previously unseen pulmonary patterns. The two cited studies used an IOS, which has differences from the classical FOT, including data processing and the parameters used to interpret raw data [18,19]. In addition, from a system identification point of view, the impulse excitation signal used in IOS is a much worse excitation signal than a Multisine used in FOT. This difference is associated with a worse crest factor in the impulse signal. In this context, we observed that there was no data in the literature concerning the use of ML algorithms associated with classical FOT measurements to aid clinicians in the identification of COPD. To contribute to elucidate this question, our group recently investigated this possibility using the classical FOT associated with a classifier based on ANN [20]. Two feature selection methods (the analysis of the linear correlation and forward search) were used in order to identify a reduced set of the most relevant parameters. Two different training strategies for the ANNs were used and the performance of resulting networks were evaluated by the determination of accuracy, sensitivity (Se), specificity (Sp) and AUC. The ANN classifiers presented high accuracy (Se>0.9, Sp>0.9 and AUC>0.9) both in the complete and the reduce sets of FOT parameters. This indicates that ANNs classifiers may contribute to easy the diagnostic of COPD using FOT measurements. Although these results were very promising, this initial work was limited to the investigation of an ANN based classifier because we were interested in a direct comparison with the two previously cited works. The purpose of the present study is to evaluate the performance of several ML algorithms in the development of an automatic classifier to help the diagnostic of COPD using forced oscillation measurements. The paper is organized as follows: a discussion of the design principles and implementation goals is presented in the next section. The healthy group and the COPD group are characterized in Section 4, along with a description of the measurement protocol. This section also presents the evaluated classifiers and describes the methods used for performance evaluation, comparisons among classifiers and feature selection. Section 5 presents the results and Section 6 discusses the results with respect to the search for the best classifier and parameters. Section 7 summarizes the main outcomes of this investigation and points to future steps in this research topic. 3 Design considerations 3.1 Classification system The basic structure of a classification system is the input, the classifier and the output. In the present work, the inputs are the parameters provided by the FOT, the classifier is one of the pattern recognition algorithms chosen, and the output tells if the input parameters indicate COPD or not. The design process of a classification system presents several important aspects such as: the evaluation of the classifiers, choice of the algorithms to be used, feature selection, selection of the best parameters and comparison of classifiers performance. In the following sections, these aspects will be briefly described. 3.2 The studied classifiers In this particular study, the following classification algorithms were evaluated: \u2022 Linear Bayes Normal Classifier [21,22] \u2022 K nearest neighbor [21] \u2022 Decision trees [23,24] \u2022 Artificial neural networks [25] \u2022 Support vector machines [26] These algorithms were chosen because they represent wide variety of classifier algorithms as seen in Lippmann's list of types of classifiers [21]. They will be briefly described. The complete full description of the algorithms can be found in the references. The Linear Bayes Normal Classifier (LBNC) presents the minimum-error, according to the Bayesian Decision Theory, when the classes are normally distributed with equal covariance matrixes. The Linear Bayes is fast and simple to compute from the training data and provides a very straight interpretation, since it is decision boundary is a hyperplane. In spite of its simplicity, it is reasonably robust, i.e., it can deliver surprisingly good results even when the classes do not follow normal distributions with equal covariance matrixes [21]. The K nearest neighbor (KNN) is one of the most simple and elegant classification methods in pattern recognition [21]. It is a type of instance-based learning, or lazy learning, which means that in the learning stage, it simply stores a set of labeled instances (training set). When a new query instance has to be classified, the algorithm finds K number of training instances closest to the query point, using a similarity function usually based on the Euclidean distance. The classification is done using the majority vote among the classification of the K objects. If K =1, then the object is simply assigned to the class of its nearest neighbor. A decision tree (DTREE) is a hierarchical structure that consists of nodes and branches [23]. There are three types of nodes: the root that has only outgoing branches, the internal nodes that have one incoming and two or more outgoing branches and terminal (leaf) nodes that have no outgoing branches. All terminal nodes have a class label assigned to them [23]. Each non terminal node in the tree represents a test on one of the attributes and each branch that comes out of the node represents one of the possible outcomes of the test performed. A query instance is classified by starting at the root node, testing the attribute specified by this node, and then moving down the tree branch corresponding to the outcome of the test for this attribute. This process is repeated until it gets to a terminal node, where the class label is given to the query instance. An ANN is a massive parallel system [25] composed of many simple processing elements (neurons) whose function is determined by the network architecture, connection strengths (synaptic weights) and the processing performed at the neurons. Neural networks are capable of acquiring knowledge through a learning process and to store that knowledge in the synaptic weights. One of the most successful neural network architecture is the multilayer perceptron (MLP). It has been successfully applied to a variety of pattern recognition problems in industry, business, science [27] and in medical diagnosis [27,28]. One of the most important features of a neural network is the ability to generalize what it has learned from the training procedure. This allows the network to deal with noise in the input data and to provide the correct outputs to new data patterns, i.e., data that were not used to train the network. Support vector machines (SVM) are learning systems based on statistical learning theory [26] and they have been successfully used in a variety of classification and regression problems. For a two-class classification problem, the basic form SVM is a linear classifier that performs a classification constructing a hyperplane that optimally separates the classes. The optimal hyperplane is the one that provides the maximal margin. (The margin is defined as the distance from a training sample and the hyperplane.) It can be proven that this particular solution has the highest generalization ability. This formulation can be generalized applying a non-linear mapping of the training set. The data is transformed to a new feature high-dimensional space where the classes are more easily separable and an optimal hyperplane can be found. The radial basis function Kernel is frequently using in accomplishing this non linear mapping and it is frequently the first non linear mapping to consider. Although the decision surface (hyperplane) is linear in the high dimensional space, however, when it is seen in the original low-dimensional feature space, it is no longer linear, meaning that SVM can also be applied to data that is not linearly separable [29]. 3.3 Feature selection The purpose of the input feature selection is to find the smallest number of relevant and informative features that can result in a satisfactory performance [30]. Other motivations to perform feature selection are: general data reduction, to limit storage requirements, increase the algorithm speed and to gain knowledge about the process that generates the data and to allow data visualization (2D or 3D) [30]. It is also important because a large number of inputs imply in the estimation of a large number of model parameters, which can be difficult in limited size datasets [28]. Basically there are three types of features selection methods: filters, wrappers and embedded methods [30]. Filter methods provide a ranking order of the features using a relevant index such as correlation coefficients or classical statistical tests (T-test, F-test, Chi-squared, etc.). Wrappers normally apply an efficient search strategy to find the best features based on the machine learning algorithm performance, such as the classification accuracy. Embedded methods perform feature selection in the process of training and are usually specific to some given learning machines, such as decision trees [30]. 3.4 Performance evaluation The evaluation of the classifiers plays a key role in classification system design. Its primary goal is to choose the best classifier and estimates its performance on future examples (generalization accuracy) [31]. The main components in this evaluation are: the choice of the performance function, the evaluation structure and the comparison of different classifiers. There are several measures that can be used to access the performance of the classifier, depending on the specific domain of application. Some of the common used measures are: accuracy, sensitivity, specificity, True Positive Rate, False Positive Rate, Recall, Precision and the area under the Receiver Operating Characteristic (ROC) curve (AUC) [32]. The evaluation structure is an important part of the design. In order to decide the best classifier, one has to look into the generalization accuracy. This can be done using either Hold-out or K-fold cross-validation procedures. In Hold out, the available data is divided in training and test datasets. The classifier is trained with the training data set and the performance of the trained classifier is evaluated in the test data set to estimate the generalization accuracy. The problem with Hold out is that different Hold out sets (different splits) leads to different results. Also, depending on the available data, it is possible to end up with a very wide confidence interval for the accuracy [24]. In a K-fold cross-validation, all the available data is partitioned into k equal (or approximately equal) data sets or folds [33]. For each fold in turn, use that folder for testing and the remaining k \u22121 folders are use for training a classifier. The performance of each learning algorithm on each fold can be tracked using some pre-determined measure such as accuracy. Upon completion, k samples of the performance metric will be available and different methodologies such as averaging can be used to obtain an aggregate measure from these samples, or these samples can be used in a statistical Hypothesis test to compare two or more machine learning algorithms. The use of K-fold cross-validation allows us to estimate performance of the learned model from available data using one algorithm. In other words, it is possible to estimate its performance in unseen examples (the generalization capability of the algorithm). It can also be used to compare the performance of two or more different algorithms and realize the best algorithm for the available data, or alternatively, it can help the designer to choose the best set of parameters of a particular model. The Hypothesis test is another important element when one desire to compare two or more machine learning algorithms. In the Hypothesis test, we want to verify if there is no difference in the performance of two classifiers (Null Hypothesis) under a certain confidence level (usually 95%). For a comparison in one data set, one can use the Student\u0301s test (t-test) or one of its variations, for example the corrected resample [24]. Dietterich [31] points out that the use of a t-test has a right risk of a Type I error, i.e., a risk of find a difference where none exists, recommending the 5\u00d72 cross-validation or the use of McNema\u0155s test. In the case of multiple data sets from different domains, Demsar [34] recommends Wilcoxon's Signed Ranks test, Friedman tests and Post hoc tests. It is also important to mention that sometimes classifiers are evaluated not only by their performance measures, but also by the speed and scalability, robustness and interpretability. When one looks at speed and scalability, he (she) is interested to know how long it takes to construct the classifier, how long it takes to use classifier and if it is able to deal with data sets with several thousand points. If robustness is important, one tries to evaluate its capability of handling noise, missing values and irrelevant features. If the interpretability is important, one tries to find if the classifier can give some explanation on how it achieved the classification for a certain point of the data set. 4 Methods 4.1 Subjects and spirometry The objectives of the study were explained to all individuals and their written consent was obtained before inclusion in the study. The study was approved by the Medical Research Ethics Committee of the State University of Rio de Janeiro. The study involved a group of COPD patients with 25 subjects and a control group formed by 25 never smoking subjects. The group was formed basically by students and employees of the State University of Rio de Janeiro, and was composed by healthy subjects who presented normal spirometry and no history of pulmonary or cardiac disease. The patients with COPD were coming from the Ambulatory of COPD of the Service of Pneumology of our University Hospital. The patients were in stable clinical condition. COPD patients presented mild (n =8), moderate (n =9) and severe (n =8) airflow obstruction, which was evaluated using the following parameters [6,14,35]: forced Expiratory Volume in the first second (FEV1), Forced Vital Capacity (FVC), FEV1/FVC ratio and the Forced Expiratory Flow (FEF) between 25% and 75% of FVC, and FVC (FEF/FVC) ratio. These measurements were obtained for all patients in a sitting position, using a closed circuit spirometer (Vitrace VT-139; Pro-m\u00e9dico, Rio de Janeiro, Brazil), and were presented as raw data and percentile of the predicted values (% pred). 4.2 Forced oscillation technique The instrumentation used for evaluation of respiratory impedance by FOT has been described in other studies [36,37]. Briefly, a pseudorandom sinusoidal signal with 2cm H2O peak-to-peak of amplitude, containing all harmonic of 2Hz between 4 and 32Hz, was applied by a loudspeaker. The pressure input was measured with a Honeywell 176 PC pressure transducer (Microswitch, Boston, MA, USA), and the airway flows with a screen pneumothacograph coupled to a similar transducer with a matched frequency response. The signals were digitized at a rate of 1024Hz, for periods of 16s, by a personal computer, and a fast Fourier transform was computed using blocks of 4096 points with 50% overlap. To perform the FOT analysis the volunteer remained in a sitting position, keeping the head in a normal position and breathing spontaneously through a mouthpiece. During the measurements, the subjects firmly supported his/her cheeks and mouth floor using both hands, while a nose clip was worn. A minimal coherence function of 0.9 was considered adequate [6,38]. Anytime the coherence computed, (for any of the studied frequencies) was less than this threshold, the maneuver was not considered valid and the exam was repeated. Three measurements were made and the final result of the test was calculated as the mean of these three measurements. To describe the resistive component of the FOT data, an analysis of linear regression in the frequency range between 4 and 16Hz was used in order to achieve intercept resistance (R 0) and the slope of the resistive component of the impedance (S). Using the same frequency range, a parameter commonly related to airways dimensions, the mean resistance (R m) was also calculated [6,12,38]. The results associated with the reactance were interpreted using the mean reactance (X m), the resonance frequency (f r) and the dynamic compliance of the respiratory system (C rs,dyn) [6,12,38]. The C rs,dyn was estimated considering respiratory reactance at the oscillatory frequency of 4Hz (X rs4Hz) and using the equation X rs4Hz =\u22121/(2\u03c0fC rs,dyn) [6,12,38]. The same frequency was used to evaluate the absolute value of respiratory impedance (Z 4Hz), which represents the total mechanical load of the respiratory system, including resistive and elastic effects [38]. 4.3 Feature selection In order to find the appropriate set of inputs, all three feature selection methods cited in the previous section were used. The chosen filter method used the correlation coefficients as a ranking index. The analysis of the linear correlation coefficients was done calculating the matrix C of correlation coefficients. Each element of this matrix represents the correlation coefficient between two features, C(feature i , feature j ) or between the features and the output, C(feature i , Output). The procedure to find the most relevant features was started by looking for a feature that possess the highest correlation coefficient with the output, C(feature i , Output). If this is called HCCFO (Highest Correlation Coefficient Feature with Output). The next step was to eliminate features where the following relation holds: (1) | C ( H C C F O , Feature ) | > | C ( Output , H C C F O ) | > | C ( Output , Feature ) | It was done because if the relation (1) holds for a specific feature, the information it carries can be represented by the feature that has highest correlation coefficient with the output (HCCFO). The process of feature selection using the correlation coefficients was performed using the cross-validation method [33]. The available dataset was divided in a fixed number of folds. Each fold had the same number of normal and COPD measurements. One of the folders is the test set and remaining folders used as training set. The feature selection using the correlation coefficients were applied only on the training sets. There was used three search strategies (forward, backward, forward floating) in the wrapper methods, and the performance index was 1-nearest neighbor leave-one-out classification performance. The embedded method was used only in the training of the decision tree. 4.4 Search for the best classifier parameters The five classifiers (LBNC, KNN, DTREE, ANN and SVM) were implemented with a pattern recognition toolbox (prtools) for Matlab [39]. The LBNC was used with the default parameters, i.e., with no regularization. In the KNN, K was set 1, so we have the one nearest neighbor classifier. In all the other classifiers, the search for the best parameters was done with a 10-fold cross-validation using the average classification accuracy in the test folds as a performance index. In the decision process, the used parameters were the binary splitting criterion (information gain, purity, fisher criterion) and the pruning type (Quinlan pruning, no pruning or the use of a tuning set for pruning) [39]. In the ANN classifier the parameter to be search is the number of neurons in the hidden layer. On the other hand, concerning the SVM classifier with radial basis function kernel has only two parameters to be found: the regularization parameter C, that express the choice of having a large margin with more training samples wrongly classified or having a small margin with less classification errors and the parameter, and r, that is the radius of the radial basis kernel. Since these parameters are not discrete values, it was used a grid-search. Various pairs of (C, r) values were tried and the one with the best cross-validation accuracy was picked. Since doing a complete grid search may still be time consuming, Hsu [40] recommended to use a coarse grid first to find the \u201cbest region\u201d and the use a finer grid to search this region. It is important to notice that the this parameter search has to be done for each set of selected features, i.e., there is no guarantee that the same parameter setting will work for all sets of selected features. For all experiments, the features were normalized to have zero mean and unit standard deviation. This is necessary to remove scale effects caused by the use of features that has different measurement scales [41]. All the classifiers were trained and evaluated with the same training and test sets generated by a 10 fold cross-validation in available dataset. The accuracy, Se, Sp and the AUC were calculated in the 10 tests sets. Also, it was assigned to each test example in the test sets two possible outcomes: 1 meaning that the classification provided by the classifier was correct and 0, otherwise. It allowed us to apply the Cochran's Q test [42] to determine whether significant differences existed in the classification results. Besides the Cochran's test, the McNemars test [31] was applied between each pair of classifiers to find whether significant differences existed [43]. These tests were assumed to be statistically significant at p <0.05 and were implemented in Matlab 7 .4.0 using the Statistics Toolbox 6.0. 5 Results 5.1 Characteristics of the subjects The biometric and spirometric characteristics of the studied subjects are given in Table 1 . The biometric characteristics of the two studied groups were well matched, and there were not significant differences between the groups. As can be seen in Table 1, patients with COPD presented significant reductions in the spirometric parameters (p <0.0001). 5.2 Feature selection The most common selected features using correlation in the different training sets were: (f r, R 0, C rs,dyn) and (R 0, C rs,dyn). The small number of selected features shows that parameters are highly correlated. The results of the feature selection using the different search strategies (forward, backward, forward floating) using 1-nearest neighbor leave-one-out classification accuracy as performance index are shown in Table 2 . The search strategies were configured to find the number of features that gives the highest performance. 5.3 Performance of the studied classifiers using different feature selection methods 5.3.1 Experiment 1\u2014use of all features (FOT parameters) Fig. 1 shows the average ROC curve for each classifier, while Table 3 presents the average and the standard deviation of the derived parameters calculated in the 10 test folds, for all of the studied classifiers. The results presented were obtained with the best parameters found for each classifier. LBNC presented the best average Sp (1.00), KNN presented the best average Acc (0.97) and AUC (1.00). On the other hand, SVM presented the best average Se (0.97) and AUC (1.00). The application of the Cochran test has shown statistically significant difference in the classifiers, and the McNemars test applied to all pairs of classifiers indicated that there was a statistically significant difference between KNN and LBNC, and between KNN and DTREE. 5.3.2 Experiment 2\u2014forward selection search The second experiment was carried out using the selected features chosen by the forward selection search strategy (f r, X m, R 0, C rs,dyn, |Z rs|). These results are described in Fig. 2 , which shows the average ROC curve for each classifier, and Table 4 . These results were obtained with the best parameters found for each classifier. LBNC presented the best average Sp (1.00), while KNN presented the best Acc (0.95) and AUC (0.99). SVM presented the best average Acc (0.95) and Se (0.95). The application of the Cochran test has shown statistically significant difference, in the classifiers and the McNemars test applied to all pairs of classifiers indicated that there was a statistically significant difference between: KNN and DTREE. 5.3.3 Experiment 3\u2014forward floating selection The third experiment was carried out using the selected features chosen by the forward floating selection and the backward search strategies since both chose the same features (f r, X m, R 0, |Z rs|). These results are described in Fig. 3 , which shows the average ROC curve for each classifier, and Table 5 . According to the results, LBNC presents the best average Sp (1.00), KNN present the best average Acc (0.95) and AUC (0.99). On the other hand, SVC presents the best average Acc (0.95) and Se (0.94). The application of the Cochran test has not shown a statistically significant difference. 5.3.4 Experiment 4\u2014analysis of correlation coefficients The fourth experiment was carried out using the features (f r, R 0, C rs,dyn) selected by the analysis of the correlation coefficients. Theses results are presented in Fig. 4 , shows the average ROC curve for each classifier, and Table 6 . Using these features, LBNC presented the best average Sp (1.00), KNN presented the best average Acc (0.95), Se (0.93) and AUC (0.99). The application of the Cochran test has not shown a statistically significant difference between the classifier results. 5.3.5 Experiment 5\u2014correlation coefficients The fifth experiment was carried out also using the features (R 0, C rs,dyn) selected by the analysis of the correlation coefficients. Fig. 5 shows the average ROC curve for each classifier, while Table 7 shows the associated parameters. In these conditions, LBNC presented the best average Sp (1.00), KNN and ANN presented the best average Acc (0.93), KNN, DTREE and ANN presented the best average Se (0.91) and KNN presented the best AUC (0.99). The application of the Cochran test has not shown a statistically significant difference between the classifier results. 5.4 Search for the best classifier parameters Tables 8 and 9 show the best parameters for each classifier and their average accuracies. 5.5 Performance of the KNN classifiers using different feature selection methods Table 10 lists the results achieved by KNN in all of the experiments. 6 Discussion The purpose of the present study was to develop an ML system classifier that may contribute to easy the diagnostic of COPD using FOT measurements. Although previous conference papers have investigated the potential of ANN to easy the diagnostic of COPD using IOS [16,17] and FOT [20], to the authors\u2019 knowledge, this is the first study dedicated to compare the performance of several ML algorithms in the development of an automatic classifier to help the diagnostic of COPD using FOT measurements. More specifically, we investigated the performance of the LBNC, KNN, DTREE, ANN and SVM algorithms. We also performed an input feature selection in order to find the smallest number of relevant and informative features that can result in a satisfactory performance [30]. Finally, we compared the performance of classifiers in order to evaluate the most adequate method to detect COPD. It has been shown that, in general, all of the studied algorithms were able to adequately detect COPD. However, it is important to point out that some classifier will perform this work better than others. Interestingly, the feature selection allowed the reduction of the used features without a significant reduction in performance. Furthermore, ROC analysis showed that particularly three of the studied algorithms presented a great potential to contribute to the automatic detection of the respiratory effects of COPD in a clinical setting. The analysis of ROC curves is performed by plotting sensitivity versus 1-specificity for each possible cut-off level. This way, the larger the area under the curve (AUC), the more valid the diagnostic test is. This parameter has the clinically useful interpretation of representing the probability of correctly discriminating between two subjects in a randomly selected pair of abnormal and normal subjects [44,45]. According to the literature, ROC curves with AUCs between 0.50 and 0.70 indicate low diagnostic accuracy, AUCs between 0.70 and 0.90 indicate moderate accuracy, and AUCs between 0.90 and 1.00 indicate high accuracy [46,47]. Taking into consideration these values, all of the studied classifiers reached high levels of accuracy when all features were used (experiment 1, Fig. 1 and Table 2). KNN was the most adequate algorithm to correctly identify COPD (AUC=1.00), followed by SVM (AUC=1.00) and ANN (AUC=0.97). Statistical comparisons showed that KNN was significantly better than LBNC and DTREE. The results obtained using the five selected features chosen by the forward selection search strategy, described in Fig. 2 and Table 4, were coherent with that obtained using all of the seven features, showing that KNN was the most adequate algorithm to correctly identify COPD (AUC=1.00), followed by SVM (AUC=0.98) and ANN (AUC=0.96). Once gain, statistical comparisons showed a better performance of the KNN when compared with DTREE. Although we could not observe statistically significant differences among the performance of the classifiers, the KNN algorithm also presented the highest value of AUC (0.99) considering the results obtained using the four selected features chosen by the forward floating selection and the backward search strategies. These results are described in Fig. 3 and Table 7. The performance of the KNN was followed by ANN (AUC=0.97) and SVM and LBNC (AUC=0.96). These results were similar to that observed further reducing the number of used features (Fig. 4 and Table 6), which was conducted using three features selected by the analysis of the correlation coefficients. In the last experiment, conducted using only two features selected by the analysis of the correlation coefficients (Fig. 5 and Table 7), KNN was also the classifier with the highest AUC (0.99). However, in this experiment, one cannot say that a particular classifier dominates the others. Different sections of the curve are dominated by different classifiers (Fig. 5). Recommendations for research in COPD [48] include the need for improved noninvasive mechanical tests of lung function. The present study was conducted as an effort to contribute in this direction, and showed that FOT measurements, integrated with machine learning algorithms, may constitute a very promising system able to non-invasively and accurately diagnose COPD. We observed high values of AUC in all of the classifiers and features studied, and that there are statistically significant differences in the first experiment between KNN and LBNC, KNN and DTREE, and in the second experiment between KNN and DTREE. It means for all other cases one can use any of the five classifiers. However, if one looks on the average values of the performance measures (Tables 3\u20137), the classifiers that performed best in all experiments were KNN and SVM. They were followed by the ANN and then by the LBNC and DTREE. In fact, KNN was the most adequate classifier to use to correctly identify the respiratory modifications in the studied COPD patients. Although the FOT may be very useful in clinical practice, this technique has not been widely used in the medical community due to the lack of specificity, which is associated with the bias from the upper airway shunt. It is interesting to note that the use of machine learning algorithms resulted in very accurate results (Tables 3\u20137 and 10). We believe that these results may help to increase the acceptation of the FOT in the medical community. The five experiments were made using different feature selection methods. None of them provided better results than the experiment that use all features, i.e., all FOT parameters. This means that all FOT parameters are relevant. However, by analyzing the experiments that selected specific features, one can observe a rank between the FOT parameters. This can be shown in Table 10 which lists the results achieved by KNN in all the experiments. The small decrease in the performance measurements indicates that f r, R 0, C rs,dyn are the most important parameters. This agrees with the analysis of the correlation coefficients. The use of fewer parameters (f r, R 0, C rs,dyn or R 0, C rs,dyn) simplifies the analysis and still keep a high degree of accuracy. In relation to the speed, it known that KNN is very powerful and very fast to build, but it can take a long time to perform a classification if the training set is large [21]. Since in this case the dataset is small, the KNN did not take long to perform a classification. The SVM classifier presents very good results. It is very fast to train and to perform the classification. The ANN takes a long time to build a classifier due to the training procedures, but it is very fast to perform a classification. It also does not provide any explanation on how it achieved the classification. The classifiers that present more interpretable results (LBNC and DTREE) have very similar performance. The DTREE suffered from the fact that the features are highly correlated.In the author's opinion, considering the trade-off among accuracy, the time to build, to train, and the time to perform a classification, if we use all the FOT parameters, the KNN is the most appropriate choice. It allows us to achieve a high degree of accuracy and an intuitive interpretation of the classification. In this case, the exam under test is classified as normal or COPD according to the training set that is closest to it. This classifier is also a good choice if we want to use only two parameters, as described in Section 5.3.5. It is important to point out that the feature selection and associated results of this study are specific for the COPD. Other diseases will result in different changes in the respiratory system and, thus, other parameters may be better suited to the identification of the respiratory changes. Even in pure emphysema, which is a disease associated with COPD, the authors recommend that a similar study be conducted and the optimized conditions are obtained and used. 7 Conclusions In this paper, we designed and evaluated several classifiers systems and feature selection methods to develop a clinical decision support system to help the diagnostic of COPD using FOT measurements. KNN, SVM and ANN classifiers were the most adequate, reaching values that allow a very accurate clinical diagnosis. These classifiers allowed the identification of the respiratory modifications with a minimum sensitivity of 87% and a minimum specificity of 94%. The use of the analysis of correlation as a ranking index of the FOT parameters, allowed us to simplify the analysis of the FOT parameters, while still maintaining a high degree of accuracy. 8 Future plans Based on these promising results, future work includes the following goals: (1) to add to the classification system the ability of identifying the level of airflow obstruction in COPD (mild, moderate or severe); (2) to apply this methodology in the detection of early smoking-induced respiratory changes, and (3) to contribute to the diagnosis of airway obstruction in asthma. Conflict of interest None declared. Acknowledgements The authors would like to thank Josiel G. Santos for their technical assistance. The Brazilian Council for Scientific and Technological Development (CNPq) and Rio de Janeiro State Research Supporting Foundation (FAPERJ) supported this study. References [1] The global initiative for chronic obstructive lung disease. Available from: <http://www.goldcopd.com> (accessed March 2011). [2] World Health Organization. Available from: <http://www.who.int/respiratory/copd/burden/en/index.html> (accessed March 2011). [3] P.L. Enright R.M. Crapo Controversies in the use of spirometry for early recognition and diagnosis of chronic obstructive pulmonary disease in cigarette smokers Clin. Chest Med. 21 4 2000 645 652 [4] L. Ljung System Identification: Theory for the User 1987 Prentice-Hall Inc. Londres [5] A.B. Dubois A.W. Brody D.H. Lewis B.F. Burges Jr. Oscillation mechanics of lungs and chest in man J. Appl. Physiol. 8 1956 587 594 [6] A.C.D. Faria A.J. Lopes J.M. Jansen P.L. Melo Evaluating the forced oscillation technique in the detection of early smoking-induced respiratory changes Biomed. Eng. Online 25 2009 8 22 [7] A.J. Orr D.R. Westenskow A breathing circuit alarm system based on neural networks J. Clin. Monit. 10 1994 101 109 [8] P. Bright M.R. Miller J.A. Franklyn M.C. Sheppard The use of a neural network to detect upper airway obstruction caused by goiter Am. J. Respir. Crit. Care Med. 157 1998 1885 1891 [9] M.A. Leon J. R\u00e4s\u00e4nen D. Mangar Neural network-based detection of esophageal intubation Anesth. Analg. 78 1994 548 553 [10] J. R\u00e4s\u00e4nen M.A. Le\u00f3n Detection of lung injury with conventional and neural network-based analysis of continuous data J. Clin. Monit. 14 1998 433 439 [11] G. Perchiazzi M. H\u00f6gman C. Rylander R. Giuliani T. Fiore G. Hedenstierna Assessment of respiratory system mechanics by artificial neural networks: an exploratory study J. Appl. Physiol. 90 2001 1817 1824 [12] U. Unc\u00fc Evaluation of pulmonary function tests by using fuzzy logic theory J. Med. Syst. 34 3 2010 241 250 [13] A.J. Lopes D. Capone R. Mogami R.S. Lanzillotti P.L. Melo J.M. Jansen Severity classification for idiopathic pulmonary fibrosis by using fuzzy logic Clinics 66 6 2011 1015 1019 [14] A.M.G.T. Di Mango A.J. Lopes J.M. Jansen P.L. Melo Changes in respiratory mechanics with degrees of airway obstruction in COPD: detection by forced oscillation technique Respir. Med. 100 3 2006 399 410 [15] C. Ionescu E. Derom R. De Keyser Assessment of respiratory mechanical properties with constant-phase models in healthy and COPD lungs Comput. Methods Programs Biomed. 97 1 2010 78 85 [16] M. Bar\u00faa H. Nazeran P. Nava V. Granda B. Diong Classification of pulmonary diseases based on impulse oscillometric measurements of lung function using neural networks Conf. Proc. IEEE Eng. Med. Biol. Soc. 2004 3848 3851 [17] M. Bar\u00faa H. Nazeran P. Nava B. Diong M. Goldman Classification of impulse oscillometric patterns of lung function in asthmatic children using artificial neural networks Conf. Proc. IEEE Eng. Med. Biol. Soc. 2005 327 331 [18] D. Macleod M. Birch Respiratory input impedance measurements: forced oscillation methods Med. Biol. Eng. Comput. 39 2001 505 516 [19] J. Hellinckx M. Cauberghs K. De Boeck M. Demedts Evaluation of impulse oscillation system: comparison with forced oscillation technique and body plethysmography Eur. Respir. J. 18 2001 564 570 [20] J.L.M. Amaral A.C.D. Faria A.J. Lopes J.M. Jansen P.L. Melo Automatic identification of chronic obstructive pulmonary disease based on forced oscillation measurements and artificial neural networks 32nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Buenos Aires, Argentina 2010 [21] L.I. Kuncheva Combining Pattern Classifiers: Methods and Algorithms 2004 Wiley-Interscience [22] R.O. Duda P.E. Hart D.G. Stork Pattern Classification 2000 Wiley-Interscience [23] P.N. Tan M. Steinbach V. Kumar Introduction to Data Mining 2006 University of Minnesota Publisher, Addison-Wesley Copyright [24] I.H. Witten E. Frank Data Mining: Practical Machine Learning Tools and Techniques 2nd ed. 2005 Morgan Kaufmann [25] S. Haykin Neural Networks a Comprehensive Foundation 1994 Macmillan College Publishing Company Englewood Cliffs [26] V.N. Vapnik The Nature of Statistical Learning Theory 2nd ed. 2000 Springer New York [27] G.P. Zhang Neural networks for classification: a survey IEEE Trans. Syst. Man Cybern. C: Appl. Rev. 30 2000 451 462 [28] C.E. Pedreira L. Macrini M.G. Land E.S. Costa New decision support tool for treatment intensity choice in childhood acute lymphoblastic leukemia IEEE Trans. Inf. Technol. Biomed. 13 2009 284 290 [29] M.H. Goldbaum P.A. Sample K. Chan J. Williams T.-W. Lee E. Blumenthal C.A. Girkin L.M. Zangwill C. Bowd T. Sejnowski R.N. Weinreb Comparing machine learning classifiers for diagnosing glaucoma from standard automated perimetry Invest. Ophthalmol. Vis. Sci. 43 1 2002 162 169 [30] I. Guyon A. Elisseeff An introduction to variable and feature selection J. Mach. Learn. Res. 3 2003 1157 1182 [31] D.T. Dietterich Approximate statistical tests for comparing supervised classification learning algorithms Neural Comput. 10 1998 1895 1923 [32] T. Fawcett An introduction to roc analysis Pattern Recogn. Lett. 27 8 2006 861 874 [33] P. Refaeilzadeh L. Tang H. Liu Cross Validation, Encyclopedia of Database Systems 2009 Springer [34] J. Demsar Statistical comparisons of classifiers over multiple data sets J. Mach. Learn. Res. 7 2006 1 30 [35] J.V. Cavalcanti A.J. Lopes J.M. Jansen P.L. Melo Detection of changes in respiratory mechanics due to increasing degrees of airway obstruction in asthma by the forced oscillation technique Respir. Med. 100 12 2006 2207 2219 [36] P.L. Melo New impedance spectrometer for scientific and clinical studies on the respiratory system Rev. Sci. Instrum. 71 7 2000 2867 2872 [37] P.L. Melo M.M. Werneck A. Giannella-Neto Influence of the pressure generator non-linearities in the accuracy of respiratory input impedance measured by forced oscillation Med. Biol. Eng. Comput. 38 2000 102 108 [38] A.C.D. Faria A.J. Lopes J.M. Jansen P.L. Melo Assessment of respiratory mechanics in patients with sarcoidosis using forced oscillation: correlations with spirometric and volumetric measurements and diagnostic accuracy Respiration 78 1 2009 93 104 [39] R.P.W. Duin P. Juszczak P. Paclik E. Pekalska D. de Ridder D.M.J. Tax S. Verzakov PRTools4.1, A Matlab Toolbox for Pattern Recognition 2007 Delft University of Technology [40] C.W. Hsu, C.C. Chang, C.J. Lin, A practical guide to support vector classification. Available from: <www.csie.ntu.edu.tw/\u223ccjlin/papers/guide/guide.pdf> (accessed October 2010). [41] W.L. Martinez A.R. Martinez Exploratory Data Analysis with MATLAB 2005 CRC Press [42] W.J. Conover Practical Nonparametric Statistics 3rd ed. 1999 Wiley [43] S.J. Delany P. Cunningham L. Coyle An assessment of case-based reasoning for spam filtering Artif. Intell. Rev. 24 3 2005 359 378 [44] J.A. Hanley B.J. McNeil The meaning and use of the area under a receiver operating characteristic (ROC) curve Radiology 143 1982 29 36 [45] J.A. Swets R.M. Picket Evaluation of diagnostic systems: methods from signal detection theory Med. Phys. 10 2 1983 266 267 [46] J.A. Swets Measuring the accuracy of diagnostic systems Science 240 1988 1285 1293 [47] R. Golpe A. Jim\u00e9nez R. Carpizo J.M. Cifrian Utility of home oximetry as a screening test for patients with moderate and severe symptoms of obstructive sleep apnea Sleep 22 7 1999 932 937 [48] T.L. Croxton G.G. Weinmann R.M. Senior J.R. Hoidal Future research directions in chronic obstructive pulmonary disease Am. J. Respir. Crit. Care Med. 165 2002 838 844", "scopus-id": "84856228303", "pubmed-id": "22018532", "coredata": {"eid": "1-s2.0-S0169260711002562", "dc:description": "Abstract The purpose of this study is to develop a clinical decision support system based on machine learning (ML) algorithms to help the diagnostic of chronic obstructive pulmonary disease (COPD) using forced oscillation (FO) measurements. To this end, the performances of classification algorithms based on Linear Bayes Normal Classifier, K nearest neighbor (KNN), decision trees, artificial neural networks (ANN) and support vector machines (SVM) were compared in order to the search for the best classifier. Four feature selection methods were also used in order to identify a reduced set of the most relevant parameters. The available dataset consists of 7 possible input features (FO parameters) of 150 measurements made in 50 volunteers (COPD, n =25; healthy, n =25). The performance of the classifiers and reduced data sets were evaluated by the determination of sensitivity (Se), specificity (Sp) and area under the ROC curve (AUC). Among the studied classifiers, KNN, SVM and ANN classifiers were the most adequate, reaching values that allow a very accurate clinical diagnosis (Se>87%, Sp>94%, and AUC>0.95). The use of the analysis of correlation as a ranking index of the FOT parameters, allowed us to simplify the analysis of the FOT parameters, while still maintaining a high degree of accuracy. In conclusion, the results of this study indicate that the proposed classifiers may contribute to easy the diagnostic of COPD by using forced oscillation measurements.", "openArchiveArticle": "false", "prism:coverDate": "2012-03-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0169260711002562", "dc:creator": [{"@_fa": "true", "$": "Amaral, Jorge L.M."}, {"@_fa": "true", "$": "Lopes, Agnaldo J."}, {"@_fa": "true", "$": "Jansen, Jos\u00e9 M."}, {"@_fa": "true", "$": "Faria, Alvaro C.D."}, {"@_fa": "true", "$": "Melo, Pedro L."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0169260711002562"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0169260711002562"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0169-2607(11)00256-2", "prism:volume": "105", "prism:publisher": "Elsevier Ireland Ltd.", "dc:title": "Machine learning algorithms and forced oscillation measurements applied to the automatic identification of chronic obstructive pulmonary disease", "prism:copyright": "Copyright \u00a9 2011 Elsevier Ireland Ltd.", "openaccess": "1", "prism:issn": "01692607", "prism:issueIdentifier": "3", "dcterms:subject": [{"@_fa": "true", "$": "Clinical decision support"}, {"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Classification"}, {"@_fa": "true", "$": "Forced oscillation technique"}, {"@_fa": "true", "$": "Respiratory system"}, {"@_fa": "true", "$": "Chronic obstructive pulmonary disease"}], "openaccessArticle": "true", "prism:publicationName": "Computer Methods and Programs in Biomedicine", "prism:number": "3", "openaccessSponsorType": "FundingPartnerOpenArchive", "prism:pageRange": "183-193", "prism:endingPage": "193", "prism:coverDisplayDate": "March 2012", "prism:doi": "10.1016/j.cmpb.2011.09.009", "prism:startingPage": "183", "dc:identifier": "doi:10.1016/j.cmpb.2011.09.009", "openaccessSponsorName": "Brazilian Government"}, "objects": {"object": [{"@category": "thumbnail", "@height": "45", "@width": "305", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1849", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1388", "@width": "1663", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "203300", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1385", "@width": "1663", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "216738", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1389", "@width": "1663", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "211177", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1391", "@width": "1663", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "227583", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1234", "@width": "1663", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "225115", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "313", "@width": "375", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27555", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "312", "@width": "375", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "29256", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "313", "@width": "375", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28921", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "314", "@width": "375", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "29935", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "278", "@width": "375", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28049", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "196", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4810", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "197", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5105", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "196", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4820", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "196", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5232", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260711002562-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5694", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84856228303"}}