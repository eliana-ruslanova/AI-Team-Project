{"scopus-eid": "2-s2.0-85063115212", "originalText": "serial JL 280852 291210 291703 291901 291926 31 Diagnostic and Interventional Imaging DIAGNOSTICINTERVENTIONALIMAGING 2019-03-23 2019-03-23 2019-04-04 2019-04-04 2019-04-04T11:55:42 1-s2.0-S2211568419300580 S2211-5684(19)30058-0 S2211568419300580 10.1016/j.diii.2019.03.002 S300 S300.1 FULL-TEXT 1-s2.0-S2211568419X00042 2020-04-30T03:57:57.363265Z 0 0 20190401 20190430 2019 2019-03-23T04:09:23.373399Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast orcid primabst pubtype ref 2211-5684 22115684 true 100 100 4 4 Volume 100, Issue 4 7 235 242 235 242 201904 April 2019 2019-04-01 2019-04-30 2019 Original articles Computer developments article fla \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. AUTOMATICKNEEMENISCUSTEARDETECTIONORIENTATIONCLASSIFICATIONMASKRCNN COUTEAUX V Introduction Method Knee meniscus tear challenge Morphological pre-processing of images Meniscus localization and tear detection Training Ensemble aggregation Tear orientation classification Results Score and ranking Visual inspection Discussion Human and animal rights Informed consent and patient details Funding Author contributions Credit author statement Disclosure of interest References CULLEN 2009 1 25 K PACHE 2018 250 259 S LECOUVET 2018 55 64 F BONIATIS 2008 335 339 I KOSE 2009 1208 1216 C RAMAKRISHNA 2009 1308 1316 B SAYGILI 2017 1 4 A MENISCUSSEGMENTATIONTEARDETECTIONINKNEEMRIMAGESBYFUZZYCMEANSMETHOD SAYGILI 2018 1 4 A MENISCUSTEARCLASSIFICATIONUSINGHISTOGRAMORIENTEDGRADIENTSINKNEEMRIMAGES GARCIAGARCIA 2017 A AREVIEWDEEPLEARNINGTECHNIQUESAPPLIEDSEMANTICSEGMENTATIONCOMPUTERVISIONPATTERNRECOGNITION LECUN 2015 436 444 Y SIMONYAN 2014 K DEEPCONVOLUTIONALNETWORKSFORLARGESCALEIMAGERECOGNITIONARXIVPREPRINTARXIV14091556 XU 2017 1067 Y HE 2017 2980 2988 K MASKRCNN LIN 2014 740 755 T MICROSOFTCOCOCOMMONOBJECTSINCONTEXT HE 2016 770 778 K DEEPRESIDUALLEARNINGFORIMAGERECOGNITION RUSSAKOVSKY 2015 211 252 O KRIZHEVSKY 2012 25 A COUTEAUXX2019X235 COUTEAUXX2019X235X242 COUTEAUXX2019X235XV COUTEAUXX2019X235X242XV Full 2020-04-01T00:01:35Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ 2020-04-04T00:00:00Z http://www.elsevier.com/open-access/userlicense/1.0/ https://vtw.elsevier.com/content/oragreement/10178 NR_HAL publishAcceptedManuscriptIndexable http://creativecommons.org/licenses/by-nc-nd/4.0/ 2020-04-04T00:00:00Z \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. This article is made available under the Elsevier license. item S2211-5684(19)30058-0 S2211568419300580 1-s2.0-S2211568419300580 10.1016/j.diii.2019.03.002 280852 2020-04-30T03:57:57.363265Z 2019-04-01 2019-04-30 1-s2.0-S2211568419300580-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/MAIN/application/pdf/280bb94f62e22153fa12b1ede9e3d124/main.pdf main.pdf pdf true 1804777 MAIN 8 1-s2.0-S2211568419300580-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/PREVIEW/image/png/e4583697352edab44805aed88cef7294/main_1.png main_1.png png 43366 849 656 IMAGE-WEB-PDF 1 1-s2.0-S2211568419300580-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr1/THUMBNAIL/image/gif/e3f514d6c7cb4e189c43a66161fd2be8/gr1.sml gr1 gr1.sml sml 28514 163 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr2/THUMBNAIL/image/gif/aebd3ff8a1cd7789962a67d17d167004/gr2.sml gr2 gr2.sml sml 17545 79 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr3/THUMBNAIL/image/gif/f8e1070ddefb2e7525e29ac9cf2e8622/gr3.sml gr3 gr3.sml sml 20421 107 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr4/THUMBNAIL/image/gif/7290385ce105a3a59197e7c6e7faa2b8/gr4.sml gr4 gr4.sml sml 15668 59 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr5/THUMBNAIL/image/gif/33938e78f5f260655f23b20146bca0f8/gr5.sml gr5 gr5.sml sml 13077 60 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr6/THUMBNAIL/image/gif/d5fe08fdf4d363d73377f7021a019891/gr6.sml gr6 gr6.sml sml 15339 79 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr7/THUMBNAIL/image/gif/7521a4a64b3f1616469b56b1aef500e3/gr7.sml gr7 gr7.sml sml 13116 48 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr8/THUMBNAIL/image/gif/9a64b77472fe4b82b1cc05b628536ef5/gr8.sml gr8 gr8.sml sml 21457 164 206 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr9/THUMBNAIL/image/gif/abd8c57b32a1c82a34b43b1971066651/gr9.sml gr9 gr9.sml sml 27010 152 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300580-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr1/DOWNSAMPLED/image/jpeg/87afb5d4c500a62cb840b05b33b8542f/gr1.jpg gr1 gr1.jpg jpg 88089 407 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr2/DOWNSAMPLED/image/jpeg/739acf6b516ccdcb0764943481b96715/gr2.jpg gr2 gr2.jpg jpg 40067 197 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr3/DOWNSAMPLED/image/jpeg/2e3078f41fe2ca85b765d0e4374a8c3b/gr3.jpg gr3 gr3.jpg jpg 33372 175 360 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr4/DOWNSAMPLED/image/jpeg/e4fc4c0429bda8ec5cdbcbff59776507/gr4.jpg gr4 gr4.jpg jpg 44327 147 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr5/DOWNSAMPLED/image/jpeg/05279f68193338390e31c97c922a3337/gr5.jpg gr5 gr5.jpg jpg 38240 149 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr6/DOWNSAMPLED/image/jpeg/c6327620548d8bc989a55c927d05848d/gr6.jpg gr6 gr6.jpg jpg 41500 197 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr7/DOWNSAMPLED/image/jpeg/5c305a39654d2f15557cd80d435d8b90/gr7.jpg gr7 gr7.jpg jpg 31686 120 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr8/DOWNSAMPLED/image/jpeg/eca46548fc7e19e1e8081c78aa9b02c0/gr8.jpg gr8 gr8.jpg jpg 83049 437 549 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr9/DOWNSAMPLED/image/jpeg/7f93d66e89f1d699c19437c0adf2a094/gr9.jpg gr9 gr9.jpg jpg 76631 380 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300580-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr1/HIGHRES/image/jpeg/e9f35216995e9343f206656fff5aaa4c/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 291524 1080 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr2/HIGHRES/image/jpeg/eeed6640a626d31ef3b1946807439ed8/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 120050 523 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr3/HIGHRES/image/jpeg/f95d13dffacc235d7ae978808586bef6/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 91412 465 956 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr4/HIGHRES/image/jpeg/9c7f1f6cac8954dff6025f5f381420ff/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 155642 390 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr5/HIGHRES/image/jpeg/0523b02908025397a7af20cbb3aaab74/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 125969 395 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr6/HIGHRES/image/jpeg/4b52d7592b120642b1973bd8ee4cae41/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 240116 870 2421 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr7/HIGHRES/image/jpeg/285b275deccefefd2f5d38647aaef992/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 92043 318 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr8_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr8/HIGHRES/image/jpeg/50377cbf820514cdb914cefa57bfade3/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 525207 1935 2431 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-gr9_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/gr9/HIGHRES/image/jpeg/6a372095dfc5873de9ee57b99495baa9/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 509657 1683 2421 IMAGE-HIGH-RES 1-s2.0-S2211568419300580-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/STRIPIN/image/gif/4d1a0dba46f7bed9110e1368e063dcfe/si1.gif si1 si1.gif gif 1356 41 275 ALTIMG 1-s2.0-S2211568419300580-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/STRIPIN/image/gif/f9232ba2514ebbda5818a00f6994b5cb/si2.gif si2 si2.gif gif 1031 15 281 ALTIMG 1-s2.0-S2211568419300580-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300580/STRIPIN/image/gif/8020251c64599f9d9f0ee64e88370481/si3.gif si3 si3.gif gif 997 20 241 ALTIMG 1-s2.0-S2211568419300580-am.pdf am am.pdf pdf 3602730 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:100W14ZLS46/MAIN/application/pdf/439706d6d71ffe25cddc915352e00695/am.pdf DIII 1167 S2211-5684(19)30058-0 10.1016/j.diii.2019.03.002 Soci show\u00e9t show\u00e9 fran\u00e7aises de radiologie Figure 1 Database contains either medial e.g. (a) or lateral e.g. (b) MR images of the knee. (a\u2013b) MR images shows healthy menisci. (c\u2013f) MR images shows examples of tears as present in the database. (c) Horizontal tear in posterior meniscus, (d) Horizontal tear in anterior meniscus, (e) Vertical tear in posterior meniscus and (f), Vertical tear in anterior meniscus. Arrows point out tears. Figure 2 MR images illustrate challenging cases. (a) Potentially misleading lesion. (b) Barely visible meniscus. (c) Multiple tears in the same meniscus. Figure 3 Figure shows two MR images from the training database illustrating the menisci annotation process. Each colored dot is a vertex of a triangle that approximates the segmentation of a meniscus. Figure 4 (a) Image from the training database with a clearly visible torn posterior meniscus that was correctly classified by the ConvNet. (b) Same image with a superimposed saliency mask indicating that the network focuses on non-relevant regions and barely considers the posterior meniscus itself. (c) Image (a) after applying a black top-hat filter with a disk structuring element of radius 5 pixels. (d) Saliency map for the processed image. Figure 5 Pre-processing of the data used as input of the Mask R-CNN. (a) Original image. (b) 5\u00d75 white top-hat. (c) 11\u00d711 white top-hat. (d) 21\u00d721 white top-hat. Figure 6 Output of Mask R-CNN. (a\u2013b) Correct results. (c) Posterior meniscus incorrectly segmented and labeled as torn. Figure 7 Patch extraction for orientation classification. (a) Extracted patch, resized to 47\u00d747. (b) Local orientation map, \u03c3=3. (c) Local orientation map, \u03c3=1. (d) Black top-hat, r=4. (e) Black top-hat, r=8. Figure 8 Prediction results on the testing batch. Most results seem correct, e.g. (a\u2013b). However, some predictions are suspicious, e.g. (d\u2013e). (a) No tear. (b) Horizontal tear on the posterior meniscus. (d) P(Ant) \u223c 0.45 but the anterior meniscus looks torn. (e) P(F) \u223c 0 but a tear is visible in the anterior meniscus. (c) Distribution of P(F). (f) Distribution of P (H) for cases satisfying P(F) >0.5. Figure 9 Cases for which P (F) (a\u2013c) or P (H) (d\u2013f) were close to 0.5. (a) Tear on the anterior meniscus but a slice where the menisci are connected was selected which does not meet the inclusion criteria. (b) Damaged anterior meniscus, but the presence of a tear is unclear. Yet the algorithm focused on the anterior meniscus: P (Ant)>0.99. (c) Untypical lesion on the anterior meniscus. (d\u2013e) Extensively damaged meniscus. (f) Several tears in one meniscus. Original article Computer developments Automatic knee meniscus tear detection and orientation classification with Mask-RCNN V. Couteaux a b \u204e vincent.couteaux@telecom-paristech.fr S. Si-Mohamed c d O. Nempont a T. Lefevre a A. Popoff a G. Pizaine a N. Villain a I. Bloch b A. Cotten e L. Boussel c d a Philips Research France, 33, rue de Verdun, 92150 Suresnes, France Philips Research France 33, rue de Verdun Suresnes 92150 France b LTCI, T\u00e9l\u00e9com ParisTech, universit\u00e9 Paris-Saclay, 46, rue Barrault, 75013 Paris, France LTCI, T\u00e9l\u00e9com ParisTech, universit\u00e9 Paris-Saclay 46, rue Barrault Paris 75013 France c Inserm U1206, INSA-Lyon, Claude-Bernard-Lyon 1 University, CREATIS, CNRS UMR 5220, 69100 Villeurbanne, France Inserm U1206, INSA-Lyon, Claude-Bernard-Lyon 1 University, CREATIS, CNRS UMR 5220 Villeurbanne 69100 France d Department of Radiology, hospices civils de Lyon, 69002 Lyon, France Department of Radiology, hospices civils de Lyon Lyon 69002 France e Department of Musculoskeletal Radiology, CHRU de Lille, 59000 Lille, France Department of Musculoskeletal Radiology, CHRU de Lille Lille 59000 France \u204e Corresponding author. Philips Research France, 33, rue de Verdun, 92150 Suresnes, France. Philips Research France 33, rue de Verdun Suresnes 92150 France Abstract Purpose This work presents our contribution to a data challenge organized by the French Radiology Society during the Journ\u00e9es Francophones de Radiologie in October 2018. This challenge consisted in classifying MR images of the knee with respect to the presence of tears in the knee menisci, on meniscal tear location, and meniscal tear orientation. Materials and methods We trained a mask region-based convolutional neural network (R-CNN) to explicitly localize normal and torn menisci, made it more robust with ensemble aggregation, and cascaded it into a shallow ConvNet to classify the orientation of the tear. Results Our approach predicted accurately tears in the database provided for the challenge. This strategy yielded a weighted AUC score of 0.906 for all three tasks, ranking first in this challenge. Conclusion The extension of the database or the use of 3D data could contribute to further improve the performances especially for non-typical cases of extensively damaged menisci or multiple tears. Keywords Knee meniscus Artificial intelligence Mask region-based convolutional neural network (R-CNN) Meniscal tear detection Orientation classification Introduction Meniscal lesions are a frequent and common cause of knee pain, responsible for approximately 700,000 arthroscopic partial meniscectomies per year in the United States [1]. They are defined as a tear within the meniscus, and can lead to articular cartilage degeneration over time, further necessitating surgical treatment. Magnetic resonance imaging (MRI) plays a central role in the diagnosis of meniscus lesions, the preoperative planning and the postoperative rehabilitation of the patient [2,3]. As meniscal lesions are very frequent, their diagnosis could certainly benefit from a quantitative and automated solution giving more accurate results in a faster way. Computer-aided detection systems for meniscal tears were thus proposed whereby regions of interest in the image are extracted and classified based on handcrafted image features [4\u20138]. The Journ\u00e9es Francophones de Radiologie was held in Paris in October 2018. For the first time, the French Society of Radiology organized an artificial intelligence (AI) competition involving teams of industrial researchers, students and radiologists. This paper presents our contribution to the knee meniscus tear challenge, where participants had to classify sagittal MRI slices cropped around the knee depending on the presence of tears in anterior and posterior menisci and on their orientation (horizontal or vertical). We proposed a method that takes advantage of recent advances in deep learning [9,10]. More precisely, we propose to localize, segment, and classify healthy and torn menisci using a mask region-based convolutional neural network (R-CNN) approach that is cascaded into a shallow ConvNet to classify tear orientation. Method Knee meniscus tear challenge Sagittal MR images centered around the knee were provided with the following annotations: \u2022 position of the image (medial or lateral); \u2022 presence of a tear in the posterior meniscus; \u2022 presence of a tear in the anterior meniscus; \u2022 orientation of the tear in the posterior meniscus (if any); \u2022 orientation of the tear in the anterior meniscus (if any). Two training batches were provided; the first made of 257 images was shared one month before the conference and the other, made of 871 images, 2 days before the end of the challenge. The first batch contained 55/257 (21.4%) images with horizontal posterior tears, 46/257 (17.9%) with vertical posterior tears, 13/257 (5.1%) with horizontal anterior tears and 8/257 (3.1%) with vertical anterior tears. The second batch contained 107/871 (12.3%) images with horizontal posterior tears, 60/871 (6.9%) with vertical posterior tears, 8/871 (0.9%) with horizontal anterior tears and 3/871 (0.3%) with vertical anterior tears. The classes were imbalanced, with horizontal tears and posterior meniscus tears being more frequent, and a low number of anterior tears were available for training. We reviewed the database and removed any ambiguous annotations from the training set. Images of size 256\u00d7256, either of the medial or of the lateral plane of the knee, were provided, as illustrated in Fig. 1 a, b. The femur was always on the left and the tibia on the right, with the anterior meniscus at the top and the posterior meniscus at the bottom of the image. Horizontal tears appeared vertical and vice versa. The grey level scale was in an arbitrary unit scaled between 0 and 1, and the images did not have consistent brightness and contrast. On MRI, meniscal tears appear as thin, hyperintense lines that cut across the menisci, but we can observe various hyper-intense signals in the menisci (Fig. 1). For instance, hyper-intense lines that only partially cut across the menisci should not be classified as tears (Fig. 2 a). Moreover, the menisci may be barely visible (Fig. 2b) or have multiple tears (Fig. 2c). In the latter case, the provided orientation may be ill-defined. Menisci are small structures and tears present as thin abnormalities within menisci on MRI. To facilitate tear detection, we first localized the menisci. However, the localization of the menisci was not part of the provided annotations. To efficiently perform this annotation, we chose to approximate menisci by triangles resulting in a coarse segmentation of menisci (Fig. 3 ). To detect tears in both menisci and identify their orientation, we opted for a cascaded approach. First, menisci were localized and tears were identified. Then the orientation of torn menisci was classified. For both tasks, we applied a morphological pre-processing, as described below, to enhance the relevant structures in the image. Morphological pre-processing of images In a first attempt to better understand the dataset and the classification task at hand, we trained simple neural network classifiers on the training dataset in order to classify the posterior (resp. anterior) meniscus into healthy and torn cases. Both networks were based on a simplified VGG-like architecture: the whole image is taken as the input, on which four 2D-convolution/ReLu/Maxpool layers are applied followed by two dense layers and a final softmax-activated output layer [11]. Note that these neural networks were only meant to explore the given dataset. An accuracy of 83% could readily be obtained (precision, 0.76; recall, 0.8). However, when analyzing the interpretation of the ConvNet classification, it appeared that the network used non-relevant features in the image to provide the result (Fig. 4 ). This phenomenon was consistently observed on other images in the dataset and is probably attributed to the variability of the images. Given that the dataset is small, the network is not able to properly generalize on so few samples. This prompted us to consider pre-processing the images in order to bring robustness to the classification. The approach we propose is close to that for detecting white matter hyper-intensities in brain MRI [12]. Since meniscus tears are primarily characterized by distinct morphological features, it seems reasonable to assume that any image pre-processing step that would retain these characteristics while limiting the influence of other structures may be beneficial. Fig. 4c shows the result of applying a black top-hat morphological filter on the image of Fig. 4a. A black top-hat filter outputs an image wherein the bright regions correspond to regions in the original image which are smaller than the structuring element and darker than their surroundings. As can be clearly observed, the torn posterior meniscus can still be identified. A ConvNet classifier trained on black top-hat filtered images shows performance similar to the initial one (accuracy, 83%; precision, 86%; recall, 67%) but is able to focus precisely on the meniscus region (Fig. 4d). In this case, the saliency map clearly indicates that only the meniscus region is relevant for the classification. Meniscus localization and tear detection To localize both menisci and identify tears in each meniscus, we used the Mask R-CNN framework, a state-of-the-art approach for Instance Segmentation. It performs object detection, segmentation and classification in a single forward pass [13]. We trained the model to detect and segment four objects: \u2022 healthy anterior meniscus; \u2022 torn anterior meniscus; \u2022 healthy posterior meniscus; \u2022 torn posterior meniscus. In this way, we obtained the localization of each meniscus, the classification of healthy vs. torn, and a classification score. We chose to perform the classification of tear orientation independently on the segmented meniscus region only, as explained below because the classes would have been too imbalanced otherwise (only 11 vertical tears in the anterior meniscus for instance). We used a Mask R-CNN model pre-trained on the common object in context (COCO) dataset [14] whose input is a three channel image. We applied three white top-hat filters (the dual of the black top-hat filters described above) on original MRI slices with square structuring elements of size 5\u00d75, 11\u00d711 and 21\u00d721 (Fig. 5 ) to generate network inputs. Note that we did not constrain the model to return exactly one result for each meniscus because the two menisci were correctly detected in almost all cases. We illustrate in Fig. 6 the output of the Mask R-CNN. In Fig. 6a, the two healthy menisci are properly detected. In Fig. 6b, the posterior meniscus is appropriately identified as torn. However, the posterior meniscus is too widely segmented and incorrectly labeled as torn (Fig. 6). Training We fine-tuned a Mask R-CNN with a ResNet-101 backbone, pretrained on COCO dataset) [13\u201315]. The training was done using an Adam optimizer, 1.10 \u22123 learning rate and batches of 8 images, during 1000 epochs of 100 batches. Ensemble aggregation To improve the robustness of our model, we applied ensemble aggregation. We trained five models on random folds of the full training data set (1128 images) and retained five additional models trained on random folds of the first training batch only (the first 257 images). We aggregated the results differently for anterior and posterior menisci. We classified the anterior meniscus as torn when at least one network had detected a torn anterior meniscus, with a probability Pant(F) equal to the mean classification score of all detected torn anterior menisci by the ensemble. We classified the posterior meniscus as torn when the strict majority of the networks had detected a torn posterior meniscus. The probability Ppost(F) is equal to the mean classification score of all detected torn posterior menisci by the ensemble. We used different aggregation methods as a large majority of anterior menisci are healthy. Some networks may not have seen enough torn anterior menisci in order to recognize them. Tear orientation classification To classify the orientation of torn menisci as horizontal or vertical, we trained a neural network on images cropped to the bounding boxes of detected torn menisci, resized to 47\u00d747 pixels. This network was fed with pre-processed patches, each input having five channels illustrated in Fig. 7 : \u2022 unprocessed patch; \u2022 local orientation map, computed with \u03c3=3 (see below); \u2022 local orientation map, computed with \u03c3=1; \u2022 black top-hat transform, with a disk structuring element of radius 4 pixels; \u2022 black top-hat transform, with a disk structuring element of radius 8 pixels. The local orientation map represents the angle of the smallest eigenvector of the Hessian matrix at each pixel. The Hessian matrix was computed with the second derivative of a Gaussian kernel, whose standard deviation \u03c3 is a parameter. Only 300 torn menisci were provided for training. Therefore, we trained a very shallow CNN based on a VGG-like architecture: \u2022 Convolution, 3\u00d73 kernel, 8 filters, ReLU activation; \u2022 Max-pooling, 2\u00d72; \u2022 Convolution, 3\u00d73 kernel, 16 filters, ReLU activation; \u2022 Max-pooling, 2\u00d72; \u2022 Convolution, 3\u00d73 kernel, 32 filters, ReLU activation; \u2022 Max-pooling, 2\u00d72; \u2022 Dense Layer with 1024 units, ReLU activation, P =0.5 dropout; \u2022 Dense Layer with 1024 units, ReLU activation, P =0.5 dropout; \u2022 Dense Layer with 2 units and a softmax activation. We trained this network on 246 torn menisci of the training database with a Stochastic Gradient Descent, 1.10 \u22123 learning rate and batches of 32 images, during 800 epochs (approximately 5min). We validated the method on the remaining 54 cases and selected the model with the highest validation accuracy. Results Score and ranking Teams were ranked according to a weighted average of the area under the ROC curves (AUC) of the tear detection task Det (tear in any meniscus), the tear localization task Loc (anterior or posterior) and the orientation classification task Or (horizontal or vertical), according to Eq. 1 (E1): (E1) S c o r e = 0.4 \u00d7 A U C D e t + 0.3 \u00d7 A U C L o c + 0.3 \u00d7 A U C O r The organizers therefore removed from the database cases where both menisci had tears and the following values were submitted for each image: \u2022 Probability of a tear in any meniscus P(F); \u2022 Probability that the tear (if any) is in the anterior meniscus P(Ant); \u2022 Probability that the tear (if any) is horizontal P(H). The Mask R-CNN ensemble outputs a probability P ant (F) that the anterior meniscus is torn, and a probability P post (F) that the posterior meniscus is torn, both being independent a priori. This results in Eq. 2 (E2) (E2) P F = P p o s t F + P a n t F \u2212 P p o s t F P a n t F where P(Ant) is defined by Equation 3 (E3) (E3) P A n t = P a n t F / P a n t F + P p o s t F To obtain P(H), we applied the orientation classifier on the anterior meniscus when P(Ant)> 0.5 and on the posterior meniscus otherwise. A test set of 700 images was used for ranking. We obtained a score of 0.906 and shared the first place with another team (score 0.903). Visual inspection In most cases, the prediction was in line with our interpretation as illustrated in Fig. 8 , but a few cases seemed suspicious. The resulting classification scores were almost binary, either very close to 1 or very close to 0, especially P(F). However, for some images, the predictor returned classification scores close to 0.5 (Fig. 9 ). Discussion The knee meniscus tear challenge posed an image classification problem. Image classification tasks in computer vision aim to discriminate many classes from the prominent object in the image [16]. However, in this problem, the classification result should be based on thin details at a specific location in the image. Moreover, only a small database was available. Training a standard classifier from the image may therefore result in sub-optimal performances as observed in our initial experiments. We chose to localize and segment the menisci and perform the classification within the anterior and posterior menisci. We opted for a Mask R-CNN approach as it can perform both tasks jointly [13]. Due to the class imbalance, we did not classify the tear orientation using this approach, but cascaded the Mask R-CNN into a shallow ConvNet to classify tear orientation [17]. Moreover, we provided pre-processed images to the networks to focus on relevant parts of the images by enhancing the tears. In conclusion, our approach ranked first in the challenge by predicting accurately tears in the database provided for the challenge. The extension of the database or the use of 3D data could contribute to further improve the performances especially on untypical cases such as very damaged menisci or multiple tears. Human and animal rights The authors declare that the work described has been carried out in accordance with the Declaration of Helsinki of the World Medical Association revised in 2013 for experiments involving humans as well as in accordance with the EU Directive 2010/63/EU for animal experiments. Informed consent and patient details The authors declare that this report does not contain any personal information that could lead to the identification of the patient(s). The authors declare that they obtained a written informed consent from the patients and/or volunteers included in the article. The authors also confirm that the personal details of the patients and/or volunteers have been removed. Funding This work has been partially funded by the ANRT (Association nationale de la recherche et de la technologie) Author contributions All authors attest that they meet the current International Committee of Medical Journal Editors (ICMJE) criteria for Authorship. Credit author statement Vincent Couteaux: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Salim Si-Mohamed: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Olivier Nempont: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Thierry Lefevre: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Alexandre Popoff: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Guillaume Pizaine: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Nicolas Villain: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Isabelle Bloch: conceptualization and design; data curation; writing \u2014 original draft preparation; review & editing. Anne Cotten: conceptualization and design; data curation; resources; review & editing. Lo\u00efc Boussel: conceptualization and design; supervision; writing \u2014 original draft preparation; review & editing. Disclosure of interest The authors declare that they have no competing interest. References [1] K.A. Cullen M.J. Hall A. Golosinskiy Ambulatory surgery in the United States, 2006 Natl Health Stat Rep 11 2009 1 25 [2] S. Pache Z.S. Aman M. Kennedy G.Y. Nakama G. Moatshe C. Ziegler Meniscal root tears: current concepts review Bone Joint Surg. 6 2018 250 259 [3] F. Lecouvet T. Van Haver S. Acid V. Perlepe T. Kirchgesner B. Vande Berg Magnetic resonance imaging (MRI) of the knee: Identification of difficult-to-diagnose meniscal lesions Diagn Interv Imaging 99 2018 55 64 [4] I. Boniatis G.S. Panayiotakis E. Panagiotopoulos A computer-based system for the discrimination between normal and degenerated menisci from magnetic resonance images Int Workshop Imaging Syst Techn IEEE 2008 335 339 [5] C. K\u00f6se O. Gen\u00e7alio\u011flu U.U. \u015eevik An automatic diagnosis method for the knee meniscus tears in MR images Expert Syst Appl 36 2009 1208 1216 [6] B. Ramakrishna W. Liu G. Saiprasad N. Safdar C.I. Chang K. Siddiqui An automatic computer-aided detection system for meniscal tears on magnetic resonance images IEEE Trans Med Imaging 28 2009 1308 1316 [7] A. Saygili S. Albayrak Meniscus segmentation and tear detection in the knee MR images by fuzzy c-means method 2017 Signal Processing and Communications Applications Conference (SIU) 1 4 [8] A. Saygili S. Albayrak Meniscus tear classification using histogram of oriented gradients in knee MR images 2018 Signal Processing and Communications Applications Conference (SIU) 1 4 [9] A. Garcia-Garcia S. Orts S. Oprea V. Villena-Martinez J.G. Rodr\u00edguez A review on deep learning techniques applied to semantic segmentation. Computer Vision and Pattern Recognition 2017 Cornell University [10] Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 2015 436 444 [11] K. Simonyan A. Zisserman Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv :1409.1556 2014 [12] Y. Xu T. G\u00e9raud E. Puybareau I. Bloch J. Chazalon White matter hyperintensities segmentation in a few seconds using fully convolutional network and transfer learning, brain lesion: glioma, multiple sclerosis, stroke and traumatic brain injuries Lect Notes Comp Sci 2017 1067 [13] K. He G. Gkioxari P. Doll\u00e1r R. Girshick Mask R-CNN 2017 International Conference on Computer Vision (ICCV). IEEE 2980 2988 [14] T.Y. Lin M. Maire S.J. Belongie L.D. Bourdev R.B. Girshick J. Hays Microsoft COCO: Common objects in context 2014 European Conference on Computer Vision (ECCV) 740 755 [15] K. He X. Zhang S. Ren J. Sun Deep residual learning for image recognition Conference on Computer Vision and Pattern Recognition (CVPR). IEEE 2016 770 778 [16] O. Russakovsky J. Deng H. Su J. Krause S. Satheesh S. Ma Imagenet large scale visual recognition challenge Int Comput Vision 115 2015 211 252 [17] A. Krizhevsky I. Sutskever G.E. Hinton Imagenet classification with deep convolutional neural networks Adv Neural Info Proc Syst 2012 25", "scopus-id": "85063115212", "pubmed-id": "30910620", "coredata": {"eid": "1-s2.0-S2211568419300580", "dc:description": "Abstract Purpose This work presents our contribution to a data challenge organized by the French Radiology Society during the Journ\u00e9es Francophones de Radiologie in October 2018. This challenge consisted in classifying MR images of the knee with respect to the presence of tears in the knee menisci, on meniscal tear location, and meniscal tear orientation. Materials and methods We trained a mask region-based convolutional neural network (R-CNN) to explicitly localize normal and torn menisci, made it more robust with ensemble aggregation, and cascaded it into a shallow ConvNet to classify the orientation of the tear. Results Our approach predicted accurately tears in the database provided for the challenge. This strategy yielded a weighted AUC score of 0.906 for all three tasks, ranking first in this challenge. Conclusion The extension of the database or the use of 3D data could contribute to further improve the performances especially for non-typical cases of extensively damaged menisci or multiple tears.", "openArchiveArticle": "true", "prism:coverDate": "2019-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S2211568419300580", "dc:creator": [{"@_fa": "true", "$": "Couteaux, V."}, {"@_fa": "true", "$": "Si-Mohamed, S."}, {"@_fa": "true", "$": "Nempont, O."}, {"@_fa": "true", "$": "Lefevre, T."}, {"@_fa": "true", "$": "Popoff, A."}, {"@_fa": "true", "$": "Pizaine, G."}, {"@_fa": "true", "$": "Villain, N."}, {"@_fa": "true", "$": "Bloch, I."}, {"@_fa": "true", "$": "Cotten, A."}, {"@_fa": "true", "$": "Boussel, L."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S2211568419300580"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S2211568419300580"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S2211-5684(19)30058-0", "prism:volume": "100", "prism:publisher": "Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "dc:title": "Automatic knee meniscus tear detection and orientation classification with Mask-RCNN", "prism:copyright": "\u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "openaccess": "1", "prism:issn": "22115684", "prism:issueIdentifier": "4", "dcterms:subject": [{"@_fa": "true", "$": "Knee meniscus"}, {"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Mask region-based convolutional neural network (R-CNN)"}, {"@_fa": "true", "$": "Meniscal tear detection"}, {"@_fa": "true", "$": "Orientation classification"}], "openaccessArticle": "true", "prism:publicationName": "Diagnostic and Interventional Imaging", "prism:number": "4", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "235-242", "prism:endingPage": "242", "pubType": "Original article Computer developments", "prism:coverDisplayDate": "April 2019", "prism:doi": "10.1016/j.diii.2019.03.002", "prism:startingPage": "235", "dc:identifier": "doi:10.1016/j.diii.2019.03.002", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "163", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "28514", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "79", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "17545", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "107", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "20421", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "59", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "15668", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "60", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13077", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "79", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "15339", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "48", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13116", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "206", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "21457", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "152", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "27010", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "407", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "88089", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "197", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "40067", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "175", "@width": "360", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33372", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "147", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "44327", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "149", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "38240", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "197", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "41500", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "120", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31686", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "437", "@width": "549", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "83049", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "380", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "76631", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1080", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "291524", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "523", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "120050", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "465", "@width": "956", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "91412", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "390", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "155642", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "395", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "125969", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "870", "@width": "2421", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "240116", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "318", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "92043", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1935", "@width": "2431", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr8_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "525207", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1683", "@width": "2421", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-gr9_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "509657", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "41", "@width": "275", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1356", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "281", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1031", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "241", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "997", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300580-am.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "AAM-PDF", "@size": "3602730", "@ref": "am", "@mimetype": "application/pdf"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85063115212"}}