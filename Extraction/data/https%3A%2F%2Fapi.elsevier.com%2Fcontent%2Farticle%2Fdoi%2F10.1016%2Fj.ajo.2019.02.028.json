{"scopus-eid": "2-s2.0-85066264736", "originalText": "serial JL 271967 291210 291684 31 90 American Journal of Ophthalmology AMERICANJOURNALOPHTHALMOLOGY 2019-03-06 2019-03-06 2019-05-29 2019-05-29 2019-07-02T00:43:47 1-s2.0-S000293941930087X S0002-9394(19)30087-X S000293941930087X 10.1016/j.ajo.2019.02.028 S300 S300.1 FULL-TEXT 1-s2.0-S0002939419X00057 2019-08-23T02:06:14.108588Z 0 0 20190701 20190731 2019 2019-03-06T02:23:33.344975Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor grantsponsorid primabst pubtype ref 0002-9394 00029394 UNLIMITED NONE true 203 203 C Volume 203 6 37 45 37 45 201907 July 2019 2019-07-01 2019-07-31 2019 Original Articles article fla \u00a9 2019 The Authors. Published by Elsevier Inc. ADEEPLEARNINGSYSTEMFORAUTOMATEDANGLECLOSUREDETECTIONINANTERIORSEGMENTOPTICALCOHERENCETOMOGRAPHYIMAGES FU H Methods AS-OCT Dataset Quantitative Feature-Based System Deep Learning System Gonioscopy and Clinician Grading of AS-OCT Images Evaluation and Statistical Analysis Interpretation of Deep Learning Features Results Discussion References QUIGLEY 2006 262 267 H FOSTERPAULANDJOHNSON 2001 1277 1282 G THAM 2014 2081 2090 Y ANG 2018 132 156 M THOMAS 2007 2362 2363 R NONGPIUR 2013 48 54 M NONGPIUR 2017 252 258 M WONG 2009 256 260 H SAKATA 2008 181 185 L FU 2016 1288 1291 H XU 2013 7380 7383 Y FU 2018 H MEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTIONMICCAI MULTICONTEXTDEEPNETWORKFORANGLECLOSUREGLAUCOMASCREENINGINANTERIORSEGMENTOCT XU 2012 3167 3170 Y LECUN 2015 436 444 Y SCHMIDHUBER 2015 85 117 J BEJNORDI 2017 2199 2210 B ESTEVA 2017 115 118 A KERMANY 2018 1122 1131 D POPLIN 2018 158 164 R GULSHAN 2016 649 656 V TING 2017 2211 2223 D BURLINA 2017 1170 1176 P GRASSMANN 2018 1 11 F LONG 2017 0024 E ASAOKA 2016 1974 1980 R LI 2018 1199 1206 Z FU 2018 1597 1605 H FU 2018 2493 2501 H FU 2017 1930 1938 H TAN 2012 39 46 G NONGPIUR 2010 1967 1973 M WU 2011 569 574 R NARAYANASWAMY 2010 1321 1327 A CHANG 2011 1 27 C SIMONYAN 2015 K DEEPCONVOLUTIONALNETWORKSFORLARGESCALEIMAGERECOGNITIONPRESENTEDINTERNATIONALCONFERENCELEARNINGREPRESENTATIONS PAN 2010 1345 1359 S RUSSAKOVSKY 2015 211 252 O KRIZHEVSKY 2017 84 90 A SPAETH 1971 709 739 G SCHEIE 1957 510 512 H VANDERMAATEN 2008 2579 2605 L CONSOLE 2008 1612 1616 J NINI 2014 942367 S WOO 1999 43 47 E FUX2019X37 FUX2019X37X45 FUX2019X37XH FUX2019X37X45XH Full 2019-03-06T14:45:40Z Author http://creativecommons.org/licenses/by-nc-nd/4.0/ 2020-05-29T00:00:00.000Z 2020-05-29T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license. \u00a9 2019 The Authors. Published by Elsevier Inc. item S0002-9394(19)30087-X S000293941930087X 1-s2.0-S000293941930087X 10.1016/j.ajo.2019.02.028 271967 2019-08-23T02:06:14.108588Z 2019-07-01 2019-07-31 UNLIMITED NONE 1-s2.0-S000293941930087X-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/MAIN/application/pdf/493bf3a94027f7f1fe677b2da5b72acc/main.pdf main.pdf pdf true 1804851 MAIN 9 1-s2.0-S000293941930087X-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/PREVIEW/image/png/392946e57282d7089b286626dc628db6/main_1.png main_1.png png 91440 849 656 IMAGE-WEB-PDF 1 1-s2.0-S000293941930087X-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr3/THUMBNAIL/image/gif/54783efe78f1a44a29752a948410fc82/gr3.sml gr3 gr3.sml sml 14486 163 201 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr1/THUMBNAIL/image/gif/222fddb1381e1520fa27d31377310e00/gr1.sml gr1 gr1.sml sml 16181 92 219 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr6/THUMBNAIL/image/gif/d07f782bb8d410da355901eb5a66ed35/gr6.sml gr6 gr6.sml sml 14031 125 219 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr4/THUMBNAIL/image/gif/eef0c18e28df6c692238a1e3936945f5/gr4.sml gr4 gr4.sml sml 19522 127 219 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr2/THUMBNAIL/image/gif/9b848661a7d30dda4a277c1ba7773579/gr2.sml gr2 gr2.sml sml 14374 65 219 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr5/THUMBNAIL/image/gif/6d0dbbcc1397fcfd4a20afa383ea17c3/gr5.sml gr5 gr5.sml sml 13360 82 219 IMAGE-THUMBNAIL 1-s2.0-S000293941930087X-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr3/DOWNSAMPLED/image/jpeg/724bdd9c80098cba474347c669a4569d/gr3.jpg gr3 gr3.jpg jpg 39865 276 339 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr1/DOWNSAMPLED/image/jpeg/30a9e5c55a0d74c35c84e605b022c9b0/gr1.jpg gr1 gr1.jpg jpg 91684 310 734 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr6/DOWNSAMPLED/image/jpeg/891c9300f7a67fe881750a67e70f8c88/gr6.jpg gr6 gr6.jpg jpg 50763 332 583 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr4/DOWNSAMPLED/image/jpeg/d3c7532f87faf063c55970e65fea0166/gr4.jpg gr4 gr4.jpg jpg 91406 421 728 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr2/DOWNSAMPLED/image/jpeg/b18f1023462d698fe1962f9f37d3e242/gr2.jpg gr2 gr2.jpg jpg 82391 219 734 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr5/DOWNSAMPLED/image/jpeg/08d1d0bff4a4adf1f0e007df24566993/gr5.jpg gr5 gr5.jpg jpg 69145 220 583 IMAGE-DOWNSAMPLED 1-s2.0-S000293941930087X-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr3/HIGHRES/image/jpeg/a207e8f51766504787e37b7f629fd0e1/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 97139 732 900 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr1/HIGHRES/image/jpeg/f382263b1db4c4d6723ed98140d79cac/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 643489 1370 3249 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr6/HIGHRES/image/jpeg/ffdc00095bc8342579c8abe5428c2d26/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 366469 1472 2583 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr4/HIGHRES/image/jpeg/695cba7018e3c5d22b1060c5bbdcc74a/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 414029 1118 1935 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr2/HIGHRES/image/jpeg/f4a853e7d540c874ce90d7b6ad747083/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 616272 972 3251 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S000293941930087X/gr5/HIGHRES/image/jpeg/3b3e7ad1914b1a0df1680078a95d9c0b/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 538485 973 2584 IMAGE-HIGH-RES 1-s2.0-S000293941930087X-am.pdf am am.pdf pdf 4805625 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10ZLR5V3DTX/MAIN/application/pdf/076cf6b1765466d59b915deb4fb34dae/am.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/egi:10ZLR5V3DTX/MAIN/application/pdf/076cf6b1765466d59b915deb4fb34dae/am.pdf AJOPHT 10861 S0002-9394(19)30087-X 10.1016/j.ajo.2019.02.028 The Authors Figure 1 Overview of 3 types of automated angle closure detection systems. (A) Quantitative feature-based method, where clinical quantitative features were extracted based on segmentation results and fed into a support vector machine classifier. (B) Deep learning method, where convolutional neural layers were used to extract deep features and fully connected layers were used to produce the detection result. The convolutional layer parameters were denoted as the \u201cnumber of channels @ receptive field size.\u201d Figure 2 Illustration of clinical quantitative parameters. AC area = anterior chamber area; ACD = anterior chamber depth; ACV = anterior chamber volume; ACW = anterior chamber width; AOD = angle opening distance; ARA = angle recess area; IA = iris area; IC = iris curvature; IT = iris thickness; LV = lens vault; PD = pupil diameter; TIA = trabecular iris angle area; SS = scleral spur. Figure 3 The average receiver operating characteristic curves (AUC) derived from 5-fold cross-validation on the anterior segment optical coherence tomography data set against clinicians' grading of anterior segment optical coherence tomography images as the reference standard. The dotted line represented the tradeoff that would result from random chance. The points shown on each curve indicated the minimal tradeoff between sensitivity and specificity. Figure 4 Feature visualization for anterior chamber angle images. (A) The input anterior segment optical coherence tomography image. (B) Attention map of deep learning features. Figure 5 A t-distributed stochastic neighbor embedding (t-SNE) visualization of the validation data set within the feature representation. Red and blue dots denote the angle-closure and open angle anterior chamber angle images, respectively. (A) The t-SNE visualization of quantitative features. (B) The t-SNE visualization of deep learning features. Figure 6 The typical false-negative cases from the deep learning system. (A) Angle-closure with a poorly delineated scleral spur. (B) Angle-closure with shadow interference (yellow arow). Table Summary Statistics of Data n Mean (95% CI) Median \u00b1 SD Refractive error 3896 0.23 (0.148\u20130.311) 0.75 \u00b1 2.60 Axial length 4025 23.89 (23.85\u201323.93) 23.66 \u00b1 1.325 CCT 3238 0.447 (0.446\u20130.448) 0.441 \u00b1 0.037 CDR 3812 0.42 (0.41\u20130.42) 0.40 \u00b1 0.11 Intraocular pressure 4082 14.90 (14.83\u201314.98) 15 \u00b1 2.39 CCT = central corneal thickness; CDR = cup-to-disc ratio; CI = confidence interval; SD = standard deviation. Original article A Deep Learning System for Automated Angle-Closure Detection in Anterior Segment Optical Coherence Tomography Images Huazhu Fu a b c \u2217 huazhufu@gmail.com Mani Baskaran d e Yanwu Xu c f \u2217\u2217 Stephen Lin g Damon Wing Kee Wong d j Jiang Liu a h Tin A. Tun d Meenakshi Mahesh d Shamira A. Perera d e Tin Aung d e i a Cixi Institute of Biomedical Engineering, Ningbo Institute of Industrial Technology, Chinese Academy of Sciences, Zhejiang, China Cixi Institute of Biomedical Engineering Ningbo Institute of Industrial Technology Chinese Academy of Sciences Zhejiang China Cixi Institute of Biomedical Engineering, Ningbo Institute of Industrial Technology, Chinese Academy of Sciences, Zhejiang, China b Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates Inception Institute of Artificial Intelligence Abu Dhabi United Arab Emirates Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates (H.F.) c Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore Institute for Infocomm Research Agency for Science, Technology and Research Singapore Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore (H.F., Y.X., D.W.K.W., J.L.) d Singapore Eye Research Institute, Singapore National Eye Center, Singapore Singapore Eye Research Institute Singapore National Eye Center Singapore Singapore Eye Research Institute, Singapore National Eye Center, Singapore (M.B., D.W.K.W., T.A.T., M.M., S.A.P., T.A.) e EYE-ACP, Duke-NUS Medical School, Singapore EYE-ACP Duke-NUS Medical School Singapore EYE-ACP, Duke-NUS Medical School, Singapore f Department of Artificial Intelligence Innovation Business, Baidu Inc., Beijing, China Department of Artificial Intelligence Innovation Business Baidu Inc. Beijing China Department of Artificial Intelligence Innovation Business, Baidu Inc., Beijing, China g Microsoft Research, Beijing, China Microsoft Research Beijing China Microsoft Research, Beijing, China h Department of Computer Science and Engineering, Southern University of Science and Technology, Guangzhou, China Department of Computer Science and Engineering Southern University of Science and Technology Guangzhou China Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, China i Yong Loo Lin School of Medicine, National University of Singapore, Singapore Yong Loo Lin School of Medicine National University of Singapore Singapore Yong Loo Lin School of Medicine, National University of Singapore, Singapore j Nanyang Technological University, Singapore Nanyang Technological University Singapore Nanyang Technological University, Singapore \u2217 Inquiries to: Huazhu Fu, Al Khatem Building, Abu Dhabi Global Market, Al Maryah Island, Abu Dhabi, United Arab Emirates Al Khatem Building Abu Dhabi Global Market Al Maryah Island Abu Dhabi United Arab Emirates \u2217\u2217 Yanwu Xu, Baidu campus, No. 10 Shangdi 10th Street, Haidian District, Beijing 100085, China Baidu campus No. 10 Shangdi 10th Street Haidian District Beijing 100085 China Purpose Anterior segment optical coherence tomography (AS-OCT) provides an objective imaging modality for visually identifying anterior segment structures. An automated detection system could assist ophthalmologists in interpreting AS-OCT images for the presence of angle closure. Design Development of an artificial intelligence automated detection system for the presence of angle closure. Methods A deep learning system for automated angle-closure detection in AS-OCT images was developed, and this was compared with another automated angle-closure detection system based on quantitative features. A total of 4135 Visante AS-OCT images from 2113 subjects (8270 anterior chamber angle images with 7375 open-angle and 895 angle-closure) were examined. The deep learning angle-closure detection system for a 2-class classification problem was tested by 5-fold cross-validation. The deep learning system and the automated angle-closure detection system based on quantitative features were evaluated against clinicians' grading of AS-OCT images as the reference standard. Results The area under the receiver operating characteristic curve of the system using quantitative features was 0.90 (95% confidence interval [CI] 0.891\u20130.914) with a sensitivity of 0.79 \u00b1 0.037 and a specificity of 0.87 \u00b1 0.009, while the area under the receiver operating characteristic curve of the deep learning system was 0.96 (95% CI 0.953\u20130.968) with a sensitivity of 0.90 \u00b1 0.02 and a specificity of 0.92 \u00b1 0.008, against clinicians' grading of AS-OCT images as the reference standard. Conclusions The results demonstrate the potential of the deep learning system for angle-closure detection in AS-OCT images. Glaucoma is the leading cause of irreversible blindness worldwide, with primary angle-closure glaucoma being a major cause of blindness in Asia. 1,2 Vision loss from primary angle-closure glaucoma cannot be reversed, and therefore identifying people with the early asymptomatic stages of the disease may allow for prophylactic treatment by laser iridotomy to prevent visual loss. 3 With an estimated 23 million people estimated to be afflicted with primary angle-closure glaucoma worldwide by 2020, 1 the disease will be a serious challenge for preventive health care systems. Gonioscopy is a convenient tool for simple opportunistic case detection in clinics, but imaging is envisaged to reduce this burden of screening for angle closure. In this context, an effective, automated system to assist with detection of angle closure via imaging is inevitable. 4 Anterior segment optical coherence tomography (AS-OCT) technology enables the acquisition and visualization of high-resolution images of the anterior segment structures. 4,5 However, AS-OCT imaging has limitations when used in the clinical management of patients with angle closure because of the lack of automated methods to interpret AS-OCT images for the presence of angle closure. Previous studies have suggested that several anterior chamber measurements, such as anterior chamber width (AC width), lens vault (LV), iris curvature (IC), and iris thickness (IT), are associated with gonioscopic angle closure, and >80% of the variation in angle width is explained by these anatomic risk factors. 6,7 When clinicians use AS-OCT to image the angles to diagnose angle closure in their patients, the AS-OCT images require a degree of interpretation. The key feature for angle closure diagnosis is irido-trabecular contact (ITC), but on AS-OCT images the trabecular meshwork (TM) cannot be reliably identified in most images because of limited resolution, making it difficult to ascertain if there was ITC. Clinicians need to use the scleral spur (SS) as a landmark for the TM (which lies just anterior to the SS), and clinicians look for iris-angle contact anterior to the SS to diagnose angle closure on AS-OCT images. However, the SS may also be difficult to identify on AS-OCT images as much as 20% to 30% of the time. 8,9 An automated system that could identify angle closure in AS-OCT images that is not dependent on SS localization would therefore be of immense clinical use. Automated assessment involving computer vision and machine learning techniques may be helpful 10\u201313 in this context. In an automated detection system, image features play the most important role because they provide an image representation used in machine learning classifiers to predict the result. In contrast to clinical quantitative features, visual features may extract image information beyond what clinicians recognize as relevant and account for detailed visual context in the image. Recently, deep learning techniques have learned discriminative representations directly from images, and this has been shown to surpass the performance of hand-crafted features in many computer vision tasks. 14,15 Based on advances in discriminative representations and large-scale data analysis, deep learning has recently demonstrated revolutionary performance in many medical image understanding tasks. 16\u201319 For fundus images, there have also been several recent studies that showed good screening performances for diabetic retinopathy, 20,21 age-related macular degeneration, 22,23 cataracts, 24 and glaucoma. 25\u201328 However, few works exist for the automated interpretation of AS-OCT images. In this study, we developed a deep learning system for automated detection of angle-closure in AS-OCT images and compared this with the automated angle-closure detection system based on quantitative features against clinician's grading of AS-OCT images as the reference standard. Methods AS-OCT Dataset A total of 2113 subjects, including those of Chinese, Indian, and Malay descent, were recruited from glaucoma clinics of the Singapore National Eye Center. Both eyes from 1 patient were included in our study. The study was approved by the SingHealth Centralised institutional review board and was conducted according to the tenets of the Declaration of Helsinki, with written informed consent obtained from all study participants. Subjects underwent a standardized ophthalmic examination including slit lamp examination, angle evaluation by gonioscopy by a fellowship-trained glaucoma specialist, and fundus examination with a 78 diopters (D) biomicroscopic examination. Phakic subjects were included in the study if they had open or closed angles on gonioscopy (see below). Subjects who had previous laser iridotomy were not excluded. Study participants underwent AS-OCT imaging (Visante; Carl Zeiss Meditec, Dublin, CA) with the examiner masked to gonioscopic findings. Seated subjects were asked to fixate on the internal fixation light in the primary gaze position under darkroom conditions. Cross-sectional AS-OCT scanning was performed for each eye for the vertical and horizontal meridia. The mode used was low scan Visante AS-OCT imaging with 16 mm \u00d7 6 mm and 256 A-scans per line (optical resolution axial 18 \u03bcm; transverse (center) 60 \u03bcm). Subjects with corneal conditions precluding good quality AS-OCT images, pseudophakia, laser procedures other than iridotomy, and those with motion artefacts and incomplete images were excluded from the study. In total, 4135 AS-OCT images from the 2113 subjects were collected to construct the AS-OCT dataset. Because each AS-OCT image contains 2 anterior chamber angle (ACA) regions, each image was split into 2 ACA images (8270 ACA images in total), with right-side ACA images flipped horizontally to match the orientations of left-side images. For each ACA image, the ground truth label of open-angle or angle-closure was determined by a single observer. This procedure yielded an AS-OCT dataset containing 8270 ACA images with 7375 open-angle and 895 angle-closure images. Quantitative Feature-Based System The quantitative features-based system consists of physical measurements deemed to be significant for angle closure (described below). To obtain these features in this study, we used the AS-OCT segmentation method, 10,29 a unified framework that included structure segmentation, clinical quantitative parameter measurement, and angle-closure classification (Figure 1 A). This method firstly generated a reference dataset composed of various AS-OCT images containing markers labeled manually by ophthalmologists, identifying landmarks such as SS and corneal boundary. Given a new AS-OCT image, the K-nearest similar reference images were retrieved from the reference dataset. The manual labels of these reference images were transferred to the new AS-OCT image as the initial markers. Then, these initial markers were used to estimate the major clinical structures (eg, SS, iris region, and corneal boundary). Finally, the standard quantitative parameters were calculated. A total of 6 global AS-OCT parameters (pupil diameter [PD], LV, anterior chamber width, anterior chamber depth, area [AC area], and anterior chamber volume) were calculated for each ACA image (Figure 2 ). PD was defined as shortest distance between the 2 iris endpoints. LV was the perpendicular distance between the anterior pole of the lens and the horizontal line joining the 2 SS points. 30 Anterior chamber width was defined as the horizontal scleral spur-to-spur distance. 31 Anterior chamber depth was the perpendicular distance between the anterior chamber and the horizontal line joining the 2 SS points. AC area and anterior chamber volume were defined as the cross-sectional area and volume of anterior segment, respectively. 32 Moreover, 6 local angular parameters (iris area [IA], iris curvature [IC], iris thickness [IT], trabecular iris angle area [TIA], angle opening distance [AOD], and angle recess area [ARA]) were also introduced (Figure 2). IA was cumulative cross-sectional area along the full length of the iris. IC was calculated by drawing a line from the most peripheral to the most central points of the iris pigment epithelium, extending a perpendicular line to the iris pigment epithelium at its point of greatest convexity and taking the length of this line. 7 IT was defined as the thickness of the iris at the specific distance from SS. TIA was the angle at the apex of the iris recess defined between lines extending through a point on the TM at the specific distance from SS. AOD was the length of the line segment between the cornea and iris at the specific distance from SS. ARA was the area bounded by the AOD line, corneal endothelium and the iris. For TIA, AOD, ARA and IT, each of them also included 3 measured values at different specific distances of 750, 1000, and 1200 \u03bcm from SS point, respectively. 33 Finally, totally 20 AS-OCT parameters were calculated as the quantitative parameters. We used a linear support vector machine classifier 34 to predict the probability of angle-closure. Deep Learning System The deep learning technique provides a powerful system that integrates feature representation and classification. In this study, we introduced the deep learning technique to detect angle-closure directly from AS-OCT images. The VGG-16 network was used as our basic network architecture, which is a state-of-the-art deep learning system and obtained satisfactory performance on the ImageNet Large Scale Visual Recognition Challenge. 35 The VGG-16 network consisted of 13 convolutional neural network layers and 3 fully connected layers (Figure 1B). The convolutional neural network layer applied a series of convolution operations to the input and extracted local feature representations to the next layer, while the fully connected layer served as the traditional multilayer perception neural network to predict the binary classification (angle closure yes/no) result with a soft-max activation function in the final output layer. The deep learning network was trained by an end-to-end way and optimized the weights of each layer via a back-propagation process using the stochastic gradient descent method to minimize the cross-entropy between labels and network outputs. Our VGG-16\u2013based deep network included many layer weights (approximately 138 million) to be optimized by the learning process. To handle this, we used 2 techniques for better convergence. The first was to use transfer learning, 36 in which the model weights were initialized by a pretrained model trained from ImageNet, 37 and then fine-tuned model weights on our AS-OCT dataset. The second technique was to apply data augmentation. 38 The AS-OCT images were captured by the standardized position and direction, and therefore we used only randomly shifting with 0.2 scale, randomly rotating with 15 degrees, and randomly zooming with 0.2 scale as image augmentation. Gonioscopy and Clinician Grading of AS-OCT Images Gonioscopy was performed with a Goldmann 2-mirror lens (Ocular Instruments, Inc., Bellevue, WA) under standard dark illumination in all participants. A narrow vertical beam 1 mm in length was offset vertically for superior and inferior quadrants and horizontally for nasal and temporal quadrants. Dynamic indentation gonioscopy with a 4-mirror Sussman gonioscope (Ocular Instruments, Inc., Bellevue, WA) was used to determine the presence of peripheral anterior synechiae. We used gonioscopy classification systems by Spaeth 39 and Scheie. 40 A closed ACA was diagnosed if the posterior trabecular meshwork was seen for \u2264180\u00b0 during static gonioscopy. 40 The clinician's qualitative evaluation of AS-OCT images was done after masking gonioscopic findings. An angle closure in AS-OCT images were defined as at least substantial ITC beyond the SS (at least one-third of the visible TM should be in contact with the iris). In those images where the highly reflective TM or SS was not visible, the ITC should be beyond the limbal reflections with a narrow approach. In the latter scenario, adjudication of the findings was done with the help of a senior glaucoma specialist. Evaluation and Statistical Analysis One advantage of a machine learning\u2013based technique is that it can produce an explicit probability result for each ACA classification. By applying a threshold to this probability to determine referability, the algorithm's sensitivity and specificity can be tuned for a particular use case. Performances across many different thresholds were summarized with area under the receiver operating characteristic curve (AUC) with 95% confidence intervals (CIs). Moreover, we represented the highest point on the ROC curve that offers minimal tradeoff between sensitivity and specificity (Figure 3 ) against clinician grading of AS-OCT images as reference standard. The deep learning method was implemented with Python based on Keras with Tensorflow backend. We used a gradually decreasing learning rate starting from 0.0001 and a momentum of 0.9. The method based on quantitative features was implemented with Matlab 2016b, and all statistical analyses were also performed using Matlab 2016b. In our study, we used 5-fold stratified cross-validation on our AS-OCT dataset of 8270 images to evaluate the performance, which is a resampling procedure widely used to evaluate machine learning models on a limited data sample. In our study, the whole dataset was split into five individual groups (1654 ACA images per each group). One group was then taken as a test data set to evaluate performance, while the other 4 groups were used as training set to train the model. Finally, the cross-validation process was repeated 5 times, with each of the subsamples used exactly once as the validation data, to report the average performance in our study. The data were divided to ensure that ACA images acquired from the same patient (eg, left and right ACA from one AS-OCT image, or the left and right eye from the same patient) were not split across training and validation data. We performed the analysis to compare automated angle-closure detection systems against clinician's grading of AS-OCT images as reference standard, and the statistical significance for P values was set at .05. Interpretation of Deep Learning Features The attention map visualizations were presented to identify patterns of deep learning features. The deep learning network was structured as a hierarchy of layers to explore different levels of features from AS-OCT images. The convolutional layers in the deep learning network scanned over the whole image to learn features from local region and aggregate these features to generate the feature map for the full image. The feature maps were thenused as input to pass on to the following layer. This hierarchy structure of deep learning networks produced the discriminative features on the different levels and scales, which mined a broader range of AS-OCT image details\u2014beyond what humans can observe as relevant\u2014and led to better performance than quantitative parameters. Moreover, we could not observe the specific deep feature exactly, but the attention map visualization provided an effective way to interpret the response region, which contributed most to the deep learning prediction in AS-OCT image. The AS-OCT image was fed into deep learning network, and the feature maps from the final convolutional layer were outputted. The attention map of the most important features used in angle-closure detection was generated by taking the weighted sum of all the feature maps using their associated weights in the fully connected layer (Figure 4 B). The highlighted response map is important to the predictions of deep learning center on the ACA region, which is coincident to what ophthalmologists use to make a diagnosis. Moreover, The t-distributed stochastic neighbor embedding (t-SNE) 41 was used to visualize the features in 2 dimensions. Figure 5 shows the t-SNE visualization for different features with each dot denoting an individual AS-OCT image in the feature space. Red and blue dots denote the angle-closure and open angle cases, respectively. From the t-SNE visualization, the 2 clusters (angle-closure and open angle images) overlap when using quantitative features. By contrast, the deep learning features resulted in two objectively separated clusters. Results The evaluation AS-OCT dataset contained 8270 ACA images with 7375 open-angle and 895 angle-closure images from 2113 subjects (954 males and 1159 females; mean age 63 \u00b1 8 years), and other clinical characteristics are reported in Table . Gonioscopy diagnosed 6971 open-angles and 1299 angle-closures. Of the AS-OCT images analyzed, 16% (1323/8270) were found to have poorly discernible TM or SS and needed adjudication by a second examiner. In addition, we performed an earlier pilot study done in this sample (n = 342) for interobserver reproducibility for angle closure as open/closed based on this definition and found kappa > 0.8. The AUC for angle closure detection for using quantitative features in comparison with clinician's interpretation of the AS-OCT image was 0.90 (95% CI 0.891\u20130.914; P < .0001) with a sensitivity of 0.79 \u00b1 0.037 and a specificity of 0.87 \u00b1 0.009. Correspondingly, the deep learning method obtained the best performance, with AUC 0.96 (95% CI 0.953\u20130.968; P < .0001) with a sensitivity of 0.90 \u00b1 0.020 and a specificity of 0.92 \u00b1 0.008. Some failure cases of angle closure are also shown in Figure 6 . One common reason for false negative classification was poorly delineated SS or TM (Figure 6A). The poorly delineated SS or TM cases may lose the discriminating information and cause the misdetection by computer vision system. The second reason of failure was because of shadow interference (Figure 6B). The shadow produced a low-contrast region, which could mislead the computer vision system. However, this case could be relieved by using extra image enhancement preprocessing. Discussion Several methods have recently been proposed to identify gonioscopic angle-closure from AS-OCT imaging. Various subsets of quantitative AS-OCT parameters were reported as being associated with the development of angle closure. 6,7 The reported AUCs were 0.83 with LV and AOD at 750 \u03bcm7 and AUC 0.95 with a combination of 6 quantitative AS-OCT features. 6 However, the quantitative parameters used in these studies were calculated manually or by semiautomated means. 42 By contrast, visual feature\u2013based methods are automated. Xu and associates 13 reported a 0.83 AUC with HOG feature, and Ni and associates 43 reported a 0.98 AUC with shape analysis on a swept source OCT. In this study, we developed a deep learning system for angle-closure detection and compared this system with an AS-OCT detection system based on quantitative features on the same AS-OCT dataset. The quantitative feature-based system obtained a lower performance (AUC = 0.90) than the deep learning features (AUC = 0.96). However, the clinical quantitative features were predefined based on anatomic structures, which had specific physical and clinical significance that clinicians take into consideration in making a diagnosis. By contrast, the visual features mined a broader range of AS-OCT image details, beyond what humans can observe as relevant, and led to better performance than quantitative parameters. The deep learning method learned the highly discriminative representations from the AS-OCT images directly by using the multiple CNN layers. The results show that the deep learning method enables automated identification of angle-closure with a high AUC score. We reported the ROC curves of 2 automated methods using the t test. The P value of the ROC comparisons between quantitative feature-based system and deep learning system indicated a statistically significant difference. Therefore, the deep learning system achieved a detection result that was significantly better than the detection result of a quantitative feature-based system. One limitation of this study was a specific Asian population (Chinese, Indian, and Malay) was evaluated and the results may not apply to other ethnic groups. Another potential limitation is that the AS-OCT images were taken from the Visante AS-OCT device (Model 1000; Carl Zeiss Meditec). This could negatively affect the quality and performance when the network is applied on images from other AS-OCT acquisition devices. Second, in this study gonioscopy was not the reference standard because this automated solution was only meant for comparison with clinician's interpretation of AS-OCT images. Moreover, in this study, we mainly analyzed the nasal/temporal scans to avoid influence by the eyelids. There was no significant difference in angle width among superior, nasal, inferior, and temporal quadrants as was found earlier. 44 The automated methods could therefore work well on nasal/temporal and superior/inferior scans. For the whole eye prediction, it could input the image one by one for different degrees to predict angle closure for each degree individually, and then count the results in 180 to 270 degrees. In summary, we developed and investigated a deep learning system for angle-closure detection in AS-OCT images. Deep learning was shown to be a promising technology for helping clinicians in reliably identifying angle-closure in AS-OCT images with high sensitivity and specificity compared to clinician graded ASOCT images. Additional studies are required to explore the usefulness of deep learning algorithms deployed in different population settings, with the use of multiple devices and for larger AS-OCT datasets. All authors have completed and submitted the ICMJE form for disclosure of potential conflicts of interest. Financial Disclosures: The sponsor or funding organization had no role in the design or conduct of this research. Funding/Support: This work was supported by ASTAR Biomedical Engineering Programme grant 1521480034 under the Biomedical Research Council and the Singapore Translational Research Investigator Award (NMRC/STAR/0023/2014) from the Singapore Ministry of Health\u2019s National Medical Research Council. Full access to all the data in the study and take full responsibility for the integrity of the data and the accuracy of the data analysis (H.F., D.W.K.W.); concept and design (H.F, Y.X., D.W.K.W., J.L., M.B.); acquisition, analysis, or interpretation of data (H.F, Y.X.); drafting of the manuscript (H.F., S.L.); critical revision of the manuscript for important intellectual content (H.F., Y.X., S.L., D.W.K.W.); statistical analysis (H.F, Y.X.); obtained funding (Y.X., D.W.K.W., T.A.); and supervision (D.W.K.W., J.L., T.A.). All authors attest that they meet the current ICMJE requirements to qualify as authors. References 1 H.A. Quigley A.T. Broman The number of people with glaucoma worldwide in 2010 and 2020 Br J Ophthalmol 90 3 2006 262 267 Quigley HA, Broman AT. The number of people with glaucoma worldwide in 2010 and 2020. Br J Ophthalmol 2006;90(3):262-267. 2 G. Foster Pauland Johnson Glaucoma in China: how big is the problem? Br J Ophthalmol 85 11 2001 1277 1282 Foster Pauland Johnson G. Glaucoma in China: how big is the problem? Br J Ophthalmol 2001;85(11):1277-1282. 3 Y.C. Tham X. Li T.Y. Wong H.A. Quigley T. Aung C.Y. Cheng Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis Ophthalmology 121 11 2014 2081 2090 Tham YC, Li X, Wong TY, Quigley HA, Aung T, Cheng CY. Global prevalence of glaucoma and projections of glaucoma burden through 2040: a systematic review and meta-analysis. Ophthalmology 2014;121(11):2081-2090. 4 M. Ang M. Baskaran R.M. Werkmeister Anterior segment optical coherence tomography Prog Retin Eye Res 66 2018 132 156 Ang M, Baskaran M, Werkmeister RM, et al. Anterior segment optical coherence tomography. Prog Retin Eye Res 2018;66:132-156. 5 R. Thomas Anterior segment optical coherence tomography Ophthalmology 114 12 2007 2362 2363 Thomas R. Anterior segment optical coherence tomography. Ophthalmology 2007;114(12):2362-2363. 6 M.E. Nongpiur B.A. Haaland D.S. Friedman Classification algorithms based on anterior segment optical coherence tomography measurements for detection of angle closure Ophthalmology 120 1 2013 48 54 Nongpiur ME, Haaland BA, Friedman DS, et al. Classification algorithms based on anterior segment optical coherence tomography measurements for detection of angle closure. Ophthalmology 2013;120(1):48-54. 7 M.E. Nongpiur I.F. Aboobakar M. Baskaran Association of baseline anterior segment parameters with the development of incident gonioscopic angle closure JAMA Ophthalmol 135 3 2017 252 258 Nongpiur ME, Aboobakar IF, Baskaran M, et al. Association of baseline anterior segment parameters with the development of incident gonioscopic angle closure. JAMA Ophthalmol 2017;135(3):252-258. 8 H.T. Wong M.C. Lim L.M. Sakata High-definition optical coherence tomography imaging of the iridocorneal angle of the eye Arch Ophthalmol 127 3 2009 256 260 Wong HT, Lim MC, Sakata LM, et al. High-definition optical coherence tomography imaging of the iridocorneal angle of the eye. Arch Ophthalmol 2009;127(3):256-260. 9 L.M. Sakata R. Lavanya D.S. Friedman Assessment of the scleral spur in anterior segment optical coherence tomography images Arch Ophthalmol 126 2 2008 181 185 Sakata LM, Lavanya R, Friedman DS, et al. Assessment of the scleral spur in anterior segment optical coherence tomography images. Arch Ophthalmol 2008;126(2):181-185. 10 H. Fu Y. Xu D. Wong Automatic anterior chamber angle structure segmentation in AS-OCT image based on label transfer Conf Proc IEEE Eng Med Biol Soc 2016 2016 1288 1291 Fu H, Xu Y, Wong D, et al. Automatic anterior chamber angle structure segmentation in AS-OCT image based on label transfer. Conf Proc IEEE Eng Med Biol Soc 2016;2016:1288-1291. 11 Y. Xu J. Liu J. Cheng Automated anterior chamber angle localization and glaucoma type classification in OCT images Conf Proc IEEE Eng Med Biol Soc 2013 2013 7380 7383 Xu Y, Liu J, Cheng J, et al. Automated anterior chamber angle localization and glaucoma type classification in OCT images. Conf Proc IEEE Eng Med Biol Soc 2013;2013:7380-7383. 12 H. Fu Y. Xu S. Lin Multi-context deep network for angle-closure glaucoma screening in anterior segment OCT A. Frangi J. Schnabel C. Davatzikos C. Alberola-L\u00f3pez G. Fichtinger Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2018 11071:356-363 Fu H, Xu Y, Lin S, et al. Multi-context deep network for angle-closure glaucoma screening in anterior segment OCT. In: Frangi A, Schnabel J, Davatzikos C, Alberola-Lopez C, Fichtinger G, eds. Medical Image Computing and Computer Assisted Intervention - MICCAI 2018;11071:356-363. 13 Y. Xu J. Liu N.M. Tan Anterior chamber angle classification using multiscale histograms of oriented gradients for glaucoma subtype identification Conf Proc IEEE Eng Med Biol Soc 2012 2012 3167 3170 Xu Y, Liu J, Tan NM, et al. Anterior chamber angle classification using multiscale histograms of oriented gradients for glaucoma subtype identification. Conf Proc IEEE Eng Med Biol Soc 2012;2012:3167-3170. 14 Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 7553 2015 436 444 LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521(7553):436-444. 15 J. Schmidhuber Deep learning in neural networks: an overview Neural Netw 61 2015 85 117 Schmidhuber J. Deep learning in neural networks: an overview. Neural Netw 2015;61:85-117. 16 B.E. Bejnordi M. Veta PJ van Diest Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer JAMA 318 22 2017 2199 2210 Bejnordi BE, Veta M, Diest PJ van, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA 2017;318(22):2199-2210. 17 A. Esteva B. Kuprel R.A. Novoa Dermatologist-level classification of skin cancer with deep neural networks Nature 542 7639 2017 115 118 Esteva A, Kuprel B, Novoa RA, et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 2017;542(7639):115-118. 18 D. Kermany M. Goldbaum W. Cai E. Al Identifying medical diagnoses and treatable diseases by image-based deep learning Cell 172 5 2018 1122 1131 Kermany D, Goldbaum M, Cai W, Al E. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell 2018;172(5):1122-1131. 19 R. Poplin A.V. Varadarajan K. Blumer Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning Nat Biomed Eng 2 3 2018 158 164 Poplin R, Varadarajan A V, Blumer K, et al. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat Biomed Eng 2018;2(3):158-164. 20 V. Gulshan L. Peng M. Coram Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs JAMA 304 6 2016 649 656 Gulshan V, Peng L, Coram M, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 2016;304(6):649-656. 21 D. Ting C. Cheung G. Lim E. Al Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes JAMA 318 22 2017 2211 2223 Ting D, Cheung C, Lim G, Al E. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. JAMA 2017;318(22):2211-2223. 22 P.M. Burlina N. Joshi M. Pekala K.D. Pacheco D.E. Freund N.M. Bressler Automated grading of age-related macular degeneration from color fundus images using deep convolutional neural networks JAMA Ophthalmol 135 11 2017 1170 1176 Burlina PM, Joshi N, Pekala M, Pacheco KD, Freund DE, Bressler NM. Automated grading of age-related macular degeneration from color fundus images using deep convolutional neural networks. JAMA Ophthalmol 2017;135(11):1170-1176. 23 F. Grassmann J. Mengelkamp C. Brandl A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography Ophthalmology 2018 1 11 Grassmann F, Mengelkamp J, Brandl C, et al. A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography. Ophthalmology 2018:1-11. 24 E. Long H. Lin Z. Liu An artificial intelligence platform for the multihospital collaborative management of congenital cataracts Nat Biomed Eng 1 24 2017 0024 Long E, Lin H, Liu Z, et al. An artificial intelligence platform for the multihospital collaborative management of congenital cataracts. Nat Biomed Eng 2017;1(24):0024. 25 R. Asaoka H. Murata A. Iwase M. Araie Detecting preperimetric glaucoma with standard automated perimetry using a deep learning classifier Ophthalmology 123 9 2016 1974 1980 Asaoka R, Murata H, Iwase A, Araie M. Detecting preperimetric glaucoma with standard automated perimetry using a deep learning classifier. Ophthalmology 2016;123(9):1974-1980. 26 Z. Li Y. He S. Keel W. Meng R.T. Chang M. He Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs Ophthalmology 125 8 2018 1199 1206 Li Z, He Y, Keel S, Meng W, Chang RT, He M. Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs. Ophthalmology 2018;125(8):1199-1206. 27 H. Fu J. Cheng Y. Xu D. Wong J. Liu X. Cao Joint optic disc and cup segmentation based on multi-label deep network and polar transformation IEEE Trans Med Imaging 37 7 2018 1597 1605 Fu H, Cheng J, Xu Y, Wong D, Liu J, Cao X. Joint optic disc and cup segmentation based on multi-label deep network and polar transformation. IEEE Trans Med Imaging 2018;37(7):1597-1605. 28 H. Fu J. Cheng Y. Xu Disc-aware ensemble network for glaucoma screening from fundus image IEEE Trans Med Imaging 37 11 2018 2493 2501 Fu H, Cheng J, Xu Y, et al. Disc-aware ensemble network for glaucoma screening from fundus image. IEEE Trans Med Imaging 2018;37(11):2493-2501. 29 H. Fu Y. Xu S. Lin Segmentation and quantification for angle-closure glaucoma assessment in anterior segment OCT IEEE Trans Med Imaging 36 9 2017 1930 1938 Fu H, Xu Y, Lin S, et al. Segmentation and quantification for angle-closure glaucoma assessment in anterior segment OCT. IEEE Trans Med Imaging 2017;36(9):1930-1938. 30 G.S. Tan M. He W. Zhao Determinants of lens vault and association with narrow angles in patients From Singapore Am J Ophthalmol 154 1 2012 39 46 Tan GS, He M, Zhao W, et al. Determinants of lens vault and association with narrow angles in patients From Singapore. Am J Ophthalmol 2012;154(1):39-46. 31 M.E. Nongpiur L.M. Sakata D.S. Friedman Novel association of smaller anterior chamber width with angle closure in Singaporeans Ophthalmology 117 10 2010 1967 1973 Nongpiur ME, Sakata LM, Friedman DS, et al. Novel association of smaller anterior chamber width with angle closure in Singaporeans. Ophthalmology 2010;117(10):1967-1973. 32 R.Y. Wu M.E. Nongpiur M.G. He Association of narrow angles with anterior chamber area and volume measured with anterior-segment optical coherence tomography Arch Ophthalmol 129 5 2011 569 574 Wu RY, Nongpiur ME, He MG, et al. Association of narrow angles with anterior chamber area and volume measured with anterior-segment optical coherence tomography. Arch Ophthalmol 2011;129(5):569-574. 33 A. Narayanaswamy L. Sakata M. He Diagnostic performance of anterior chamber angle measurements for detecting eyes with narrow angles Arch Ophthalmol 128 10 2010 1321 1327 Narayanaswamy A, Sakata L, He M, et al. Diagnostic performance of anterior chamber angle measurements for detecting eyes with narrow angles. Arch Ophthalmol 2010;128(10):1321-1327. 34 C.C. Chang C.J. Lin LIBSVM: a library for support vector machines ACM Trans Intell Syst Technol 2 3 2011 1 27 Chang CC, Lin CJ. LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 2011;2(3):1-27. 35 K. Simonyan A. Zisserman Very deep convolutional networks for large-scale image recognition. Presented at: International Conference on Learning Representations 2015 May 7-9, 2015, San Diego, CA Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. Presented at: International Conference on Learning Representations 2015, May 7-9, 2015, San Diego, CA. 36 S.J. Pan Q. Yang A Survey on Transfer Learning IEEE Trans Knowl Data Eng 22 10 2010 1345 1359 Pan SJ, Yang Q. A Survey on Transfer Learning. IEEE Trans Knowl Data Eng 2010;22(10):1345-1359. 37 O. Russakovsky J. Deng H. Su ImageNet large scale visual recognition challenge Int J Comput Vis 115 3 2015 211 252 Russakovsky O, Deng J, Su H, et al. ImageNet large scale visual recognition challenge. Int J Comput Vis 2015;115(3):211-252. 38 A. Krizhevsky I. Sutskever G. Hinton ImageNet classification with deep convolutional neural networks Communications of the ACM 60 6 2017 84 90 Krizhevsky A, Sutskever I, Hinton G. ImageNet classification with deep convolutional neural networks. Communications of the ACM 2017;60(6):84-90. 39 G.L. Spaeth The normal development of the human anterior chamber angle: a new system of descriptive grading Trans Ophthalmol Soc U K 91 1971 709 739 Spaeth GL. The normal development of the human anterior chamber angle: a new system of descriptive grading. Trans Ophthalmol Soc U K 1971;91:709-739. 40 H.G. Scheie Width and pigmentation of the angle of the anterior chamber; a system of grading by gonioscopy AMA Arch Ophthalmol 58 4 1957 510 512 Scheie HG. Width and pigmentation of the angle of the anterior chamber; a system of grading by gonioscopy. AMA Arch Ophthalmol 1957;58(4):510-512. 41 L. van der Maaten G. Hinton Visualizing Data using t-SNE J Mach Learn Res 9 Nov 2008 2579 2605 van der Maaten L, Hinton G. Visualizing Data using t-SNE. J Mach Learn Res 2008;9(Nov):2579-2605. 42 J. Console L. Sakata T. Aung D. Friedman M. He Quantitative analysis of anterior segment optical coherence tomography images: the Zhongshan Angle Assessment Program Br J Ophthalmol 92 12 2008 1612 1616 Console J, Sakata L, Aung T, Friedman D, He M. Quantitative analysis of anterior segment optical coherence tomography images: the Zhongshan Angle Assessment Program. Br J Ophthalmol 2008;92(12):1612-1616. 43 S. Ni Ni J. Tian P. Marziliano H.T. Wong Anterior chamber angle shape analysis and classification of glaucoma in SS-OCT images J Ophthalmol 2014 2014 942367 Ni Ni S, Tian J, Marziliano P, Wong HT. Anterior chamber angle shape analysis and classification of glaucoma in SS-OCT images. J Ophthalmol 2014;2014:942367. 44 E.K. Woo C.J. Pavlin A. Slomovic N. Taback Y.M. Buys Ultrasound biomicroscopic quantitative analysis of light-dark changes associated with pupillary block Am J Ophthalmol 127 1 1999 43 47 Woo EK, Pavlin CJ, Slomovic A, Taback N, Buys YM. Ultrasound biomicroscopic quantitative analysis of light-dark changes associated with pupillary block. Am J Ophthalmol 1999;127(1):43-47.", "scopus-id": "85066264736", "pubmed-id": "30849350", "coredata": {"eid": "1-s2.0-S000293941930087X", "dc:description": "Purpose Anterior segment optical coherence tomography (AS-OCT) provides an objective imaging modality for visually identifying anterior segment structures. An automated detection system could assist ophthalmologists in interpreting AS-OCT images for the presence of angle closure. Design Development of an artificial intelligence automated detection system for the presence of angle closure. Methods A deep learning system for automated angle-closure detection in AS-OCT images was developed, and this was compared with another automated angle-closure detection system based on quantitative features. A total of 4135 Visante AS-OCT images from 2113 subjects (8270 anterior chamber angle images with 7375 open-angle and 895 angle-closure) were examined. The deep learning angle-closure detection system for a 2-class classification problem was tested by 5-fold cross-validation. The deep learning system and the automated angle-closure detection system based on quantitative features were evaluated against clinicians' grading of AS-OCT images as the reference standard. Results The area under the receiver operating characteristic curve of the system using quantitative features was 0.90 (95% confidence interval [CI] 0.891\u20130.914) with a sensitivity of 0.79 \u00b1 0.037 and a specificity of 0.87 \u00b1 0.009, while the area under the receiver operating characteristic curve of the deep learning system was 0.96 (95% CI 0.953\u20130.968) with a sensitivity of 0.90 \u00b1 0.02 and a specificity of 0.92 \u00b1 0.008, against clinicians' grading of AS-OCT images as the reference standard. Conclusions The results demonstrate the potential of the deep learning system for angle-closure detection in AS-OCT images.", "openArchiveArticle": "false", "prism:coverDate": "2019-07-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S000293941930087X", "dc:creator": [{"@_fa": "true", "$": "Fu, Huazhu"}, {"@_fa": "true", "$": "Baskaran, Mani"}, {"@_fa": "true", "$": "Xu, Yanwu"}, {"@_fa": "true", "$": "Lin, Stephen"}, {"@_fa": "true", "$": "Wong, Damon Wing Kee"}, {"@_fa": "true", "$": "Liu, Jiang"}, {"@_fa": "true", "$": "Tun, Tin A."}, {"@_fa": "true", "$": "Mahesh, Meenakshi"}, {"@_fa": "true", "$": "Perera, Shamira A."}, {"@_fa": "true", "$": "Aung, Tin"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S000293941930087X"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S000293941930087X"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0002-9394(19)30087-X", "prism:volume": "203", "prism:publisher": "The Authors. Published by Elsevier Inc.", "dc:title": "A Deep Learning System for Automated Angle-Closure Detection in Anterior Segment Optical Coherence Tomography Images", "prism:copyright": "\u00a9 2019 The Authors. Published by Elsevier Inc.", "openaccess": "1", "prism:issn": "00029394", "openaccessArticle": "true", "prism:publicationName": "American Journal of Ophthalmology", "openaccessSponsorType": "Author", "prism:pageRange": "37-45", "prism:endingPage": "45", "pubType": "Original article", "prism:coverDisplayDate": "July 2019", "prism:doi": "10.1016/j.ajo.2019.02.028", "prism:startingPage": "37", "dc:identifier": "doi:10.1016/j.ajo.2019.02.028", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "163", "@width": "201", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14486", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "92", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "16181", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "125", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14031", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "127", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "19522", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "65", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14374", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "82", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13360", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "276", "@width": "339", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "39865", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "310", "@width": "734", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "91684", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "332", "@width": "583", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "50763", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "421", "@width": "728", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "91406", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "219", "@width": "734", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "82391", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "220", "@width": "583", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "69145", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "732", "@width": "900", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "97139", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1370", "@width": "3249", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "643489", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1472", "@width": "2583", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "366469", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1118", "@width": "1935", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "414029", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "972", "@width": "3251", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "616272", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "973", "@width": "2584", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "538485", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S000293941930087X-am.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "AAM-PDF", "@size": "4805625", "@ref": "am", "@mimetype": "application/pdf"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85066264736"}}