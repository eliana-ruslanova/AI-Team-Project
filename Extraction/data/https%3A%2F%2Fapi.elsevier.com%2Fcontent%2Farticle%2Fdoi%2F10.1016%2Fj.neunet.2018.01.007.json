{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608018300078", "dc:identifier": "doi:10.1016/j.neunet.2018.01.007", "eid": "1-s2.0-S0893608018300078", "prism:doi": "10.1016/j.neunet.2018.01.007", "pii": "S0893-6080(18)30007-8", "dc:title": "A novel type of activation function in artificial neural networks: Trained activation function ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "99", "prism:startingPage": "148", "prism:endingPage": "157", "prism:pageRange": "148-157", "dc:format": "application/json", "prism:coverDate": "2018-03-31", "prism:coverDisplayDate": "March 2018", "prism:copyright": "\u00a9 2018 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Ertu\u011frul, \u00d6mer Faruk"}], "dc:description": "\n               Abstract\n               \n                  Determining optimal activation function in artificial neural networks is an important issue because it is directly linked with obtained success rates. But, unfortunately, there is not any way to determine them analytically, optimal activation function is generally determined by trials or tuning. This paper addresses, a simpler and a more effective approach to determine optimal activation function. In this approach, which can be called as trained activation function, an activation function was trained for each particular neuron by linear regression. This training process was done based on the training dataset, which consists the sums of inputs of each neuron in the hidden layer and desired outputs. By this way, a different activation function was generated for each neuron in the hidden layer. This approach was employed in random weight artificial neural network (RWN) and validated by 50 benchmark datasets. Achieved success rates by RWN that used trained activation functions were higher than obtained success rates by RWN that used traditional activation functions. Obtained results showed that proposed approach is a successful, simple and an effective way to determine optimal activation function instead of trials or tuning in both randomized single and multilayer ANNs.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Activation function"}, {"@_fa": "true", "$": "Trained activation function"}, {"@_fa": "true", "$": "Artificial neural network"}, {"@_fa": "true", "$": "Random weight artificial neural network"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608018300078", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608018300078", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "85041400956", "scopus-eid": "2-s2.0-85041400956", "pubmed-id": "29427841", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/85041400956", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20180131", "$": "2018-01-31"}}}}}