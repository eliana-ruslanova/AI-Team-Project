{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0895611114000986", "dc:identifier": "doi:10.1016/j.compmedimag.2014.06.005", "eid": "1-s2.0-S0895611114000986", "prism:doi": "10.1016/j.compmedimag.2014.06.005", "pii": "S0895-6111(14)00098-6", "dc:title": "Improved medical image modality classification using a combination of visual and textual features ", "prism:publicationName": "Computerized Medical Imaging and Graphics", "prism:aggregationType": "Journal", "prism:issn": "08956111", "prism:volume": "39", "prism:startingPage": "14", "prism:endingPage": "26", "prism:pageRange": "14-26", "dc:format": "application/json", "prism:coverDate": "2015-01-31", "prism:coverDisplayDate": "January 2015", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "Medical visual information analysis and retrieval", "dc:creator": [{"@_fa": "true", "$": "Dimitrovski, Ivica"}, {"@_fa": "true", "$": "Kocev, Dragi"}, {"@_fa": "true", "$": "Kitanovski, Ivan"}, {"@_fa": "true", "$": "Loskovska, Suzana"}, {"@_fa": "true", "$": "D\u017eeroski, Sa\u0161o"}], "dc:description": "\n               Abstract\n               \n                  In this paper, we present the approach that we applied to the medical modality classification tasks at the ImageCLEF evaluation forum. More specifically, we used the modality classification databases from the ImageCLEF competitions in 2011, 2012 and 2013, described by four visual and one textual types of features, and combinations thereof. We used local binary patterns, color and edge directivity descriptors, fuzzy color and texture histogram and scale-invariant feature transform (and its variant opponentSIFT) as visual features and the standard bag-of-words textual representation coupled with TF-IDF weighting. The results from the extensive experimental evaluation identify the SIFT and opponentSIFT features as the best performing features for modality classification. Next, the low-level fusion of the visual features improves the predictive performance of the classifiers. This is because the different features are able to capture different aspects of an image, their combination offering a more complete representation of the visual content in an image. Moreover, adding textual features further increases the predictive performance. Finally, the results obtained with our approach are the best results reported on these databases so far.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Image modality classification"}, {"@_fa": "true", "$": "Visual image descriptors"}, {"@_fa": "true", "$": "Feature fusion"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0895611114000986", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0895611114000986", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84920261662", "scopus-eid": "2-s2.0-84920261662", "pubmed-id": "24997992", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84920261662", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20140619", "$": "2014-06-19"}}}}}