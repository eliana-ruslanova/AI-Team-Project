{"scopus-eid": "2-s2.0-40049102115", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2007-10-22 2007-10-22 2010-10-07T15:33:45 1-s2.0-S1532046407001086 S1532-0464(07)00108-6 S1532046407001086 10.1016/j.jbi.2007.10.002 S300 S300.1 FULL-TEXT 1-s2.0-S1532046408X00036 2015-05-15T06:30:58.184067-04:00 0 0 20080401 20080430 2008 2007-10-22T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 1532-0464 15320464 41 41 2 2 Volume 41, Issue 2 16 371 386 371 386 200804 April 2008 2008-04-01 2008-04-30 2008 article fla Copyright \u00a9 2007 Elsevier Inc. All rights reserved. BUILDINGAHOSPITALREFERRALEXPERTSYSTEMAPREDICTIONOPTIMIZATIONBASEDDECISIONSUPPORTSYSTEMALGORITHM CHI C 1 Introduction 1.1 Introduction to the hospital referral problem 1.2 An introduction to the PODSS algorithm 2 Methods 2.1 Dataset design 2.2 Building a predictive model 2.3 Extracting recommendation information 2.4 Building a validation map 3 Results 3.1 Single-objective optimization 3.2 Multi-objective optimization 4 Discussion and conclusions Acknowledgment References ABIDI 2005 193 204 S ALLISON 2000 1256 1262 J AYANIAN 2002 569 593 J BALI 2005 157 161 R BIRKMEYER 2006 411 417 J BIRKMEYER 2001 415 422 J BIRKMEYER 2002 1128 1137 J BIRKMEYER 2003 2703 2708 J CHEN 2003 243 254 J CLANCEY 1985 289 350 W COENEN 1992 76 84 F DEYO 1992 613 619 R DIMICK 2002 828 832 J ELIXHAUSER 2003 167 177 A ELIXHAUSER 1998 8 27 A GANDJOUR 2003 1129 1141 A GLANCE 2003 1155 1162 L HALM 2002 511 520 E HILLNER 2000 2327 2340 B IHSE 2003 777 781 I JEFFREY 1992 L MULTIOBJECTIVEOPTIMIZATIONBEHAVIORALCOMPUTATIONALCONSIDERATIONS 2000 ERRHUMANBUILDINGASAFERHEALTHSYSTEM KUPERSMITH 2005 458 466 J NALLAMOTHU 2005 333 337 B NEMATI 2002 143 161 H PLATT 1999 61 74 J ADVANCESINLARGEMARGINCLASSIFIERS PROBABILISTICOUTPUTSFORSUPPORTVECTORMACHINESCOMPARISONREGULARIZEDLIKELIHOODMETHODS QUINLAN 1993 J C45PROGRAMSFORMACHINELEARNING SHORTLIFFE 1975 303 320 E VAPNIK 1995 V NATURESTATISTICALLEARNINGTHEORY WARD 2004 344 354 M WATSON 1992 189 196 I WATSON 1994 355 381 I CHIX2008X371 CHIX2008X371X386 CHIX2008X371XC CHIX2008X371X386XC 2013-07-17T11:42:39Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(07)00108-6 S1532046407001086 1-s2.0-S1532046407001086 10.1016/j.jbi.2007.10.002 272371 2010-11-08T20:52:02.353937-05:00 2008-04-01 2008-04-30 1-s2.0-S1532046407001086-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/MAIN/application/pdf/e372545c6596d3e0b5fb58ff6a236b87/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/MAIN/application/pdf/e372545c6596d3e0b5fb58ff6a236b87/main.pdf main.pdf pdf true 955745 MAIN 16 1-s2.0-S1532046407001086-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/PREVIEW/image/png/720a9582013df00003ff3cfbe51551d9/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/PREVIEW/image/png/720a9582013df00003ff3cfbe51551d9/main_1.png main_1.png png 67691 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046407001086-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif si9 si9.gif gif 473 21 71 ALTIMG 1-s2.0-S1532046407001086-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/2592ab6cbd41a0dc808b83424a7b8c9c/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/2592ab6cbd41a0dc808b83424a7b8c9c/si8.gif si8 si8.gif gif 242 15 21 ALTIMG 1-s2.0-S1532046407001086-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/ce3ea5a34730bdaa3291ebcb3fa8799f/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/ce3ea5a34730bdaa3291ebcb3fa8799f/si7.gif si7 si7.gif gif 195 19 9 ALTIMG 1-s2.0-S1532046407001086-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/da93d5861279b941ca0f1254c6e38a02/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/da93d5861279b941ca0f1254c6e38a02/si6.gif si6 si6.gif gif 1812 77 188 ALTIMG 1-s2.0-S1532046407001086-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/ee5d5ff5cd6ac78c0d9d17806bb12155/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/ee5d5ff5cd6ac78c0d9d17806bb12155/si5.gif si5 si5.gif gif 187 17 9 ALTIMG 1-s2.0-S1532046407001086-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/65eac565c50c040625feb48261cc81f9/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/65eac565c50c040625feb48261cc81f9/si4.gif si4 si4.gif gif 616 19 107 ALTIMG 1-s2.0-S1532046407001086-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/6583e22486732eb87017f457354ef36b/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/6583e22486732eb87017f457354ef36b/si3.gif si3 si3.gif gif 885 18 187 ALTIMG 1-s2.0-S1532046407001086-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/3f8c0e372e3d9ed7266402a4369aa1d7/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/3f8c0e372e3d9ed7266402a4369aa1d7/si2.gif si2 si2.gif gif 875 19 242 ALTIMG 1-s2.0-S1532046407001086-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif si17 si17.gif gif 473 21 71 ALTIMG 1-s2.0-S1532046407001086-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif si16 si16.gif gif 473 21 71 ALTIMG 1-s2.0-S1532046407001086-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/a7ec9284670f326eca42fefef97649c9/si16.gif si15 si15.gif gif 473 21 71 ALTIMG 1-s2.0-S1532046407001086-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/2592ab6cbd41a0dc808b83424a7b8c9c/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/2592ab6cbd41a0dc808b83424a7b8c9c/si8.gif si14 si14.gif gif 242 15 21 ALTIMG 1-s2.0-S1532046407001086-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/e1bd95df6a35c9e3ced8a17e81360a03/si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/e1bd95df6a35c9e3ced8a17e81360a03/si13.gif si13 si13.gif gif 1052 50 167 ALTIMG 1-s2.0-S1532046407001086-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/eab642198a265f921fb0ecf17b7ded53/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/eab642198a265f921fb0ecf17b7ded53/si12.gif si12 si12.gif gif 1399 33 308 ALTIMG 1-s2.0-S1532046407001086-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/ce34d6ca2973d572e0f16f0668e9e282/si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/ce34d6ca2973d572e0f16f0668e9e282/si11.gif si11 si11.gif gif 1149 38 241 ALTIMG 1-s2.0-S1532046407001086-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/24ee0c7c6262129cef222df0e7730c4f/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/24ee0c7c6262129cef222df0e7730c4f/si10.gif si10 si10.gif gif 1880 52 279 ALTIMG 1-s2.0-S1532046407001086-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/STRIPIN/image/gif/6583e22486732eb87017f457354ef36b/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/STRIPIN/image/gif/6583e22486732eb87017f457354ef36b/si3.gif si1 si1.gif gif 885 18 187 ALTIMG 1-s2.0-S1532046407001086-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr2/DOWNSAMPLED/image/jpeg/605aeb584cc3ba5336bd11c0b3db3f4d/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr2/DOWNSAMPLED/image/jpeg/605aeb584cc3ba5336bd11c0b3db3f4d/gr2.jpg gr2 gr2.jpg jpg 17739 235 263 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr2/THUMBNAIL/image/gif/714ed8e62292f2cd1afb17ee45e41dcf/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr2/THUMBNAIL/image/gif/714ed8e62292f2cd1afb17ee45e41dcf/gr2.sml gr2 gr2.sml sml 1430 94 105 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr3/DOWNSAMPLED/image/jpeg/c0e459996c17f663060e7616491473a9/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr3/DOWNSAMPLED/image/jpeg/c0e459996c17f663060e7616491473a9/gr3.jpg gr3 gr3.jpg jpg 31361 348 381 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr3/THUMBNAIL/image/gif/d8536012efa5039df80fbe2048b5c851/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr3/THUMBNAIL/image/gif/d8536012efa5039df80fbe2048b5c851/gr3.sml gr3 gr3.sml sml 2405 93 102 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr4/DOWNSAMPLED/image/jpeg/fe4f463ea0103de6853a73f7efcde2f6/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr4/DOWNSAMPLED/image/jpeg/fe4f463ea0103de6853a73f7efcde2f6/gr4.jpg gr4 gr4.jpg jpg 30746 432 378 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr4/THUMBNAIL/image/gif/3878dbb5299d33c4017e236ae8e17a13/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr4/THUMBNAIL/image/gif/3878dbb5299d33c4017e236ae8e17a13/gr4.sml gr4 gr4.sml sml 1847 94 82 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr5/DOWNSAMPLED/image/jpeg/e87a57dcfbaa09d44e1b04ce4cfe1ab7/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr5/DOWNSAMPLED/image/jpeg/e87a57dcfbaa09d44e1b04ce4cfe1ab7/gr5.jpg gr5 gr5.jpg jpg 30503 416 378 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr5/THUMBNAIL/image/gif/2ce2afe3060280c09f200f9d3b452c7b/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr5/THUMBNAIL/image/gif/2ce2afe3060280c09f200f9d3b452c7b/gr5.sml gr5 gr5.sml sml 1909 93 85 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr7/DOWNSAMPLED/image/jpeg/c92385ff4467d6f8fb5176f9c8a8336d/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr7/DOWNSAMPLED/image/jpeg/c92385ff4467d6f8fb5176f9c8a8336d/gr7.jpg gr7 gr7.jpg jpg 33596 394 428 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr7/THUMBNAIL/image/gif/2c9e65575cf98ae6d6d7ff4311b654b3/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr7/THUMBNAIL/image/gif/2c9e65575cf98ae6d6d7ff4311b654b3/gr7.sml gr7 gr7.sml sml 2424 93 101 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr8/DOWNSAMPLED/image/jpeg/4a85eb87d6c91114b2128b0b328065ff/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr8/DOWNSAMPLED/image/jpeg/4a85eb87d6c91114b2128b0b328065ff/gr8.jpg gr8 gr8.jpg jpg 121531 956 450 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr8/THUMBNAIL/image/gif/43965924fd4ffbd399dbd58da95a6a02/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr8/THUMBNAIL/image/gif/43965924fd4ffbd399dbd58da95a6a02/gr8.sml gr8 gr8.sml sml 2082 93 44 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr1/DOWNSAMPLED/image/jpeg/aa8d3b7a531cda54620462bf6c3daa8f/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr1/DOWNSAMPLED/image/jpeg/aa8d3b7a531cda54620462bf6c3daa8f/gr1.jpg gr1 gr1.jpg jpg 17957 282 338 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr1/THUMBNAIL/image/gif/4ea36322a1b4544bf05141eaf2740301/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr1/THUMBNAIL/image/gif/4ea36322a1b4544bf05141eaf2740301/gr1.sml gr1 gr1.sml sml 1315 93 112 IMAGE-THUMBNAIL 1-s2.0-S1532046407001086-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr6/DOWNSAMPLED/image/jpeg/8161c27ae7e272f32d270796245bca34/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr6/DOWNSAMPLED/image/jpeg/8161c27ae7e272f32d270796245bca34/gr6.jpg gr6 gr6.jpg jpg 43266 465 379 IMAGE-DOWNSAMPLED 1-s2.0-S1532046407001086-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046407001086/gr6/THUMBNAIL/image/gif/4cb368bf3149fb47c61d2c3b024b2e06/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046407001086/gr6/THUMBNAIL/image/gif/4cb368bf3149fb47c61d2c3b024b2e06/gr6.sml gr6 gr6.sml sml 2068 93 76 IMAGE-THUMBNAIL YJBIN 1389 S1532-0464(07)00108-6 10.1016/j.jbi.2007.10.002 Elsevier Inc. Fig. 1 The process to capture and apply knowledge. Fig. 2 Separating hyperplane with maximum margin created by a support vector machine. + and \u2212 represent the class of each data point. We assume + is the desired result (survival). The decision function, d ( x ) = \u2211 i = 1 n y i \u03b1 i K ( x , x i ) + b , can decide the location of a point as the prediction result. We can improve the probability of a point being positive by improving the decision value, d(x), of this point, for example, moving the point A\u2212 to A+ or A\u2217. Fig. 3 The validation maps of (a) all AMI, (b) nonsurgery, (c) CABG (mortality), and (d) CABG (complication) patients. Fig. 4 Training process in the PODSS algorithm. Fig. 5 Knowledge application process for single-objective optimization. Fig. 6 Knowledge application process for multi-objective optimization. Fig. 7 The visualization of hospitals. The solution space (hospital) can be demonstrated in location (x, y)=(freedom-from-complication probability, survival probability). Fig. 8 Visualization of hospital-selection trend. The distance parameters are 30, 50, 100, and 200 miles. Each star is a patient. The survival (left column) and the freedom-from-complication probabilities (right column) of all patients can be observed. Table 1 Variables description Data type 1 Patient age Numeric 2 Patient sex Male, female 3 Patient race White, other 4 Patient admission type Emergency, urgent, elective 5 Patient comorbidity severity Numeric 6 Patient payment type Medicare, Blue Cross, Commercial, other 7 Hospital ownership type Government owned or not 8 Hospital bed size Numeric 9 Hospital metropolitan status Numeric, from 0 to 6 based on population size 10 Hospital JCAHO status JCAHO accreditation or not 11 Hospital surgery volume Numeric, total surgical operations 12 Hospital discharge volume Numeric 13 Hospital CABG volume Numeric Table 2 Description of four datasets Desired outcome (%) Data size All AMI 93.0 6599 Nonsurgery 93.4 5846 CABG 95.0 466 CABG\u2013FFC 81.8 466 Desired outcome for the first three databases is survival and the last database is free from complications (FFC). Table 3 Mean square error of predicted probability of survival for SVM and logistic regression Regression Calibrated SVM P-value All AMI 0.2137 0.0626 <0.0001 Nonsurgery 0.2136 0.0593 <0.0001 CABG 0.4461 0.0462 <0.0001 CABG\u2013FFC 0.2616 0.1481 <0.0001 Table 4 Average predicted survival probabilities, average predicted scores, and distance comparison between originally chosen hospitals and recommended hospitals # Original choice Maximum distance (miles) 30 50 100 200 1 Avg. dist. 20 17 28 58 104 Avg. prob. (%) 92.8 93.6 94.2 94.9 95.2 Avg. scr. 0.575 0.714 0.830 0.973 1.047 2 Avg. dist. 19 17 27 58 104 Avg. prob. (%) 93.6 94.3 94.8 95.4 95.7 Avg. scr. 0.545 0.660 0.764 0.900 0.968 3 Avg. dist. 29 26 32 52 85 Avg. prob. (%) 95 95.4 95.7 96.6 97.2 Avg. scr. 0.3897 0.534 0.674 1.206 1.507 Table 5 Statistical test between actual outcome and prediction # True Estimation Orig. 30 50 100 200 1 Avg (%) 93.0 92.8 93.6 94.2 0.949 95.2 P \u2014 0.4548 0.0246 <0.0005 <0.0005 <0.0005 2 Avg (%) 93.4 93.6 94.3 94.8 95.4 95.7 P \u2014 0.6239 0.0036 <0.0005 <0.0005 <0.0005 3 Avg (%) 95.0 95.0 95.4 95.7 96.6 97.2 P \u2014 0.9670 0.3851 0.2765 0.0599 0.0169 4 Avg (%) 81.8 81.8 81.8 82.0 82.5 82.9 P \u2014 0.9966 0.4811 0.4442 0.3342 0.2692 Table 6 Frequency distributions of different improvement groups # Maximum distance (miles) 30 50 100 200 1 Negative 775 384 41 34 Improve (11.7%) (5.8%) (0.6%) (0.5%) 0\u20131% 3838 3705 3332 2950 Improve (58.2%) (56.1%) (50.5%) (44.7%) 1\u20133% 1060 1203 1508 1631 Improve (16.1%) (18.2%) (22.9%) (24.7%) 3\u20135% 504 654 826 848 Improve (7.6%) (9.9%) (12.5%) (12.9%) 5\u20137% 224 332 413 562 Improve (3.4%) (5.0%) (6.3%) (8.5%) 7\u20139% 122 183 254 292 Improve (1.8%) (2.8%) (3.8%) (4.4%) 9-% 76 138 225 282 Improve (1.2%) (2.1%) (3.4%) (4.3%) 2 Negative 706 354 44 36 Improve (12.1%) (6.1%) (0.8%) (0.6%) 0\u20131% 3579 3500 3212 2886 Improve (61.2%) (59.9%) (54.9%) (49.4%) 1\u20133% 890 1030 1284 1341 Improve (15.2%) (17.6%) (22.0%) (22.9%) 3\u20135% 380 507 664 781 Improve (6.5%) (8.7%) (11.4%) (13.4%) 5\u20137% 157 231 311 407 Improve (2.7%) (4.0%) (5.3%) (7.0%) 7\u20139% 99 153 193 219 Improve (1.7%) (2.6%) (3.3%) (3.7%) 9-% 35 71 138 176 Improve (0.6%) (1.2%) (2.4%) (3.0%) 3 Negative 62 42 7 1 Improve (13.3%) (9%) (1.5%) (0.2%) 0\u20131% 320 303 186 110 Improve (68.7%) (65%) (39.9%) (23.6%) 1\u20133% 63 89 185 216 Improve (13.5%) (19.1%) (39.7%) (46.4%) 3\u20135% 17 26 74 118 Improve (3.6%) (5.6%) (15.9%) (25.3%) 5\u20137% 4 6 13 20 Improve (0.9%) (1.3%) (2.8%) (4.3%) 7\u20139% 0 0 1 1 Improve (0) (0) (0.2%) (0.2%) Table 7 Survival rates for the negative to 1% improvement groups # 30 50 100 200 1 94.41% 95.08% 95.91% 96.35% 2 94.87% 95.33% 96.22% 96.65% 3 95.03% 95.65% 96.37% 97.3% Table 8 Survival rates for the 1\u20133% improvement groups # 30 50 100 200 1 90.94% 91.44% 92.51% 93.38% 2 90.79% 91.55% 91.43% 93.06% 3 1% 98.88% 96.76% 96.3% Table 9 Survival rates for the above 3% improvement group # 30 50 100 200 1 88.44% 87.99% 87.78% 87.70% 2 87.48% 87.63% 88.28% 87.68% 3 80.95% 78.13% 88.64% 91.37% Building a hospital referral expert system with a Prediction and Optimization-Based Decision Support System algorithm Chih-Lin Chi a \u204e chih-lin-chi@uiowa.edu W. Nick Street b Marcia M. Ward c a Health Informatics Program, 3087 Main Library, The University of Iowa, Iowa City, IA 52242, USA b Department of Management Sciences, S232 Pappajohn Business Building, The University of Iowa, Iowa City, IA 52242, USA c Department of Health Management and Policy, E210 General Hospital, The University of Iowa, Iowa City, IA 52242, USA \u204e Corresponding author. Present address: Department of Management Sciences, S210 Pappajohn Business Building, The University of Iowa, Iowa City, IA 52242, USA. Abstract This study presents a new method for constructing an expert system using a hospital referral problem as an example. Many factors, such as institutional characteristics, patient risks, traveling distance, and chances of survival and complications should be included in the hospital-selection decision. Ideally, each patient should be treated individually, with the decision process including not only their condition but also their beliefs about trade-offs among the desired hospital features. An expert system can help with this complex decision, especially when numerous factors are to be considered. We propose a new method, called the Prediction and Optimization-Based Decision Support System (PODSS) algorithm, which constructs an expert system without an explicit knowledge base. The algorithm obtains knowledge on its own by building machine learning classifiers from a collection of labeled cases. In response to a query, the algorithm gives a customized recommendation, using an optimization step to help the patient maximize the probability of achieving a desired outcome. In this case, the recommended hospital is the optimal solution that maximizes the probability of the desired outcome. With proper formulation, this expert system can combine multiple factors to give hospital-selection decision support at the individual level. Keywords Decision support systems Expert systems Data mining Machine learning Support vector machines Optimization Artificial intelligence Hospital referral Hospital quality 1 Introduction Health care quality is a very important issue in the US. Tens of thousands of Americans die each year, and many more suffer from nonfatal injuries due to errors in the health care system [29]. Several approaches have been directed toward solving this problem. For example, the Health IT Framework [39] proposed several strategies, such as promoting IT adoption, fostering collaborations, and enhancing informed consumer choice of clinicians or institutions. The last strategy is important because variation in the quality of care across health care institutions is large. Choosing a health care institution that has a track record of providing quality care can make the difference between desired outcomes or unsatisfactory outcomes, including death. Experienced physicians gain knowledge about which facilities provide the best care. Physicians typically play a key role in recommending specific hospitals to their patients. Recent efforts by the Centers for Medicare and Medicaid Services and other organizations (e.g., www.healthgrades.com) are making hospital performance data available to physicians and to the public to assist this decision. It has been estimated that about 2600 lives could be saved each year by improved hospital referral [7]. Without analyzing the aggregated measures, taking a different approach, we address the problem of hospital referral by using techniques from the fields of machine learning and knowledge discovery to create automated, personalized hospital referral recommendations that take patient characteristics and preferences into consideration. 1.1 Introduction to the hospital referral problem Hospital referral criteria usually come from research studies and personal experience. Many researchers have examined the relationship between outcomes of hospitals and various institutional characteristics. In particular, a large number of studies have related the volume of hospital surgical procedures to decreased in-hospital mortality [23,25,20,8]. Likewise, teaching hospitals have been shown in several studies to have lower in-hospital mortality [30,4]. Chen et al. [11] concluded that hospitals participating in the JCAHO survey process reported superior quality and outcomes. Elixhauser et al. [17] and others have reported that staffing affects quality. Among these institutional characteristics, the volume of patients or procedures is the most consistent predictor of in-hospital mortality and is broadly used as a hospital-selection criteria. Although the volume-outcome relationship holds for a number of complex surgeries, the magnitude of association varies across procedures [8,23]. Both \u201cpractice makes perfect\u201d and \u201cselective referral\u201d appear to play a role in the volume-outcome relationship [26]. Usually, large institutions have favorable characteristics, such as technical sophistication and more staffing, and they are usually preferred for referral. Although surgical volume is a strong predictor of outcomes, the usage of this indicator is sometimes criticized. Nallamothu et al. [33] explained three reasons that the quality of high-volume hospitals looks better than low-volume ones. First, low-volume hospitals may be less inclined to turn down high-risk cases. Second, large-volume hospitals attract more cases through physician referral or self-referral. Third, patients with opportunity and desire to be referred may be healthier because of several factors. These reasons can help to explain variations in the volume-outcome relationship. Many low-volume centers have very good performance, while some high-volume hospitals have poor performance because volume is an imperfect proxy measure of quality [16,26,8,23]. While many of these studies examine only one or two predictive variables, for practical usage, a good hospital referral decision should consider numerous factors, specifically including geography. Some medical situations are time-critical, and transportation time plays a very important role in outcomes. For patients living in rural and underserved areas, distance is often the most important concern when selecting a hospital. Even for nonemergency conditions, proximity is highly desirable. Therefore, several studies [9,15,16,33] have shown that patients often prefer local higher-risk hospitals over traveling to lower-risk hospitals. Geographic factors may influence the effect of institutional predictors. Ward et al. [41] indicated that the volume threshold suggested by The Leapfrog Group [22] does not perform well in a largely rural state. Other factors, such as a patient\u2019s physical condition, should also be considered. Glance et al. [21] stated that the risk reductions of high- and low-risk patients in different volume hospitals vary. If we considered the distance to an institution, the hospital referral recommendation for a healthier patient and a sicker patient can be dramatically different. A good hospital referral recommendation considers not only institutional but also patient factors, including the travel distance a patient can tolerate, and the patient\u2019s risk factors. Not surprisingly, it is challenging to give hospital-selection advice that considers these multiple complex and interdependent issues. Some practical problems may arise if we consider only institutional factors. For example, should an acute myocardial infarction (AMI) patient go to a mid-size teaching hospital with JCAHO accreditation 20 miles away or a large-volume nonteaching hospital 40 miles away? The Leapfrog Group [22] suggested that a good hospital would have a procedure volume greater than 450 for coronary artery bypass graft (CABG) surgery. Should an 70-year-old AMI patient with congestive heart failure and diabetes who needs an emergency CABG choose a hospital with CABG procedure volume of 300, 30 miles away or another hospital with CABG procedure size of 450, 40 miles away? How about a younger and healthier patient who is not in an emergency situation but needs surgery? Obviously, the answers would be different for different people. It is hard to tell which hospital is better when we consider only institutional characteristics. The hospital referral problem is even more complex if we add other practical concerns, such as insurance coverage and estimated charge during a hospital stay. From the perspective of knowledge management, the above research studies regarding identification of high quality institutional characteristics are called explicit knowledge [1]. Tacit knowledge, such as personal experience and working knowledge, plays an important role in health care settings. Experts can rely on it to derive solutions to their problems. Physicians refer a patient to a specific hospital considering the patient\u2019s physical condition and the travel distance involved. An experienced physician can choose the hospital that minimizes the patient\u2019s risk. Such a customized recommendation has a higher chance of being accepted by a patient. The purpose of this project is to build an expert system that can assist such a customized hospital-selection decision. An expert system is a computer program that can make inferences and give conclusions using the knowledge of specialists. A knowledge-based system (KBS) is an early and well-known type of expert system. The knowledge typically exists in the form of atomic facts about the domain of interest and rules for inferring new facts, but may also be in the form of graphs, trees, or networks. They are stored in a specific location called a knowledge base. The KBS uses an inference engine and the knowledge base to make inferences. MYCIN [38] was one of the earliest knowledge-based expert systems and can provide diagnosis and therapy recommendations. The knowledge in MYCIN is stored in the form of rules. These rules were derived from the knowledge of infectious disease experts. The process of transforming knowledge from human to machine is called knowledge acquisition, and is time- and labor-intensive [19]. In addition, maintaining the knowledge base is very difficult [42,13]. Case-based reasoning [43] is another technique for knowledge acquisition. This method does not require a knowledge base. Instead, all previously solved cases are stored in one place, called the case library, which is the knowledge source. Typically, a case comprises the problem, the solution, and the outcome. To obtain the solution for a new case, one simply finds the case with the most similar problem in the case library. The proposed solution is then the same as was observed in the retrieved case. Machine learning methods are well known for knowledge discovery. They can help to elicit knowledge (explicit and tacit) [10,34] from data and generalize that knowledge to new, previously unseen cases. Machine learning methods have been successfully applied in several medical areas such as classification, diagnosis, and prognosis. In general, these methods can be classified as supervised or unsupervised learning methods. Supervised learning methods learn a function that maps features (the independent variables used to represent a case) to the corresponding labels (dependent variables, typically outcomes). The labeled samples for the unsupervised learning methods are not necessary because these methods cluster samples based on similarity of variables. In this project, we are only interested in supervised learning methods. Supervised learning methods such as C4.5 [37] that learn patterns in the form of rules provide a partially automatic method for knowledge acquisition in traditional expert systems. However, much time and labor may still be required to construct and maintain the knowledge base. Other supervised learning methods induce patterns in the form of mathematical functions, which can be either linear or nonlinear. Knowledge is therefore encoded in these mathematical functions. They can be applied very successfully for their original design purpose, e.g., classification. However, methods for applying captured knowledge in other purposes are limited [5]. It is difficult to use the knowledge in a mathematical function to assist in an action decision, such as a customized hospital-selection decision. This paper proposes a new method, called Prediction and Optimization-Based Decision Support System (PODSS) to extract knowledge in the form of the mathematical function from a classifier and apply optimization methods to use this knowledge source to solve the complex hospital referral problem. Similar to case-based reasoning, the knowledge source is data instead of a knowledge base. The difference is that the knowledge has been compiled in the form of mathematical functions. In addition, we obtain the action solution based on optimization methods instead of similarity. 1.2 An introduction to the PODSS algorithm The purpose of the PODSS algorithm is to generate a suggested action that leads to a higher probability of the desired outcome. This type of problem can be found in many domains. Stock investors choose stocks carefully in order to maximize profit and minimize risk. Marketing managers design strategies to maximize their product sales under budgetary constraints. Experts are often consulted because they know how to maximize the probability of the desired results while considering multiple and sometimes competing factors. The proposed algorithm can simulate these experts by recommending actions that maximize the probability of the desired result. There are many approaches that can help to understand factors (variables) that lead to a desired outcome. Careful use of regression techniques can allow us to determine the relative importance of variables by observing the sign and magnitude of coefficients (e.g., [2]). Sensitivity analysis provides a way to observe how sensitive a result is to variations in the variables of interest, thus determining the importance of these variables (e.g., [6]). These methods can identify important factors for achieving a desired outcome. However, they cannot tell us what to do, how to do it, and how to resolve trade-offs among alternatives. For example, the above methods can identify which interventions are most likely to cure a disease, however, it usually takes an experienced doctor to know how to choose among alternative interventions, and how to tailor the intervention to the needs of a specific patient. The proposed algorithm is a decision tool that can provide suggestions by utilizing captured knowledge and optimizing the effectiveness of the chosen action. This algorithm can recommend an action decision based on multiple variables and the interactions among them. Fig. 1 illustrates the process of constructing and using this decision support tool. Examples with known outcomes are used to capture knowledge in the form of a predictive model (classifier) and a validation map that estimates the probability of the desired outcome for any patient/action pair. A query will activate an optimization method which finds the best course of action using the captured knowledge, feasible choices, and information provided about the patient. Communication between the decision support system and a user is required. The user provides information regarding a patient\u2019s characteristics and the maximal distance to a hospital that the patient can tolerate, and then the system can generate a customized hospital choice. This customized choice not only satisfies the given maximum tolerated distance but also identifies the hospital with the highest probability of the desired outcome. The maximum tolerated distance parameter should be decided by a patient and his/her doctor to ensure that the travel distance does not become a risk factor and is acceptable to the patient. For some healthier patients, this parameter value can be high. For an emergency case, this parameter must be very low. Travel distance and survival probability are two important targets. The trade-off between different objectives can be addressed explicitly when the hospital choice decision is customized. In this study, the problem is first formulated as single-objective optimization, and can be solved by an exhaustive search because of the small solution space (only the number of hospitals). In this optimization, a query provides a patient\u2019s characteristics to the system, such as age, admission type, comorbidities, and the maximum tolerated distance. The optimization process will combine the provided information with the captured knowledge to generate a customized hospital selection. The objective of the knowledge extraction tool is to find a hospital with the highest survival probability under the constraint of the maximum tolerated distance to a hospital. If we also want to consider other issues in the hospital-selection decision, such as reduced likelihood of complications, then the problem is formulated as multi-objective optimization. We present the entire solution space to the user in an intuitive format, so that the relative importance among the targets can be decided by the user. Due to the small solution space, presenting the predicted outcomes of each hospital in an organized way to a user is more effective and efficient than having the system decide on a single optimal choice. The rest of the article is organized as follows. In Section 2, we discuss how to capture knowledge, transform captured knowledge into an objective function, apply the captured knowledge, and use the algorithm in a hospital referral problem. We demonstrate both single- and multi-objective optimization examples. Section 3 discusses experimental results of these examples, and introduces an indirect evaluation method for determining the effectiveness of the method. Discussion of the computational experiments and possible extensions of this algorithm to other problems are included in Section 4. 2 Methods There are a series of stages in the PODSS algorithm. As illustrated in Fig. 1, the algorithm relies on classifiers to capture knowledge. This step is the same as training a prediction model. Independent and dependent variables are required to train the model. The output score of the prediction model, which we convert to a probability, can be interpreted as the confidence level of the desired class prediction. The purpose of optimization is to maximize the confidence level of the desired class label. 2.1 Dataset design Our approach depends on the problem having two distinct types of independent variables. The first type is uncontrollable (unchangeable) variables. The values of these variables are given and cannot be changed. For example, patient variables such as demographic data, medical test results, diagnostic results, admission type, surgery status, comorbidity scores [14], and payment type are uncontrollable variables in this study. The second type is controllable (changeable) variables, whose values can be changed. The recommendation can be made based on these variables. In our application, each set of values of these variables describes a hospital. These hospital descriptive variables are owner type, hospital location, JCAHO accreditation, total number of surgical operations, AMI patient discharge volume, and the volume of CABG surgeries. Table 1 summarizes the variables used for classifier training. Variables 1\u20136 relate to the patient\u2019s characteristics and are uncontrollable; variables 7\u201313 relate to the hospital\u2019s characteristics and are controllable. In the knowledge application stage, each type of variable plays a different role in the optimization process. The first set is constant and is provided by a user when querying. The solution variables comprise the second set. The optimal solution is the hospital with the most favorable descriptive variables that results in the highest optimum value (desired outcome with the highest probability). In a nonlinear model, the optimal solution may depend on the given uncontrollable variables due to variable interaction. The 2004 State Inpatient Dataset (SID) for Iowa from the Agency for Healthcare Research and Quality (AHRQ) Healthcare Cost and Utilization Project (HCUP) [24] was used in our study. There are almost 360,000 discharge records in the SID. For this project we chose to build a hospital referral algorithm for patients with a principal diagnosis of acute myocardial infarction (AMI) with ICD-9-CM codes of 410.01 to 410.91. We selected AMI for several reasons. First, it is relatively common and easy to identify in the datasets. Second, the outcomes of interest, including mortality, are also relatively common which facilitated model building. Third, AMI in-hospital mortality is being introduced by the Centers for Medicare and Medicaid Services as a required publicly reported performance indicator for all hospitals, thus the algorithm described here could find application in the near future. While we use AMI for this first demonstration, the algorithm can be easily modified to work with nearly any disease of interest where referral is an issue. The SID can be linked to hospital descriptive data from the American Hospital Association (AHA) [3] by a hospital identification number. There are 116 nonfederal acute-care hospitals in Iowa. The SID contains zip codes for each patient and hospital, which permit the Euclidean distance between a patient and any hospital to be computed. The location (longitudinal and latitudinal) data were retrieved from http://www.brainyzip.com/. The distance estimation from MapQuest or Google Maps could be used to compute road distance estimation. Road distance estimation more accurately represents travel distance and is longer than Euclidean distance [27]. We choose the Euclidean distance estimation in our study because it was readily available for each patient/hospital pairing, whereby road distance estimation is not, and the difference between the two methods is relatively consistent in Midwestern states. Four datasets were used in this study. The labels of the first three datasets are patients\u2019 in-hospital survival status, and the labels of the fourth dataset are hospital-acquired complication status. The complication labels are identified using ICD-9-CM codes of complication defined by Elixhauser and colleagues [18]. The first dataset includes all AMI patients whether surgery is performed or not. The second dataset is designed for AMI patients who do not have any surgery. In this dataset, patients with any surgical Diagnosis Related Groups (DRG) were excluded. The third dataset includes only AMI patients who have coronary artery bypass graft (CABG) surgeries (ICD-9 36.10 to 36.19). The last dataset contains the same patients as the third one but with a different label type. The data show only 12 hospitals in Iowa perform CABG surgeries. Thus, hospital selection is limited to these 12 hospitals in the third and fourth dataset. The size and percentage of the desired outcome label in each dataset are shown in Table 2 . Dependent variables, such as survival status and complication status, are usually used to find the predictors of hospital quality. This study shows two types of expert system based on the number of desired targets. The first is called single-objective optimization, in which we use a classifier to find the relationship between independent variables and patients\u2019 survival status. Datasets 1, 2, and 3 are used in this study. The second is called multi-objective optimization. In addition to the survival status classifier, we add a second classifier for the complication status. The hospital-acquired complication is a very important concern to surgical patients, and the third and fourth datasets are used in this study. 2.2 Building a predictive model Knowledge capturing in our model relies on supervised learning, which learns the relationship between independent and dependent variables. We used support vector machines (SVMs) [40] as the classifier to construct a separating surface between point sets of different classes. There are an infinite number of surfaces that can perform the separation. In order to generalize to unseen points well, the SVM classifier finds the separating surface with the greatest margin, or the distance from a point to the surface, during the training process. Fig. 2 shows an example of a linear separating surface. In our example, the surface separates negative from positive cases, with the negative area to the left of the plane (d(x)<0), and the positive region to the right (d(x)>0). If a test case falls to the right of the separation surface, the predicted result will be positive. In general, there are some points that cannot be separated and are classified incorrectly. A high prediction score means a high probability that a patient will have the desired outcome. In other words, the confidence level of the desired outcome is high. On the other hand, if we can increase a data point\u2019s predictive score by changing certain independent variables (controllable), we may improve the probability to have the desired outcome. For example, the probability of point A\u2217 corresponding to a positive outcome is higher than A+, and the probability of point A+ to be positive is higher than A\u2212 (Fig. 2). In many problems, linear surfaces are insufficiently flexible to separate the point sets well. Projecting the points to a higher-dimensional space and building a linear surface in this space can solve this problem, but complicates the learning process. For example, consider a problem in which the input space has three dimensions x 1, x 2, and x 3. We could map them into a higher-dimensional feature space x 1 , x 2 , x 3 , x 1 2 , x 1 x 2 , x 2 x 3 , x 1 x 3 , x 2 2 , x 3 2 \u2026 for model construction. SVMs use a kernel function, K(x, x i ), to avoid the need to explicitly perform such a mapping. The radial basis function (RBF) kernel was used in our experiments. Fig. 2 shows an example of a simple separating hyperplane, d(x)=0, where d ( x ) = \u2211 i = 1 n y i \u03b1 i K ( x , x i ) + b . \u03b1 is a vector of the Lagrange multipliers, y is the class label, and b is the bias term. By using a nonlinear kernel function K, the separation can be performed in a high-dimensional space without explicitly performing the projection. SVMs have been shown to perform extremely well on a wide range of problems and are generally considered to be one of the best classification algorithms. We built a predictive model using independent variables of patients\u2019 characteristics (or uncontrollable variables), x 1i i \u2208patients, and their chosen hospitals\u2019 descriptive features (or controllable variables), x 2 j \u00af j \u2208 hospitals , to predict whether or not a patient will survive (or be free from complication) during a hospital stay. Note that we use the notation j \u00af to indicate the index of the hospital actually chosen by the particular patient. After training, the decision function is represented as d(x 1 \u222a x 2). As in Fig. 2, the value of d can decide the confidence level of the desired outcome. Our goal is to increase this confidence level. 2.3 Extracting recommendation information In this problem, we assume that the only way to improve a patient\u2019s expected outcome is to change hospitals, and hence, change the values of x 2. Optimization methods provide a scientific way to improve the confidence level to the desired outcome and find the values of x 2. The decision function can naturally become the source of the objective function, since we want to maximize the confidence that the desired outcome occurs. The idea of using optimization is intuitive. In real life, the probability of survival in different hospitals for the same patient will vary. After training, a classifier can estimate the score of survival or freedom-from-complication (FFC) for a query patient in each hospital by an evaluation function. A good hospital will result in a higher predictive score, so the evaluation function value will be higher. In other words, the optimization method can help to identify such a hospital. The mathematical model can represent the hospital referral scenario in real life, since a high quality institution can prevent medical error and increase the chance of survival (or FFC). The survival function optimization is formulated as follows: (1) maximize x 2 j d ( x 1 \u222a x 2 j ) subject to dist ( j , x ) \u2a7d DL x 2 j \u2208 X 2 where x 1 is the characteristic variables of the query patient, and x 2j is the set of descriptive variables describing the hospital j. dist(j, x) is the Euclidean distance from the patient to the hospital j. DL is the maximum tolerated traveling distance parameter given by the user. The purpose of the optimization process is to find the hospital j \u02c6 with the most favorable descriptive features x 2 j \u02c6 , such that the objective function d ( x 1 \u222a x 2 j \u02c6 ) is the maximum. Although a high quality hospital can improve a patient\u2019s survival probability, the effect is sometimes limited. The physical condition, x 1, of each patient is different, and is a more important factor in deciding the survival probability. There are three possible situations when patients change their original chosen hospital to the referred one: 1. Patients may move from predicted negative to less negative (less predicted probability of death). 2. Patients may move from predicted negative to positive (predicted death to predicted survival). 3. Patients may move from predicted positive to more positive (increasing the probability of the prediction of survival). We note that in the general case that the problem in Eq. (1) might be formulated as a linear, nonlinear, or mixed integer problem to reflect the nature of the variables x 2. These methods can construct the characteristic of a \u201cperfect\u201d hospital. However, in our application this approach makes no sense, since such a hospital does not necessarily exist. To maintain feasibility, the optimization should not create the value of x 2j . Instead, we should evaluate the value of d(x 1 \u222a x 2j ) from all hospitals j within the distance limit, and find one with the highest value of d. Due to the small search space in the hospital referral problem, exhaustive search of the possible hospitals is fast enough to examine the evaluation function value in each hospital j within the given distance limit. To demonstrate the flexibility of our mathematical formulation, we move the distance constraint into the objective function. In the multi-objective optimization formulation (2) maximize x 2 j D ( x ) = ( d 1 ( x ) , d 2 ( x ) , - d 3 ( x ) ) subject to x 2 j \u2208 X 2 x = x 1 \u222a x 2j , d 1 represents the survival decision function, d 2 represents the FFC decision function, and d 3 is the Euclidean distance function, which is the same as dist(j, x). The goal in this formulation is to optimize the three desired objectives. These objectives can be combined in several ways, such as additive, multiplicative, and multi-linear forms [28]. Each objective should be assigned a weight when combining. The relative importance (weight) of each objective can be decided based on a user\u2019s consideration. In order to give customized decision support, the system should not decide the balance of weight for the user. For this problem, we present the individualized solution space to the query patient rather than determine a single choice for the patient. With sufficient information and visual presentation, a patient can decide his/her best solution easily. The solution space contains information of the query patient\u2019s survival probability, the FFC probability, and the distance to each hospital. 2.4 Building a validation map The SVM scores are not probabilities. Although we can improve the predictive score by recommending a hospital to the query patient, we still want to understand how much the survival probability can be improved. The validation method is also a problem. The dataset recorded historical events of patients\u2019 characteristics, the hospital they chose, and the outcomes. We have no way to know if sending those patients to the recommended hospitals would have changed their results. However, we can observe the distribution of predictive scores corresponding to survival by learning the relationship between class labels and predictive scores (Eq. (3)). Platt\u2019s calibration method [36] provides a computational solution to this problem. This method provides an approach to map SVM scores, d, into probabilities, p, through a sigmoid function (3) P ( y = + 1 | d ) = 1 1 + exp ( Ad ( x ) + B ) The parameters A and B are found from the negative log likelihood of the data, which is a cross-entropy error function (4) minimize A , B \u2211 i t i log ( p i ) + ( 1 - t i ) log ( 1 - p i ) where p i =1/(1+exp(Ad(x i )+ B)) and t i = N + + 1 N + + 2 , if y i = + 1 1 N - + 2 , if y i = - 1 We used a variation of Platt\u2019s method [32] which avoids numerical difficulties to build a smooth mapping from the SVM score to a posterior probability of survival. Niculescu-Mizil and Caruana [35] tested several classifiers using Platt\u2019s calibration and isotonic regression [44] and indicate that SVM is one of the best methods to predict probabilities after calibration. Using the calibration function we can construct a validation map based on the relationship between scores and probabilities. Fig. 3 shows the validation maps for the four datasets. The predicted survival probability of a patient in each hospital can be found on this map. For example, in the all-AMI-patients experiment (Fig. 3a), the predicted survival probability of a patient may improve about 5% if the query patient\u2019s predictive score improves from 0 to 1 by switching to the recommended hospital. Fig. 4 shows pseudocode for the PODSS algorithm in knowledge capturing. Figs. 5 and 6 show the algorithms for single- and multi-objective optimization, respectively. The main purpose of the knowledge capturing (training) is to learn a decision function and probability transfer function from the data. In other words, knowledge is stored in these functions. In the single-objective optimization, given the query patient\u2019s characteristic variables and the maximum tolerated distance, the optimization will find the hospital with the highest survival probability satisfying the distance constraint. In the multi-objective optimization, a patient does not need to give the maximum tolerated distance. Instead, the algorithm will give information including the survival probability, the freedom-from-complication probability, and the distance to each hospital. They comprise the patient-specific information, and can be expressed visually, as a consumer report. If there are two identical patients, their hospital selection could be different because their ideas of importance of each objective may vary. 3 Results Table 2 shows class distributions of the four datasets. The desired class (positive) is the majority in all datasets. Directly modeling these sets results in highly accurate but useless classifiers simply predicting all (or nearly all) points to be positive. We used over-sampling of the minority class [31] to balance each dataset according to the proportion of positive to negative classes. For example, the survival (positive class) probability is 95% in the CABG survival experiment, so we used the ratio 1/19 to balance positive and negative classes. We compare the mean square error of the probabilities generated by calibrated SVM with those created using logistic regression in Table 3 . This table shows that the mean square errors of the calibrated SVM are significantly lower than logistic regression in all experiments. In the following sections, we present the results using single- and multi-objective optimization. In actual application of the single-objective optimization, the maximum tolerated distance should be decided by a user, and the returned optimal solution is customized. We varied this parameter in order to present results. In the application of multi-objective optimization, a user does not need to give a parameter. Instead, the user needs to choose the optimal solution in the solution space considering three desired targets. Similar to the single-objective optimization, we varied the distance target and discuss the user\u2019s decision considering the other two desired targets. 3.1 Single-objective optimization Table 4 summarizes the results of three analyses using (1) all AMI patients, (2) nonsurgery patients, and (3) CABG patients. In each analysis, the table shows the average miles between patients and recommended hospitals, the average predicted hospital-stay survival probabilities, and the average predicted SVM scores. The hospital originally chosen by each patient is referred to as the original choice. From this column, we can observe that the average distance between CABG patients and the hospital they choose is longer than the other datasets because of the limited number of hospitals that can perform CABG surgery. For the recommendation portion, new hospitals are recommended when we gave 30, 50, 100, and 200 mile maximum distance parameters. The average results of improvement can be observed in this table. For example, the average survival probability can be improved from 92.8% to 93.6% and the average distance decreased from 20 miles to 17 miles when we give a 30-mile search limit in analysis 1. The average probability will become 94.2% if the maximum distance is 50 miles. This is because the optimization method can find a higher optimal predicted value when we change the restriction of distance parameter from 30 to 50 miles. Patients in analyses 1 and 2 can choose among 116 hospitals in Iowa, but patients in analysis 3 can choose among only 12 hospitals because not all hospitals can perform CABG surgeries. Usually hospitals that can perform CABG surgery are larger and have more patient visits than other hospitals and the survival probabilities of patients in these hospitals are higher than average. Therefore, not surprisingly, the actual survival probabilities of patients who stay in the hospitals that can perform CABG surgeries in \u2018AMI all\u2019 and \u2018nonsurgery\u2019 datasets are 94.29% and 95.11%, which are higher than the survival probabilities, 93.0% and 93.4%, of all patients (Table 2). Further, the survival probabilities of the CABG surgery are usually very high. The physical condition of each patient is the most important factor in predicting survival. Therefore, it is not easy to improve survival probabilities for the CABG patients when they change hospitals. The average survival probabilities only improve from 95% to 97.2% when the given distance is 200 miles although the improvement of SVM scores is higher than the other two datasets. The shape of each graph in Fig. 3 can explain why the probability in (c) is harder to improve than (a and b). Table 5 shows the statistical test examining the true outcome and all estimated outcomes, which includes the original choice and the recommendations. They are compared using two independent samples with unequal variance t-test. For the true outcome vs. original choice, we examined whether or not they are different. For the true outcome vs. recommendation, the test examined the improvement of survival probability. As expected, the test results show that the predicted survival probabilities of all original choices are not significantly different from the true outcomes. In addition, the improvement works well for datasets 1 and 2, but not for datasets 3 and 4 unless the given distance is long enough. For all analyses, the degree of significance increases with the given distance. The retrospective data do not allow a direct comparison between true outcomes of the original choice and the recommendation. However, we can explore the classification power of recommendations by examining the actual outcomes of the predictive groups (defined in Table 6 ), as shown in Tables 7\u20139 . Table 6 shows the frequency in each improvement group for the three analyses with mileage limits of 30, 50, 100, and 200 miles. The second column indicates the improvement percentage after changing to the recommended hospital. For example, in analysis 1, given 30 miles, the predicted survival probabilities of recommended hospitals for 775 patients are lower than the hospitals originally chosen by patients. These patients are in the \u2018negative improvement\u2019 group. The recommended hospitals only improve 0\u20131% for 3838 patients. If the predicted survival probabilities can classify the improvement well, we can expect three things: (1) the actual survival probability for the patients who stay in the \u2018negative improvement\u2019 or \u20180\u20131% improvement\u2019 groups will be higher than the average of all patients, (2) if patients still stay in the \u2018negative improvement\u2019 or \u20180 \u20131% improvement\u2019 groups when we increase the given distance, the actual survival probability of these patients is higher than those with a shorter distance parameter, and (3) patients in a larger percentage-improvement group have a lower actual survival probability, and therefore more room for improvement. Table 7 shows the actual survival probability of patients in the \u2018negative\u20131% improvement\u2019 (combined \u2018negative improvement\u2019 and \u20180 \u20131% improvement\u2019) group in the three analyses. The average actual survival probabilities of all patients in the three analyses are 93.0%, 93.4%, and 95.0% (Table 2). For point (1) above, we find that the survival probabilities in Table 7 are all higher than that in Table 2. The recommended hospital is x 2 j \u02c6 , which attains the optimal value, d ( x 1 \u222a x 2 j \u02c6 ) . If the improvement of the predicted survival probability of the recommended hospital is small or negative, the value in the objective function of the hospital originally chosen by a patient is close to or larger than the optimal value, d ( x 1 \u222a x 2 j \u02c6 ) . In other words, this patient already stayed in a hospital close to or better than the recommended hospital. Therefore, the actual survival probabilities of patients who are already in good hospitals would be higher than average, and little improvement is possible. For the second point, we can find that the survival probabilities for patients in the \u2018negative\u20131% improvement\u2019 group increase with the given distance. When we gradually improve the optimal value (finding new recommended hospital) by increasing the search distance limit, the new optimal value may be much larger than the value in the objective function of the originally chosen hospital. Therefore, these patients will move to other improvement groups. The hospitals for those patients who still stay in the \u2018negative\u20131% improvement\u2019 are still close to or better than the newly recommended hospital. Therefore, the actual survival probability for these patients would be better than all patients or previous patients staying in the \u2018negative\u20131% improvement\u2019 group. For the third point, we can compare the results from Tables 7\u20139. Similar to Tables 7, Tables 8 and 9 are actual survival probabilities of patients in \u20181\u20133% improvement\u2019 group and \u2018more than 3% improvement\u2019 group (combined 3\u20135%, 5\u20137%, 7\u20139%, and 9-% improvement groups) from all analyses. These tables show that actual survival probabilities are lower for patients in the higher percentage-improvement groups. The only exception is analysis 3 in Table 8. This variation may result from the small sample. For example, the actual survival probability is 94.41% in the \u2018negative\u20131% improvement\u2019 group (given 30 miles in Table 7), 90.94% in \u20181\u20133% improvement\u2019 group (given 30 miles in Table 8), and 88.44% in the \u2018above 3% improvement\u2019 group (given 30 miles in Table 9). The explanation here is similar to the first point. Instead of staying in hospitals with high survival rates, patients stayed in hospitals with relatively poor ones. The value in the objective function of the hospital originally chosen by a patient in a high-probability improvement group is less than the optimal value, d ( x 1 \u222a x 2 j \u02c6 ) . The difference will become larger for patients in a higher probability improvement group. In other words, the hospitals originally chosen by patients who are in a higher probability improvement group are worse than hospitals chosen by patients in a lower probability improvement group. Therefore, the actual survival probability of patients in a higher improvement group would be smaller than those in lower probability improvement groups. In Table 6, we can also observe that the frequency is moving toward higher improvement when we increase the given distance. Most cases in analysis 3 improve less than 1% unless we allow a maximum travel distance limit greater than 100 miles. 3.2 Multi-objective optimization This analysis uses visual presentation of a three-dimensional solution space for each query patient. The balance of the three targets, survival probability, FFC probability, and distance can be easily decided by a user through the three-dimensional solution space. We present a user\u2019s decision of hospital choice considering survival and FFC in different distances, and decide the best balance of the three targets according to the patient\u2019s degree of emergency. We use this case study to demonstrate how this expert system can help a user to choose a hospital. Then, we show group results for survival and FFC. Fig. 7 shows a case study example using this expert system. The location (x, y) represents the FFC (x) and survival (y) probabilities of a hospital. This study picks a sample patient, P299, to demonstrate how the system can provide decision support. Hospital 1 is P299\u2019s originally chosen hospital (Fig. 7a) with (x, y)=(0.803,0.915), and the distance to this hospital is about 7 miles. Although this hospital has a large discharge and CABG surgery volume in a highly metropolitan area, both estimated probabilities are below average. There are two possible factors that explain the low probabilities when we consider only the patient\u2019s characteristic: (1) P299 is 70-year-old, and (2) P299 is an emergency admission case. Both probabilities may improve when a hospital with higher survival rates is chosen. After entering the patient\u2019s characteristic data and 30 miles as the maximum distance parameter, the system shows three hospitals within this distance limit (Fig. 7b). Hospital 3 is a good choice not only because of higher survival and FFC probabilities but also because the traveling distance is only 8 miles. Searching ranges of 30 and 50 miles results in the same candidate hospitals, but 100 miles will generate five hospital candidates (Fig. 7c). There is a trade-off between FFC and survival probabilities in deciding between hospitals 3 and 5. Hospital 5 is in a rural area, and the volume is less than both hospitals 1 and 3. However, our 2004 SID data show that this hospital did not have any in-hospital deaths for AMI patients who had CABG surgery. The average age of these patients (72.7 years) is higher than hospitals 1 (67.2 years) and 3 (65.6 years). In addition, the average comorbidity score of these patients (0.83) is higher than hospitals 1 (0.47) and 3 (0.61). The frequency of emergency admissions (11/13) is also relatively higher than hospitals 1 (32/57) and 3 (50/64). These results indicate that although hospital 5 is located in a rural area, sicker patients do well. Although hospital 5 shows a higher survival probability than hospital 3, the FFC probability is lower than hospital 3. The patient may still prefer hospital 3 because hospital 5 is 75 miles away. Finally, a search range of 200 miles shows all hospitals that can perform CABG surgeries in Iowa. Both probabilities of hospital 12 are the highest. However, distance is the critical issue. The originally chosen hospital is the closest one. If the patient can be admitted to hospital 3, the distance is also small and both probabilities will increase around 1\u20131.5%. For other cases, such as nonemergencies, patients may have more options. Fig. 8 shows the improvement trend of all query patients. The distance parameters are 30, 50, 100, and 200 miles. The balance of survival and FFC should be decided by a query patient. In order to present the results clearly in two dimensions, we show the maximum tolerated distance parameter together with either the FFC or the survival probability in Fig. 8. Fig. 8a shows both probabilities for hospitals originally chosen by patients. In other words, each point represents a predicted result for a patient. When patients switch to the recommended hospital, the results will become (b\u2013e). The average distances of FFC in each graph are 29, 26, 31, 52, and 89 miles in the order of (a\u2013e), and the expected FFC probabilities are 81.8%, 81.8%, 82%, 82.5%, and 82.9%, respectively. The expected probability and average distance of survival graphs are in Table 4 (analysis 3). Although there is a distance constraint, some patients live very far away from any hospital, so that there are no hospitals recommended for them. For these cases, the recommended hospitals are the ones that least violate the distance constraint. These points appear at Fig. 8b\u2013d. The distances of these points are higher than the distance parameter. The movement of points in Fig. 8a and b shows an interesting pattern. Many points move toward the top left. The left side indicates that the system can find local hospitals within the distance parameter for patients who originally chose further ones. The top indicates that survival or FFC probabilities are improved. The trend in Fig. 8a and b shows that the optimal solution can identify local hospitals that improve both survival and FFC probabilities while decreasing patients\u2019 traveling distance. The same situation can also be observed in Table 4. Both probabilities in Fig. 8b\u2013e show that points move toward the top-right direction when the distance parameter increases. In other words, both probabilities of most patients can improve if they are capable of traveling a longer distance. 4 Discussion and conclusions We proposed a new data mining process to construct an expert system. Each element in this process (see Fig. 1) has been widely used previously in several areas. For example, classifiers can facilitate decision support for diagnosis and optimizers can help to find the shortest route for vehicle transportation. The main contribution of this project is the development and exploration of a new method for constructing an expert system using this process. This process extends the utility of a classifier, whose captured pattern is in the form of a mathematical formulation (i.e., decision function). This type of pattern is directly transformed into the knowledge source of an expert system (equivalent to the knowledge base of a rule-based expert system). The process also relies on an optimizer to choose an action that maximizes the confidence in the desired outcome (equivalent to the inference engine that maps a query into an action). The structure of this process has several advantages. First, the construction and maintenance are automatic. Thus, compared to a rule-based system, the cost is low, as the \u201cknowledge acquisition\u201d phase requires only the collection of a set of labeled examples. Second, this process enables an expert system to use a nonlinear pattern that provides a flexible estimation of the real situation; thus, the retrieved solution can be closer to the truth. Third, the recommendation is the best solution that can be found. In a solution space, there may be several applicable solutions that can improve outcome, and the recommendation is the one that can improve it most. The PODSS process is flexible regarding potential applications. The outcome is not limited to being binary, as numeric or censored outcomes may also be included. The number of outcomes can be either single or multiple. The number of solutions can be a single best one or multiple ones with ranked scores. In the optimization formulation, a decision function is not necessarily the objective function. The decision function may become a constraint because often users want to force one target to an acceptable level and find the optimal result of other targets within the acceptable level of the first target. For example, in a cost-effectiveness recommendation, the goal is to minimize cost and ensure that effectiveness is acceptable. In addition, the optimization formulation structure allows incorporating expert knowledge into the recommendation. We can hard-code the expert knowledge in either constraints or the objective function to avoid or enforce some real-life requirement, such as drug-interaction avoidance. The demonstrated hospital referral application has shown this example. Each query user can be seen as an expert to decide the patient\u2019s maximum-tolerated distance, and this expert knowledge becomes a constraint. The choice of optimization methods depends on the problem. Expert system problems can be classified as selection or construction [12]. The hospital referral problem, described here, is a selection problem, which selects the most appropriate item from a list. In the proposed process, the selection problem can be solved by several searching methods, e.g., exhaustive search, simulated annealing, or genetic algorithms. The last two searching methods can solve hard problems (i.e., an extensive range of possible choices). The number of possible choices in this problem is very small; therefore, the easiest one, exhaustive search, is sufficient for the optimizer. The construction problem is more complex. Unlike selection, the solution must be constructed from scratch with appropriate constraints to avoid generating an impractical recommendation. Several mathematical programming methods provide the way to construct a solution, e.g., linear or nonlinear programming can decide the values of the decision variables. We do not use an explicit optimization step in the multi-objective problems. Our analysis could simply assign a weight to each target and choose an optimal solution, but the balance of weights would depend on the preferences of the user. Therefore, we change the role of users to \u201coptimizers\u201d, who decide the user-specific balance of weight, which in turn generate the best solution for each user. We propose two types of optimization formulation. The first type simply recommends the best solution under certain constraints provided by a user. The maximum-tolerated distance is a parameter quantifying the user\u2019s tolerance for travel based on the disease urgency and the accessibility to care institutions. When there are more than two targets, we merely present the solution space to a user. The optimal solution in the second type of formulation is directly searched by the user instead of using optimization methods. The abundant information of each hospital from the solution space and the visual presentation can aid a user\u2019s decision. Each user\u2019s feeling about these concerns is different. Some users may choose a local institution, but others may want to have the best institution using the fastest vehicle, such as helicopter emergency medical service transport. Sometimes, there may be a trade-off among targets, and the user is the one who can decide the individualized optimal solution. Users are not limited to patients. They can be the physician, medical staff, family or other people who know the patient well. It is interesting to note that the globally best hospital can be different for two patients in the same locale because of differences in patient characteristics. For example, there are three globally best hospitals (instead of one) in Iowa (the conclusions from survival and freedom-from-complications (FFC) classifiers are the same). This result was generated from giving a large search range, such as 300 miles, for all patients to the system. Hospitals 3, 5, and 12 (see Fig. 7) are the three globally best ones. Hospitals 3 and 12 are large hospitals, but hospital 5 is a relatively small hospital. This result may be surprising because many people would assume that large hospitals provide better outcomes for AMI patients. However, the data show that this is not uniformly the case. These results indicate that the recommendation of hospital selection includes nonlinear relationships and variable interactions. Our classifiers use the RBF kernel function. When we perform optimization, the optimal solution is generated with a nonlinear decision function. One possible explanation to the different best hospitals is that each of the three hospitals has their unique advantage. The different advantages provide the different best fit for each patient. This point can become an interesting research topic. Compared to the traditional approach of considering only institutional factors, the individualized hospital referral expert system has several advantages. First, several factors, including a patient\u2019s characteristics and institutional factors, are interactively combined into a number. For example, the probability estimation considers a patient\u2019s age, comorbidity, and a hospital\u2019s discharge and surgical volume, JCAHO status, etc. The number is comprehensive in that it factors in multiple considerations, yet it provides a simple presentation of recommendations. Second, the probabilities of survival or FFC are estimated by RBF SVM with these multiple considerations. RBF SVM is a very flexible technique and hence potentially more accurate than other methods. Third, once this system structure has been built, the construction process for other diseases is almost automatic. As described previously, the knowledge elicitation process is replaced by classifier training, and the optimizer is a well-developed technique for inferring advice. To construct hospital referral expert systems for other diseases, we simply need to provide the dataset to the system, decide on an appropriate set of variables for training (or choose them automatically via feature selection), and determine the disease of interest (giving ICD-9-CM codes to the system). Fourth, the proposed system can facilitate the decision-making process for problems involving the trade-off of multiple objectives. It is difficult to select a hospital when considering several trade-off targets using traditional methods. The comprehensive estimation can provide more choices. For example, the hospital with the most favorable characteristics may be very far away. The proposed system may allow users to find some closer hospitals whose effectiveness is very close to or even better than the hospital with the most favorable characteristics. The trade-off problem between distance and effectiveness can be alleviated because patients have more information for decision making. Fifth, the system may help to allocate resources better. A recommendation considering only institutional factors would encourage patients to flock to certain hospitals. The hospital recommendation of the proposed system is much more diverse because of nonlinearity and communication with users. For example, for patients that only consider local hospitals (assuming 30 miles is the limit), they can still find hospitals that improve the estimated survival probabilities (Table 4). On the other hand, the patients that can tolerate a longer travel distance will have more options. The decisions are even more diverse when we include consideration of complication rates. The overall result of the widespread use of such a system would be more patients going to hospitals that are better for their specific situation. Hospital referral is important when the disease outcome has a large influence on a patient, e.g., survival, surgery complications, length of stay, quality of life. A high quality hospital may improve the probabilities of good outcomes. An extreme example is liver transplant, in which case patients may highly prefer the best hospital. We selected AMI as the example because this disease is often used as a quality indicator. Survival probability is not the only concern for this disease because the survival probability is universally high; therefore, the balance of survival, freedom-from-complications, and distance is less likely to be uniform. In other words, the disease selection of AMI can help to demonstrate customization decisions that rely on communication with users. Although we chose AMI as an example, in the real situation, patients who have less acute diseases may have more time to consider hospital-selection decisions. However, this algorithm can be duplicated easily to other diseases or situations. The results of this application are limited to the specific type of disease, area, and time period. The best hospital for CABG surgery is not necessarily the best hospital for complex cancer surgeries. If the data for training are too dated, such as 10-year-old, the recommendation may not reflect current practices. We must update knowledge by training the system with new data because the outcomes for a hospital are likely to change over time. For example, a hospital may adopt new technologies, promote quality improvement, or experience surgeon turnover. The outcome improvement is estimated by a validation map, which is trained with an independent calibration set, and such training allows the storing of validation information. Thus, we can validate and observe if there is any improvement from the probability movement. Since the recommended case does not actually exist in the data, the validation method is performed indirectly. The indirect evaluation of the validation map and the classification of improvement groups compared to actual outcomes suggest that the system has the potential to improve patients\u2019 results given the fact that a better evaluation method is still necessary. In fact, the evaluation method is a common difficult problem for expert systems. The development of new validation methods to measure the performance of the recommendation is an interesting research topic. The best way to evaluate is to actually use the system to provide recommendations in a follow-up study. We can compare outcomes of patients who take the advice with patients who do not. An alternative way is to devise a mathematical model to evaluate improvement using the same retrospective data. The proposed process can be adapted to solve many types of problems. We will explore new adaptations of the proposed process in future work. We can add more issues to the hospital referral problem, such as factoring in a patient\u2019s insurance coverage, the hospital charges, or usual length of stay, and we can apply the concept to other diseases. This algorithm can also solve the recommendation problem in other domains, e.g., the recommendation of treatment in clinical care and the recommendation of healthy lifestyle choices to lower disease occurrence probability in public health. Acknowledgment Analysis for this paper was partially supported by grant R01 HS015009 from the Agency for Healthcare Research and Quality. References [1] S.S.R. Abidi Y.-N. Cheah J. Curran A knowledge creation info-structure to acquire and crystallize the tacit knowledge of health-care experts IEEE Trans Inform Technol Biomed 9 2 2005 193 204 [2] J.J. Allison C.I. Kiefe N.W. Weissman S.D. Person M. Rousculp J.G. Canto Relationship of hospital teaching status with quality of care and mortality for medicare patients with acute MI J Am Med Assoc 284 10 2000 1256 1262 [3] The American Hospital Association. Data source, <http://www.thirdwaveresearch.com/aha_wizard/default.aspx/> [Accessed in April 2007]. [4] J.Z. Ayanian J.S. Weissman Teaching hospitals and quality of care: a review of the literature Milbank Q 80 3 2002 569 593 [5] R.K. Bali D.D. Feng F. Burstein A.N. Dwivedi Introduction to the special issue on advances in clinical and health-care knowledge management IEEE Trans Inform Technol Biomed 9 2 2005 157 161 [6] J.D. Birkmeyer J.B. Dimick D.O. Staiger Operative mortality and procedure volume as predictors of subsequent hospital performance Ann Surg 243 3 2006 411 417 [7] J.D. Birkmeyer E.V. Finlayson C.M. Birkmeyer Volume standards for high-risk surgical procedures: potential benefits of the Leapfrog initiative Surgery 130 3 2001 415 422 [8] J.D. Birkmeyer A.E. Siewers E.V.A. Finlayson T.A. Stukel F.L. Lucas I. Batista Hospital volume and surgical mortality in the United States New Engl J Med 346 15 2002 1128 1137 [9] J.D. Birkmeyer A.E. Siewers N.J. Marth D.C. Goodman Regionalization of high-risk surgery and implications for patient travel times J Am Med Assoc 290 20 2003 2703 2708 [10] Brohman, MK. Knowledge creation opportunities in the data mining process. In: HICSS\u201906: Proceedings of the 39th annual hawaii international conference on system sciences, Washington, DC: IEE Computer Society; 2006, p. 170.3. [11] J. Chen S.S. Rathore M.J. Radford H.M. Krumholz JCAHO accreditation and quality of care for acute myocardial infarction Health Aff 22 2 2003 243 254 [12] W.J. Clancey Heuristic classification Artif Intell 27 3 1985 289 350 [13] F. Coenen T.J.M. Bench-Capon Maintenance and maintainability in regulation based systems ICL Tech J 1992 76 84 [14] R.A. Deyo D.C. Cherkin M.A. Ciol Adapting a clinical comorbidity index for use with ICD-9-CM administrative databases J Clin Epidemiol 45 6 1992 613 619 [15] J.B. Dimick G. Ailawadi The volume-outcome effect for abdominal aortic surgery Arch Surg 137 7 2002 828 832 [16] Dimick JB, Finlayson SRG, Birkmeyer JD. Regional availability of high-volume hospitals for major surgery. Health Aff, Web Exclusive: VAR45-VAR53, 2004. [17] A. Elixhauser C. Steiner I. Fraser Volume thresholds and hospital characteristics in the United States Health Aff 22 2 2003 167 177 [18] A. Elixhauser C. Steiner D.R. Harris R.M. Coffey Comorbidity measures for use with administrative data Med Care 36 1 1998 8 27 [19] Feigenbaum EA. The art of artificial intelligence: 1. Themes and case studies of knowledge engineering. Technical report, Stanford, CA, USA; 1977. [20] A. Gandjour A. Bannenberg K.W. Lauterbach Threshold volumes associated with higher survival in health care: a systematic review Med Care 41 10 2003 1129 1141 [21] L.G. Glance A.W. Dick D.B. Mukamel T.M. Osler Is the hospital volume-mortality relationship in coronary artery bypass surgery the same for low-risk versus high-risk patients? Ann Thorac Surg 76 2003 1155 1162 [22] The Leapfrog Group, 2004. Evidence-Based Hospital Referral, <http://www.leapfroggroup.org/media/file/Leapfrog-Evidence-based_HospitalReferralFact_Sheet.pdf/>; 2004 [Accessed in March 2006]. [23] E.A. Halm C. Lee M.R. Chassin Is volume related to outcome in health care? a systematic review and methodologic critique of the literature Ann Int Med 137 6 2002 511 520 [24] Healthcare Cost and Utilization Project (HCUP). The project description and data, <http://www.ahrq.gov/data/hcup/> [Accessed in April 2007]. [25] B.E. Hillner T.J. Smith C.E. Desch Hospital and physician volume or specialization and outcomes in cancer treatment: importance in quality of cancer care J Clin Oncol 18 11 2000 2327 2340 [26] I. Ihse The volume-outcome relationship in cancer surgery: a hard sell Ann Surg 238 6 2003 777 781 [27] Jaana M, Wakefield D, Rosenthal G. Access to healthcare services within the VA system: comparison of two methods for computing travel distances for CABG patients. Poster presented at the COM/CPH Research Week; 2003. [28] L.R. Jeffrey Multiobjective optimization: behavioral and computational considerations 1992 Kluwer Academic Publishers Norwell, Massachusetts [29] L.T. Kohn J.M. Corrigan M.S. Donaldson To err is human: building a safer health system 2000 National Academy Press Washington, DC [30] J. Kupersmith Quality of care in teaching hospitals: a literature review Acad Med 80 5 2005 458 466 [31] Ling CX, Li C. Data mining for direct marketing: problems and solutions. In: Proceedings of the forth ACM SIGKDD international conference on knowledge discovery and data mining, AAAI Press: New York, NY; 1998, p. 73\u201379. [32] Lin H-T, Lin C-J, Weng C-H. A note on Platt\u2019s probabilistic outputs for support vector machines, <http://www.csie.ntu.edu.tw/~cjlin/papers.html/>; 2003 [Accessed in February 2006]. [33] B.K. Nallamothu S. Saint T.P. Hofer Impact of patient risk on the hospital volume-outcome relationship in coronary artery bypass grafting Arch Int Med 165 3 2005 333 337 [34] H.R. Nemati D.M. Steiger L.S. Iyer R.T. Herschel Knowledge warehouse: an architectural integration of knowledge management, decision support, artificial intelligence and data warehousing Decision Support Syst 33 2 2002 143 161 [35] Niculescu-Mizil A, Caruana R. Predicting good probabilities with supervised learning. In: Proceedings 22th international conference on machine learning, New York, NY: ACM Press; 2005, p. 625\u2013632. [36] J. Platt Probabilistic outputs for support vector machines and comparison to regularized likelihood methods Advances in large margin classifiers 1999 MIT Press 61 74 [37] J.R. Quinlan C4.5: programs for machine learning 1993 Morgan Kaufman San Mateo, CA [38] E.H. Shortliffe R. Davis S.G. Axline B.G. Buchanan C.C. Green S.N. Cohen Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system Comput Biomed Res 8 4 1975 303 320 [39] Thompson TG, Brailer DJ, 2004. The decade of health information technology: delivering consumer-centric and information-rich health care: Framework for strategic action. Washington, DC: Department of Health and Human Services, National Coordinator for Health Information Technology, <http://www.hhs.gov/healthit/documents/hitframework.pdf/> [Accessed in March 2007]. [40] V. Vapnik The nature of statistical learning theory 1995 Springer NY [41] M.W. Ward M. Jaana D.S. Wakefield R.L. Ohsfeldt What would be the effect of referral to high-volume hospitals in a largely rural state? J Rural Health 20 4 2004 344 354 [42] I.D. Watson A. Basden P.S. Brandon The client centered approach: expert system maintenance Expert Syst 9 4 1992 189 196 [43] I. Watson F. Marir Case-based reasoning: a review Knowledge Eng Rev 9 4 1994 355 381 [44] Zadrozny B, Elkan C. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In: Proceedings of the eighteenth international conference on machine learning, San Francisco, CA: Morgan Kaufman; 2001, p. 609\u2013616.", "scopus-id": "40049102115", "pubmed-id": "18054523", "coredata": {"eid": "1-s2.0-S1532046407001086", "dc:description": "Abstract This study presents a new method for constructing an expert system using a hospital referral problem as an example. Many factors, such as institutional characteristics, patient risks, traveling distance, and chances of survival and complications should be included in the hospital-selection decision. Ideally, each patient should be treated individually, with the decision process including not only their condition but also their beliefs about trade-offs among the desired hospital features. An expert system can help with this complex decision, especially when numerous factors are to be considered. We propose a new method, called the Prediction and Optimization-Based Decision Support System (PODSS) algorithm, which constructs an expert system without an explicit knowledge base. The algorithm obtains knowledge on its own by building machine learning classifiers from a collection of labeled cases. In response to a query, the algorithm gives a customized recommendation, using an optimization step to help the patient maximize the probability of achieving a desired outcome. In this case, the recommended hospital is the optimal solution that maximizes the probability of the desired outcome. With proper formulation, this expert system can combine multiple factors to give hospital-selection decision support at the individual level.", "openArchiveArticle": "true", "prism:coverDate": "2008-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046407001086", "dc:creator": [{"@_fa": "true", "$": "Chi, Chih-Lin"}, {"@_fa": "true", "$": "Street, W. Nick"}, {"@_fa": "true", "$": "Ward, Marcia M."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046407001086"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046407001086"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(07)00108-6", "prism:volume": "41", "prism:publisher": "Elsevier Inc.", "dc:title": "Building a hospital referral expert system with a Prediction and Optimization-Based Decision Support System algorithm", "prism:copyright": "Copyright \u00a9 2007 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Decision support systems"}, {"@_fa": "true", "$": "Expert systems"}, {"@_fa": "true", "$": "Data mining"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Support vector machines"}, {"@_fa": "true", "$": "Optimization"}, {"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Hospital referral"}, {"@_fa": "true", "$": "Hospital quality"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "371-386", "prism:endingPage": "386", "prism:coverDisplayDate": "April 2008", "prism:doi": "10.1016/j.jbi.2007.10.002", "prism:startingPage": "371", "dc:identifier": "doi:10.1016/j.jbi.2007.10.002", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "21", "@width": "71", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "473", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "242", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "77", "@width": "188", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1812", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "187", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "107", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "616", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "187", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "885", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "242", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "875", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "71", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "473", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "71", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "473", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "71", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "473", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "242", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "50", "@width": "167", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1052", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "33", "@width": "308", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1399", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "241", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1149", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "52", "@width": "279", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1880", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "187", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "885", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "235", "@width": "263", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "17739", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "94", "@width": "105", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1430", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "348", "@width": "381", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31361", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "102", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2405", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "432", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "30746", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "94", "@width": "82", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1847", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "416", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "30503", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "85", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1909", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "394", "@width": "428", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33596", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "101", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2424", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "956", "@width": "450", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "121531", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "44", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2082", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "282", "@width": "338", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "17957", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "112", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1315", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "465", "@width": "379", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "43266", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "76", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046407001086-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2068", "@ref": "gr6", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/40049102115"}}