{"scopus-eid": "2-s2.0-33748182760", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2006-02-02 2006-02-02 2010-10-06T18:12:05 1-s2.0-S1532046406000025 S1532-0464(06)00002-5 S1532046406000025 10.1016/j.jbi.2005.12.009 S300 S300.2 FULL-TEXT 1-s2.0-S1532046406X00321 2015-05-15T06:30:58.184067-04:00 0 0 20061001 20061031 2006 2006-02-02T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 39 39 5 5 Volume 39, Issue 5 9 541 555 541 555 200610 October 2006 2006-10-01 2006-10-31 2006 Dialog Systems for Health Communications Timothy Bickmore Special Issue Articles article fla Copyright \u00a9 2006 Elsevier Inc. All rights reserved. AUTOMATICANALYSISMEDICALDIALOGUEINHOMEHEMODIALYSISDOMAINSTRUCTUREINDUCTIONSUMMARIZATION LACSON R 1 Introduction 2 Related work 3 Method 3.1 Data collection 3.2 Structure induction 3.2.1 Semantic taxonomy 3.2.1.1 Kappa agreement 3.2.2 Basic model 3.2.2.1 Feature selection 3.2.2.1.1 Lexical features 3.2.2.1.2 Durational features 3.2.2.1.3 Contextual features 3.2.2.2 Feature weighting and combination 3.2.3 Data augmentation with background knowledge 3.2.4 Results of semantic type classification 3.3 Summarization 3.3.1 Predicted semantic type vs. true semantic type 4 Evaluation method 4.1 The \u201cgold standard\u201d\u2014manual dialogue turn extraction 4.1.1 Measure of agreement 4.2 Baseline summary 4.3 Intrinsic vs. extrinsic evaluation 4.3.1 Intrinsic evaluation Extrinsic (task-based) evaluation 5 Summarization results 5.1 Intrinsic evaluation 5.2 Extrinsic (task-based) evaluation 6 Conclusion and future work Acknowledgments Appendix A Request for annotation A.1 Instructions A.2 Sample of annotated dialogue Appendix B Instructions given to physicians for manually selecting dialogue turns B.1 Instructions B.2 Example Appendix C Complete and summarized dialogues C.1 Complete dialogue with each turn labeled with corresponding summaries that contain this turn (G: gold standard, R: random, T: true semantic type, P: predicted semantic type) C.2 The four summaries (P: patient, N: nurse) Appendix D Instructions given to evaluators D.1 Instructions References HAMPTON 1975 486 489 J LOCKRIDGE 1999 16 R MCCRAY 1993 184 194 A XU 2004 565 572 H HSIEH 2004 511 515 Y LANDIS 1977 159 174 J SCHAPIRE 2000 135 168 R BROWN 1992 467 479 P MILLER 2004 337 342 S BLAND 2000 1468 J 1996 EVALUATINGNATURALLANGUAGEPROCESSINGSYSTEMSANALYSISREVIEW LEHOUX 2004 28 P LAFFERTY 2001 282 289 J LACSONX2006X541 LACSONX2006X541X555 LACSONX2006X541XR LACSONX2006X541X555XR Full 2014-11-20T07:55:50Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(06)00002-5 S1532046406000025 1-s2.0-S1532046406000025 10.1016/j.jbi.2005.12.009 272371 2010-11-08T12:11:50.868941-05:00 2006-10-01 2006-10-31 1-s2.0-S1532046406000025-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/MAIN/application/pdf/82a0ead71960bc68426788375c8deb30/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/MAIN/application/pdf/82a0ead71960bc68426788375c8deb30/main.pdf main.pdf pdf true 221368 MAIN 15 1-s2.0-S1532046406000025-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/PREVIEW/image/png/62b72723860d110140ea8e05746d4168/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/PREVIEW/image/png/62b72723860d110140ea8e05746d4168/main_1.png main_1.png png 67981 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046406000025-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/STRIPIN/image/gif/ce16f320d0c560bb58a6aea0be04307e/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/STRIPIN/image/gif/ce16f320d0c560bb58a6aea0be04307e/si3.gif si3 si3.gif gif 1564 38 234 ALTIMG 1-s2.0-S1532046406000025-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/STRIPIN/image/gif/a537fc062939275b8c2f7c2bbfe265e9/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/STRIPIN/image/gif/a537fc062939275b8c2f7c2bbfe265e9/si2.gif si2 si2.gif gif 3074 38 408 ALTIMG 1-s2.0-S1532046406000025-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/STRIPIN/image/gif/e1e5e47920e0bdc8080d681bb15fe49e/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/STRIPIN/image/gif/e1e5e47920e0bdc8080d681bb15fe49e/si1.gif si1 si1.gif gif 2300 38 336 ALTIMG 1-s2.0-S1532046406000025-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr2/DOWNSAMPLED/image/jpeg/a57315e3fe2dfcce5359928cb2fed018/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr2/DOWNSAMPLED/image/jpeg/a57315e3fe2dfcce5359928cb2fed018/gr2.jpg gr2 gr2.jpg jpg 10347 93 378 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000025-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr2/THUMBNAIL/image/gif/0f2178d60370a7ace3f5b316cd2a1d5f/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr2/THUMBNAIL/image/gif/0f2178d60370a7ace3f5b316cd2a1d5f/gr2.sml gr2 gr2.sml sml 1586 31 125 IMAGE-THUMBNAIL 1-s2.0-S1532046406000025-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr3/DOWNSAMPLED/image/jpeg/aa0e8d3b4ce9ae005a232993622c6076/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr3/DOWNSAMPLED/image/jpeg/aa0e8d3b4ce9ae005a232993622c6076/gr3.jpg gr3 gr3.jpg jpg 117809 831 623 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000025-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr3/THUMBNAIL/image/gif/1d7e4e9ca824a4488f476f758bffff45/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr3/THUMBNAIL/image/gif/1d7e4e9ca824a4488f476f758bffff45/gr3.sml gr3 gr3.sml sml 2206 93 70 IMAGE-THUMBNAIL 1-s2.0-S1532046406000025-gr3a.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr3a/DOWNSAMPLED/image/jpeg/27ac0e717b9ba5c31a973417049aa970/gr3a.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr3a/DOWNSAMPLED/image/jpeg/27ac0e717b9ba5c31a973417049aa970/gr3a.jpg gr3a gr3a.jpg jpg 113705 791 623 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000025-gr3a.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr3a/THUMBNAIL/image/gif/631d326fef501885d17eb879d066499b/gr3a.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr3a/THUMBNAIL/image/gif/631d326fef501885d17eb879d066499b/gr3a.sml gr3a gr3a.sml sml 2287 93 73 IMAGE-THUMBNAIL 1-s2.0-S1532046406000025-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr1/DOWNSAMPLED/image/jpeg/be7c30241938c25bf04193a00b63556b/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr1/DOWNSAMPLED/image/jpeg/be7c30241938c25bf04193a00b63556b/gr1.jpg gr1 gr1.jpg jpg 44750 422 622 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000025-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000025/gr1/THUMBNAIL/image/gif/79d35385bccd8d6ab6664f17566ba58e/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000025/gr1/THUMBNAIL/image/gif/79d35385bccd8d6ab6664f17566ba58e/gr1.sml gr1 gr1.sml sml 2340 85 125 IMAGE-THUMBNAIL YJBIN 1266 S1532-0464(06)00002-5 10.1016/j.jbi.2005.12.009 Elsevier Inc. Fig. 1 Transcribed segment of a phone dialogue. Fig. 2 An example of a cluster. Fig. 3 Segmented dialogue and the summarized version (P, patient; N, nurse). Table 1 Semantic type distribution of dialogue turns in training and testing data sets Category Training (n =1281) (%) Testing (n =293) (%) Clinical 33.4 20.8 Technical 14.6 18.1 Backchannel 27.2 34.5 Miscellaneous 24.7 26.6 Table 2 Examples of dialogue for each semantic type Clinical: 1. Ok, how\u2019s the Vioxx helping your shoulder? 2. You see, his pressure is dropping during his treatments. Technical: 1. Umm, I\u2019m out of kidneys. 2. That\u2019s where you spike it; the second port is the one where you draw from. Miscellaneous: 1. Martha wants me to remind you of your appointment today at 8:30. 2. I\u2019m just helping out \u2019til they get back from vacation. Backchannel: 1. Hello. How are you doing? 2. Yeah. Table 3 Accuracy of the models based on various feature combinations Models Accuracy (%) Dialogue turn 69 Dialogue turn with length 70 Dialogue turn with previous turn 68 Basic model (dialogue turn with length and previous turn) 70 Knowledge-augmented model 73 Table 4 Examples of predictive features Category Current dialogue turn Previous dialogue turn Clinical weight, blood, low, feel, pulse weight, take integer, you Technical filter, box, leaking machine, a little Backchannel thanks, ok, and, umm hi, make, sure, lab Miscellaneous appointment, hold, phone can, o clock, what, time Table 5 Questions used in task-based evaluation 1. Did a clinical problem require urgent intervention? 2. Did the patient mention either his vital signs (blood pressure, pulse rate, temperature), his weight, any symptoms, or his medications? 3. Was there a problem with the machine that required technical support? 4. Did the call require a follow-up (i.e., need to consult with another nurse, a physician, a technician or a supplier and/or require further laboratory investigation outside of the current call)? 5. Did the patient need to make, verify, cancel or reschedule an appointment? 6. Did the patient need to be dialyzed in-center? Table 6 Answer distribution across six questions based on full transcripts of dialogues Number of dialogues 40 Average number of dialogue turns per dialogue 13 Number of \u201cyes\u201d answers to question 1 12 (0.30) Number of \u201cyes\u201d answers to question 2 33 (0.41) Number of \u201cyes\u201d answers to question 3 20 (0.25) Number of \u201cyes\u201d answers to question 4 38 (0.48) Number of \u201cyes\u201d answers to question 5 26 (0.32) Number of \u201cyes\u201d answers to question 6 8 (0.10) Total number of \u201cyes\u201d answers 143 (0.30) Table 7 Intrinsic evaluation results with precision, recall, and F measure for 40 dialogues Random Computer generated using true semantic type Computer generated using predicted semantic type Precision 62/183 (33.88%) 107/199 (53.77%) 139/277 (50.18%) Recall 62/177 (35.03%) 107/177 (60.45%) 139/177 (78.53%) F measure 34.45 56.91 61.23 Number of dialogue turns 183/516 (35.47%) 199/516 (38.57%) 277/516 (53.68%) Table 8 Fisher\u2019s Exact test comparing the precision and recall of pairs of summary types (p <0.05 is statistically significant) Computer generated using true semantic type vs. random Computer generated using predicted semantic type vs. random Computer generated using predicted semantic type vs. true semantic type Precision 1.38\u00d710\u22124 7.94\u00d710\u22124 0.4580 Recall 2.53\u00d710\u22126 1.03\u00d710\u221216 3.23\u00d710\u22124 Table 9 Number of correct responses for each summary type Q1 Q2 Q3 Q4 Q5 Q6 Total Random 27 (67.5%) 28 (70.0%) 33 (82.5%) 26 (65.0%) 29 (72.5%) 38 (95.0%) 181 (75.4%) Manual 31 (77.5%) 34 (85.0%) 35 (87.5%) 28 (70.0%) 24 (60.0%) 38 (95.0%) 190 (79.2%) Computer generated using true-label 31 (77.5%) 34 (85.0%) 37 (92.5%) 27 (67.5%) 33 (82.5%) 38 (95.0%) 200 (83.3%) Computer generated using predicted-label 29 (72.5%) 32 (80.0%) 38 (95.0%) 28 (70.0%) 29 (72.5%) 39 (97.5%) 195 (81.2%) Table 10 Comparison of the accuracy of the summaries using Sign test (p <0.05 is statistically significant, NS, not significant) Sign test (one-tailed, n =5) Sign test (two-tailed, n =5) Computer generated using true semantic type vs. random p =0.031 p =0.062 Computer generated using predicted semantic type vs. random p =0.031 p =0.062 Computer generated using true semantic type vs. Manual NS NS Computer generated using predicted semantic type vs. Manual NS NS Computer generated using predicted semantic type vs. true semantic type NS NS Automatic analysis of medical dialogue in the home hemodialysis domain: Structure induction and summarization Ronilda C. Lacson \u204e rclacson@mit.edu Regina Barzilay William J. Long Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA, USA \u204e Corresponding author. Fax: +1 781 652 0404. Abstract Spoken medical dialogue is a valuable source of information for patients and caregivers. This work presents a first step towards automatic analysis and summarization of spoken medical dialogue. We first abstract a dialogue into a sequence of semantic categories using linguistic and contextual features integrated in a supervised machine-learning framework. Our model has a classification accuracy of 73%, compared to 33% achieved by a majority baseline (p <0.01). We then describe and implement a summarizer that utilizes this automatically induced structure. Our evaluation results indicate that automatically generated summaries exhibit high resemblance to summaries written by humans. In addition, task-based evaluation shows that physicians can reasonably answer questions related to patient care by looking at the automatically generated summaries alone, in contrast to the physicians\u2019 performance when they were given summaries from a na\u00efve summarizer (p <0.05). This work demonstrates the feasibility of automatically structuring and summarizing spoken medical dialogue. Keywords Spoken dialogue analysis Medical discourse Summarization Natural language processing Information processing 1 Introduction Medical dialogue occurs in almost all types of patient\u2013caregiver interaction, and forms a foundation for diagnosis, prevention, and therapeutic management. In fact, studies show that up to 80% of diagnostic assessments are based solely on the patient\u2013caregiver interview [1]. Automatic processing of medical dialogue is desirable in multiple contexts\u2014from clinical and educational, to financial and legal. Caregivers can use the results of this processing for informed decision making, researchers can benefit from large volumes of patient-related data currently unavailable in medical records, and health-care providers can enhance communication with patients by understanding their concerns and needs. All of these users share a common constraint: none of them wants to wade through a recording or transcript of the entire interaction. To illustrate the difficulty of accessing medical dialogue, consider 30s of an error-free transcript of an interaction between a dialysis patient and a nurse (see Fig. 1 ). This excerpt exhibits an informal, verbose style of medical dialogue\u2014interleaved false starts (such as \u201c I \u2019 ll pick up , I \u2019 ll give you a box of them \u201d), extraneous filler words (such as \u201c ok \u201d), and non-lexical filled pauses (such as \u201c Umm \u201d). This exposition also highlights the striking lack of structure in the transcript: a request for more supplies (e.g., \u201c kidney ,\u201d which in this context refers to a dialyzer) switches to a question about a patient\u2019s symptom (e.g., shoulder pain) without any visible delineation customary in written text. Therefore, a critical problem for processing dialogue transcripts is to provide information about their internal structure. This paper presents the first attempt to analyze, structure, and summarize dialogues in the medical domain. Our method operates as part of a system that analyzes telephone consultations between nurses and dialysis patients in the home hemodialysis program at Lynchburg Nephrology, the largest such program in the United States [2]. By identifying the type of a turn\u2014Clinical, Technical, Backchannel or Miscellaneous\u2014we are able to render the transcript into a structured format, amenable to automatic summarization. The Clinical category represents the patient\u2019s health, the Technical category encompasses problems with operating dialysis machines, the Miscellaneous category includes mostly scheduling and social concerns, while Backchannels capture greetings and acknowledgments. In addition, automatically processing medical dialogue has important implications for the development and evaluation of conversational systems. Current methods for developing automated dialogue systems rely on large amounts of labeled data for training (ref); human annotation of this material is an expensive and lengthy process. Our system can provide an initial annotation which can be further refined by a human, if necessary. Furthermore, for evaluation of automated dialogue systems, structure of the dialogue can be analyzed and compared to human\u2013human dialogues. An interesting direction in analyzing the performance of automated dialogue systems is their comparison with human\u2013human dialogues. Understanding similarities and differences in structure between human\u2013human and machine\u2013human dialogues can further advance the development of automated systems. Our method may also be used for mixed conversational systems, in which part of the dialogue is routed to an automated system (i.e., scheduling), as opposed to a clinical or technical query, which requires the attention of a human caregiver. Finally, our classification allows a provider to distill the portions of the dialogue that support medical reasoning and are of primary interest to clinicians. In the long run, knowing the distribution of patient requests can improve the allocation of resources, and ultimately provide better quality of health care. Our system has two main components: Structure induction. We present a machine-learning algorithm for classifying dialogue turns with respect to their semantic type. The algorithm\u2019s input is a transcription of spoken dialogue, where boundaries between speakers are identified, but the semantic type of the dialogue turn is unknown. The algorithm\u2019s output is a label for each utterance, identifying it as Clinical, Technical, Backchannel, and Miscellaneous. Our algorithm makes this prediction based on a shallow meaning representation encoded in lexical and contextual features. We further improve the classification accuracy by augmenting the input representation with background medical knowledge. Summarization. We introduce a novel way to extract essential dialogue turns within our domain of spoken medical dialogue using the discourse structure just described. Our goal is to provide a caregiver with a succinct summary that preserves the content of a medical dialogue, thereby reducing the need to leaf through a massive amount of unstructured and verbose transcript. To assess the performance of the summarizer and the contribution of structure induction, we describe a framework for evaluation of medical dialogues. Our first evaluation method follows an intrinsic methodology, commonly used in the text summarization community [3]. We compare automatically generated summaries with a \u201cgold standard\u201d summary created by humans, assuming that a better automatic summary exhibits high overlap with a \u201cgold standard\u201d summary. Our second evaluation is task based. Doctors were asked to use our summaries to answer questions concerning various aspects of patient care, ranging from clinical assessment to scheduling issues. We compare their responses to randomly generated summaries and to a \u201cgold standard\u201d summary. 2 Related work In recent years, a variety of summarization algorithms have been developed for text [4,5], and are primarily applied for summarizing newspaper articles [6,7]. Our work builds on these approaches in the design of a summarization algorithm for medical dialogues. For instance, some of the features used in our algorithm, such as position information and sentence length, have been shown useful in summarization of written materials [8\u201310]. Our emphasis on spoken discourse sets us apart from the efforts to interpret written medical text [11\u201313]. In particular, our work differs in two significant directions: 1. The essential component of our method is structural representation of dialogue content, tailored to the medical domain. We show that this scheme can be reliably annotated by physicians, effectively computed and integrated within a summarization system. 2. We propose a novel task-based evaluation method that assesses usefulness of our summaries in the medical setting. Research in text summarization has revealed that designing a task-based evaluation is challenging; frequently a task does not effectively discriminate between systems. In contrast, we show that our task-based evaluation does not suffer from this drawback, and can be used to evaluate other summarization systems for medical dialogues. 3 Method 3.1 Data collection We collected our data from the Lynchburg Nephrology program, the oldest and largest home hemodialysis program in the United States [2]. All phone conversations between nurses and 25 adult patients treated in the program from July to September of 2002 were recorded using a telephone handset audio tap (\u201cQuickTap,\u201d made by Harris, Sandwich, IL) and a recorder. The home hemodialysis nurses recorded the conversations whenever a call was made and stopped the recorder when the conversation ended. All patients and nurses whose questions and answers were recorded read and signed an informed consent form approved by the MIT Committee on the Use of Humans as Experimental Subjects. At the end of the study period, we received a total of six cassette tapes, consisting of 118 phone calls, containing 1574 dialogue turns with 17, 384 words. The conversations were transcribed, maintaining delineations between calls and speaker turns. The data were then divided chronologically into training and testing sets. The distribution of semantic types for each set is shown in Table 1 . 3.2 Structure induction 3.2.1 Semantic taxonomy Our annotation scheme was motivated by the nature of our application\u2014analysis of phone consultations between a nurse and a dialysis patient. It is defined by four semantic types\u2014Clinical, Technical, Backchannel, and Miscellaneous. Examples of utterances in each semantic type are shown in Table 2 . Dialogue turns are labeled Clinical if they pertain to the patient\u2019s health, medications, laboratory tests (results) or any concerns or issues the patient or nurse has regarding the patient\u2019s health. These discussions become the basis from which a patient\u2019s diagnostic and therapeutic plans are built. Dialogue turns are labeled Technical if they relate to machine problems, troubleshooting, electrical, plumbing or any other issues that require technical support. This category also includes problems with performing a procedure or laboratory test because of the lack of materials, as well as a request for necessary supplies. Utterances in the Technical category typically do not play a substantial role in clinical decision making, but are important for providing quality health care. We label as Miscellaneous any other concerns primarily related to scheduling issues and family concerns. Finally, the Backchannel category covers greetings and confirmatory responses, and they carry little information value for health-care providers. 3.2.1.1 Kappa agreement Two domain experts, specializing in Internal Medicine and Nephrology, independently labeled each dialogue turn with its semantic type. Each annotator was provided with written instructions that define each category and was given multiple examples (see Appendix A). To validate the reliability of the annotation scheme, we computed agreement using the kappa coefficient [14]. Complete agreement would correspond to a kappa of 1.0. We computed the kappa to be 0.80, which is \u201csubstantial\u201d agreement [14]. This kappa suggests that our dialogue can be reliably annotated using the scheme we developed. 3.2.2 Basic model Our goal is to identify features of a dialogue turn that are indicative of its semantic type and effectively combine them. Our discussion of the selected features is followed by a presentation of the supervised framework for learning their relative weights. 3.2.2.1 Feature selection Our basic model relies on three features that can be easily extracted from the transcript: words of a dialogue turn, its length, and words of the previous turn. 3.2.2.1.1 Lexical features Clearly, words of an utterance are highly predictive of its semantic type. We expect that utterances in the Clinical category would contain words like \u201c pressure ,\u201d \u201c pulse ,\u201d and \u201c pain ,\u201d while utterances in the Technical category would consist of words related to dialysis machinery, such as \u201c catheter \u201d and \u201c port .\u201d To capture colloquial expressions common in everyday speech, our model includes bigrams (e.g., \u201cI am\u201d) in addition to unigrams (e.g., \u201cI\u201d). 3.2.2.1.2 Durational features We hypothesize that the length of a dialogue turn helps to discriminate certain semantic categories. For instance, utterances in the Backchannel category are typically shorter than Technical and Clinical utterances. The length is computed by the number of words in a dialogue turn. 3.2.2.1.3 Contextual features Adding the previous dialogue turn is also likely to help in classification, since it adds important contextual information about the utterance. If a dialogue is focused on a Clinical topic, succeeding turns frequently remain Clinical. For example, the question \u201c How are you doing ?\u201d might be a Backchannel if it occurs in the beginning of a dialog whereas it would be considered Clinical if the previous statement is \u201c My blood pressure is really low .\u201d 3.2.2.2 Feature weighting and combination We learn the weights of the rules in the supervised framework using Boostexter [15], a state-of-the-art boosting classifier. Each object in the training set is represented as a vector of features and its corresponding class. Boosting works by initially learning simple weighted rules, each one using a feature to predict one of the labels with some weight. It then searches greedily for the subset of features that predict a label with high accuracy. On the test data set, the label with the highest weighted vote is the output of the algorithm. 3.2.3 Data augmentation with background knowledge Our basic model relies on the shallow representation of dialogue turns, and thus lacks the ability to generalize at the level of semantic concepts. Consider the following scenario: the test set consists of an utterance \u201c I have a headache \u201d but the training set does not contain the word \u201c headache .\u201d At the same time, the word \u201c pain \u201d is present in the training set, and is found predictive of the Clinical category. If the system knows that \u201c headache \u201d is a type of \u201c pain ,\u201d it will be able to classify the test utterance into a correct category. In our previous work, we described methods that bridge this gap by leveraging semantic knowledge from readily available data sources [16]. These methods identify the semantic category for each word, and use this information to predict the semantic type of a dialogue turn. Our best algorithm derives background knowledge from clusters of semantically related words automatically computed from a large text corpus. Clustering provides an easy and robust solution to the problem of coverage as we can always select a large and stylistically appropriate corpus for cluster induction. This is especially important for our application, since patients often use colloquial language and jargon. In addition, similarity-based clustering has been successfully used in statistical natural language processing for such tasks as name entity recognition and language modeling [17,18]. To construct word classes, we employ a clustering algorithm that groups together words with similar distributional properties [17]. In our experiments, we applied clustering to a corpus in the domain of medical discourse that covers topics related to dialysis. We downloaded the data from a discussion group for dialysis patients available in the following url: http://health.groups.yahoo.com/group/dialysis_support. Our corpus contains more than one million words corresponding to discussions within a ten-month period. We empirically determined that the best classification results are achieved for 2000 clusters. We added cluster-based substitutions to the feature space of the basic model by substituting each word of text with their corresponding cluster identifier. An example of a cluster is shown in Fig. 2 . When feature space is augmented with clustering information (computed outside of Boostexter), the number of features is increased by the number of clusters. We also used semantic types from a large-scale human-crafted resource, UMLS, for data augmentation. Unfortunately, the results we obtained are less successful than those using word clusters. 1 Lacson R, Barzilay R. Automatic processing of spoken dialogue in the home hemodialysis domain. AMIA Annual Fall Conference, 2005. 1 3.2.4 Results of semantic type classification Table 3 displays the results of various configurations of our model on the 293 dialogue turns of the test set, held out during the development time. The basic model and the knowledge-augmented model are shown in bold. All the presented models significantly outperform the 33.4% accuracy (p <0.01) of a baseline model in which every turn is assigned to the most frequent class (Clinical). The best model achieves an accuracy of 73%, and it combines lexical, durational, and contextual features augmented with background information obtained through statistical clustering. The first four rows of Table 3 show the contribution of different features of the basic model. Words of the dialogue turn alone combined with both the length of the turn and the words of the previous utterance achieve an accuracy of 70%. Table 4 shows the most predictive features for each category. The last row in Table 3 demonstrates that adding background knowledge improves the performance of the model modestly, achieving a 3% gain over the basic model. Even at the current level of performance (73%), we are able to use this model\u2019s predicted semantic types to generate summaries that are comparable to manual summaries created by physicians (see Section 5). In an effort to determine whether predictive features are easily identifiable by humans, we also compare the accuracy of our model to that of a classification algorithm that uses manually identified words chosen from our data [19]. In this approach, a domain expert manually identifies the most predictive features for each category, instead of automatically learning it from the data. For every category, an expert assigned a set of representative words. The weight of the words is determined by the count of instances it occurred in the training data for that particular category. When a new data segment is presented, we computed the score for each class by summing the word scores derived from the training data for every word in the segment that appears in the expert\u2019s list. The class with the highest vote wins. This model achieves an accuracy of 61%. This unexpectedly low result demonstrates the complexity of semantic annotation for medical dialogues, and justifies the use of machine-learning methods. 3.3 Summarization In the next section, we describe our method for automatically extracting key dialogue turns using the semantic types we defined. The extracted dialogue turns will comprise the summary for each dialogue. Telephone dialogues between caregivers and patients may provide additional information to the health team for individual management of patients as well as for identifying the bulk of patient requests. Availability of this data is important for continuity of individual patient care as well as for proper allocation of health resources. However, an entire transcript of dialogue is not helpful for caregivers who are often pressed for time. Summarized versions of the transcript, which preserves the main contents, will provide the information in a more concise form. Our extraction method consists of three consecutive steps: Step 1: Remove Backchannels\u2014By definition, backchannels contain greetings and acknowledgements that carry very little information value for health-care providers. Removing backchannels should not affect the quality of information that is essential in summarization. Examples of backchannels are \u201c Hello .,\u201d \u201c Hi , is Martha there ?,\u201d \u201c That \u2019 s ok .,\u201d and \u201c Thank you .\u201d We remove all backchannels from the dialogues at the beginning of the process. After this, each dialogue only contains dialogue turns from the following three categories: Clinical, Technical and Miscellaneous. Step 2: Dialogue segmentation\u2014Our manual corpus analysis revealed that a typical dialogue in our domain contains more than one topic [20]. Therefore, a summary has to include dialogue turns representative of each topic. We computed topics by segmenting a dialogue into blocks of consecutive turns of the same semantic type. In other words, consecutive dialogue turns with the same semantic type are considered to belong to a segment with a single topic. For instance, a dialogue with six turns of the following type \u201cclinical, clinical, miscellaneous, technical, technical, miscellaneous\u201d is abstracted into a sequence of four topics \u201cclinical, miscellaneous, technical, miscellaneous. An example of such segmentation is shown in Fig. 3 . Step 3: Dialogue turn extraction\u2014Next, we extract key utterances from each segment. Following a commonly used strategy in text summarization, we select the leading utterance of each segment [21]. We hypothesize that the initial utterance in a segment introduces a new topic and is highly informative of the segment\u2019s content. This extraction strategy may be deficient for long segments since such segments may discuss several topics of the same semantic type. For instance, a patient may discuss his vital signs while doing dialysis and then proceed to talk about back pain. Thus, for segments with more than two dialogue turns, we select the longest dialogue turn in addition to the initial one. We hypothesize that introducing a new topic will contain a lot of new information and will therefore contain more words. Fig. 3 shows one run of the algorithm. The summarizer compresses a conversation of 14 into five key dialogue turns. 3.3.1 Predicted semantic type vs. true semantic type Our summarization takes as input a dialogue in which every turn is annotated with its semantic type. An obvious way to obtain this information is to use an automatic classification method described in Section B for generating semantic types for each dialogue turn. We refer to these automatically generated labels as \u201cpredicted semantic types.\u201d In our experiments, we also consider summaries that use \u201ctrue semantic types,\u201d that is, types manually assigned by human experts to each dialogue turn. Analyzing the performance of the model based on the \u201ctrue semantic types\u201d would allow us to measure whether structural information helps. Comparing summaries based on \u201ctrue semantic types\u201d with summaries based on \u201cpredicted semantic types\u201d would reveal the impact of classification accuracy on the quality of the produced summaries. Note that there is one caveat in this comparison: summaries of the two types may have different lengths for the same dialogue. This happens because our summarization method captures changes in conversation topics by identifying switches in semantic types of the dialogue turns. We found that summaries based on \u201ctrue semantic types\u201d contain 38% of the original dialogues, compared to the summaries based on \u201cpredicted semantic types\u201d which contained 53% of the original dialogues. The discussion of our evaluation results in the next section takes this discrepancy into account. 4 Evaluation method We first describe two alternative summarization strategies that we use for comparison with our system. We then introduce two evaluation frameworks for testing our summarizer. 4.1 The \u201cgold standard\u201d\u2014manual dialogue turn extraction We created a \u201cgold standard\u201d summary for evaluating our automatically extracted dialogue turns. Two physicians were given instructions to select dialogue turns that cover the most essential topics within each dialogue. For each dialogue, we limited the number of dialogue turns the human subjects could select ranging from a single turn up to 1/3 of the total turns in a dialogue (see instructions in Appendix B). We obtained summaries for 80 dialogues. Twenty summaries were summarized by two physicians while the remaining 60 were summarized by a single physician. 4.1.1 Measure of agreement We assess the degree of agreement between two humans by comparing selected dialogue turns for 20 dialogues that both physicians summarized. First, we calculated their percentage of agreement in manually selecting dialogue turns that best represent each dialogue. Second, we calculated an odds ratio to further illustrate agreement. Percentage of agreement is defined as the number of dialogue turns that both physicians included in the summary, divided by the total number of dialogue turns in the summary. The actual observed agreement is 81.8% between the two physicians. In addition, we computed the kappa to be 0.5, which is \u201csubstantial\u201d agreement [14]. We also computed the odds ratio, which shows the relative increase in the odds of one subject making a given decision, given that the other subject made the same decision, is 10.8. It indicates that the odds of Subject 2 making a positive decision increases 10.8 times for cases where Subject 1 makes a positive decision, which is statistically significant (p <0.0003, log odds ratio) [22]. These two measurements indicate that dialogue turn extraction can be reliably performed by humans in our domain. 4.2 Baseline summary The baseline summaries were produced by randomly selecting a third of the dialogue turns within each dialogue, independently of their semantic types. Random baselines are routinely used for comparison in the natural language domain [23,24]. In a task-based evaluation, random extraction methods commonly rival automatic methods since humans can compensate for poor summary quality by their background knowledge. We therefore have the complete dialogue and four types of summaries for each dialogue: the \u201cgold standard,\u201d a randomly generated baseline, summaries based on \u201ctrue semantic types,\u201d and summaries based on \u201cpredicted semantic types.\u201d Appendix C shows a sample of all four summaries with the original complete dialogue. 4.3 Intrinsic vs. extrinsic evaluation Our evaluation is composed of two parts\u2014intrinsic and extrinsic [25,26]. In the intrinsic part, we compare the automatically generated summaries to the \u201cgold standard.\u201d The key assumption is that automatically generated summaries that have higher overlap with the \u201cgold standard\u201d are better summaries. In the extrinsic part, we do a task-based evaluation and measure how useful the summaries are in preserving information important in the medical setting. 4.3.1 Intrinsic evaluation To measure the degree of overlap between an automatically computed summary and the \u201cgold standard,\u201d we use precision and recall. Precision penalizes false positives chosen by the system in question. It is similar to positive predictive value in the biomedical literature and is expressed as: Precision \u2261 # Dialogue Turns Correctly Chosen # Dialogue Turns Chosen Recall penalizes false negatives chosen by the system. It is similar to sensitivity in the biomedical literature and is expressed as: Recall \u2261 # Dialogue Turns Correctly Re cognized # Dialogue Turns Should Have Been Recognized To have a single measure of a system\u2019s performance, we also use the F measure, defined as a weighted combination of precision and recall. It is expressed as: F measure \u2261 2 \u2217 precision \u2217 recall precision + recall Using these measures, we compare automatically generated summaries using \u201cpredicted semantic types\u201d and \u201ctrue semantic types\u201d with the \u201cgold standard\u201d and the random baseline. We use two-tailed Fisher\u2019s Exact test to determine statistical significance. Extrinsic (task-based) evaluation Our goal in this section is to determine whether the summaries are sufficient to provide caregivers with information that is important for patient care. We consulted with dialysis physicians and nurses to create a list of key questions based on topics that commonly arise between hemodialysis patients and caregivers [2,27] (see Table 5 ). The questions address relevant issues in clinical assessment, technical support, and overall delivery of quality patient care. We distributed 200 dialogues, comprised of the complete version of 40 dialogues and four \u201csummaries\u201d of these same dialogues: (1) the manually created summaries; (2) the summaries based on randomly extracted dialogue turns; (3) summaries based on the \u201ctrue semantic types\u201d of the dialogue turns; and (4) summaries based on the \u201cpredicted semantic types\u201d of the dialogue turns. We had five licensed physicians (who did not participate in the selection of questions or in the manual summarization process) answer each of the six \u201cyes/no\u201d questions using each of 40 dialogues. It has been noted previously that when humans are asked to answer otherwise (e.g., NA or unknown), other factors come into play, such as a person\u2019s degree of decisiveness in committing to a response. Humans who are indecisive may tend to answer \u201cNA\u201d to a lot of questions and further bias the results. A similar experimental design has been adopted in other summarization systems [28]. The physicians received written instructions prior to performing their task (see Appendix D). Each physician only saw one version of every dialogue. Based on self-reporting, they completed the task of answering six questions for 40 dialogues in approximately 1h. Based on the complete dialogue, 30% of the answers to these questions are \u201cyes\u201d and 70% are \u201cno.\u201d The characteristics of the complete data set are provided in Table 6 below. We compare the number of questions that physicians answered correctly using our summaries with answers based on the \u201cgold standard\u201d and the random baseline. Sign test was used to measure statistical significance. 5 Summarization results We report the results of the intrinsic and extrinsic evaluation. 5.1 Intrinsic evaluation The precision, recall, and F measure for the random baseline and the computer-generated summaries are shown in Table 7 . The results indicate that machine-generated summaries outperform random summaries by a wide margin. The results of two-tailed Fisher\u2019s Exact test comparing various summaries are shown in Table 8 . As expected, recall was better for the summary that was generated using the predicted semantic types compared to true semantic types because it contained more dialogue turns. It is more important to note the effect on precision, which is less influenced by the length of the summaries. Precision was significantly better for both summaries compared to the random baseline and there was no difference between the precision of the two summaries. These results demonstrate the contribution of structural information to text summarization. 5.2 Extrinsic (task-based) evaluation We report the results of physicians\u2019 answers to each of our six questions when given various summaries for 40 dialogues. We assume that answers based on the complete dialogues are the correct ones. The numbers of correct responses are shown in Table 9 for each summary type. The summaries based on true semantic types outperformed all other summaries. Computer-generated summaries based on predicted semantic types performed comparably, allowing physicians to correctly answer 81% of questions. Statistical significance was measured using Sign test comparing summaries generated using our method to random summaries as shown in Table 10 . Sign test has been used in the speech recognition domain to show systematic evidence of differences in a consistent direction, even if the magnitudes of the differences are small [29]. The automatically generated summaries outperform random summaries on five questions, with a tie for the sixth (see Table 9). Using one-tailed Sign test, this difference was significant. This test is applicable for our evaluation: we want to measure the degree of improvement our method has over the random baseline. Using two-tailed Sign test, there is no significant difference between computer-generated summaries and manually generated or random summaries. 6 Conclusion and future work This work presents a first step towards automatic analysis of spoken medical dialogue. The backbone of our approach is an abstraction of a dialogue into a sequence of semantic categories. This abstraction uncovers structure in informal, verbose conversation between a caregiver and a patient, thereby facilitating automatic processing of dialogue content. Our method induces this structure based on a range of linguistic and contextual features that are integrated in a supervised machine-learning framework. We demonstrate the utility of this structural abstraction by incorporating it into an automatic dialogue summarizer. Our evaluation results indicate that automatically generated summaries exhibit high resemblance to summaries written by humans. Our task-based evaluation shows that physicians can reasonably answer questions related to patient care by looking at the summaries alone, without reading a full transcript of a dialogue. We believe that further refinement of the presented summarizer would ultimately spare the physician from the need to wade through irrelevant material ample in dialogue transcripts. Automatic segmentation of dialogues by topics and the use of more expressive statistical models to capture the sequential structure of dialogue will likely enhance the summarizer\u2019s performance. In the future, we plan to extend this work in three main directions. First, we will apply our method to automatically recognized conversations. Clearly, automatic speech recognition will introduce mistakes in a transcript. At the same time, we will have access to a wealth of acoustic features that provide additional cues about dialogue content. For instance, a pause may be a strong indicator of topic switch. Therefore, we will explore the use of acoustic features to compensate for recognition errors in the transcript. Second, we will refine our annotation scheme to include more semantic categories. This would support a deeper analysis of medical dialogue. To achieve this goal, we will experiment with more expressive statistical models able to capture the sequential structure of medical dialogue. Possible modeling methods include hidden Markov models and conditional random fields [30,31]. Finally, we will explore query-based summarization as opposed to generic summarization [32]. In our current implementation, the summaries are not tailored to specific information needs of a care provider. By knowing what information is of interest to different categories of care providers, we can personalize the summaries towards their needs. Acknowledgments The authors acknowledge the support of the National Science Foundation (Barzilay; Career grant IIS-0448168 and grant IIS-0415865). We thank Dr. Robert Lockridge and the nurses at the Lynchburg Nephrology nightly home hemodialysis program, Mary Pipken, Viola Craft, and Maureen Spencer for recording the conversations. We thank Percy Liang for letting us use his word clustering software, Peter Szolovits and Eduardo Lacson Jr. for reviewing the manuscript and all the physicians who participated in the annotation and evaluation of the system. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation. Appendix A Request for annotation We provide here the instructions and examples for annotating dialogue turns within our dialogues. A.1 Instructions Dear Doctor, I would like to request your participation in annotating a transcription of a telephone dialogue between dialysis nurses and patients. This annotation will be used to help identify the most frequent reasons for calls to a dialysis unit by actual patients. It will be used in conjunction with other methods in helping identify the topics that are pertinent to patients who undergo home hemodialysis. The dialog will be segmented by utterances or each person\u2019s turn in the actual dialogue. Each turn will be labeled as belonging to one of several categories: 1. Clinical 2. Technical 3. Greetings and acknowledgements 4. Miscellaneous As implied by the category names, a clinical utterance is anything that pertains to a clinical topic, such as the patient\u2019s health, medications, laboratory tests (results) or any concerns or issues the patient or nurse has regarding the patient\u2019s health. Examples include: 1. You see, his pressure\u2019s dropping during his treatments. 2. Do you want me to do blood test? A technical utterance relates to machine problems, troubleshooting, electrical, plumbing or any other issues that require technical support. This also includes problems with performing a procedure or laboratory test because of lack of or defective materials, as well as a request for necessary supplies. Procedures for doing a laboratory test will also be classified as technical. Examples include: 1. The machine is stuck. 2. That\u2019s where you spike it, the second port is the one where you draw from. Greetings include \u201chellos\u201d and \u201cgoodbyes\u201d that are typically located at the beginning and end of a call. Acknowledgements and confirmatory responses to questions include \u201caha,\u201d \u201cok,\u201d \u201calright,\u201d \u201cyes,\u201d etc. Examples of this category include: 1. Hello, is S__ there? 2. Thanks for calling. Any other utterances can be classified as miscellaneous. These include (but are not exclusive to) scheduling (a clinical or technical meeting or appointment), personal conversations, etc. Examples include: 1. I\u2019ll call you back. 2. I\u2019m just helping out till they get back from vacation. An utterance should be taken within the context of the conversation. (e.g., \u201cI\u2019m taking two\u201d should be categorized as clinical if the conversation is regarding how many tablets a patient is taking.) However, \u201cok,\u201d \u201cyes\u201d and other acknowledgements should be categorized as confirmations. Please indicate the categorizations by marking the clinical utterances with \u201cC,\u201d the technical utterances with \u201cT,\u201d acknowledgements/greetings with \u201cA,\u201d and miscellaneous utterances with \u201cM.\u201d A sample annotation is given below. An utterance can be categorized into more than one topic. If any utterance appears to belong to more than one topic, please indicate both categories. For example, 1. \u201cYou know the meter on the machine, and I couldn\u2019t get it to come out so I called technical support. He said someone will call him but nobody called me.\u201d This can be technical because it concerns the machine or miscellaneous because it refers to someone who needs to call. You can indicate \u201cT\u201d or \u201cM\u201d in this case. 2. \u201cOk, how many hours did you run M_.\u201d This can be clinical because knowing how long the patient dialyzed impacts their health. It can also be technical if taken in context with the machine not working anymore after this run. You can indicate \u201cC\u201d or \u201cT\u201d in this case. This participation is voluntary and any specific data you provide will not be published or made available without your consent. Thank you. A.2 Sample of annotated dialogue C: Just changed it this morning, he said it\u2019s not sore. It\u2019s still got the dressing on it, didn\u2019t take it out last night in case it drains again. C: Have you looked at it this morning? C: It hasn\u2019t drained overnight. Just a little bit. It\u2019s not clear, it\u2019s pussy looking. A: Ok. C: It\u2019s not red like it was last night. C: Ok, let me Dr. M_ is on call for the weekend, let me give her a call. See if he wants to put him on any antibiotics. You, know, preventatively. A: Ok. M: And I\u2019ll call you back. T, M: You know the meter on the machine, and I couldn\u2019t get it to come out so I called technical support. He said someone will call him but nobody called me. C, T: Ok, how many hours did you run M_. C, T: 3 and a half. C, T: You ran 3 and a half? A: Aha. M: Ok, well nobody will be coming out here today anyway to do anything about your machine. A: Aha. M: At least, till tomorrow morning. And I will go ahead and call them to see if we can get somebody to come out there tomorrow to do something. M: It\u2019s the same thing. M: Oh you\u2019re kidding. Appendix B Instructions given to physicians for manually selecting dialogue turns B.1 Instructions Dear Doctor, 1. Please select dialogue turns from each phone call, which are most representative of the entire dialogue and would give the reader an idea about the topics within the conversation. In particular, please pick dialogue turns that are important to the patient\u2019s health and dialysis management. Information about their relatives, their homes, etc., is not relevant unless these impact the delivery of their care. 2. A dialogue turn starts with N: (for a nurse\u2019s turn) or P: (for a patient\u2019s turn). 3. You are allowed to pick at least one dialogue turn, up to a specified number of turns that will best summarize the conversation, at your discretion. 4. Please highlight your choices with the highlighter provided. 5. See example below. Thank you. B.2 Example Select up to 3 turns N: ok P: I was making cabbage rolls and a little bit of rice. And I have to cook the rice and put it in there. And it\u2019s the regular long grain rice. And I thought it would cook, you know, in the rolls. N: Right P: But it appears not to get done so the first half of the cabbage rolls I ate was crunchy rice. N: Oh, ok. P: I just wanted to ask if there\u2019s anything I should watch out for because I know raw rice is not a good thing for you. (laughs) N: I\u2019ll ask Dr. LAWSON ok, coz I\u2019m not sure to be honest with you, but I\u2019ll ask Dr. LAWSON. I\u2019ll call you back and let you know, ok? P: Ok. I\u2019m just concerned because people stop throwing rice at weddings because birds would eat it. And they get stuck in their stomachs. Now they probably don\u2019t have enough enzymes, but we can probably break down rice and stuff but I just called to make sure. N: Ok, well I\u2019ll ask her and I\u2019ll call you back and let you know, ok? P: ok, Thanks. N: Bye-bye. Appendix C Complete and summarized dialogues C.1 Complete dialogue with each turn labeled with corresponding summaries that contain this turn (G: gold standard, R: random, T: true semantic type, P: predicted semantic type) R T P P: It\u2019s the machine, I couldn\u2019t turn it on P N: What\u2019s the matter? G P P: The pressure, arterial pressure, I mean the venous pressure, I couldn\u2019t even turn the pump on G R P N: Did you have the transducer hooked up? Your monitor is on? G R T P P: Yes ma\u2019am, my blood won\u2019t flush, every time I try to turn the pump on, its either I got a negative pressure, arterial has a pressure now, and both of my catheters, I have an arterial pressure of 220 and a venous pressure of 180. I don\u2019t even have my pump open. R N: You don\u2019t have any pumps open where? On your catheter? P: I have pressures a little bit there. R N: I can hear the warning. Does it flush ok? P: Yeah N: I will try switching the ports. Start the pump and clamp off your lines and try switching the ports. And then turn it on and see what happens G N: Can you come off and put your blood in recirculation? I\u2019ll go ahead and call technical support and see if they have any suggestions. I can\u2019t think of anything else that can be causing it. T P N: How are you feeling? R P: I feel fine. G R N: You feel better? Your target weight\u2019s ok? G T P P: My blood pressure was 147/79, when I sit it drops to 139/73. My pulse is good, 80 and 84. R N: And how\u2019s your weight now P: 129.2 G N: Your blood pressure medicine, I\u2019ll have you finish that. P: I finished taking that on Friday G N: Oh, so you finished taking that Friday, and the diarrhea and nausea, all that stopped. P: Yuh T P N: Ok, that\u2019s good. Go ahead and call technical support and then just call me back and let me know what they say, ok? C.2 The four summaries (P: patient, N: nurse) Gold standard P: The pressure, arterial pressure, I mean the venous pressure, I couldn\u2019t even turn the pump on N: Did you have the transducer hooked up? Your monitor is on? P: Yes ma\u2019am, my blood won\u2019t flush, every time I try to turn the pump on, its either I got a negative pressure, arterial has a pressure now, and both of my catheters, I have an arterial pressure of 220 and a venous pressure of 180. I don\u2019t even have my pump open. N: Can you come off and put your blood in recirculation? I\u2019ll go ahead and call technical support and see if they have any suggestions. I can\u2019t think of anything else that can be causing it. N: You feel better? Your target weight\u2019s ok? P: My blood pressure was 147/79, when I sit it drops to 139/73. My pulse is good, 80 and 84. N: Your blood pressure medicine, I\u2019ll have you finish that. N: Oh, so you finished taking that Friday, and the diarrhea and nausea, all that stopped. Random P: It\u2019s the machine, I couldn\u2019t turn it on N: Did you have the transducer hooked up? Your monitor is on? P: Yes ma\u2019am, my blood won\u2019t flush, every time I try to turn the pump on, its either I got a negative pressure, arterial has a pressure now, and both of my catheters, I have an arterial pressure of 220 and a venous pressure of 180. I don\u2019t even have my pump open. N: You don\u2019t have any pumps open where? On your catheter? N: I can hear the warning. Does it flush ok? P: I feel fine. N: You feel better? Your target weight\u2019s ok? N: And how\u2019s your weight now \u201cTrue semantic type\u201d based P: It\u2019s the machine I couldn\u2019t turn it on P: Yes ma\u2019am, my blood won\u2019t flush, every time I try to turn the pump on, its either I got a negative pressure, arterial has a pressure now, and both of my catheters, I have an arterial pressure of 220 and a venous pressure of 180. I don\u2019t even have my pump open. N: How are you feeling P: My blood pressure was 147/79, when I sit it drops to 139/73. My pulse is good, 80 and 84. N: Ok, that\u2019s good. Go ahead and call technical support and then just call me back and let me know what they say, ok? \u201cPredicted semantic type\u201d based P: It\u2019s the machine, I couldn\u2019t turn it on N: What\u2019s the matter? P: The pressure, arterial pressure, I mean the venous pressure, I couldn\u2019t even turn the pump on N: Did you have the transducer hooked up? Your monitor is on? P: Yes ma\u2019am, my blood won\u2019t flush, every time I try to turn the pump on, its either I got a negative pressure, arterial has a pressure now, and both of my catheters, I have an arterial pressure of 220 and a venous pressure of 180. I don\u2019t even have my pump open. N: How are you feeling? P: My blood pressure was 147/79, when I sit it drops to 139/73. My pulse is good, 80 and 84. N: ok, that\u2019s good. Go ahead and call technical support and then just call me back and let me know what they say, ok? Appendix D Instructions given to evaluators D.1 Instructions Dear Doctor, Below are some dialogues between dialysis nurses and patients. After reading each dialogue, please answer the 6 (yes/no) questions that follow. Some dialogues are incomplete, so just answer the best you can. Thanks a lot for doing this amidst your busy schedule. Questions: 1. Did a clinical problem require urgent intervention? 2. Did the patient mention either his vital signs (blood pressure, pulse rate, temperature), his weight, any symptoms, or his medications? 3. Was there a problem with the machine that required technical support? 4. Did the call require a follow-up (i.e., need to consult with another nurse, a physician, a technician or a supplier and/or require further laboratory investigation outside of the current call)? 5. Did the patient need to make, verify, cancel or reschedule an appointment? 6. Did the patient need to be dialyzed in-center? References [1] J.R. Hampton M.J. Harrison J.R. Mitchell J.S. Prichard C. Seymour Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients BMJ 2 5969 1975 486 489 [2] R.S. Lockridge Jr. Daily dialysis and long-term outcomes-the Lynchburg nephrology NHHD experience Nephrol News Issues 13 12 1999 16 19, 23\u20136 [3] Document Understanding Workshop. HLT/NAACL Annual Meeting. Boston, MA. May, 2004. In: http://duc.nist.gov/. Accessed on: June 16, 2005. [4] Kupiec J, Pedersen J, Chen F. A trainable document summarizer. Research and development in information retrieval. 1995;68\u201373. In: http://citeseer.csail.mit.edu/kupiec95trainable.html. [5] Edmundson HP. New methods in automatic extracting. In: Mani, Maybury, editor. Advances in automatic text summarization. Cambridge, MA: MIT Press; 1999. p. 23\u201342. [6] McKeown K, Radeev DR. Generating summaries of multiple news articles. In: Mani, Maybury, editor. Advances in automatic text summarization. Cambridge, MA: MIT Press; 1999. p. 381\u201390. [7] Merlino A, Maybury M. An empirical study of the optimal presentation of multimedia summaries of broadcast news. In: Mani, Maybury, editor. Advances in automatic text summarization. Cambridge, MA: MIT Press; 1999. p. 391\u2013402. [8] Marcu D. Discourse trees are good indicators of importance in text. In: Mani, Maybury, editor. Advances in automatic text summarization. Cambridge, MA: MIT Press; 1999. p. 123\u201336. [9] Teufel S, Moens M. Argumentative classification of extracted sentences as a first step towards flexible abstracting. In: Mani, Maybury, editor. Advances in automatic text summarization. Cambridge, MA: MIT Press; 1999. p. 155\u201376. [10] McKeown K, Hirschberg J, Galley M, Maskey S. From text to speech summarization. ICASSP. 2005. Philadelphia, PA. In: http://www1.cs.columbia.edu/~galley/papers/from_txt_to_speech.pdf. Last accessed: June 20, 2005. [11] A.T. McCray A.R. Aronson A.C. Browne T.C. Rindflesch A. Razi S. Srinivasan UMLS knowledge for biomedical language processing Bull Med Libr Assoc 81 2 1993 184 194 [12] H. Xu K. Anderson V.R. Grann C. Friedman Facilitating cancer research using natural language processing of pathology reports Medinfo 2004 565 572 [13] Y. Hsieh G.A. Hardardottir P.F. Brennan Linguistic analysis: terms and phrases used by patients in e-mail messages to nurses Medinfo 2004 511 515 [14] J.R. Landis G.G. Koch The measurement of observer agreement for categorical data Biometrics 33 1977 159 174 [15] R. Schapire Y. Singer Boostexter: a boosting-based system for text categorization Machine Learning 39 2/3 2000 135 168 [16] Lacson R, Barzilay R. Automatic processing of spoken dialogue in the home hemodialysis domain. In: Proceedings of the AMIA annual fall syposium. Bethesda, MD: AMIA; 2005. [17] P.F. Brown P.V. DeSouza R. Mercer V.J. Della Pietra J.C. Lai Class-based n-gram models of natural language Computational Linguistics 18 1992 467 479 [18] S. Miller J. Guinness A. Zamanian Name tagging with word clusters and discriminative training HLT-NAACL 2004 337 342 [19] Lacson R, Long W. How accurate are experts in spotting the right words in a dialogue? In: http://people.csail.mit.edu/rlacson/dialogue_poster.doc. Last accessed: June 20, 2005. [20] Lacson R, Lacson E, Szolovits P. Discourse structure of medical dialogue. In: Proceedings of Medinfo. Amsterdam, Netherlands: IOS Press; 2004;1703. [21] Goldstein J, Kantrowitz M, Mittal V, Carbonell J. Summarizing text documents: sentence selection and evaluation metrics. In: Proceedings of the 22nd ACM SIGIR. New York: ACM Press; 1999. p. 121\u20138. [22] J.M. Bland D.G. Altman The odds ratio BMJ 320 2000 1468 [23] Zechner K, Waibel A. Diasumm: flexible summarization of spontaneous dialogues in unrestricted domains. In: Proceedings of 18th international conference on computational linguistics, vol. 1. Germany: Morgan Kaufman Publishers; 2000. p. C00\u20132140. [24] Allan J, Gupta R, Khandelwal V. Temporal summaries of news topics. In: Proceedings of SIGIR. New York: ACM Press; 2001;10\u20138. [25] K.S. Jones J.R. GAlliers Evaluating natural language processing systems: an analysis and review 1996 New York Springer [26] Jing H, Barzilay R, McKeown K, Elhadad M. Summarization evaluation methods: experiments and analysis. AAAI intelligent text summarization workshop (Stanford, CA): Mar. 1998. p. 60\u20138. [27] P. Lehoux Patients\u2019 perspectives on high-tech home care: a qualitative inquiry into the user-friendliness of four technologies BMC Health Serv. Res. 4 1 2004 28 [28] Teufel S. Task-based evaluation of summary quality: describing relationships between scientific papers. In: Proceedings of NAACL-01 workshop on automatic text summarization. Germany: Morgan Kaufman Publishers; 2001. [29] Pallett D, Fiscus J, Garofolo J. Resource management corpus: September 1992 test set benchmark test results. In: Proceedings of ARPA microelectronics technology office continuous speech recognition workshop (Stanford, CA): September 1992. p. 21\u20132. [30] Conroy J, O\u2019Leary D. Text summarization via hidden Markov models. In: Proceedings of 24th annual international ACM SIGIR conference on research and development in information retrieval. 2001. New York: ACM Press; 2001. p. 406\u20137. [31] J. Lafferty F. Pereira A. McCallum Conditional random fields: probabilistic models for segmenting and labeling sequence data International Conference on Machine Learning 2001 282 289 [32] Sakai T, Sparck-Jones K. Generic summaries for indexing in information retrieval. In: Proceedings of 24th annual international ACM SIGIR conference on research and development in information retrieval. New York: ACM Press; 2001. p. 190\u20138.", "scopus-id": "33748182760", "pubmed-id": "16488194", "coredata": {"eid": "1-s2.0-S1532046406000025", "dc:description": "Abstract Spoken medical dialogue is a valuable source of information for patients and caregivers. This work presents a first step towards automatic analysis and summarization of spoken medical dialogue. We first abstract a dialogue into a sequence of semantic categories using linguistic and contextual features integrated in a supervised machine-learning framework. Our model has a classification accuracy of 73%, compared to 33% achieved by a majority baseline (p <0.01). We then describe and implement a summarizer that utilizes this automatically induced structure. Our evaluation results indicate that automatically generated summaries exhibit high resemblance to summaries written by humans. In addition, task-based evaluation shows that physicians can reasonably answer questions related to patient care by looking at the automatically generated summaries alone, in contrast to the physicians\u2019 performance when they were given summaries from a na\u00efve summarizer (p <0.05). This work demonstrates the feasibility of automatically structuring and summarizing spoken medical dialogue.", "openArchiveArticle": "true", "prism:coverDate": "2006-10-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046406000025", "dc:creator": [{"@_fa": "true", "$": "Lacson, Ronilda C."}, {"@_fa": "true", "$": "Barzilay, Regina"}, {"@_fa": "true", "$": "Long, William J."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046406000025"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046406000025"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(06)00002-5", "prism:volume": "39", "prism:publisher": "Elsevier Inc.", "dc:title": "Automatic analysis of medical dialogue in the home hemodialysis domain: Structure induction and summarization", "prism:copyright": "Copyright \u00a9 2006 Elsevier Inc. All rights reserved.", "prism:issueName": "Dialog Systems for Health Communications", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "5", "dcterms:subject": [{"@_fa": "true", "$": "Spoken dialogue analysis"}, {"@_fa": "true", "$": "Medical discourse"}, {"@_fa": "true", "$": "Summarization"}, {"@_fa": "true", "$": "Natural language processing"}, {"@_fa": "true", "$": "Information processing"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "5", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "541-555", "prism:endingPage": "555", "prism:coverDisplayDate": "October 2006", "prism:doi": "10.1016/j.jbi.2005.12.009", "prism:startingPage": "541", "dc:identifier": "doi:10.1016/j.jbi.2005.12.009", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "38", "@width": "234", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1564", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "408", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "3074", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "336", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2300", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "93", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "10347", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "31", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1586", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "831", "@width": "623", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "117809", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "70", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2206", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "791", "@width": "623", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr3a.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "113705", "@ref": "gr3a", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "73", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr3a.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2287", "@ref": "gr3a", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "422", "@width": "622", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "44750", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "85", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000025-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2340", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33748182760"}}