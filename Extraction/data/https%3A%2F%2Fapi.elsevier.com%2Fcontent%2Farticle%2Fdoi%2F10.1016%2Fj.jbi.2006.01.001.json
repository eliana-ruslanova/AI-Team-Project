{"scopus-eid": "2-s2.0-33748168229", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2006-02-20 2006-02-20 2010-10-06T18:12:05 1-s2.0-S1532046406000177 S1532-0464(06)00017-7 S1532046406000177 10.1016/j.jbi.2006.01.001 S300 S300.2 FULL-TEXT 1-s2.0-S1532046406X00321 2015-05-15T06:30:58.184067-04:00 0 0 20061001 20061031 2006 2006-02-20T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 39 39 5 5 Volume 39, Issue 5 7 514 531 514 531 200610 October 2006 2006-10-01 2006-10-31 2006 Dialog Systems for Health Communications Timothy Bickmore Special Issue Articles article fla Copyright \u00a9 2006 Elsevier Inc. All rights reserved. USERMODELINGADAPTATIONINHEALTHPROMOTIONDIALOGSANIMATEDCHARACTER ROSIS F 1 Introduction 2 Motivation 2.1 Previous experiences 2.2 Theoretical background 2.2.1 Affective state 2.2.2 Transtheoretical Model of Change 3 Our first prototype of dialog simulator 3.1 Method 3.1.1 The corpus 3.1.2 Corpus labeling 3.1.3 Measures of agreement among raters 3.1.4 Move classification 3.2 Results 3.3 Prototype design 3.3.1 User modeling 3.3.2 Dialog adaptation 3.3.3 An example 4 Is interacting with an ECA the same as interacting with a human counselor? 5 A corpus of wizard of Oz dialogs 5.1 Method 5.1.1 The corpus 5.1.2 Method of analysis 5.1.3 Corpus labeling 5.2 Results 6 Building a model of the user 6.1 Move parsing 6.2 Integration of signs in a Bayesian network 6.3 Model validation 7 Conclusions Acknowledgments References ANG 2002 J BALL 2003 E EMOTIONSINHUMANSARTIFACTS ABAYESIANHEARTCOMPUTERRECOGNITIONSIMULATIONEMOTION BATES 1994 J BATLINER 2003 A BERRY 2005 D BICKMORE 2005 21 30 T CARBERRY 2002 S CARLETTA 1996 J CASSELL 2003 J 2000 EMBODIEDCONVERSATIONALAGENTS CAVALLUZZI 2004 A AFFECTIVEDIALOGUESYSTEMS AFFECTIVEADVICEGIVINGDIALOGS COLBY 1981 K COLBY 1990 K COLBY 1999 K MACHINECONVERSATIONS HUMANCOMPUTERCONVERSATIONINACOGNITIVETHERAPYPROGRAM CONATI 2005 C USERMODELING DATADRIVENREFINEMENTAPROBABILISTICMODELUSERAFFECT COOPER 1992 G COWIE 2000 R DECAROLIS 2003 B LIFELIKECHARACTERSTOOLSAFFECTIVEFUNCTIONSAPPLICATIONS APMLAMARKUPLANGUAGEFORBELIEVABLEBEHAVIORGENERATION DIEUGENIO 2004 B EKMAN 1992 P GRASSO 2000 F LAFORGE 1994 361 374 R LEE 2002 C LI 2000 777 781 R LITMAN 2003 D MANNING 1999 C FOUNDATIONSSTATISTICALNATURALLANGUAGEPROCESSING NASS 2000 C EMBODIEDCONVERSATIONALAGENTS TRUTHBEAUTYRESEARCHINGEMBODIEDCONVERSATIONALAGENTS NICHOLSON 1994 1593 1610 A NIJHOLT 2004 A ORTONY 1988 A COGNITIVESTRUCTUREEMOTIONS OVIATT 2000 S EMBODIEDCONVERSATIONALAGENTS DESIGNINGEVALUATINGCONVERSATIONALINTERFACESANIMATEDCHARACTERS PEARL 1988 J PROBABILISTICREASONINGININTELLIGENTSYSTEMSNETWORKSPLAUSIBLEINFERENCE PELACHAUD 2003 C COMMUNICATIONINMULTIAGENTSYSTEMSBACKGROUNDCURRENTTRENDSFUTURE COMPUTATIONALMODELBELIEVABLECONVERSATIONALAGENTS PENNEBAKER 2003 J PICARD 1997 R AFFECTIVECOMPUTING PICARD 2002 R EMOTIONSINHUMANSARTEFACTS MEANFORACOMPUTEREMOTIONS 1996 CIRCUMPLEXMODELSPERSONALITYEMOTIONS PROCHASKA 1992 1102 1114 J 2004 BROWSTILLTRUSTEVALUATINGEMBODIEDCONVERSATIONALAGENTS SILLINCE 1991 J SILVERMAN 2001 B STOCK 2003 O STORM 1987 C VELICER 1998 W WALTON 1992 D PLACEEMOTIONINARGUMENT WEGMAN 1988 C COGNITIVEPERSPECTIVESEMOTIONMOTIVATION EMOTIONARGUMENTATIONINEXPRESSIONOPINION 1999 MACHINECONVERSATIONS ROSISX2006X514 ROSISX2006X514X531 ROSISX2006X514XF ROSISX2006X514X531XF Full 2014-11-20T07:55:50Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(06)00017-7 S1532046406000177 1-s2.0-S1532046406000177 10.1016/j.jbi.2006.01.001 272371 2010-11-08T12:11:50.870484-05:00 2006-10-01 2006-10-31 1-s2.0-S1532046406000177-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/MAIN/application/pdf/9472039cfa3b56bdf3df032d08188c75/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/MAIN/application/pdf/9472039cfa3b56bdf3df032d08188c75/main.pdf main.pdf pdf true 493197 MAIN 18 1-s2.0-S1532046406000177-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/PREVIEW/image/png/5fe1053c24f9ae7e27e533576fad4710/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/PREVIEW/image/png/5fe1053c24f9ae7e27e533576fad4710/main_1.png main_1.png png 64342 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046406000177-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/STRIPIN/image/gif/acbe0838bebf3de0a58d9b37254cee36/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/STRIPIN/image/gif/acbe0838bebf3de0a58d9b37254cee36/si1.gif si1 si1.gif gif 825 17 187 ALTIMG 1-s2.0-S1532046406000177-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/gr1/DOWNSAMPLED/image/jpeg/e0e4c71eda560d869aaad1be4314b87f/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/gr1/DOWNSAMPLED/image/jpeg/e0e4c71eda560d869aaad1be4314b87f/gr1.jpg gr1 gr1.jpg jpg 76649 324 487 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000177-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/gr1/THUMBNAIL/image/gif/09f2dacdedd0bcedf3cf9a74b85101bb/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/gr1/THUMBNAIL/image/gif/09f2dacdedd0bcedf3cf9a74b85101bb/gr1.sml gr1 gr1.sml sml 4178 83 125 IMAGE-THUMBNAIL 1-s2.0-S1532046406000177-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/gr2/DOWNSAMPLED/image/jpeg/161d66965a85b381f709fe29731de56d/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/gr2/DOWNSAMPLED/image/jpeg/161d66965a85b381f709fe29731de56d/gr2.jpg gr2 gr2.jpg jpg 82870 417 623 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000177-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000177/gr2/THUMBNAIL/image/gif/2a9b878cd1464f8a141a6a6d4aeca1ab/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000177/gr2/THUMBNAIL/image/gif/2a9b878cd1464f8a141a6a6d4aeca1ab/gr2.sml gr2 gr2.sml sml 3494 84 125 IMAGE-THUMBNAIL YJBIN 1268 S1532-0464(06)00017-7 10.1016/j.jbi.2006.01.001 Elsevier Inc. Fig. 1 The character employed in our wizard of Oz studies. Fig. 2 Structure of the dynamic user model. Table 1 Markup language for emotion and stage of change Sign Definition Values Examples (with shared interpretation) Emotional state Valence Whether the state is perceived as pleasant to the individual in that state Positive/negative/unknown Well, it does help to talk with someone (positive) I feel I\u2019m being lectured rather than listened to! (negative) Intensity Whether the manifestation of the state is strong High/low/unknown When the doctor told me I could never work again, I was very depressed (high) Well, it does help to talk to someone (low) Believes behavior wrong Whether the subject is aware that his/her behavior is \u201cwrong\u201d Yes/no/maybe Yes, I know it\u2019s bad for me (yes) Well, I\u2019m not really sure if it\u2019s a problem at all (no) Intends to change Whether the subject wants to change behavior Yes/no/maybe Well, I want to do something. I do not want to just let this go on (yes) Well, it\u2019s like this: I\u2019d like to give up, but it is just too much for me at the moment (no) Knows about a plan Whether the subject knows about a plan to follow to change behavior or, in general, if he/she has enough information about how to change Yes/no/maybe I think I should cut down on caffeine (yes) Accepts plan Whether the subject accepts the plan proposed by the therapist Yes/no/maybe That seems best (yes) Is following a plan Whether the subject is already following a plan Yes/no/maybe I changed my diet in 1992 (yes) Table 2 Agreement rate and frequency for emotion and stage of change tags Frequency (%) (in the subset of labeled moves) Full agreement Weak agreement Disagreement \u03ba Valence 83 .55 .02 .43 .27 Intensity 65 .45 .20 .34 .17 Believes behavior wrong 17 .80 .04 .11 .26 Table 3 Markup language and agreement among raters for signs of social attitude towards the agent Sign with definition Values Example Friendly self-introduction The subjects introduce themselves with a friendly attitude (e.g., by giving her name or by explaining the reasons why they are participating in the dialog) Yes/no Oz: Hi. My name is Valentina. I\u2019m here to suggest how to improve your diet S: Hi, my name is Isa and I\u2019m curious to get some information about healthy eating Familiar style Whether the subject employs a current language, dialectal forms, proverbs, etc Yes/no Oz: Are you attracted by sweets? S: I\u2019m crazy about them Talks about self Whether the subject provides more personal information about self than requested by the agent Yes/no Oz: Do you like sweets? Do you ever stop in front of the display window of a beautiful bakery? S: Very much! I\u2019m greedy! Personal questions about or suggestions to the agent Whether the subjects tries to know something about the agent preferences, lifestyle, etc., or to give suggestions rather than receiving them Yes/no Oz: What did you eat at lunch? S: Meet-stuffed peppers. How about you? Humor and irony Whether the subjects make any kind of verbal joke in their move Yes/no Oz: I know we risk entering into private issues. But did you ever ask yourself what are the reasons for your eating habits? S: Unbridled life, with a light aversion towards healthy food Comments on the dialog Whether the subjects comments the agent behavior in the dialog: comments may be about the agent experience, its degree of domain knowledge, the length of its moves etc Positive Oz: I\u2019m sorry, I\u2019m not much of an expert in this domain S: OK: but try to get more informed, right? Negative Oz: Good bye S: What are you doing? You leave me this way? You are rude!! Friendly farewell This may consist in using a friendly farewell form or in asking to carry-on the dialog Yes/no Oz: Goodbye. It was really pleasant to interact with you. Come back when you wish S: But I would like to chat a bit more with you Table 4 Agreement among raters for signs of social attitude towards the agent Sign Values Frequency (%) (subset of labeled moves) Frequency (%) (whole corpus) Agreement rate \u03ba Friendly self-introduction Yes/no 5 2 .98 .87 Familiar style Yes/no 85 28 .33 .16 Talks about self Yes/no 57 19 .73 .64 Personal questions about or suggestion to the agent Yes/no 38 12 .70 .56 Humor and irony Yes/no 7 2 .84 .36 Comments on the dialog Positive 13 4 .82 .42 Negative 16 5 .86 Friendly farewell Yes/no 11 4 .93 .65 Table 5 Agreement among raters for emotion valence and stage of change, in the subset of selected moves Frequency (%) (subset of labeled moves) Frequency (%) (whole corpus) Full agreement Weak agreement Disagreement \u03ba Believes behavior wrong 48 10 .65 .26 .09 .46 Intends to change 28 6 .76 .21 .02 .54 Table 6 Recognition criteria and predictive capacity of the parser Signs Criteria Sensitivity Specificity Proportion of correctly classified cases Friendly self-introduction Expressions of greetings (\u2018ciao,\u2019 \u2018hello,\u2019\u2026) or of self-presentation (\u2018my name is\u2026\u2019) 0.91 0.98 0.97 Familiar style Agent name (\u2018Valentina\u2019), interjections (\u2018!,\u2019 \u2018Hurrah,\u2019\u2026), friendly lexicon (\u2018papa,\u2019 \u2018mummy,\u2019 \u2018greedy,\u2019 \u2018chat,\u2019 \u2018my passion,\u2019 \u2018dear,\u2019\u2026), dialectal expressions (\u2018cute,\u2019 \u2018espressino,\u2019\u2026), diminutive or expressive forms (\u2018little sweet,\u2019 \u2018fatty,\u2019\u2026) 0.36 0.96 0.79 Talks about self Personal pronouns (\u2018I,\u2019 \u2018my,\u2019 \u2018to me,\u2019..), auxiliary verbs (\u2018I have,\u2019 \u2018I am,\u2019\u2026), expressions of knowledge (\u2018I know,\u2019 I believe,\u2019\u2026), of attitude (\u2018I try,\u2019 \u2018I think,\u2019 \u2018I tend to,\u2019 \u2018I care of,\u2019\u2026), domain verbs (\u2018I eat,\u2019 \u2018I drink,\u2019.. all at the first person) 0.81 0.79 0.80 Question about the agent Similar to the previous one, but with the second pronoun 0.80 0.92 0.91 Positive comments Expressions of agreement (\u2018OK,\u2019 \u2018right,\u2019 \u2018good,\u2019 \u2018true,\u2019\u2026), of attitude (\u2018I agree,\u2019 \u2018I trust,\u2019\u2026), of opinion about the agent (\u2018That\u2019s kind of you,\u2019\u2026) 0.56 0.94 0.92 Negative comments Objections (\u2018no,\u2019 \u2018but,\u2019\u2026) negative evaluations about the agent (\u2018you are rude,\u2019 \u2018you don\u2019t know,\u2019 \u2018you don\u2019t understand\u2019) or about the message received (\u2018this is too much,\u2019 \u2018too little,\u2019\u2026) 0.16 0.98 0.93 Friendly farewell Expressions of farewell (\u2018bye,\u2019 \u2018see you soon,\u2019\u2026), of thanking and wishes (\u2018thanks,\u2019\u2026) 0.83 0.97 0.96 Believes behavior wrong Declaration of own \u2018wrong\u2019 behaviors (\u2018I don\u2019t believe I take all the substances I need\u2019 or \u2018\u2019I can\u2019t follow a more correct dietary habit,\u2019\u2026); description of own shape as being \u2018not ideal\u2019 (\u2018I\u2019m overweight\u2019) 0.40 0.97 0.92 Intends to change Weak or strong manifestations of desires to change (\u2018I would like to,\u2019 \u2018I should,\u2019 I must,\u2019\u2026 \u2018change my dietary habits,\u2019\u2026 or similarities). Requests of suggestion (\u2018What should I do?\u2019) 0.51 0.97 0.95 Table 7 Variables included in the model Variable category Variable name Label Stable user characteristics Background Back Gender Gend Context Type of last agent move Ctext Type of user move Mtype Monitored variables User attitude towards the agent Satt Stage of change SoC Signs of social attitude Familiar style Fstyl Friendly self-introduction Fsint Talks about self Perin Questions about agent Qagt Friendly farewell F-Fw Comments Comm Signs of stage of change Believes behavior wrong Bbw Intends to change Itc Results of parsing Cues of familiar style Pfstyl Cues of friendly self-introduction Pfsint Cues of talks about self Pperin Cues of questions to the agent Pqagt Cues of friendly farewell Pffw Cues of comments Pcomm Cues of belief that behavior is wrong Pbbw Cues of intention to change Pitc User modeling and adaptation in health promotion dialogs with an animated character Fiorella de Rosis \u204e derosis@di.uniba.it http://www.di.uniba.it/intint/ Nicole Novielli novielli@di.uniba.it Valeria Carofiglio carofiglio@di.uniba.it Addolorata Cavalluzzi cavalluzzi@di.uniba.it Berardina De Carolis decarolis@di.uniba.it Department of Informatics, University of Bari, Italy \u204e Corresponding author. Fax: +39 080 544 3284. Abstract In this paper, we describe our experience with the design and implementation of an embodied conversational agent (ECA) that converses with users to change their dietary behavior. Our intent is to develop a system that dynamically models the agent and the user and adapts the agent\u2019s counseling dialog accordingly. Towards this end, we discuss our efforts to automatically determine the user\u2019s dietary behavior stage of change and attitude towards the agent on the basis of unconstrained typed text dialog, first with another person and then with an ECA controlled by an experimenter in a wizard of Oz study. We describe how the results of these studies have been incorporated into an algorithm that combines the results from simple parsing rules together with contextual features using a Bayesian network to determine user stage and attitude automatically. Keywords User and agent modeling Healthy eating dialog Embodied conversational agents Bayesian networks Wizard of Oz studies 1 Introduction Conversational systems can be employed in support of health care in limited domains given their potential for low-cost and wide accessibility. Such systems may be especially efficacious for patients and consumers who are used to using computer systems and the web to obtain health information. Conversational systems that counsel users on dietary behavior represent an especially promising application in this area. Fruit and vegetable consumption alone plays a protective role in a large number of cancers, and is associated with reduced risk for heart disease, stroke, and hypertension, yet only a small percentage of adults meet the government guidelines for daily fruit and vegetable consumption [37]. In behavior change counseling, expert counselors must be finely attuned to their patients\u2019 emotional state, including their attitude towards the counselor, and adapt their dialog accordingly. This adaptivity is one of the features which gives new conversational systems the potential to become, if not competitive, at least supportive of encounters with human therapists, when compared with previous seminal experiences like, e.g., PARRY or GURU [15,16]. 1 A reflection on these experiences may be found in [17] with an interesting discussion of early works on computer conversations in the same volume [68]. 1 Embodied conversational agents (ECAs) are a new metaphor of human\u2013computer interaction which aims at providing the users with the illusion of cooperating with a human partner rather than just \u2018using a tool\u2019: their application to health promotion dialogs might therefore be of benefit to increase the usability of these systems [12]. The more ECAs become \u2018believable,\u2019 with all the shades this term has acquired after the initial definition in [3], the more users can be expected to show some sign of \u2018social relationship\u2019 with them: in addition to understanding the user problems, agents should be equipped to perceive these signs and to respond appropriately. In this paper, we describe our experience with the design and implementation of an ECA that converses with users to change their dietary behavior. Two theories guided us in designing this system: the Transtheoretical Model of health behavior change and theories on the role of affect in persuasion. Prochaska and Di Clemente\u2019s Transtheoretical Model of health behavior change outlines a series of stages that people naturally go through when changing their health behavior, as well as the subset of behavior change techniques that are especially effective in each stage [57]. In addition, several persuasion theories state that the emotional state of the persuadee\u2014including their attitude towards the persuader\u2014should be considered as a significant factor in selecting and applying persuasion strategies [67,60,66,41]. The conversational system we wanted to implement therefore had to be endowed with the following features: \u2022 dynamic user modeling, to recognize the users state and revise this image during interaction; \u2022 dynamic agent modeling, to represent the agent\u2019s emotional reaction to what the user says; \u2022 double adaptation of the dialog, to the user and the agent. In the dialog system we were ultimately designing, agent utterances will be spoken by an ECA. However, to reproduce the typical interaction mode of information systems on the web, user utterances would be typed. Thus, a prerequisite for our system was the development of methods to determine users\u2019 mental and affective state on the basis of typed text utterances within the context of a diet counseling dialog. As many researchers have pointed out, affect recognition requires integration of cues from multiple modalities to achieve an acceptable level of accuracy: see [51] and [52] and the website of HUMAINE, the European Human\u2013Machine Interaction Network of Excellence on Emotions. 2 http://emotion-research.net. 2 Uncertainty is an unavoidable factor of any emotion recognition process. To address this problem, we represent the user model with an appropriate formalism (Bayesian networks) in which knowledge about the context in which the user utterance was made can be integrated to clarify recognition and to increase the predictive value of the model. The following are the main design, implementation, and revision steps we followed in our research: 1. definition of a theoretical background for our research (Section 2 of this paper); 2. development of a first prototype (Section 3) after analysis of a corpus of \u2018natural\u2019 dialogs to identify signs of \u2018stage of change\u2019 and the emotional state of the user, and adapt the dialog strategies accordingly; 3. informal evaluation of this prototype, (Section 4) with reflection on the difference between affective problems related to interacting with a conversational character vs. a therapist; 4. collection of a corpus of dialogs with an ECA by means of a wizard of Oz study (Section 5); 5. design of a dynamic user modeling component (Section 6) to integrate the dialog system with the ability to deal with \u2018social relationship\u2019 factors. We will discuss the interest and limits of our work in Section 7 by reflecting, in particular, on the role that embodied agents might play in this application domain. 2 Motivation Health promotion is considered a promising application domain and, at the same time, a fertile testing arena for computer-simulated dialogs in general and for socially intelligent animated characters in particular. Successful experiences in this application domain, together with a body of established theories, were the background on which we built our experience. 2.1 Previous experiences As far as we know, Daphne [30] was one of the first systems in which the problem of promoting health nutrition was considered. Although the project was aimed at developing a theory of \u2018informal argumentation\u2019 rather than implementing a conversational agent in the domain, the system adapted its argumentation strategy to the respondent\u2019s preferences rather than attempting to solve conflicts due to differences in opinions between the two participants to the dialog. More focused on ECAs, Silverman et al. [61] combined a generic simulation package with animated pedagogical agents to promote health behavior shifts in heart attacks. Bickmore employed Laura in FitTrack, by endowing this character with the ability to establish a working alliance, an essential prerequisite of any successful therapist\u2013client relationship. Trust and empathy were considered, in particular, as key factors of the bond relationship between the two participants to the dialog, to be achieved by means of adequate dialog strategies [6]. By extending their previous experience with Carmen\u2019s Bright IDEAS [40], DESIA integrated an embodied pedagogical agent into a psychosocial intervention deployed on a handheld computer, to help mothers of pediatric cancer patients to cope with problems encountered in care giving [34]. As in the case of FitTrack, this system was designed to work on long-term interaction. It adapts the choice of presentation modes to the context of use and keeps track of information about past encounters to tailor its explanations. The common denominator of these applications is in considering forms of social intelligence like encouragement, display of empathy, promotion of \u2018positive face,\u2019 and developing of a rapport as key factors in motivating the users in their application domain. 2.2 Theoretical background 2.2.1 Affective state Affective factors may include long-term personality traits or shorter-term states ranging from \u2018affect dispositions,\u2019 \u2018attitudes\u2019 (liking, loving, hating,\u2026), \u2018interpersonal stances\u2019 (distant, cold, warm,\u2026), \u2018moods\u2019 (cheerful, irritable, depressed,\u2026) or \u2018real emotions\u2019 (K. Scherer, Glossary in the HUMAINE website). Emotions have been defined, in particular, according to at least two views: as points in a two-dimensional space of valence and arousal (according to the \u2018Circumplex Model of Emotions\u2019: [54]) or as a set of \u2018basic emotions\u2019 [28], possibly classified according to their activation factors [45]. Other authors proposed a categorization into \u2018individual\u2019 emotions, referring to self (fear, hope, joy, etc.) and \u2018social emotions,\u2019 which originate from relationships with others: sympathy, antipathy, tenderness, sense of friendship etc. [55]. The second category is tightly related to Scherer\u2019s concept of interpersonal stance. Both individual and social emotions are expected to occur in client\u2013therapist dialogs, with a relative frequency that depends on the client\u2019s problems discussed. 2.2.2 Transtheoretical Model of Change Interventions to promote a health-behavior change often adopt the Transtheoretical Model of Change [57]. According to this theory, health promotion plans should be adapted to the degree of advancement of the process followed by subjects in changing their beliefs and attitudes, that is their \u2018stage of change.\u2019 Expert therapists apply this theory by selecting intervention techniques that are known to be particularly effective for the stage of change their client is currently in. Some dialog systems try to simulate this situation: in the most famous of them, which is due to Bickmore (Laura), dialogs alternate between a listening and a persuading phase. The encounters of the client with the artificial agent are repeated at fixed time intervals by adapting, every time, to the evolved stage of the subject. The Stage of Change (SoC) Model identifies five main steps in changing behavior : \u2022 in the pre-contemplation stage, subjects believe that their behavior is acceptable and do not want to change it; \u2022 in the contemplation stage, they doubt that their behavior is acceptable, seriously consider the opportunity of changing it but do not want to commit to do it soon; \u2022 in the preparation stage, they believe that their behavior should be changed and intend to do it soon; \u2022 in the action stage, they are following a plan to change their behavior (for some months); \u2022 in the maintenance stage, they are maintaining the change for more than 6 months. These definitions suggest how stages may be recognized from a set of \u2018signs\u2019 which are related to the mental state of the subjects: \u2022 belief that their behavior is \u2018right\u2019 or \u2018wrong,\u2019 value attributed to the \u2018right\u2019 behavior and knowledge of reasons influencing the adoption of a problem behavior; \u2022 intention to change their own behavior if wrong; \u2022 belief that (internal and external) conditions exist to change this behavior; \u2022 knowledge of an acceptable plan which enables achieving this intention; \u2022 level of perseverance in following the plan. The actions which the therapist may apply at every \u2018stage of change\u2019 to promote a correct behavior respond to the following goals: (i) to recognize the situation, (ii) to inform and encourage about the evaluation processes rather than enforcing persuasion, (iii) to influence intentions, (iv) to check abilities, (v) to suggest plans, and (vi) to offer support during plan execution. The theory may therefore be adopted as a powerful source of knowledge to build cognitive models of the users\u2019 attitude towards the problem behavior and to decide how to tailor advice-giving to their stage of change. As suggested in [65], stage of change and emotional state are strongly interrelated. For instance, in the pre-contemplation stage subjects may be demoralized about their ability to change if they have tried to do it and failed; in the maintenance stage, people are increasingly more confident that they can progress in this change, and so on. Recognizing some aspects of the emotional state (valence and intensity, for instance) may therefore contribute to inferring the stage of change and the inverse; at the same time, it may influence the choice of an appropriate persuasion strategy. On the other hand, recognizing the attitude of users towards the virtual therapist enables adapting other aspects of the dialog: level of familiarity of the style employed, introduction of small talk and similarities. Accurate measurement of the stage of change requires acquiring some information on the clients\u2019 behavior and their mental state: accurate collection of this information is a critical step for applying the model successfully. In health services, the stage of change is usually estimated with a questionnaire which is supplied to the patient at the beginning of interaction (e.g. [35]). Applying this procedure to a conversation with an embodied agent risks negatively influencing the attitude of the user. In addition, if the counseling dialog is successful, the mental state of the user may change during the interaction and the system will have to dynamically adapt to this situation. Our solution to this problem is to dynamically infer users\u2019 stage of change, emotional state, and attitude towards the agent on the basis of their dialog behavior during the interaction. 3 Our first prototype of dialog simulator Dialog simulation systems are usually built after careful analysis of a corpus of naturally occurring dialogs to emulate the behavior humans show when communicating among themselves. In health promotion, the ideal source of corpora is that of transcripts of conversations between patients and specialists: these, however, are generally not available for research due to privacy concerns. Instead, we focused our initial analyses on published transcripts available in specialized journals or books dealing with various aspects of health promotion in a variety of behavioral domains, including smoking and alcohol abuse cessation and dietary change. 3.1 Method 3.1.1 The corpus Our corpus included five dialogs which were published in specialized books and an email dialog with a computer scientist who played the role of a dietician [30]. The cases were patients with problems of varying severity and at various stages of change. This corpus was quite heterogeneous, due to the variety of sources employed. In particular, the email dialog was midway between human\u2013human and human\u2013computer conversations. The subject believed that he was interacting with a human dietician but conducted the interaction via a computer-mediated communication channel. He included some emotions in his text and some closing sentences which were more representative of a written communication than an oral communication style. 3.1.2 Corpus labeling We extracted from the six dialogs the moves which included potential signs of either emotional state or stage of change: overall, 78 moves were selected for analysis of the emotional state and 115 for stage of change. We then defined a markup language with which to label the selected moves and asked 10 raters to label them with this language. The emotional tags were valence, intensity, and emotion name; SoC tags were the mental state components which we called \u2018signs\u2019 in Section 2. Table 1 shows the definitions of these tags with some examples of labeled sentences for each of them: these examples show that multiple labeling was requested. For instance, the sentence: \u201cWell, it does help to talk with someone\u201d shows a positive emotional valence of low intensity. Therefore, when tagging a move for the emotional state, raters were requested to indicate the value of valence and intensity, and (if recognizable!) the emotion name. This apparent redundancy was motivated by our belief that emotions are difficult to recognize, while valence and intensity are recognized more easily. The names and descriptions of emotions were drawn from the OCC classification [45] with a very few additions: demoralization, frustration, disappointment, irritation. In defining the emotional tags, we adopted effect-type descriptions [21] which refer to the effect that emotional characteristics of speech have on the listener. Raters were asked to label the sentences according to what the language (style, syntax, lexicon, etc.) suggested to them. For all labels (both emotional and related to the stage of change), we introduced a categorical scale which enabled the raters to distinguish between three grades of the feature of interest; this has proven to be preferable for subtle phenomena, such as emotions or mental state components [22]. 3.1.3 Measures of agreement among raters Various measures of agreement among raters in labeling corpora have been proposed. In a paper aimed at describing the role of agreement measures, Craggs and McGee Wood [22] discuss the agreement statistics classification of Di Eugenio and Glass [27] by examining their advantages and limits. The class of \u2018percentage agreement\u2019 statistics (which measures the proportion of agreement among raters) has the advantage of not suffering from unequal distribution of the labels used by raters. However, it excludes any notion of the level of agreement one could expect to achieve by chance, without which any deviation from perfect agreement is not interpretable. Chance-corrected measures compute agreement by considering both the observed values and those we could expect by chance; these measures may or may not assume an equal distribution of categories between coders. The most common measures applied in computational linguistics belong to the latter category (\u03ba and \u03b1, by Krippendorff). In particular: \u03ba = ( p ( a ) - p ( e ) ) / 1 - p ( e ) , where p (a) denotes the observed agreement rate and p (e) the expected rate. Some authors (again [27]) propose to use the two categories of measures as a means to judge agreement from several viewpoints; others believe that this may be seen as a lack of confidence in each of them. In our opinion, if agreement measures are applied in order to assess the difficulty to recognize a given feature from text, combining percentage agreement statistics with chance-corrected measures is worthwhile. The first measure is immediately interpretable, while the second measure enables comparing features with different frequencies in the corpus (we will see some examples in the results). For this reason, we combined the \u03ba statistics with the following percentage agreement measures: \u2022 full agreement rate, as the proportion of moves in which the raters \u2018fully agree\u2019 on the labeling of the move: for instance, two raters are said to fully agree on labeling the emotion valence of a sentence if they both label it as \u2018positive,\u2019 \u2018negative\u2019 or \u2018unknown,\u2019 and in the case of its intensity, if they both label it as \u2018high,\u2019 \u2018low\u2019 or \u2018unknown\u2019; \u2022 weak agreement rate, as the proportion of moves in which the raters \u2018weakly agree\u2019 in the labeling of the move. A \u2018weak\u2019 agreement on a tag is defined as a case in which the values assigned to the tag by the two raters are not the same but are not opposite either: for example, \u2018positive\u2019 vs. \u2018neutral\u2019 or \u2018negative\u2019 vs. \u2018neutral\u2019 valence; \u2022 disagreement rate as the proportion of moves in which the raters \u2018strongly disagree\u2019 in the labeling of the move: for example, \u2018negative\u2019 vs. \u2018positive\u2019 valence, or \u2018high\u2019 vs. \u2018low\u2019 intensity. 3.1.4 Move classification To summarize the results of tagging by our 10 raters, we classified the moves into three broad categories: moves with a shared interpretation, for which there was a rate of full agreement in more than 60% of raters, moves with a likely interpretation, for which the rate of full agreement among raters ranged from 40 to 60%, and moves with a questionable interpretation, for which the full agreement was less than 40%. 3.2 Results Table 2 summarizes the agreement among raters for the first three signs in Table 1: valence, intensity, and believes behavior wrong. Emotion names varied considerably among the raters; for instance, some moves were interpreted, by different raters, as indicative of \u2018reproach,\u2019 \u2018anger,\u2019 \u2018frustration\u2019 or \u2018disappointment\u2019; while others as indicative of \u2018disliking\u2019 or \u2018reproach\u2019; for this reasons, we do not include emotion names in this analysis. Every dialog included moves which were classified as belonging to the pre-contemplation, contemplation or preparation stages, indicating that the subject\u2019s stage of change varied during the dialog, probably due to the interaction with the counselor. We only show results regarding the \u2018believes behavior wrong\u2019 tag, as very few moves denoting the other components were found in the corpus; these sentences received a high full agreement rate. The table shows that valence was identified a bit more easily than intensity, as the full agreement rate is higher for this tag (.55 vs. .45). The majority of moves with \u2018shared\u2019 interpretation were those labeled with a \u2018negative\u2019 valence, while the majority of moves with \u2018likely\u2019 interpretation were labeled with an \u2018unknown\u2019 valence. This may due to different reasons: first, as we said, five dialogs referred to situations involving serious behavior problems of alcohol abuse or smoking: these were likely to induce negative emotions. In the email dialog regarding dietary behavior, the problems discussed were less severe: however, in this dialog the subject showed a negative emotion (irritation) due to the overly persuasive behavior of the counselor. Moves tagged as indicative of a \u2018high\u2019 emotional intensity were more frequent than those tagged with a \u2018low\u2019 value, possibly because they were easier to recognize or for reasons similar to those mentioned for valence: health-related dialogs deal with problems which deeply involve subjects since they involve discussion of their health, family, job, etc. Therefore, it seems reasonable that becoming aware of a situation which is (potentially or actually) negative generates a negative emotional reaction of more or less high intensity, according to how serious the discussed problem is. 3.3 Prototype design 3.3.1 User modeling The results of the corpus analysis described above guided our design of the user modeling module of our dialog simulation prototype. This module parses user utterances and propagates the results of this parsing, as \u2018observed variables,\u2019 in a dynamic Bayesian network which infers the stage of change and the emotional valence of the user with some level of uncertainty. In Section 6, we will describe with more detail the method behind user model building and updating. In another paper [9], we describe how the emotional impact of the user move on the agent may be simulated, again with some uncertainty. In the next subsection, we will describe how the two models are employed to adapt the dialog plans and style. 3.3.2 Dialog adaptation As we anticipated in Section 2, the Transtheoretical Model of Change suggests specific plans to apply at every \u2018stage of change\u2019 to help the subjects in changing their behavior. For instance, in the pre-contemplation stage the plan includes the following steps: (i) validate lack of readiness, (ii) clarify, decision is yours, (iii) encourage evaluation of pros and cons of the behavior change, and (iv) identify and promote new positive outcome expectations. To apply this model, our agent requires some information about the user: it may employ uncertain default information in the first dialog steps, provided that this approximate picture of the user is subsequently refined so as to also refine the advice provided. Therefore, our dialog simulator needs on one hand a knowledge updating system which deals with uncertainty in knowledge about the user and, on the other hand, a description of the current situation and the dialog history on which to base its planning activity. This description of the problem orients the choice of the dialog management system towards an information state model. This model was developed as part of the TRINDI EC project to enable implementing flexible dialog simulation systems with a plan-based approach [64]. The information state (IS) is a blackboard on which data needed to develop the dialog are represented with a logical formalism and are revised dynamically by means of IS update rules. In our case, the IS structure includes a model of the agent and a model of the user with two main components: \u2022 permanent characteristics (in the \u2019STABLE\u2019 part) which do not change in the course of the dialog: for instance, \u2018name,\u2019 \u2018age,\u2019 \u2018personality,\u2019 and background; \u2022 transitory characteristics (in the \u2019UNSTABLE\u2019 part) which are revised during the dialog: the user\u2019s affective state and her stage of change. A set of updating rules is used to update the \u2018unstable\u2019 part of the user model following the interpretation of every user dialog move. The agent\u2019s goal can be achieved by means of one or more plans, each characterized by a set of applicability conditions on the user\u2019s mental state: a set of select rules establishes the next plan and move to perform. A plan includes a main part (the essential of the plan) and a secondary part with optional details. It may be linked to another plan by a causal link, to represent that the execution of at least its main part should precede the execution of that plan. Introduction of precedence relationships and distinction between the main and secondary parts was our solution to the problem of uncertainty in the knowledge of the user state. The system provides the user with the opportunity to react to its moves, by fragmenting its plans into short moves and by interleaving suggestions with questions. The user may answer these questions or input new comments; both kinds of moves enable the system to reason on her state and on the emotional effect produced by its move. Since interpretation of user moves is highly error prone, dialog plans are selected by default but may be revised as new information becomes available. When the user model is revised, an on-going plan may be interrupted to be substituted with a new one. In this case, priority and causal relationships insure that a \u2019believable\u2019 dialog flow is maintained after plans are changed. This distinction between the main and secondary parts ensures that a goal can be achieved with plans at different levels of detail, depending on the situation and the dialog history. 3.3.3 An example Let us describe the dynamics of interpretation and planning, in an example dialog which was generated with our prototype: we will show how the system\u2019s image of the user and the dialog plans evolve. The example starts after the user (\u2018U\u2019) declared that she feels her eating behavior is not as she would like it to be: \u2026 User\u2019s stage of change: contemplation (p =0.26) Agent plan: clarify: decision is yours S3:I will try to help you, but cannot make decisions for you. U3:I would like to change but it\u2019s so difficult! Move interpretation: (say U \u2018change is difficult\u2019) Agent emotion: light sorry-for. User\u2019s stage of change: contemplation (p =0.24) Valence: negative. Agent plan: display empathy; encourage the evaluation of the pros and cons of behavior change. S4:I know, adiet is difficult to follow, particularly if you eat out because of work. But the effects may be very positive! U4: I know! I definitely intend to change my habits. Move interpretation: (Say U \u2018intends to change\u2019) Agent emotion: light hope. User\u2019s stage of change: preparation (p =0.24) Valence: positive. Agent plan: display satisfaction; encourage initial steps S5: Good! why, for instance, don\u2019t you try to avoid going to restaurants for your daily lunch? \u2026 The user is initially assumed to be in the \u2018contemplation\u2019 stage, with a probability above the threshold (that we fixed as .20). 3 To avoid unstable changing of plans, if the probability of more than one stage is larger than the threshold and these values are similar, the last move\u2019s stage is preferred. 3 The agent adopts the plan \u2018clarify, decision is yours\u2019 which matches the goal activated by this state. After move U3, the agent takes on the emotion \u2018sorry-for\u2019 which activates a plan of \u2018empathy display\u2019: \u201cI know, a diet is difficult to follow\u2026.\u201d The inferred \u2018stage of change\u2019 does not differ from the previous one (its probability only slightly decreases) and the inferred valence is negative. Hence, a plan to \u2018encourage evaluation of pros and cons of behavior change\u2019 is carried out. At move 4, the user manifests an intention to change her habits and a positive valence. The user model is revised (the most likely \u2018stage of change\u2019 is now \u2018preparation\u2019), the agent takes on the emotion of \u2018hope\u2019 and a plan to \u2018display satisfaction and encourage initial step\u2019 is applied. The agent move is produced by an animated agent using a standardized API that consists of an APML string [25] interpreted by a \u2018wrapper\u2019 to the particular animated agent in use [27]. This API allows several different animated agents to be used, including GRETA [49], MS-Agent or Haptek. 4 http://www.haptek.com. 4 A graphical interface enables the users to interact with the system and is responsible for scheduling the various functions and activating the related modules. More details about the dialog manager may be found in [13]. 4 Is interacting with an ECA the same as interacting with a human counselor? This is a provocative question which we asked ourselves after an internal evaluation of our first prototype. Although, as we said in Section 1, the idea behind conversational agents is to endow the system with the ability to emulate human behavior, we felt that we could not assume that the subjects\u2019 behavior when interacting with our character would be the same as would be adopted when interacting with a human counselor. Therefore, we decided that the knowledge we had acquired from analysis of the natural corpus had to be integrated with some theoretical background and empirical data concerning the nature of human\u2013agent interaction. In this second step of our research, we were looking for answers to questions such as: What kind of relationship do users establish with an ECA when discussing their health-related problems? How can the nature of this relationship be recognized? How do users expect the ECA to respond to their manifestations of social relationship? A number of evaluation studies have been published, which describe how users see ECAs and how their vision is influenced by variations in the agent characteristics: see [42,5,58] for a survey of more recent results. The majority of these studies only involved agent monologs and therefore do not contribute much to the understanding of the exact nature of the human\u2013ECA relationship. In the famous media equation, the Stanford group formulated the hypothesis that social science theories may be applied in this domain [42]. Recently, however, the need to specify the applicability conditions of this hypothesis was raised. Some studies demonstrated that there are some significant differences between human\u2013human and human\u2013agent interactions. In some situations, people tend to use \u2018computer talk\u2019 when speaking to a computer agent [4], manifested by behavior such as shortened move length and adaptation of their speech level and tone to the agent\u2019s speech characteristics [46,24,20]. These changes in style may be due to a simple style-induction effect or may be an index of lack of trust in the computer\u2019s ability to recognize and interpret the user input. But they might also be explained in terms of a more complex theory of the social relationship humans tend to establish with technology in general, and with agents in particular. Several psychosocial theories have been considered by computer scientists engaged in research about ECAs. In designing the interaction attitude of REA, Cassell and Bickmore [11] referred to Svennevig\u2019s model of \u2018interpersonal relations in conversations.\u2019 They identified the dimensions of social relationship (familiarity, power, solidarity, and affect) and extended these dimensions with the concept of trust. Other authors included politeness, deception, and irony among the ingredients of social relationship. Terms like \u2018empathy\u2019 or \u2018friendship\u2019 have also been employed to denote key aspects of this relationship [47]. To Poggi [55], empathy is the ability to identify with and understand another\u2019s situation, feelings, and motives: it requires listening skills and emotional intelligence, and may occur even in absence of any emotional expression by the interlocutor. To Vaknin, 5 http://samvak.tripod.com/empathy.html. 5 this concept goes beyond pure emotion transmission as \u201cThe empathor empathizes not only with the empathee\u2019s emotions but also with his physical state and other parameters of existence.\u201d We will use the term social attitude to denote the kind of relationship users establish with our ECA. We denote, with this term, \u2018the process of entering into a warm social relationship with someone else, of being somehow involved in her goals and feelings.\u2019 Although this is a shorter-term kind of relation than friendship, it shares with this concept the characteristics of intimacy, affection, and mutual assistance; it is influenced by interpersonal attraction but also by rewards, which should outweigh costs such as irritation or disappointment. In advice-giving dialogs, rewards are affected by what subjects expect to receive from the interaction: information and, in some cases, entertainment. Therefore, the social attitude of users towards the agent is probably affected by their degree of satisfaction with the information received and by how pleasant they found the interaction. A positive social attitude may be displayed through an increase of intimacy and common ground over the course of the conversation, a decrease of interpersonal distance, the use of non-explicit ways of achieving conversational goals and the display of expertise [11]. Humorous acts may also be taken as an offer of sympathy: \u201cWhen the participants are in the mood for jokes, joke telling occurs naturally and there is some meta-level cooperation\u201d [44]. These were therefore the signs of social attitude we expected to find in human\u2013ECA dialogs. 5 A corpus of wizard of Oz dialogs To have some insight into the kind of relationships users might establish with our advice-giving ECA, as well as how this relationship may be recognized, we performed a wizard of Oz (WOZ) study. Subjects interacted with the ECA to discuss their eating habits and receive information and suggestions about problems in their eating behavior that were possibly discovered during the conversation. As in all WOZ studies, subjects believed that an automated system was generating the ECA\u2019s answers, while in actuality these were selected by a human confederate (\u201cwizard\u201d) from a set of precompiled moves [23]. This method is usually considered to produce results that are plausible simulations of real dialogs between an automated, virtual dietary expert and a client. 5.1 Method 5.1.1 The corpus We employed an experimental setup which enabled us to perform studies under diverse conditions by varying the agent\u2019s physical aspect, its expressivity, the dialog moves, the evaluation questionnaire and other factors [14]. Data of various kinds may be collected with this tool: the subjects may be asked to evaluate the agent\u2019s behavior with a questionnaire and dialogs may be recorded to be later analyzed using quantitative and qualitative methods. The tool was employed in an \u2018iterative design\u2019 mode, with data collection steps ending with the design of the next one: at every iteration, the moves that the agent could employ were revised according to the problems found in the previous iteration. Subjects involved in the study completed a pre-test questionnaire which was aimed at assessing their level of knowledge, habits, and interest for healthy eating, in addition to their cultural background. To insure the uniformity of the experimental conditions throughout the whole study, we established some rules the wizard was requested to follow. After every subject move, the wizard selected her next move so as to respect a well-defined dialog plan and at the same time to insure the internal coherence in every dialog. This was achieved by a careful preliminary training of the wizard and by employing the same wizard with all the subjects. We employed an ECA that used an animated humanoid head built with Haptek\u2019s toolkit, with a rather realistic and pleasant aspect (Fig. 1 ) and with two kinds of voices: a mechanical and not very natural one produced with the Microsoft Speech API, and a much more natural voice produced with Loquendo. 6 http://www.loquendo.com. 6 The three clickable icons on the right side of the agent enabled the subject to evaluate whether the agent move was \u2018good,\u2019 \u2018bad\u2019 or \u2018unclear\u2019 without interrupting the natural course of the dialog. At the same time, subjects could respond to the agent by typing any text in the textfield at the bottom of the window. At the end of the experiment, a final questionnaire was displayed on the same computer monitor on which the agent had been displayed, to collect the subject\u2019s evaluation of several features of the message and the agent, each with a Likert scale from 1 to 6: how credible, plausible, clear, useful, and persuasive was the message and how sincere, likable, natural, intelligent, and competent was the agent. Dialogs were stored in a log at the end of the interaction for subsequent analysis. 5.1.2 Method of analysis We defined two measures of the subject\u2019s attitude during the dialog: \u2022 level of involvement, as a function of the dialog duration (in number of adjacency pairs 7 An adjacency pair is a couple of adjacent wizard-subject moves in the dialog. 7 ) and the average length of the user moves (in number of characters), and \u2022 degree of initiative, as a function of the percentage of questions raised by the subject over all the dialog moves. These measures were integrated with a set of \u2018signs\u2019 that we defined after comparing the signs mentioned in the literature (see end of Section 4) with a preliminary analysis of the subject moves. These signs were aimed at estimating the social attitude of the subjects towards the agent and at assessing its relationship with their subjective evaluation of the agent (from the questionnaire), their level of involvement and their degree of initiative. 5.1.3 Corpus labeling We extracted two subsets from the corpus of 712 moves: \u2022 237 moves to label for \u2018social attitude\u2019 with the language described in Table 3 : this table shows the language features which we considered as signs of this attitude. For each, we provide an example of an adjacency pair which is translated from Italian: some pairs belong to several classes; \u2022 143 moves to label for signs of \u2018stage of change\u2019 with the same criteria that were employed in the first study (see Table 2). However, emotion intensity was excluded from this language because we did not notice any case of \u2018strong\u2019 emotions in the corpus. As we wanted to apply the results of this analysis to an automatic recognition of these features from a user move given as input to a parser, individual user moves were presented to raters in random order. In addition, since we wanted to develop a recognition method that incorporated some discourse context, we had raters analyze adjacency pairs, including a user move and the agent move that preceded it. Due to the large number of moves in the two sets, only three independent raters were recruited in this case. As in the first study, two measures of inter-rater agreement were computed: a percentage agreement, in which we considered the label of a move as \u2018agreed\u2019 when at least two raters gave it the same value, and a chance-corrected \u03ba statistics [8]. 5.2 Results We performed six WOZ tests, with five subjects in each with a total of 712 moves in the 30 collected dialogs. As we said, these tests were considered as steps of an iterative design of our ECA: therefore, in designing each step we considered the results of the previous ones to discover the main limits of the ECA and revise its behavior. After the first three tests, we were able to stabilize the agent moves and behavior. We also included in the study subjects with different backgrounds (humanities and computer science), to evaluate the role played by this factor. The pre-test questionnaire enabled us to verify that the six groups of subjects were comparable in their level of knowledge, habits, and interest in healthy eating. They belonged to the same age group (23\u201326 years) and the groups were gender balanced. There was a high variability among the subjects for the two measures of level of involvement and also for the subjective evaluation of the message and the agent. The dialog duration ranged from 9 to 60 moves and increased only slightly on average when the number of moves among which the wizard could choose her answers was increased. The average duration was of 22.4 moves when the wizard could select among 58 moves and 25.5 when available moves were increased to 78. The average length of moves ranged from 29 to 95 characters. The average rating of the five message features (credibility, plausibility, etc.) ranged from 1.5 to 5 and the rating of the agent features (sincerity, likeability, etc.) ranged from 1.75 to 4.7. A multiple regression analysis showed that the message rating was correlated negatively with the dialog duration, the average length of the moves and the percentage of user questions in a dialog. We cannot say whether this rating was correlated with any emotional feeling, because the subjects showed very few \u2018individual\u2019 low intensity emotions (disappointment, satisfaction, etc.) compared to our first study. This was probably due to the difference in problems declared by the two groups of subjects: serious cases of problem behavior in the natural dialogs, vs. problems of unhealthy dieting in the WOZ studies. We may, however, conclude that the subjective evaluation of the message does not seem to be a good indicator of the level of involvement of the subject in the dialog as one might expect. The dialogs included several signs of \u2018social\u2019 emotions (sympathy, appreciation or irritation, disappointment) and social attitude. The percentage of moves with these signs was positively associated with the ratings in the initial questionnaire and the subject involvement, while it was negatively correlated with their level of initiative. The subjects\u2019 background was the factor which most highly influenced their behavior: computer scientists conducted dialogs with fewer and shorter moves, a larger proportion of questions and a lower proportion of social moves when compared to subjects with a background in humanities. More detailed data on this quantitative analysis of our corpus may be found in [26] while, in this paper, we will focus on their qualitative aspect. Table 4 shows that there was a good agreement rate and \u03ba for \u2018friendly self-introduction,\u2019 \u2018talks about self,\u2019 and \u2018friendly farewell\u2019 and a reasonably good value for \u2018personal questions about the agent.\u2019 \u2018Irony\u2019 and \u2018comments\u2019 had a high rate but a low \u03ba while \u2018familiar style\u2019 had a very low rate and \u03ba. As we said in Section 3, the \u03ba statistics is a function of both the observed and the expected chance of agreement; this last measure depends, in its turn, on the distribution of values taken by the variables. Given an observed rate of agreement, the variables whose values were not uniformly distributed produced a lower \u03ba value: this is the reason why signs with low frequency (like \u2018irony\u2019 and \u2018comments\u2019) have a low \u03ba level even if the agreement among raters is good. The value of \u03ba for the \u2018comment\u2019 sign is unique because this was a three-valued variable (positive, negative or no comment). As shown in Table 5 , agreement was quite good also for the two signs of stage of change. Although these data cannot be compared with the results of the first study (in Table 2) because the number of raters is not the same (10 in the first study, and three in the second), we can say that, in this second study, raters tended to \u2018weakly agree\u2019 on tagging \u2018belief behavior wrong\u2019 more frequently than in the first study: recognizing this sign therefore seems to be easier when more serious problems are discussed. As in the natural corpus, subjects involved in WOZ studies were mostly in the pre-contemplation, contemplation, and preparation stages of change and some of them apparently changed stage during the dialog. This is not surprising, considering that the conversation dealt with simple and common dietary rules that are not particularly difficult to accept. 6 Building a model of the user The function we assign to our user model is to infer how the mental state of the user evolves during the dialog, in relation to his/her \u2018stable\u2019 characteristics and to the dialog history. The mental state components which are relevant in health promotion dialogs are stage of change and social attitude towards the agent, while emotional valence is important only in case of serious eating-related problems. Since we would like to assess these continuously throughout a dialog without asking the user to fill out a questionnaire after every dialog move, we consider these \u2018hidden\u2019 variables whose values are to be inferred. We take our \u2018observable\u2019 measures to be the user\u2019s stable characteristics, the context in which the move was entered (previous agent\u2019s move), the length of the user move and its linguistic features as recognized by parsing. Intermediate variables are the signs of mental state: belief behavior wrong, intention to change, and the features listed in Table 3 for social attitude. The model is dynamic: it is initialized by assigning a value to the stable characteristics and (at every step of the dialog) to the context and the move characteristics, and produces a revised set of values for the hidden variables after every user move. The user model was built according to the philosophy described in Section 2: very simple parsing of the moves with integration of results in a Bayesian network which handles uncertainty in the relationships among the various features. We will describe the two components separately in the next two paragraphs. 6.1 Move parsing Language resources employed by humans to express their affective state are lexical (\u2018emotional words\u2019), syntactic (e.g., emphatic construction) and morphological (e.g., terms of endearment or contempt) [56,63,4,50]. Several researchers have investigated the assessment of affective state based on analysis of written language: Poggi and Magno-Caldognetto [56] worked on the Italian emotional language; Gill and Oberlander [29] worked on recognizing personality traits in emails; Carberry et al. [7] studied the recognition of doubt; Guinn and Hubal [32] proposed a semantic grammar enriched with emotional or attitudinal tags to recognize features like politeness, urgency, and satisfaction. Other researchers have investigated affect assessment from spoken language, combining prosodic information with language features. Lee et al. [36] focused on recognizing \u2018negative\u2019 and \u2018non-negative\u2019 emotions and found that combining acoustic information with salient keywords in utterances improved the recognition performance. Ang et al. [1] aimed at detecting frustration and annoyance in telephone-based dialogs after annotating a large corpus by five raters: they found that the labeling of emotions and speaking style was a \u2018inherently difficult task\u2019 and that emotion characteristics varied enormously from person to person and from context to context; therefore, although the language analysis method they applied was refined, they could only discriminate the \u2018neutral\u2019 case from the \u2018annoyed and frustrated\u2019 category. Litman et al. [38] combined acoustic\u2013prosodic features with word analysis to recognize the valence of emotions in spoken tutoring dialogs. In working with WOZ data, Batliner et al. [4] demonstrated that the combination of prosodic with linguistic and conversational data yielded better results than the use of prosody only, for recognizing \u2018troubles in communication,\u2019 that is, the beginning of emotionally critical phases in the dialog; this last study considered features, like \u2018repetitions,\u2019 which require examining the dialog history rather than individual moves. We defined our parsing criteria based on a preliminary analysis of the moves in which the raters agreed (fully or weakly) and based them on recognition of short word sequences (one, two or three keywords). The criteria applied for each sign of social attitude (in Table 6 ) combined the knowledge about the sign semantics with the analysis of word salience in the corpus: a word was considered to be \u2018salient\u2019 for a category if it appeared more often in the category than in other parts of the corpus [36]. The predictive capacity of the parser was evaluated from confusion matrices in terms of sensitivity (true positives TP/total positive cases, also named \u2018recall\u2019), specificity (true negatives TN/total number of negative cases, which is equal to 1\u2014\u2018fallout\u2019) and proportion of correctly classified cases (TP+TN/total number of cases). The table shows that the specificity of parsing was high for all signs (ranging from 79 to 98%) while sensitivity was low for some of them; negative comments were the most difficult to recognize (sensitivity=16%), followed by a familiar style and positive comments. These values are, of course, a consequence of parsing criteria: in defining these criteria, we had to decide whether we preferred a high sensitivity or a high specificity [39]. We felt that, in the case of social attitude, the consequences of false positives were less harmful for system effectiveness than those of false negatives: for the agent, false positives imply interpreting the user move as \u2018friendly\u2019 and answering in a friendly way even if this were not the case; false negatives imply, on the contrary, a \u2018cold\u2019 answer to some user attempts to establish a \u2018warm\u2019 relationship with the agent. In our view, the second risk should be avoided while the first is more acceptable. Therefore, we did our best to design the parser so as to maximize the sensitivity even at the expense of a lower specificity. As column 3 in Table 6 shows, we succeeded in our attempt for some signs but not for all of them. In particular, recognizing negative comments, familiar style, beliefs, and intentions requires far more complex parsing methods than those we applied in this study. 6.2 Integration of signs in a Bayesian network As the relationships among the user features can only be estimated, either subjectively or objectively, with some level of uncertainty, we decided to treat uncertainty probabilistically and to represent the user model with a dynamic Bayesian network. Bayesian networks (BNs) are probabilistic models that may be based on expert knowledge, empirical data or a combination of both. A BN consists in a network of assumed causal relationships between random variables and a set of conditional probability tables that relate every variable to its assumed causal variables [48]. In their dynamic version, BNs replicate at defined time intervals by establishing links with the previous layers for some of the nodes; monitored events occurring in every time interval produce new evidence to be propagated in the subsequent layer [43]. When dynamic belief networks are applied to represent evolving user models in dialogs, the component at time t i represents the system\u2019s image of the user at the i-th step of the dialog and the monitored event is the user move in the interval (t i, t i+1). This formalism was proposed some years ago as a method to model affective human\u2013machine interaction. Ball [2] employed them to represent the relationships among emotion and personality components and their observable effects. Carofiglio et al. [9] modeled mixed emotion activation with dynamic models; they proved how these cognitive models may be employed in communication processes to represent, at the same time, prospective reasoning on possible consequences of some communicative act being planned and reasoning on the possible causes of a communication received [10]. Conati and Maclaren [18] built and refined a probabilistic affective model to represent activation of emotional states as a consequence of goal satisfaction or threatening; the model was built on the famous Ortony, Clore, and Collin\u2019s categorization of emotions [45] and its validation was based on contrasting results with the subject\u2019s \u2018feeling.\u2019 Green et al. [31] applied belief networks to model clinical genetics knowledge. We employed the dataset of our wizard of Oz studies to train the static component of the model with the K2 learning algorithm [19] which is provided by Bayesware. 8 http://www.bayesware.com. 8 This algorithm reduces the search space of possible BN structures by allowing developers to specify an ordering of the variables from which the network should be built. It is appropriate in our case (and more in general, we claim, in learning user models) because it enables distinguishing \u2018trigger\u2019 variables (which are placed at the top level of the network) from the variables which describe the resulting behavior of the user (which are placed at the lowest level, as leaf nodes). Links therefore describe the causal relationships among stable characteristics of the users, their behavior and the dialog dynamics via intermediate nodes. Table 7 describes the variables in our model, with the labels employed to denote them: a. stable user characteristics: background, in humanities or computer science; 9 As this node is settled at the beginning of the interaction and does not change during the simulation, we omitted it from Fig. 2. 9 b. context: category of the previous agent move and of the current user move; c. monitored variables: social attitude towards the ECA and stage of change; d. \u2018hidden\u2019 subject characteristics: the signs with which the user characteristics manifest themselves: believes behavior wrong and intends to change for the stage of change. Friendly self-introduction, familiar style, talks about self, personal questions to the agent, comments on the dialog, and friendly farewell for the social attitude; e. \u2018observable\u2019 linguistic features in the subject move produced by parsing. K2, as well as other current BN learning algorithms, tries to find the model that fits data best by maximizing the log likelihood (MLL), but does not care about the use of the resulting model and its predictive ability. We tested several kinds of models by automatically learning both their structure and their parameters and by introducing a few links between some nodes to avoid problems in evidence propagation due to d-separation properties. The resulting structure is shown in Fig. 2 . This figure does not include irony, that we finally excluded from the model because of the difficulty of defining parsing rules able to recognize it. Apart from the obvious associations between the agent\u2019s and the user\u2019s move types (self-introduction, farewell, question\u2013answering or question\u2013suggestion couplings), this model shows that a background in humanities (an \u2018observable\u2019 variable in our case) implies a higher level of social attitude in general, and of irony and friendly self-introduction in particular. When the level of social attitude is low, self-presentation and farewell are sometimes omitted, and few comments are made after the agent\u2019s answers or suggestions. The use of familiar style is associated with irony and non-neutral valence. Self-disclosure is frequently included in comments or answers to the agent\u2019s questions and is common in the contemplation and preparation stages of change, personal questions about the agent are more frequent when the emotional valence is not neutral and are associated with a familiar style of move. While the \u2018static\u2019 component of our model was learned from the dataset by considering every user move individually, the parameters associated with the two dynamic nodes were learnt from pairs of adjacent moves. This enabled us to estimate the probability that a move displaying a positive or a neutral \u2018social attitude\u2019 comes immediately after a move of the same or a different type (and the same for the stage of change). Probability tables associated with links between the two monitored variables in adjacent time slices t i, t i+1 are such that, in absence of any sign of social attitude or stage of change in the user move occurring between t i and t i+1, the distributions of the two variables tend to smooth. This produces a decay of the probability of the value that was found as the most likely at time t i. The model runs as follows: \u2022 at the beginning of interaction, it is initialized by propagating evidence about stable user characteristics; the probabilities of the other variables take prior values which correspond to this category of users; \u2022 after every user move, linguistic features of the move are analyzed with the parser; results are introduced and propagated in the network together with evidence about the previous agent\u2019s move; \u2022 the new probabilities of the social attitude signs are read and contribute to formulating the next agent move; the new probabilities of stage of change and social and emotional attitude (which evolve more slowly during the dialog) are employed to revise the high-level planning of the agent behavior. 6.3 Model validation Our model needs to be validated by contrasting its results with some external assessment of the two monitored variables. This external reference might consist in the observation of the user behavior (facial expression, gesturing, etc.), in some form of self-report (questionnaire-based user data) or in an independent, subjective evaluation by experts. Observation of the user behavior would probably be the preferable method. As far as the stage of change is concerned, standard self-report questionnaires might be employed as well. The problem is a bit more complex for social attitude, for which behavior observation methods and standard self-report questionnaires do not exist: in addition, for this kind of data (and, more in general, for short-term affective features) the reliability of self-report is being debated (see, e.g. [53] and [33]). This kind of data might not be collected, in any case, after every agent move without influencing the user behavior: questionnaires might only be administered at the beginning and the end of interaction, to assess whether and how the user mind changed as a consequence of the system use. For these reasons we decided, as a first step, to validate our model by comparing the model results after every user move with an independent expert evaluation. We evaluated the model ability to represent the dynamic aspects of the user with test on some of the dialogs included in the corpus, by comparing its results with the subjective estimates of the three raters (again, with a majority agreement criterion). The predictive value of the model was estimated in terms of the percentage of correctly classified cases of social attitude and was equal to .75, while we did not get enough data to attempt a similar evaluation for the stage of change. The following is an excerpt of one of the test dialogs, 10 Translated from Italian. 10 in which we show how the system\u2019s belief about dynamic features of a subject with a background in computer science evolves: TIME 0 P(SoC=pre-contemplation)=.88 P(Satt=friendly)=.37 TIME 1 A: Hi, my name is Valentina. I\u2019m here to help you understand how you can improve your diet U: Hi, my name is Carla Ctext=self-introduction the agent did a \u2018self-introduction\u2019 type of move Pfsint=Y the parser recognizes signs of a friendly self-introduction in the user move P(Satt=friendly)=.99 this move is showing a friendly social attitude TIME 2 P(Satt=friendly)=.80 decay A: What are your eating habits? U: I always try to eat in a healthy way Ctext=question Pperin=Y the parser recognizes signs of \u2018talking about self\u2019 P(Satt=friendly)=.89 the probability of social attitude increases again TIME 3 P(Satt=friendly)=.68 decay A: What did you eat for breakfast? U: Milk and biscuits. How about you? Ctext=question Pqagt=Y the parser recognizes signs of \u2018personal questions to the agent\u2019 P(Satt=friendly)=.85 the probability of social attitude increases again TIME 4 P(Satt=friendly)=.66 decay A: Maybe you forget I\u2019m only an artificial agent: I can\u2019t eat, and therefore I don\u2019t follow any particular diet U: sorry! Ctext=answer Pfstyl=Y the parser recognizes signs of \u2018familiar style\u2019 P(Satt=friendly)=.91 the probability of social attitude increases again \u2026 some \u2018neutral\u2019 moves\u2026 TIME 16 P(Satt=friendly)=.60 the probability of social attitude increases again A: Sweets should be avoided, or limited to special occasions. \u2026 U: But I can\u2019t resist a beautiful dessert!! What should I do? Ctext=suggestion Pfstyl=Y the parser recognizes signs of \u2018familiar style\u2019 and of \u2018believes behavior wrong\u2019 P(Satt=friendly)=.77 the probability of social attitude increases again P(SoC=contemplation)=.49 the subject might now be in the contemplation stage \u2026.. 7 Conclusions In this paper, we discussed the role of adaptive models in dialog systems for health promotion. We focused our attention on users\u2019 mental state: their stage of change (behavior, beliefs, intentions, knowledge of plans, etc.) and the relation between stage of change and affective characteristics, both individual and social. In our wizard of Oz studies, responding appropriately to subjects\u2019 attempts to establish a social relationship with the agent proved to be a key factor to engage subjects\u2019 attention and increase their level of involvement in the dialog: these effects provide an important platform upon which more sophisticated persuasion strategies may be implemented. In addition to providing an insight into the user\u2013agent relationship which is established in the health promotion domain, the collected dialogs served to define recognition criteria for the mental state of the users by means of linguistic analysis of their moves. To improve the parser performance, we employed dynamic Bayesian networks: these models enabled us to consider, in the recognition process, the stable characteristics of the user and the dialog history, resulting in increased accuracy of user modeling. We based our work on two kinds of corpora. The first comprised natural conversations between a human dietician and a client, and the second consisted of dialogs with an artificial agent. The two situations were very different from on another with respect to the severity of problems tackled and the characteristics of subjects involved. As for the methods employed, we found that the signs of stage of change in our corpus were infrequent and were not difficult to recognize by our raters; whereas the signs of social attitude were frequent and easy to recognize in some cases, and quite uncommon and difficult to recognize in others. In constructing our user model, we did not rely completely on automatic learning algorithms for building the Bayesian networks, but integrated this statistical method with the theoretical knowledge of relationships among variables in the model. Defining an order of search for variables and carefully revising the learned links and parameters were key steps of this interactive procedure. This corresponds to the prevailing attitude in the model learning community (see, e.g. [59]), where it is suggested that models learnt from a database should be evaluated not only according to their accuracy in explaining data, but also according to their theoretical plausibility. However, models induced from a sample of data can be over trained, and may not generalize well to new situations [39]. Therefore, learned models should be validated on additional, external data, and this is the next step in our research, with a systematic analysis of the predictive ability of the model and how it is influenced by changes in its parameters. Compared with the growing body of work in emotion recognition, to the best of our knowledge this is the first attempt to recognize and model stages of change and social attitude in user\u2013ECA interactions. One of the limits of our recognition method is that we did not measure the personality of subjects in our WOZ studies: we did this to avoid reducing their level of cooperation in the experiment by having to respond to a long initial questionnaire. Therefore, it is possible that the stable characteristics which influence the subject behavior in our studies (gender and background) hide or partially overlap with some personality traits. This hypothesis is supported by the similarity between our signs of social attitude and some of the language features which characterize the behavior of \u2018introverted\u2019 or \u2018extraverted\u2019 subjects in [29]. According to that study, extraverts tend to use a \u2018relaxed\u2019 and \u2018informal\u2019 style, make less use of the first person singular pronouns than introverts and tend to express a \u2018positive affect\u2019 more frequently. These linguistic signs are similar to those we included among our signs of \u2018social attitude,\u2019 so that it is reasonable to wonder whether social attitude is related to the extraversion dimension of personality. We learned a great deal from our experience of iterative prototyping of a health promotion dialog system. We started our research with the belief that a key requirement of dialog simulation was the recognition of the emotional state of the users. This is true when the user problems are serious and therefore produce a strong emotional state (as in the case of natural dialogs with a counselor about drinking and smoking). On the contrary, when the subjects involved are younger and their problems are less serious, different kinds of emotions emerge in the interaction: rather than strong \u2018individual\u2019 emotions like fear, joy, anxiety, relief etc., softer \u2018social\u2019 emotions like sympathy or antipathy, tenderness, contempt, and sense of belonging are expressed [56]. To increase the effectiveness of advice-giving, the ability to recognize the degree of involvement of the user and to manifest the reciprocity of social attitudes is probably even more important than displaying realistic expressions of emotions in the animated agent\u2019s face. This raises complex problems, like recognizing and responding to humor [62] or deception attempts [27] or formulating moves with appropriate \u2018politeness\u2019 which are all important areas for future research. Acknowledgments This work was financed, in part, by HUMAINE, the European Human\u2013Machine Interaction Network on Emotion (EC Contract 507422). Nicole Novielli began to work at this project when she was at the University of Liverpool under the tutorship of Floriana Grasso, in the scope of a SOCRATES agreement with the University of Bari. We thank Giuseppe Clarizio for cooperating in the implementation of the WOZ tool and Gianni Cozzolongo and Irene Mazzotta for cooperating in labeling the corpus. We thank Loquendo for providing us their software in the scope of a scientific cooperation agreement and Mrs. Pauline Butts for being so kind to correct our bad English after interpreting our ideas. We cannot conclude this list without thanking warmly Tim Bickmore, who interpreted his role of Guest Editor in a such cooperative way, to spend a lot of time in reviewing and revising our paper. References [1] J. Ang R. Dhillon A. Krupsky E. Shriberg A. Stolcke Prosody based automatic detection of annoyance and frustration in human\u2013computer dialog Proc ICSLP 2002 [2] E. Ball A bayesian heart: computer recognition and simulation of emotion R. Trappl P. Petta S. Payr Emotions in humans and artifacts 2003 The MIT Press Cambridge [3] J. Bates The role of emotions in believable agents Commun ACM 37 7 1994 [4] A. Batliner K. Fischer R. Huber J. Spilker E. Noth How to find trouble in communication Speech Commun 40 2003 [5] D. Berry L. Butler F. de Rosis Evaluating GRETA. The importance of consistency of behaviour in a multimodal animated agent Int J Hum\u2013Comput Stud 63 2005 [6] T. Bickmore A. Gruber R. Picard Establishing the computer\u2013patient working alliance in automated health behavior change interventions Patient Educ Couns 59 1 2005 21 30 [7] S. Carberry L. Lambert L. Schroeder Towards recognising and conveying an attitude of doubt via natural language Appl Artif Intell 2002 [8] J. Carletta Assessing agreement on classification tasks. The Kappa statistics Comput Linguist 22 1996 [9] Carofiglio V, de Rosis F, Grassano R. Dynamic models of mixed emotion activation. In: Canamero L, Aylett R, editors. Animating expressive characters for social interactions. John Benjamins; in press. [10] Carofiglio V, De Rosis F. In favour of cognitive models of emotions. AISB\u201905 Workshop on Mind-Minding Agents, 2005. [11] J. Cassell T. Bickmore Negotiated collusion: modelling social language and its relationship effects in intelligent agents User Model User-Adapted Interact 13 1\u20132 2003 [12] J. Cassell J. Sullivan S. Prevost E. Churchill Embodied conversational agents 2000 The MIT Press Cambridge, MA [13] A. Cavalluzzi V. Carofiglio F. de Rosis Affective advice-giving dialogs E. Andr\u00e9 L. Dybkjaer W. Minker P. Heisterkamp Affective dialogue systems LNAI 3068 2004 Springer Berlin [14] Cavalluzzi A, de Rosis F, Mazzotta I, Novielli N. Modeling the user attitude towards an ECA. UM\u201905 Workshop on Adapting the interaction style to affective factors. Edinburgh, July 2005. [15] K.M. Colby Modeling a paranoid mind Behav Brain Sci 4 1981 [16] K.M. Colby P.M. Colby R.J. Stoller Dialogues in natural language with GURU, a psychologic inference engine Philos Psychol 3 1990 [17] K.M. Colby Human\u2013computer conversation in a cognitive therapy program Y. Wilks Machine conversations 1999 Kluwer Academic Publishers Dordrecht [18] C. Conati H. Maclaren Data-driven refinement of a probabilistic model of user affect L. Ardissono P. Brna A. Mitrovic User modeling LNAI 3538 2005 Springer Berlin [19] G.F. Cooper E. Herskovitz A Bayesian method for the induction of probabilistic networks from data Mach Learn 9 1992 [20] Coulston R, Oviatt S, Darves C. Amplitude Convergence in Children\u2019s Conversational Speech With Animated Personas. In: Hansen J, Pellom B, editors. Proceedings of the 7th International Conference on Spoken Language Processing. 2002. [21] R. Cowie Describing the emotional states expressed in speech Speech Emotion 2000 [22] Craggs R, McGee Woods M. A two dimensional annotation scheme for emotion in dialogue. AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications, 2004. [23] Dahlback N, Joensson A, Ahrenberg L. Wizard of Oz Studies. Why And How? Proceedings of the International Workshop on IUI, 1993. [24] Darves S, Oviatt S. Adaptation of Users\u2019 Spoken Dialogue Patterns in a Conversational Interface. In: Hansen J, Pellom B, editors. Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP2002). 2002. [25] B. De Carolis C. Pelachaud I. Poggi M. Steedman APML, a Mark-Up Language for believable behavior generation H. Prendinger M. Ishizuka Life-like characters: tools, affective functions and applications 2003 Springer Berlin [26] de Rosis F, Cavalluzzi A, Mazzotta I, Novielli N. Can embodied conversational agents induce empathy in users? AISB Virtual Social Characters Symposium 2005. [27] Gaines BR, Di Eugenio B, Glass M. The Kappa statistics, a second look. Comput. Linguist 2004. B. Di Eugenio M. Glass The Kappa statistics, a second look Comput Linguist 2004 [28] P. Ekman An argument for basic emotions Cogn Emotion 6 1992 [29] Gill AJ, Oberlander J. Taking care of the linguistic features of extraversion. In Proceedings of the 24th Annual Conference of the Cognitive Science Society. 2002. [30] F. Grasso A. Cawsey R. Jones Dialectical argumentation to solve conflicts in advice giving: a case study in the promotion of health nutrition Int J Hum\u2013Comput Stud 53 2000 [31] Green N, Britt T, Jirak K. Communication of uncertainty in clinical genetics patient health communication systems. AAAI Fall Symposium on Dialogue Systems for Health Communication. 2004. [32] Guinn C, Hubal R. Extracting emotional information from the text of spoken dialog. In: Conati C, Hudlicka E, Lisetti C, editors. Workshop on Assessing and adapting to user attitudes and affect: why, when and how? In the scope of User Modeling 2003. [33] H\u00f6\u00f6k K, Isbister K, Laaksholahti J. Sensual evaluation instrument. CHI Workshop on Evaluating affective interfaces: innovative approaches. Portland, April 2005. [34] Johnson WL, LaBore C, Chin Y-C. A pedagogical agent for psychological intervention on a handheld computer. AAAI Fall Symposium on Dialogue Systems For Health Communication. 2004. [35] R. Laforge G. Greene J. Prochaska Psychosocial factors influencing low fruit and vegetable consumption J Behav Med 17 4 1994 361 374 [36] C.M. Lee S.S. Narayanan R. Pieraccini Combining acoustic and language information for emotion recognition Proc ICSLP 2002 [37] R. Li M. Serdula S. Bland Trends in fruit and vegetable consumption among adults in 16 US states: behavioral risk factor surveillance system, 1990\u20131996 Am J Public Health 90 2000 777 781 [38] D. Litman K. Forbes S. Silliman Towards emotion prediction in spoken tutoring dialogues Proc HLT/NAACL 2003 [39] C.D. Manning H. Schutze Foundations of statistical natural language processing 1999 The MIT Press Cambridge [40] Marsella SC, Johnson WL, Labore CM. Interactive pedagogical drama for health interventions. In: Hopper U, et al., editors. Artificial Intelligence in Education. Shaping the future of learning through intelligent technologies. 2003. [41] Miceli M, de Rosis F, Poggi I. Emotional and non emotional persuasion. Applied Artificial Intelligence, in press. [42] C. Nass K. Isbister E.-J. Lee Truth is beauty: researching embodied conversational agents J. Cassell J. Sullivan S. Prevost E. Churchill Embodied conversational agents 2000 The MIT Press Cambridge [43] A.E. Nicholson J.M. Brady Dynamic belief networks for discrete monitoring IEEE Trans Syst Man Cybern 24 11 1994 1593 1610 [44] A. Nijholt Observations on humorous act construction Proc EMCSR 2004 <http://www.home.cs.utwente.nl/~anijholt/artikelen/emcsr2004.pdf/> [45] A. Ortony G.L. Clore A. Collins The cognitive structure of emotions 1988 Cambridge University Press Cambridge [46] S. Oviatt B. Adams Designing and evaluating conversational interfaces with animated characters J. Cassell J. Sullivan S. Prevost E. Churchill Embodied conversational agents 2000 The MIT Press Cambridge [47] Paiva A, Aylett R, Marsella S. AAMAS04 Workshop on Empathic Agents. <http://gaips.inesc.pt/gaips/en/aamas-ea.html/>. 2004. [48] J.A. Pearl Probabilistic reasoning in intelligent systems: networks of plausible inference 1988 Morgan Kaufman Publishers Los Altos, CA [49] C. Pelachaud M. Bilvi Computational model of believable conversational agents M.P. Huget Communication in multiagent systems: background, current trends and future LNCS 2650 2003 Springer Verlag Berlin [50] J. Pennebaker M. Mehl K.G. Niederhoffer Psychological aspects of natural language use. Our works, our selves Annu Rev Psychol 54 2003 [51] R. Picard Affective computing 1997 The MIT Press Cambridge [52] R. Picard What does it mean for a computer to \u2018have\u2019 emotions? R. Trappl P. Petta S. Pays Emotions in humans and artefacts 2002 A Bradford Book, MIT Press Cambridge [53] Picard RW, Daily SB. Evaluating affective interactions: alternatives to asking what users feel. CHI Workshop on Evaluating affective interfaces: innovative approaches. Portland, April 2005. [54] R. Plutchik H.R. Conte Circumplex models of personality and emotions 1996 APA Books Washington, DC [55] Poggi I. Emotions from mind to mind. In: Paiva A, editor. Proceedings of the Workshop on Empathic Agents. AAMAS 2004. [56] Poggi I, Magno-Caldognetto E. Il parlato emotivo. Aspetti cognitivi, linguistici e fonetici. Proceedings of the Conference Il Parlato italiano. Naples: D\u2019Auria; 2003. [57] J. Prochaska C. Di Clemente H. Norcross In search of how people change: applications to addictive behavior Am Psychol 47 1992 1102 1114 [58] Z. Ruttkay C. Pelachaud From brows till trust: evaluating embodied conversational agents 7 2004 Kluwer Academic Publishers Kluwer Human\u2013Computer Interaction Series [59] Sebastiani P, Ramoni M, Crea A. Profiling your customers using bayesian networks. Proceedings of SIGKDD Explorations. ACM SIGKDD 2000. [60] J.A. Sillince R.H. Minors What makes a strong argument? Emotions, highly-placed values and role-playing Commun Cogn 1991 [61] B. Silverman J. Holmes S. Kimmel C. Branas D. Ivins R. Weaver Y. Chen Modeling emotion and behavior change: the case of the HEART-SENSE game Health Care Manag Sci 4 3 2001 [62] O. Stock C. Strapparava An experiment in automated humorous output production Proc Intell User Interfaces 2003 [63] C. Storm T. Storm A taxonomic study of the vocabulary of emotions J Pers Soc Psychol 53 1987 [64] Traum DR, Larsson S, editors. The information state approach to dialogue management. Current and new directions in discourse and dialog. Van Kuppevelt J, Smith R, editors. Kluwer, Dordrecht, 2003. [65] W.F. Velicer J.O. Prochaska J.L. Fava G.J. Norman C.A. Redding Smoking cessation and stress management: applications of the Transtheoretical Model of Change Homeostasis 38 1998 [66] D. Walton The place of emotion in argument 1992 The Pennsylvania State University Press PA [67] C. Wegman Emotion and argumentation in expression of opinion V. Hamilton G.H. Bower N.H Frijda Cognitive perspectives on emotion and motivation 1988 Kluwer Dordrecht [68] Y. Wilks Machine conversations 1999 Kluwer Academic Publishers Dordrecht", "scopus-id": "33748168229", "pubmed-id": "16524784", "coredata": {"eid": "1-s2.0-S1532046406000177", "dc:description": "Abstract In this paper, we describe our experience with the design and implementation of an embodied conversational agent (ECA) that converses with users to change their dietary behavior. Our intent is to develop a system that dynamically models the agent and the user and adapts the agent\u2019s counseling dialog accordingly. Towards this end, we discuss our efforts to automatically determine the user\u2019s dietary behavior stage of change and attitude towards the agent on the basis of unconstrained typed text dialog, first with another person and then with an ECA controlled by an experimenter in a wizard of Oz study. We describe how the results of these studies have been incorporated into an algorithm that combines the results from simple parsing rules together with contextual features using a Bayesian network to determine user stage and attitude automatically.", "openArchiveArticle": "true", "prism:coverDate": "2006-10-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046406000177", "dc:creator": [{"@_fa": "true", "$": "Rosis, Fiorella de"}, {"@_fa": "true", "$": "Novielli, Nicole"}, {"@_fa": "true", "$": "Carofiglio, Valeria"}, {"@_fa": "true", "$": "Cavalluzzi, Addolorata"}, {"@_fa": "true", "$": "Carolis, Berardina De"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046406000177"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046406000177"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(06)00017-7", "prism:volume": "39", "prism:publisher": "Elsevier Inc.", "dc:title": "User modeling and adaptation in health promotion dialogs with an animated character", "prism:copyright": "Copyright \u00a9 2006 Elsevier Inc. All rights reserved.", "prism:issueName": "Dialog Systems for Health Communications", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "5", "dcterms:subject": [{"@_fa": "true", "$": "User and agent modeling"}, {"@_fa": "true", "$": "Healthy eating dialog"}, {"@_fa": "true", "$": "Embodied conversational agents"}, {"@_fa": "true", "$": "Bayesian networks"}, {"@_fa": "true", "$": "Wizard of Oz studies"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "5", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "514-531", "prism:endingPage": "531", "prism:coverDisplayDate": "October 2006", "prism:doi": "10.1016/j.jbi.2006.01.001", "prism:startingPage": "514", "dc:identifier": "doi:10.1016/j.jbi.2006.01.001", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "17", "@width": "187", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000177-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "825", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "324", "@width": "487", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000177-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "76649", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "83", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000177-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4178", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "417", "@width": "623", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000177-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "82870", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "84", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000177-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3494", "@ref": "gr2", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33748168229"}}