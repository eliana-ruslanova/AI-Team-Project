{"scopus-eid": "2-s2.0-85044626626", "originalText": "serial JL 271275 291210 291727 291850 291928 31 90 Drug Discovery Today DRUGDISCOVERYTODAY 2018-01-31 2018-01-31 2018-06-13 2018-06-13 2018-06-13T19:33:24 1-s2.0-S1359644617303598 S1359-6446(17)30359-8 S1359644617303598 10.1016/j.drudis.2018.01.039 S300 S300.1 FULL-TEXT 1-s2.0-S1359644618X00075 2018-06-16T01:19:43.963398Z 0 0 20180601 20180630 2018 2018-01-31T09:58:18.022535Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authlast grantnumber grantsponsor highlightsabst primabst pubtype ref 1359-6446 13596446 UNLIMITED NONE true 23 23 6 6 Volume 23, Issue 6 11 1241 1250 1241 1250 201806 June 2018 2018-06-01 2018-06-30 2018 REVIEWS Informatics article rev \u00a9 2018 The Authors. Published by Elsevier Ltd. RISEDEEPLEARNINGINDRUGDISCOVERY CHEN H Introduction Principles of deep learning Application of deep learning in compound property and activity prediction De novo design through deep learning Application of deep learning in predicting reactions and retrosynthetic analysis Application of convolutional neural networks to predict ligand\u2013protein interactions Benchmark datasets within chemoinformatics Application of deep learning in biological imaging analysis Future development of deep learning in drug discovery Concluding remarks Conflicts of interest Acknowledgments References HOWARD 2013 J PAPADATOS 2015 885 896 G KIM 2016 D1202 D1213 S GILSON 2016 D1045 D1053 M CORTES 1995 273 297 C SALT 1992 161 170 D HO 1998 832 844 T AMMADUDDIN 2016 i455 463 M CHING 2017 T GOH 2017 1291 1307 G GAWEHN 2016 3 14 E MAMOSHINA 2016 1445 1454 P EKINS 2016 2594 2603 S BASKIN 2016 785 795 I MCCULLOCH 1943 115 133 W DREYFUS 1973 383 385 S SRIVASTAVA 2014 1929 1958 N WAN 2013 1058 1066 L PROCEEDINGS30THINTERNATIONALCONFERENCEMACHINELEARNING REGULARIZATIONNEURALNETWORKSUSINGDROPCONNECT NAIR 2010 807 814 V PROCEEDINGS27THINTERNATIONALCONFERENCEINTERNATIONALCONFERENCEMACHINELEARNING RECTIFIEDLINEARUNITSIMPROVERESTRICTEDBOLTZMANNMACHINES LEE 2011 95 103 H SZEGEDY 2015 C CVPR GOINGDEEPERCONVOLUTIONS FERNANDEZ 2007 220 229 S PROCEEDINGS17THINTERNATIONALCONFERENCEARTIFICIALNEURALNETWORKS APPLICATIONRECURRENTNEURALNETWORKSDISCRIMINATIVEKEYWORDSPOTTING HOCHREITER 1997 1735 1780 S BENGIO 2009 1 127 Y KINGMA 2013 D MA 2015 263 274 J MAYR 2016 A DAHL 2014 G RAMSUNDAR 2017 2068 2076 B KOUTSOUKAS 2017 42 A GAULTON 2012 D1100 1107 A LENSELINK 2017 45 E SUBRAMANIAN 2016 1936 1949 G ALIPER 2016 2524 2530 A MERKWIRTH 2005 1159 1168 C LUSCI 2013 1563 1575 A XU 2015 2085 2093 Y MORGAN 1965 107 113 H DUVENAUD 2015 2224 2232 D PROCEEDINGS28THINTERNATIONALCONFERENCENEURALINFORMATIONPROCESSINGSYSTEMS CONVOLUTIONALNETWORKSGRAPHSFORLEARNINGMOLECULARFINGERPRINTS KEARNES 2016 595 608 S XU 2017 2672 2685 Y LI 2017 J COLEY 2017 1757 1772 C GILMER 2017 J LI 2015 Y KIPF 2016 T BJERRUM 2017 E GOH 2017 G GOH 2017 G GOMEZBOMBARELLI 2016 R KADURIN 2017 3098 3104 A GOODFELLOW 2014 I BLASCHKE 2017 T SEGLER 2018 120 131 M YUAN 2017 875 882 W JAQUES 2016 N LEO 1971 525 616 A BICKERTON 2012 90 98 G OLIVECRONA 2017 48 M BENHENDA 2017 M METZ 2016 L UNTERTHINER 2017 T COREY 1969 178 192 E COLEY 2017 434 443 C JIN 2017 W SEGLER 2017 5966 5971 M SEGLER 2017 M LIU 2017 1103 1113 B PAGADALA 2017 91 102 N RAGOZA 2017 942 957 M TROTT 2010 455 461 O DUNBAR 2011 2036 2046 J GOMES 2017 J WALLACH 2015 I PEREIRA 2016 2495 2506 J RUSSAKOVSKY 2015 211 252 O MYSINGER 2012 6582 6594 M SUN 2017 17 J MILLER 1995 39 41 G LI 2009 1037 F WU 2017 Z ANGERMUELLER 2016 878 C KRAUS 2015 O RONNEBERGER 2015 234 241 O INTERNATIONALCONFERENCEMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTION UNETCONVOLUTIONALNETWORKSFORBIOMEDICALIMAGESEGMENTATION NING 2005 1360 1371 F FERRARI 2015 7458 7461 A 201537THANNUALINTERNATIONALCONFERENCEIEEE BACTERIALCOLONYCOUNTINGBYCONVOLUTIONALNEURALNETWORKSINENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBC CIRESAN 2013 411 418 D INTERNATIONALCONFERENCEMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTION MITOSISDETECTIONINBREASTCANCERHISTOLOGYIMAGESDEEPNEURALNETWORKS SIRINUKUNWATTANA 2016 1196 1206 K XU 2014 1626 1630 Y ACOUSTICSSPEECHSIGNALPROCESSINGICASSP2014IEEEINTERNATIONALCONFERENCE DEEPLEARNINGFEATUREREPRESENTATIONMULTIPLEINSTANCELEARNINGFORMEDICALIMAGEANALYSIS TURKKI 2016 38 R VANDENBERGHE 2017 M LITJENS 2016 26286 G BAR 2015 Y CHENG 2016 24454 J CHA 2016 1882 1896 K AVENDI 2016 108 119 M LI 2014 305 312 R INTERNATIONALCONFERENCEMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTION DEEPLEARNINGBASEDIMAGINGDATACOMPLETIONFORIMPROVEDBRAINDISEASEDIAGNOSIS LIU 2014 2028 S BEHRMANN 2017 J INGLESE 2017 3500 3511 P CHEN 2016 21471 C ZHANG 2016 W VINYALS 2016 O ALTAETRAN 2017 283 293 H GRAVES 2016 471 476 A CHEN 2016 T CHENX2018X1241 CHENX2018X1241X1250 CHENX2018X1241XH CHENX2018X1241X1250XH Full 2018-01-17T22:12:24Z Author http://creativecommons.org/licenses/by/4.0/ 2019-06-13T00:00:00.000Z 2019-06-13T00:00:00.000Z UnderEmbargo This is an open access article under the CC BY license. \u00a9 2018 The Authors. Published by Elsevier Ltd. item S1359-6446(17)30359-8 S1359644617303598 1-s2.0-S1359644617303598 10.1016/j.drudis.2018.01.039 271275 2018-06-13T21:33:54.750657Z 2018-06-01 2018-06-30 UNLIMITED NONE 1-s2.0-S1359644617303598-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/MAIN/application/pdf/550b4de9715cab7ab0301f61d3031349/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/MAIN/application/pdf/550b4de9715cab7ab0301f61d3031349/main.pdf main.pdf pdf true 1509406 MAIN 10 1-s2.0-S1359644617303598-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/PREVIEW/image/png/5aa32875469ab288dc6fd1a3d68b866e/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/PREVIEW/image/png/5aa32875469ab288dc6fd1a3d68b866e/main_1.png main_1.png png 87902 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1359644617303598-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr1/THUMBNAIL/image/gif/23a5ec5f08c8087f161aaa0d67f69aa2/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr1/THUMBNAIL/image/gif/23a5ec5f08c8087f161aaa0d67f69aa2/gr1.sml gr1 gr1.sml sml 2742 56 219 IMAGE-THUMBNAIL 1-s2.0-S1359644617303598-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr2/THUMBNAIL/image/gif/3d8d5e5fea238d0ba07481245837310b/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr2/THUMBNAIL/image/gif/3d8d5e5fea238d0ba07481245837310b/gr2.sml gr2 gr2.sml sml 6521 164 133 IMAGE-THUMBNAIL 1-s2.0-S1359644617303598-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr3/THUMBNAIL/image/gif/6116e12aa9594f7c52cdaad5fd76b9ae/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr3/THUMBNAIL/image/gif/6116e12aa9594f7c52cdaad5fd76b9ae/gr3.sml gr3 gr3.sml sml 5084 143 219 IMAGE-THUMBNAIL 1-s2.0-S1359644617303598-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr4/THUMBNAIL/image/gif/f29e6a460e08cb0f8085693f2c4f6cc3/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr4/THUMBNAIL/image/gif/f29e6a460e08cb0f8085693f2c4f6cc3/gr4.sml gr4 gr4.sml sml 2629 46 219 IMAGE-THUMBNAIL 1-s2.0-S1359644617303598-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr5/THUMBNAIL/image/gif/733af8176a67b95781228d873fbc3102/gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr5/THUMBNAIL/image/gif/733af8176a67b95781228d873fbc3102/gr5.sml gr5 gr5.sml sml 8957 133 219 IMAGE-THUMBNAIL 1-s2.0-S1359644617303598-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr1/DOWNSAMPLED/image/jpeg/a94a1cbb2b4bf2b3fe3c0a41b688d2ba/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr1/DOWNSAMPLED/image/jpeg/a94a1cbb2b4bf2b3fe3c0a41b688d2ba/gr1.jpg gr1 gr1.jpg jpg 24848 213 836 IMAGE-DOWNSAMPLED 1-s2.0-S1359644617303598-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr2/DOWNSAMPLED/image/jpeg/3b88a26b9e9ce1d6de67a81a4f91c38c/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr2/DOWNSAMPLED/image/jpeg/3b88a26b9e9ce1d6de67a81a4f91c38c/gr2.jpg gr2 gr2.jpg jpg 111147 1032 836 IMAGE-DOWNSAMPLED 1-s2.0-S1359644617303598-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr3/DOWNSAMPLED/image/jpeg/664d916085a627794968ce50a4bc85ba/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr3/DOWNSAMPLED/image/jpeg/664d916085a627794968ce50a4bc85ba/gr3.jpg gr3 gr3.jpg jpg 50598 545 836 IMAGE-DOWNSAMPLED 1-s2.0-S1359644617303598-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr4/DOWNSAMPLED/image/jpeg/8e046fa4f3cf772a0d189681227fc006/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr4/DOWNSAMPLED/image/jpeg/8e046fa4f3cf772a0d189681227fc006/gr4.jpg gr4 gr4.jpg jpg 22641 175 836 IMAGE-DOWNSAMPLED 1-s2.0-S1359644617303598-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr5/DOWNSAMPLED/image/jpeg/df5338cfa431f6f2adc1d6c3f06c0654/gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr5/DOWNSAMPLED/image/jpeg/df5338cfa431f6f2adc1d6c3f06c0654/gr5.jpg gr5 gr5.jpg jpg 61437 507 836 IMAGE-DOWNSAMPLED 1-s2.0-S1359644617303598-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr1/HIGHRES/image/jpeg/1ebf0b32d6816a068ee318222320825e/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr1/HIGHRES/image/jpeg/1ebf0b32d6816a068ee318222320825e/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 196452 943 3703 IMAGE-HIGH-RES 1-s2.0-S1359644617303598-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr2/HIGHRES/image/jpeg/a3285e171c7962ce6d538a8671d032e8/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr2/HIGHRES/image/jpeg/a3285e171c7962ce6d538a8671d032e8/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 1102351 4570 3703 IMAGE-HIGH-RES 1-s2.0-S1359644617303598-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr3/HIGHRES/image/jpeg/72eda99f7cde77e03e09d2f5c3ef9c7d/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr3/HIGHRES/image/jpeg/72eda99f7cde77e03e09d2f5c3ef9c7d/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 398993 2416 3703 IMAGE-HIGH-RES 1-s2.0-S1359644617303598-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr4/HIGHRES/image/jpeg/722651bff3cd5d04dd2bf0ffe5ac3eec/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr4/HIGHRES/image/jpeg/722651bff3cd5d04dd2bf0ffe5ac3eec/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 178643 775 3703 IMAGE-HIGH-RES 1-s2.0-S1359644617303598-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/gr5/HIGHRES/image/jpeg/c68c630773e9baddf8731949dfa71c9e/gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/gr5/HIGHRES/image/jpeg/c68c630773e9baddf8731949dfa71c9e/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 464640 2247 3703 IMAGE-HIGH-RES 1-s2.0-S1359644617303598-si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1359644617303598/STRIPIN/image/gif/a0fc7719fb5f640bb50c0755e97255db/si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1359644617303598/STRIPIN/image/gif/a0fc7719fb5f640bb50c0755e97255db/si1.gif si1 si1.gif gif 744 50 125 ALTIMG DRUDIS 2172 S1359-6446(17)30359-8 10.1016/j.drudis.2018.01.039 The Authors Figure 1 A simple illustration of neural networks (NNs). (a) A NN is composed of input, hidden and output layers. (b) The output values of a hidden unit are calculated from input values via an activation function. Figure 1 Figure 2 Architecture of several popular neural networks: (a) fully connected deep neural network (DNN), (b) convolutional neural network (CNN), (c) recurrent neural network (RNN) and (d) autoencoder (AE). Figure 2 Figure 3 Illustration of graph convolutional neural networks (CNNs) [49]. A molecular graph first goes through a convolution operation via a single layer NN to form a vector of fixed length. The convolution operation can be run at different neighbor levels. The vectors generated from different convolution operations then go through a softmax transformation and are summed up to form the neural fingerprints of the compound. The neural fingerprint is passed through another fully connected NN layer to generate the final output. The bit values in the neural fingerprint are learned through training and are differentiable. Figure 3 Figure 4 The illustration of a variational autoencoder (VAE) method. The encoder neural network (NN) converts a discrete molecule into Gaussian distribution deterministically. After the latent variables are reparameterized against the gaussian distribution with given mean and variance, a new point is sampled and fed into the decoder NN. In the generation mode, only the decoder is used to generate a new molecule from the sampled latent point. Figure 4 Figure 5 Structure generation from recurrent neural networks (RNNs). The upper plot shows how the RNN model thinks when generating the structure on the bottom right. The y axis lists all possible tokens that can be chosen at each step, the color represents the conditional probability for the character to be chosen at the current step given the previously chosen characters, and the x axis shows the character that, in this instance, was sampled. The bottom left figure demonstrates how the RNN actually works in the structure-generation mode. At each step a character was sampled based on the conditional probability distribution calculated from the RNN model and the generated character will then be used as the input character for generation of the next character. Figure 5 Review Informatics The rise of deep learning in drug discovery Hongming Chen 1 \u204e hongming.chen@astrazeneca.com Ola Engkvist 1 Yinhai Wang 2 Marcus Olivecrona 1 Thomas Blaschke 1 1 Hit Discovery, Discovery Sciences, Innovative Medicines and Early Development Biotech Unit, AstraZeneca R&D Gothenburg, M\u00f6lndal 43183, Sweden Hit Discovery Discovery Sciences Innovative Medicines and Early Development Biotech Unit AstraZeneca R&D Gothenburg M\u00f6lndal 43183 Sweden 2 Quantitative Biology, Discovery Sciences, Innovative Medicines and Early Development Biotech Unit, AstraZeneca, Unit 310, Cambridge Science Park, Milton Road, Cambridge CB4 0WG, UK Quantitative Biology Discovery Sciences Innovative Medicines and Early Development Biotech Unit AstraZeneca Unit 310 Cambridge Science Park Milton Road Cambridge CB4 0WG UK \u204e Corresponding author: Highlights \u2022 Deep learning technology has gained remarkable success. \u2022 We highlight the recent applications of deep learning in drug discovery research. \u2022 Some popular deep learning architectures are introduced in the current study. \u2022 Future development of deep learning in drug discovery is discussed. Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis. Introduction Digital data, in all shapes and sizes, is growing exponentially. According to the National Security Agency of the USA, the Internet is processing 1826 petabytes of data per day [1]. In 2011, digital information grew nine times in volume in just five years [2]; and by 2020 its amount in the world is expected to reach 35 trillion gigabytes [3]. The high demand of exploring and analyzing big data has encouraged the use of data-hungry machine learning algorithms like deep learning (DL). DL has gained huge success in a wide range of applications such as computer games, speech recognition, computer vision, natural language processing, self-driving cars, among others [4]. It is fair to say that DL is changing our everyday life. In the Gartner-selected top ten technology trends of 2018, DL-represented AI technologies were ranked at the top position [5]. Over the past decade, there has been a remarkable increase in the amount of available compound activity and biomedical data [6,7] owing to the emergence of new experimental techniques such as HTS, parallel synthesis, among others [7,8]. How to efficiently mine the large-scale chemistry data becomes a crucial problem for drug discovery. Larger data volumes in combination with increased automation technology have promoted further use of machine learning. Besides established methods like support vector machines (SVM) [9], neural networks (NN) [10] and random forest (RF) [11], which have been utilized to develop QSAR models for a long time, methods like matrix factorization [12] and DL have started to be used. DL has taken advantage of the increased amounts of data and the continuous increase of available computer power. A difference between most other machine learning methods and DL is the flexibility of the NN architecture in DL. Architectures that will be discussed in this review are convolutional neural networks (CNNs), recurrent neural networks (RNNs) and fully connected feed-forward networks. Single-layer NNs have been used in QSAR modeling for a long time [10]; and with increasing data size and computational power have made it natural to apply multilayer feed-forward networks for bioactivity predictions. A somewhat surprising development has been the use of RNNs in de novo design which could not be foreseen a few years ago. With the adoption of high-throughput imaging equipment, CNNs have gained remarkable success in computer vision and have become a natural choice for biological image processing. The field of applying DL in drug discovery is rapidly progressing with new articles published almost every week. Recently, several reviews on DL applications in computational chemistry and life sciences have been published [13\u201318]. Here, we focus more on DL applications in drug discovery particularly in the chemoinformatics and biological image analysis domains and highlight DL architectures used so far within drug discovery. Principles of deep learning DL is a class of machine learning algorithms that uses artificial neural networks (ANNs) with many layers of nonlinear processing units for learning data representations. The earliest ANN can be traced back to 1943 [19], when Warren McCulloch and Walter Pitts developed a computational model for NNs based on mathematics and algorithms called threshold logic. The basic structure of a modern ANN is represented in Fig. 1 and is inspired by the structure of the human brain. There are three basic layers in an ANN: the input layer, hidden layer and output layer. Depending on the type of ANN, the nodes, also called neurons, in neighboring layers are either fully connected or partially connected. Input variables are taken by input nodes and the variables are transformed through hidden nodes, and in the end output values are calculated at output nodes. The interrelationship between input and output values of a hidden unit can be exemplified in Fig. 1b. The output value Yi of the node i is calculated as shown in Eq. (1). (1) Y i = g ( \u2211 j W ij * a j ) where aj refers to the input variables, Wij is the weight of input node j on node i and function g is the activation function, which is normally a nonlinear function (e.g., sigmoid or Gaussian function) to transform the linear combination of input signal from input nodes to an output value. The training of an ANN is done by iterative modification of the weight values in the network to optimize the errors between predicted and true value typically through the back-propagation methods [20]. The modern ANN algorithm was developed during the 1960s to the 1980s and applications have appeared since then. But the traditional ANN method suffered from problems such as overfitting, diminishing gradients, among others, and was largely replaced by other machine learning algorithms like SVM [9] and RF [11]. The recent development of DL has given ANN a renaissance. The major difference between DL and traditional ANN is the scale and complexity of the NNs. DL uses larger numbers of hidden layers whereas traditional ANNs normally can only afford one or two hidden layers owing to the limitation of computer hardware in the early days. DL can afford to use many more nodes in each layer owing to the appearance of more-powerful CPU and GPU hardware. There are also many algorithmic improvements in DL, for example using the dropout [21] and DropConnect [22] methods to address the overfitting problem, applying rectified linear unit (ReLU) [23] to avoid vanishing gradients and introducing convolutional and pooling layers as novel network architectures to enable the usage of large numbers of input variables. Most of the DL software packages are open-sourced. TensorFlow [24], Caffe [25], PyTorch [26], Keras [27] and Theano [28] are among the most popular DL packages used in the data science community. Here, we briefly introduce several popular NN architectures used in DL (Fig. 2 ). First is the fully connected deep neural network (DNN) which contains multiple hidden layers and each layer comprises hundreds of nonlinear process units (Fig. 2a). DNNs can take large numbers of input features and the neurons in different layers of a DNN can automatically extract features at different hierarchical levels [29]. Another very popular NN is CNN, which is widely used for image recognition (Fig. 2b). It usually contains several convolution layers and subsampling layers. The convolution layer consists of a set of filters (or kernels) that have a small receptive field and learnable parameters. During the forward pass, each filter is convoluted across the width and height of the input volume, computing the dot product between the entries of the filter and its receptive field in input volume and producing a 2D feature map of that filter. The subsampling layer is used to reduce the size of feature maps. In the end, the feature maps are concatenated into fully connected layers where neurons in neighboring layers are all connected just like in a traditional ANN to give a final output value. Owing to sharing the same parameters for each filter, a CNN largely reduces the number of free parameters learned, thus lowering the consumed memory and increasing the learning speed. It has outperformed other types of machine learning algorithms in image recognition [30]. One additional variant of an ANN (Fig. 2c) is RNN. Unlike feed-forward NNs, it allows the connection among neurons in the same hidden layer to form a directed cycle. RNNs can take sequential data as input features, which is very suitable for time-dependent tasks like language modeling [31]. Using a technology called long short term memory (LSTM) [32], RNNs can reduce the vanishing gradient problem. The fourth ANN architecture shown in Fig. 2d is called autoencoder (AE) [33]. An AE is a NN used for unsupervised learning. It contains an encoder part, which is a NN to transform the information received from the input layer into a limited number of hidden units, and then couples a decoder NN with the output layer having the same number of nodes as the input layer. Instead of predicting labels of input instances, the purpose of the decoder NN is to reconstruct its own inputs from a fewer number of hidden units. Typically, the purpose of AE is for nonlinear dimensionality reduction. Recently, the AE concept has become more widely used for learning generative models from data [34]. Below, we illustrate how these DL technologies are applied in drug discovery research. Application of deep learning in compound property and activity prediction Machine learning methods including ANN have been applied in compound activity prediction for a long time. Naturally, DL methods are adopted to address the activity prediction problems in the first place. When compounds are presented by the same number of molecular descriptors, the straight forward method is to use fully connected DNNs to build models. Dahl et al. [35] applied a DNN on the Merck Kaggle challenge dataset using a large number of 2D topological descriptors; and the DNN showed slightly better performance in 13 of the total 15 targets than the standard RF method. Some of the key learnings from the study are: (i) DNNs can handle thousands of descriptors without the need of feature selection; (ii) dropout can avoid the notorious overfitting problem faced by a traditional ANN; (iii) hyper-parameter (number of layers, number of nodes per layer, type of activation functions, etc.) optimization can maximize the DNN performance; (iv) multitask DNN models perform better than single-task models. Mayr et al. [36] reported their multitask DNN models that won the Tox21 challenge on a dataset comprising 12000 compounds for 12 high-throughput toxicity assays. Similar to Dahl\u2019s architecture [35,37], dropout and ReLU activation function were used in the DNN, and model training was run in parallel on GPU machines. They used a large feature set with static descriptors (3D, 2D descriptors, predefined toxicophores) as well as dynamically generated extended connectivity fingerprint descriptors (ECFP) to enable DNN to make self-feature deduction during training. More interestingly, statistical association analysis was done for DNN models exclusively using ECFP, and substructures significantly associated with known toxicophores in each hidden layer can be identified. These benchmark results demonstrate the advantages of a multitask DNN compared with a single-task DNN and conventional machine learning methods. Recently, some other benchmark studies were published to further support the conclusion. Ramsundar et al. carried out a systematic study [38] to build multitask DNNs and compare their performance with single-task DNN models. Their results show that multitask models constantly perform better than single-task and RF models. Koutsoukas et al. [39] compared a DNN model with some commonly used machine learning methods such as SVM, RF, among others, on seven datasets selected from ChEMBL [40]. DNNs were found to statistically outperform (with P value <0.01 based on the Wilcoxon\u2019s statistical test) other machine learning methods. Lenselink et al. [41] reported another benchmark study for comparing DNN with conventional machine learning methods RF, SVM, naive Bayesian and logic regression methods taking protein descriptors into account [i.e., the proteochemometric (PCM) study]. They investigated performance of various classification models on a dataset comprising 314 767 target\u2013compound interactions. The DNN model turned out to be the best model in terms of BEDROC (Boltzmann-enhanced discrimination of receiver operating characteristic), and multitask and PCM implementations were shown to improve performance over single-task DNNs. Besides the benchmark studies of DNN, Subramanian et al. [42] reported a study using DNN with 2D topological descriptors to build a predictive BACE activity model and achieved a classification accuracy of 0.82 and standard error of pIC50 \u223c0.53 on the validation set. Aliper et al. [43] built DNN models for predicting pharmacological properties of drugs and for drug repurposing leveraging transcriptomic data from the LINCS project [44], as well as the pathway information. It has been shown that, using pathway and gene-level information, DNN models achieved high accuracy in predicting drug indications, hence they could be useful for drug repurposing. Efforts have also been made in using representation learning (i.e., enabling NNs to learn directly from the molecular structure instead of using predefined molecular descriptors). This idea was first explored by Merkwirth et al. in 2005 [45]. Several years later, two different methods were developed to address the problem. Lusci et al. [46] reported a method that employed a variant of RNN, called UGRNN, which first transforms molecular structures into vectors of the same length as the molecular representation and then passes them to a fully connected NN layer to build models. Bit values in the vectors are learned from the dataset. The UGRNN method was shown to be able to build predictive solubility models that were comparable in accuracy to models built with molecular descriptors. Xu et al. [47] applied the same method to model drug-induced liver injury (DILI). The DL models were built based on 475 drugs and validated on an external dataset of 198 drugs. The best model achieved an AUC of 0.955 exceeding the accuracy of previously reported DILI models. Another type of method is called graph convolution models. The basic idea is similar to the UGRNN method, which employs NNs to automatically generate a molecular description vector and vector values are learned by training NNs. Inspired by the Morgan circular fingerprint method [48], Duvenaud et al. [49] proposed the neural fingerprint method as one of the first efforts in creating a graph convolution model. The workflow of this method can be seen in Fig. 3 . First, the 2D molecular structure is read to form a state matrix, containing atom and bond information (based on the bonds attached to the atom) for each atom. The state matrix then goes through a convolution operation via a single-layer NN to generate a fixed length vector as the molecular representation. The convolution operation can be run at different levels by considering the contribution of neighboring atoms, which is equivalent to the circular fingerprints at different neighboring levels. The vectors generated from different convolution operations first go through a softmax transformation and then are summed up to form the final vector for the compound, which is a neural fingerprint encoding molecular level information. The neural fingerprints are passed through another fully connected NN layer to generate the final output. The bit values in the neural fingerprint are learned through training and are differentiable. In Duvenaud\u2019s three test cases, better results were obtained using neural fingerprints than with Morgan fingerprints and, more importantly, the influential substructures in the graph convolution model can be visualized to interpret the model. The advantage of the graph convolution model is that descriptors are generated automatically during the training and do not need any predefined molecular descriptor. Such a descriptor is not a general descriptor, but is task-specific and fully differentiable and hence can potentially provide better prediction. Other molecular graph convolution methods were reported by Kearnes et al. [50], Xu et al. [51], Li et al. [52] and Coley et al. [53] to extend on Duvenaud\u2019s method. Recently, researchers from Google [54] reformulated several existing graph convolution algorithms [49,50,53,55,56] into a common framework known as a message passing neural network (MPNN) and used the MPNNs to predict quantum chemical properties. Besides the graph-based representation learning methods, DL methods based on other types of molecular representation were also explored. Bjerrum [57] used a SMILES string as the input to LSTM RNNs to build predictive models without the need to generate molecular descriptors. More interestingly, it was observed that augmenting the dataset by using multiple SMILES strings to represent the same compound achieved better results than using canonical SMILES. Goh et al. [58] applied a CNN on images of 2D drawings of molecules and achieved surprisingly comparable results to DNN models trained on ECFP. Moreover [59], when the images were augmented with some basic chemical information, the model performance was further improved. The capability of learning representations from structures directly without using any predefined structure descriptor is an important feature distinguishing DL from other machine learning methods and it basically makes the traditional feature selection and reduction procedures unnecessary. De novo design through deep learning Another interesting application of DL in chemoinformatics is the generation of new chemical structures through NNs. G\u00f3mez-Bombarelli et al. proposed a novel method [60] using variational autoencoder (VAE) to generate chemical structures (Fig. 4 ). The first step is to use VAE to do unsupervised learning to map chemical structures (SMILES strings) in the ZINC database into latent space. Once the VAE training is done, the latent vector in the latent space becomes a continuous representation of molecular structure and can be reversibly transformed to a SMILES string through the trained VAE. Generation of a new structure with desirable properties can be realized by searching optimal latent solutions in the continuous latent space via any optimization method (e.g., Bayesian optimization) and then decoding the searched latent solutions into SMILES. Following on from G\u00f3mez-Bombarelli\u2019s work, Kadurin et al. [61] used VAE as a molecular descriptor generator coupled with a generative adversarial network (GAN) [62], a special NN architecture, to generate new structures that were claimed to have promising specific anticancer properties. Blaschke et al. [63] utilized VAE to generate novel structures with predicted activity against dopamine receptor type 2. RNNs have been very successful in the natural language processing area [31]. Segler et al. [64] and Yuan et al. [65] reported their studies using RNNs to generate novel chemical structures. After training the RNN on a large number of SMILES strings, the RNN method worked surprisingly well for generating new valid SMILES strings that were not included in the training set (Fig. 5 ). The RNN writes structurally valid SMILES by learning the underlying probability distribution of characters in a SMILES string and, in this case, RNN can be regarded as a generative model for molecule structures. Segler et al. [64] also explored the possibility of using RNNs to generate target-specific libraries by first training a general prior model and then a fine-tuned focused model through transfer learning on a small set of target-specific active compounds. In a retrospective analysis for testing on two antibioactive targets, their focused models were able to generate 18% unseen true actives for Staphylococcus aureus and 28% for Plasmodium falciparum. Jaques et al. [66] applied a reinforcement learning technology, called Deep Q-learning, together with an RNN to generate SMILES with desirable molecular properties such as cLogP [67] and QED drug-likeness [68]. However, their method needed a reward function that incorporates handwritten rules to penalize undesirable types of structures, which otherwise would lead to exploitation of the reward resulting in unrealistically simple molecules. To overcome the drawback, Olivecrona et al. [69] proposed a policy-based reinforcement learning approach to tune the pre-trained RNNs for generating molecules with given user-defined properties. In one test example for tuning the model toward generating compounds predicted to be active against the dopamine receptor type 2, the model generated structures of which >95% were predicted to be active, including experimentally confirmed actives that have not been included in the generative model nor the activity prediction model. The methods described above have demonstrated potentials as alternatives to the traditional rule-based approaches for de novo design. However, GANs and the reinforcement learning methods are known to be susceptible to mode collapse (i.e., the models only generate a single solution or a small family of similar solutions). This has been highlighted in a recent survey [70] on de novo structure generation using DL tools. Considerable effort [71,72] has been spent to address this issue. Application of deep learning in predicting reactions and retrosynthetic analysis Synthesis predictions have a long history dating back to rule-based methods in the 1960s [73]. Very recently some promising results were reported in reaction prediction using DL methods. Although there has been no explicit comparison with other machine learning methods, the results indicated that DL can achieve performance on-par with, or superior to, the rule-based methods. Schematically, two types of problems can be addressed with machine learning including DL in reaction informatics. One type is forward reaction prediction, where the products are predicted given a set of reactants, and the other type is retrosynthetic prediction, where given a final product the reaction steps that produce the product are predicted. Coley et al. [74] utilized NN to rank the candidate products for a set of reactions based on a training set of 15 000 reactions from US patents. The reactions were classified into templates and the trained model correctly assigned the major product rank 1 in 71.8%, rank \u22643 in 86.7% and rank \u22645 in 90.8% of cases. To overcome the coverage and efficiency issues faced with the template-based reaction prediction methods, a template-free approach was proposed [75] in a follow-up study by the same research group. They employed the Weisfeiler\u2013Lehman difference network to score the generated candidate reactions and superior performance was achieved compared with reaction template-based methods. Segler et al. [76] used 3.5 million reactions as the training set for DNN. A top-ten accuracy of 97% for reaction prediction and 95% in retrosynthetic analysis were achieved. In another study [77], they combined policy networks and Monte-Carlo tree search for retrosynthetic prediction utilizing a training set consisting of 12 million reactions from scientific literature. Their system can solve twice as many molecules\u2019 retrosynthesis plans as the rule-based method. Liu et al. [78] used neural sequence-to-sequence models for retrosynthetic prediction. They used 50 000 reactions obtained from US patents to train the network and obtained similar accuracy to rule-based methods. Application of convolutional neural networks to predict ligand\u2013protein interactions Assessing the interaction between a protein and a ligand is the crucial part of the molecular docking program and a lot of scoring functions were developed either based on forcefields or knowledge from existing protein\u2013ligand complex structures [79]. Inspired by the success of CNNs in image analysis, several studies have been recently published in applying a CNN to score protein\u2013ligand interaction. A typical example is the investigation done by Ragoza et al. [80]. The protein\u2013ligand structures were discretized into a grid with a resolution of 0.5\u00c5. The grid was 24\u00c5 on each side and centered on the binding site. Each atom was described with a function, and atom densities over the grid were generated to form the input matrix. Multilayer CNN models were defined and trained using the Caffe DL framework. The CNN scoring outperformed AutoDock Vina [81] on the CSAR inter-target pose-prediction dataset [82], but performed worse for intra-target ranking of poses. Other studies utilizing CNNs or DNNs have also been published [83\u201385]. Although some encouraging results have been obtained with convolutional networks, it is not clear whether they will consistently improve results compared to currently used scoring functions. Benchmark datasets within chemoinformatics The rapid advances made in the field of image recognition can be attributed to not only the emergence of novel algorithms but also to the existence of canonical and large datasets. The standardized dataset would allow the community to conveniently benchmark or evaluate developed machine learning methods. The yearly ImageNet Large Scale Visual Recognition Competition (ILSVRC) [86] has seen the birth of many influential CNN architectures. Although several open-source chemoinformatics datasets [87,88] are available, their impact on machine learning method development is still limited owing to the limited size of those datasets, lack of diverse ways of splitting training and test-sets and, more importantly, lack of a standard evaluation platform for proposed new algorithms. Inspired by WordNet [89] and ImageNet [90], Wu et al. [91] introduced the MoleculeNet dataset by curating a number of diverse collections including quantum mechanics, physicochemical, biophysics and physiological datasets, and developing a suite of software implementing many known molecule representations and machine learning algorithms. MoleculeNet is built on the open source package DeepChem [92] and provides easy access to some popular DL algorithms existing in DeepChem. This will largely facilitate comparison and development of novel machine learning algorithms in the future. Application of deep learning in biological imaging analysis In the drug discovery process, biological imaging and image analysis are widely used at various stages from preclinical R&D to clinical trials. Imaging enables scientists to see the phenotypes and behaviors of hosts (human or animals), organs, tissues, cells and subcellular components. Through digital image analysis, the hidden biology and pathology, as well as the drug mechanism of action, are revealed. Examples of imaging modalities are fluorescently labeled or unlabeled microscopic images, computed tomography (CT), MRI, positron emission tomography (PET), tissue pathology imaging and mass-spectrometry imaging (MSI). DL has also made its way to successes in biological image analysis and many studies reported a superior performance compared with classical classifiers. For microscopic images, CNNs have been used [93,94] for segmenting and subtyping individual fluorescently labelled cells, as well as unlabeled imageries from phase contract microscopy [95,96]. Other traditionally laborious tasks from preclinical settings, such as cell tracking [96] and colony counting [97], could also be automated using DL. Images from tissue pathology are typically complex in nature compared with the fluorescently labeled images owing to rich tissue morphology. Nevertheless, at the cellular level, the segmentation and classification of individual cells were achieved in breast and colon tissues stained with hematoxylin and eosin (H&E) staining [98,99]. At the tissue region level, the tumor regions from H&E-stained breast tissue were identified through DL [100], whereas the extra categories of leukocytes and fat tissue can also be recognized [101]. Beyond basic image segmentation, DL has already been used for the histopathological diagnosis with H&E and the immunohistochemistry stained tissue [102,103]. The application of DL was also applied for the analysis of CT [104\u2013106], MRI [107,108] and PET [108] imaging. Besides the popular application of the image segmentation [106,107] and classifications [104,105], its utilities have also been shown in content-based image retrieval [109] and it was reported that DL methods outperformed the popular ISOMAP and Elastic Net methods. For the emerging MSI, similar to the application of DL in tissue pathology, tumor subtyping can be performed by high-resolution matrix-assisted laser desorption/ionization (MALDI) MSI [110]. Given that MSI can visualize the metabolic information of a tissue, sub-regions of a tumor with metabolic heterogeneity from desorption electrospray ionization (DESI) MSI can already be detected through DL [111]. Finally, in an unusual imaging area: flow cytometry, DL enabled the cell classification in real-time for high-throughput applications [112]. The training of DNNs for imaging is time-consuming and requires dedicated GPU processing. Furthermore, in the context of high-throughput imaging screening, good-quality training sets are rare. Therefore, image features trained from natural scenes and other datasets were \u2018borrowed\u2019 to perform biological image segmentations and classifications, and robust performances were reported [101,113]. Future development of deep learning in drug discovery Machine learning methods and DL in particular generally need large datasets for training; however, the human brain has the capability of learning through only a few examples. How to learn with only a small amount of available data is therefore one of the hottest topics in machine learning. A DL example of exploiting auxiliary data to improve a model with only a few data points is matching networks [114], which was proposed as a variant of one-shot learning. Improved results were obtained when the auxiliary data were included. Methods like one-shot learning are relevant to drug discovery, where medicinal chemists often work on novel targets with limited data available. Altae-Tran et al. [115] utilized the LSTM method on chemoinformatics datasets to build models with a very small training set and promising results were reported. Very recently, a new type of architecture has been used in DL: memory augmented neural networks. The first version was the neural Turing machine. This architecture was significantly improved with a differentiable neural computer (DNC) [116]. DNCs have been applied to several problems like question-answering systems and finding the shortest path in graphs. However, these more-advanced architectures have not been applied so far in drug discovery. Concluding remarks Machine learning has been used since the late 1990s in drug discovery and has established itself as a useful tool in drug discovery. A recent extension of the machine learning toolbox is DL. In comparison with other methods, DL has a much more flexible architecture so it is possible to create a NN architecture tailor-made for a specific problem. A disadvantage is that DL in general needs very large training sets. A relevant question is: is DL is superior to other machine learning methods? We believe it is still too early to draw any firm conclusion, the results so far indicate that DL is superior for certain tasks like image analysis and very useful for de novo molecular design and reaction predictions. For tasks with structured input descriptors, DL seems to perform at least on-par with other methods. The most relevant example is bioactivity prediction where DL seems to achieve better performance overall through multitask learning. However, other machine learning methods are also improving. One example is the XGBoost [117] method, which has dominated Kaggle competitions for structured input data [118] after its introduction. Thus, in practice the choice of method used in bioactivity prediction might depend on which method the modeler is most familiar with. If different machine learning methods achieve roughly the same accuracy, the limit of what can be achieved with a machine learning model could depend on experimental uncertainty for the data and dataset size rather than the specific algorithm used. Conflicts of interest The authors have no conflicts of interest to declare. Acknowledgments The authors thank Christian Tyrchan, Lars Carlsson, Thierry Kogej and Clive Green for valuable discussion on deep learning and machine learning in general. This research has received funding from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement no. 676434, \u2018Big Data in Chemistry\u2019 (BIGCHEM). The article reflects only the authors\u2019 views and neither the European Commission nor the Research Executive Agency (RREA) are responsible for any use that could be made of the information it contains. References 1 National Security Agency statement. Available at: https://www.nsa.gov/news-features/press-room/statements/2013-08-09-the-nsa-story.shtml 2 Gantz, J. and Reinsel, D. (2011) Extracting value from chaos. Available at: https://www.emc.com/collateral/analyst-reports/idc-extracting-value-from-chaos-ar.pdf 3 Gantz, J. and Reinsel, D. (2010) The digital universe decade \u2013 are you ready? Available at: https://www.emc.com/collateral/analyst-reports/idc-digital-universe-are-you-ready.pdf 4 J. Howard The business impact of deep learning Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 2013 pp. 1135 5 Top Strategic Technology Trends for 2018. Available at: http://www.gartner.com/technology/research/top-10-technology-trends/ 6 G. Papadatos Activity, assay and target data curation and quality in the ChEMBL database J. Comput. Aided Mol. Des. 29 2015 885 896 7 S. Kim PubChem substance and compound databases Nucleic Acids Res. 44 2016 D1202 D1213 8 M.K. Gilson BindingDB in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology Nucleic Acids Res. 44 2016 D1045 D1053 9 C. Cortes V. Vapnik Support-vector networks Mach. Learn. 20 1995 273 297 10 D.W. Salt The use of artificial neural networks in QSAR Pestic. Sci. 36 1992 161 170 11 T.K. Ho The random subspace method for constructing decision forests IEEE Trans. Pattern Anal. Mach. Intell. 20 1998 832 844 12 M. Ammad-Ud-Din Drug response prediction by inferring pathway-response associations with kernelized Bayesian matrix factorization Bioinformatics 32 2016 i455 463 13 T. Ching Opportunities and obstacles for deep learning in biology and medicine bioRxiv 2017 10.1101/142760 14 G.B. Goh Deep learning for computational chemistry J. Comput. Chem. 38 2017 1291 1307 15 E. Gawehn Deep learning in drug discovery Mol. Inf. 35 2016 3 14 16 P. Mamoshina Applications of deep learning in biomedicine Mol. Pharm. 13 2016 1445 1454 17 S. Ekins The next era: deep learning in pharmaceutical research Pharm. Res. 33 2016 2594 2603 18 I.I. Baskin A renaissance of neural networks in drug discovery Expert Opin. Drug Discov. 11 2016 785 795 19 W.S. McCulloch W. Pitts A logical calculus of the ideas immanent in nervous activity Bull. Math. Biol. 52 1943 115 133 20 S.E. Dreyfus The computational solution of optimal control problems with time lag IEEE Trans. Autom. Control 18 1973 383 385 21 N. Srivastava Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 15 2014 1929 1958 22 L. Wan Regularization of neural networks using DropConnect D. Sanjoy M. David Proceedings of the 30th International Conference on Machine Learning Vol. 28 2013 PMLR 1058 1066 23 V. Nair G.E. Hinton Rectified linear units improve restricted boltzmann machines Proceedings of the 27th International Conference on International Conference on Machine Learning Omnipress 2010 807 814 24 TensorFlow\u2122. Available at: https://www.tensorflow.org/ 25 Caffe. Available at: http://caffe.berkeleyvision.org/ 26 PYTORCH. Available at: http://pytorch.org/ 27 Keras. Available at: https://keras.io/ 28 Theano. Available at: http://deeplearning.net/software/theano/ 29 H. Lee Unsupervised learning of hierarchical representations with convolutional deep belief networks Commun. ACM 54 2011 95 103 30 C. Szegedy Going deeper with convolutions CVPR IEEE Computer Society 2015 pp. 1\u20139 31 S. Fern\u00e1ndez An application of recurrent neural networks to discriminative keyword spotting Proceedings of the 17th International Conference on Artificial Neural Networks Springer-Verlag 2007 220 229 32 S. Hochreiter J. Schmidhuber Long short-term memory Neural Comput. 9 1997 1735 1780 33 Y. Bengio Learning deep architectures for AI Found. Trends Mach. Learn. 2 2009 1 127 34 D.P. Kingma M. Welling Auto-encoding variational bayes ArXiv 2013 1312.6114 35 J. Ma Deep neural nets as a method for quantitative structure\u2013activity relationships J. Chem. Inf. Model. 55 2015 263 274 36 A. Mayr DeepTox: toxicity prediction using deep learning Front. Environ. Sci. 2016 10.3389/fenvs.2015.00080 37 G.E. Dahl Multi-task neural networks for QSAR predictions ArXiv 2014 arXiv:1406.1231 38 B. Ramsundar Is multitask deep learning practical for pharma? J. Chem. Inf. Model. 57 2017 2068 2076 39 A. Koutsoukas Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data J. Cheminformatics 9 2017 42 40 A. Gaulton ChEMBL: a large-scale bioactivity database for drug discovery Nucleic Acids Res. 40 2012 D1100 1107 41 E.B. Lenselink Beyond the hype: deep neural networks outperform established methods using a ChEMBL bioactivity benchmark set J. Cheminformatics 9 2017 45 42 G. Subramanian Computational modeling of beta-secretase 1 (BACE-1) inhibitors using ligand based approaches J. Chem. Inf. Model. 56 2016 1936 1949 43 A. Aliper Deep learning applications for predicting pharmacological properties of drugs and drug repurposing using transcriptomic data Mol. Pharm. 13 2016 2524 2530 44 NIH LINCS program. Available at: http://www.lincsproject.org/LINCS/ 45 C. Merkwirth T. Lengauer Automatic generation of complementary descriptors with molecular graph networks J. Chem. Inf. Model. 45 2005 1159 1168 46 A. Lusci Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules J. Chem. Inf. Model. 53 2013 1563 1575 47 Y. Xu Deep learning for drug-induced liver injury J. Chem. Inf. Model. 55 2015 2085 2093 48 H.L. Morgan The generation of a unique machine description for chemical structures\u2014a technique developed at Chemical Abstracts Service J. Chem. Doc. 5 1965 107 113 49 D. Duvenaud Convolutional networks on graphs for learning molecular fingerprints Proceedings of the 28th International Conference on Neural Information Processing Systems MIT Press 2015 2224 2232 50 S. Kearnes Molecular graph convolutions: moving beyond fingerprints J. Comput. Aided Mol. Des. 30 2016 595 608 51 Y. Xu Deep learning based regression and multiclass models for acute oral toxicity prediction with automatic chemical feature extraction J. Chem. Inf. Model. 57 2017 2672 2685 52 J. Li Learning graph-level representation for drug discovery ArXiv 2017 arXiv:1709.03741 53 C.W. Coley Convolutional embedding of attributed molecular graphs for physical property prediction J. Chem. Inf. Model. 57 2017 1757 1772 54 J. Gilmer Neural message passing for quantum chemistry ArXiv 2017 arXiv:1704.01212 55 Y. Li Gated graph sequence neural networks ArXiv 2015 arXiv:1511.05493 56 T.N. Kipf M. Welling Semi-supervised classification with graph convolutional networks ArXiv 2016 arXiv:1609.02907 57 E.J. Bjerrum SMILES enumeration as data augmentation for neural network modeling of molecules ArXiv 2017 arXiv:1703.07076 58 G.B. Goh Chemception: a deep neural network with minimal chemistry knowledge matches the performance of expert-developed QSAR/QSPR models ArXiv 2017 arXiv:1706.06689 59 G.B. Goh How much chemistry does a deep neural network need to know to make accurate predictions? ArXiv 2017 arXiv:1710.02238 60 R. G\u00f3mez-Bombarelli Automatic chemical design using a data-driven continuous representation of molecules ArXiv 2016 arXiv:1610.02415 61 A. Kadurin druGAN: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico Mol. Pharm. 14 2017 3098 3104 62 I.J. Goodfellow Generative adversarial networks ArXiv 2014 arXiv: 1406.2661 63 T. Blaschke Application of generative autoencoder in de novo molecular design Mol. Inf. 2017 10.1002/minf.201700123 64 M.H.S. Segler Generating focussed molecule libraries for drug discovery with recurrent neural networks ACS Cent. Sci. 4 2018 120 131 65 W. Yuan Chemical space mimicry for drug discovery J. Chem. Inf. Model. 57 2017 875 882 66 N. Jaques Sequence Tutor: conservative fine-tuning of sequence generation models with KL-control ArXiv 2016 arXiv:1611.02796 67 A. Leo Partition coefficients and their uses Chem. Rev. 71 1971 525 616 68 G.R. Bickerton Quantifying the chemical beauty of drugs Nat. Chem. 4 2012 90 98 69 M. Olivecrona Molecular de-novo design through deep reinforcement learning J. Cheminformatics 9 2017 48 70 M. Benhenda ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity? ArXiv 2017 arXiv:1708.08227 71 L. Metz Unrolled generative adversarial networks ArXiv 2016 arXiv:1611.02163 72 T. Unterthiner Coulomb GANs: provably optimal Nash equilibria via potential fields ArXiv 2017 arXiv:1708.08819 73 E.J. Corey W.T. Wipke Computer-assisted design of complex organic syntheses Science 166 1969 178 192 74 C.W. Coley Prediction of organic reaction outcomes using machine learning ACS Cent. Sci. 3 2017 434 443 75 W. Jin Predicting organic reaction outcomes with Weisfeiler\u2013Lehman network ArXiv 2017 arXiv:1709.04555 76 M.H.S. Segler M.P. Waller Neural-symbolic machine learning for retrosynthesis and reaction prediction Chemistry 23 2017 5966 5971 77 M.H.S. Segler Learning to plan chemical syntheses ArXiv 2017 arXiv:1708.04202 78 B. Liu Retrosynthetic reaction prediction using neural sequence-to-sequence models ACS Central Science 3 2017 1103 1113 79 N.S. Pagadala Software for molecular docking: a review Biophys. Rev. 9 2017 91 102 80 M. Ragoza Protein\u2013ligand scoring with convolutional neural networks J. Chem. Inf. Model. 57 2017 942 957 81 O. Trott A.J. Olson AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading J. Comput. Chem. 31 2010 455 461 82 J.B. Dunbar Jr CSAR benchmark exercise of 2010: selection of the protein\u2013ligand complexes J. Chem. Inf. Model. 51 2011 2036 2046 83 J. Gomes Atomic convolutional networks for predicting protein\u2013ligand binding affinity ArXiv 2017 arXiv:1703.10603 84 I. Wallach AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery ArXiv 2015 arXiv:1510.02855 85 J.C. Pereira Boosting docking-based virtual screening with deep learning J. Chem. Inf. Model. 56 2016 2495 2506 86 O. Russakovsky ImageNet large scale visual recognition challenge Int. J. Comput. Vision 115 2015 211 252 87 M.M. Mysinger Directory of Useful Decoys, Enhanced (DUD-E): better ligands and decoys for better benchmarking J. Med. Chem. 55 2012 6582 6594 88 J. Sun ExCAPE-DB: an integrated large scale dataset facilitating Big Data analysis in chemogenomics J. Cheminformatics 9 2017 17 89 G.A. Miller WordNet: a lexical database for English Commun. ACM 38 1995 39 41 90 F.-F. Li ImageNet: constructing a large-scale image database J. Vision 9 2009 1037 91 Z. Wu MoleculeNet: a benchmark for molecular machine learning ArXiv 2017 arXiv:1703.00564 92 DeepChem package. Available at: https://deepchem.io/ 93 C. Angermueller Deep learning for computational biology Mol. Syst. Biol. 12 2016 878 94 O.Z. Kraus Classifying and segmenting microscopy images using convolutional multiple instance learning ArXiv 2015 arXiv:1511.05286 95 O. Ronneberger U-net: convolutional networks for biomedical image segmentation International Conference on Medical Image Computing and Computer-Assisted Intervention Springer 2015 234 241 96 F. Ning Toward automatic phenotyping of developing embryos from videos IEEE Trans. Image Process. 14 2005 1360 1371 97 A. Ferrari Bacterial colony counting by convolutional neural networks. In Engineering in Medicine and Biology Society (EMBC) 2015 37th Annual International Conference of the IEEE IEEE 2015 7458 7461 98 D.C. Cire\u015fan Mitosis detection in breast cancer histology images with deep neural networks International Conference on Medical Image Computing and Computer-assisted Intervention Springer 2013 411 418 99 K. Sirinukunwattana Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images IEEE Trans. Med. Imaging 35 2016 1196 1206 100 Y. Xu Deep learning of feature representation with multiple instance learning for medical image analysis Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference IEEE 2014 1626 1630 101 R. Turkki Antibody-supervised deep learning for quantification of tumor-infiltrating immune cells in hematoxylin and eosin stained breast cancer samples J. Pathol. Inf. 7 2016 38 102 M.E. Vandenberghe Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer Sci. Rep. 2017 10.1038/srep45938 103 G. Litjens Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis Sci. Rep. 6 2016 26286 104 Y. Bar Deep learning with non-medical training used for chest pathology identification Proc. SPIE Vol. 9414 2015 pp. 94140V 105 J.-Z. Cheng Computer-aided diagnosis with deep learning architecture: applications to breast lesions in US images and pulmonary nodules in CT scans Sci. Rep. 6 2016 24454 106 K.H. Cha Urinary bladder segmentation in CT urography using deep-learning convolutional neural network and level sets Med. Phys. 43 2016 1882 1896 107 M. Avendi A combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI Med. Image Anal. 30 2016 108 119 108 R. Li Deep learning based imaging data completion for improved brain disease diagnosis International Conference on Medical Image Computing and Computer-Assisted Intervention Springer 2014 305 312 109 S. Liu High-level feature based PET image retrieval with deep learning architecture J. Nucl. Med. 55 Suppl. 1 2014 2028 110 J. Behrmann Deep learning for tumor classification in imaging mass spectrometry ArXiv 2017 arXiv:1705.01015 111 P. Inglese Deep learning and 3D-DESI imaging reveal the hidden metabolic heterogeneity of cancer Chem. Sci. 8 2017 3500 3511 112 C.L. Chen Deep learning in label-free cell classification Sci. Rep. 6 2016 21471 113 W. Zhang Deep model based transfer and multi-task learning for biological image analysis IEEE Trans. Big Data 2016 10.1109/TBDATA.2016.2573280 114 O. Vinyals Matching networks for one shot learning ArXiv 2016 arXiv:1606.04080 115 H. Altae-Tran Low data drug discovery with one-shot learning ACS Cent. Sci. 3 2017 283 293 116 A. Graves Hybrid computing using a neural network with dynamic external memory Nature 538 2016 471 476 117 T. Chen C. Guestrin XGBoost: a scalable tree boosting system ArXiv 1603 2016 arXiv:1603.02754 118 A Kaggle Master Explains Gradient Boosting. Available at: http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/", "scopus-id": "85044626626", "pubmed-id": "29366762", "coredata": {"eid": "1-s2.0-S1359644617303598", "dc:description": "Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.", "openArchiveArticle": "false", "prism:coverDate": "2018-06-30", "openaccessUserLicense": "http://creativecommons.org/licenses/by/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1359644617303598", "dc:creator": [{"@_fa": "true", "$": "Chen, Hongming"}, {"@_fa": "true", "$": "Engkvist, Ola"}, {"@_fa": "true", "$": "Wang, Yinhai"}, {"@_fa": "true", "$": "Olivecrona, Marcus"}, {"@_fa": "true", "$": "Blaschke, Thomas"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1359644617303598"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1359644617303598"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1359-6446(17)30359-8", "prism:volume": "23", "prism:publisher": "The Authors. Published by Elsevier Ltd.", "dc:title": "The rise of deep learning in drug discovery", "prism:copyright": "\u00a9 2018 The Authors. Published by Elsevier Ltd.", "openaccess": "1", "prism:issn": "13596446", "prism:issueIdentifier": "6", "openaccessArticle": "true", "prism:publicationName": "Drug Discovery Today", "prism:number": "6", "openaccessSponsorType": "Author", "prism:pageRange": "1241-1250", "prism:endingPage": "1250", "pubType": "Review Informatics", "prism:coverDisplayDate": "June 2018", "prism:doi": "10.1016/j.drudis.2018.01.039", "prism:startingPage": "1241", "dc:identifier": "doi:10.1016/j.drudis.2018.01.039", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "56", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2742", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "133", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6521", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "143", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5084", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2629", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "133", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8957", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "213", "@width": "836", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24848", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "1032", "@width": "836", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "111147", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "545", "@width": "836", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "50598", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "175", "@width": "836", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "22641", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "507", "@width": "836", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "61437", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "943", "@width": "3703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "196452", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "4570", "@width": "3703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "1102351", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2416", "@width": "3703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "398993", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "775", "@width": "3703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "178643", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2247", "@width": "3703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "464640", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "50", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1359644617303598-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "744", "@ref": "si1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85044626626"}}