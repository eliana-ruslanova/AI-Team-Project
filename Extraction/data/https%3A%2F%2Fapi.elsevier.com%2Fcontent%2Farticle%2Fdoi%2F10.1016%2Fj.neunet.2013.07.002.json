{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608013001871", "dc:identifier": "doi:10.1016/j.neunet.2013.07.002", "eid": "1-s2.0-S0893608013001871", "prism:doi": "10.1016/j.neunet.2013.07.002", "pii": "S0893-6080(13)00187-1", "dc:title": "Fixed-final-time optimal control of nonlinear systems with terminal constraints ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "48", "prism:startingPage": "61", "prism:endingPage": "71", "prism:pageRange": "61-71", "dc:format": "application/json", "prism:coverDate": "2013-12-31", "prism:coverDisplayDate": "December 2013", "prism:copyright": "Copyright \u00a9 2013 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Heydari, Ali"}, {"@_fa": "true", "$": "Balakrishnan, S.N."}], "dc:description": "\n               Abstract\n               \n                  A model-based reinforcement learning algorithm is developed in this paper for fixed-final-time optimal control of nonlinear systems with soft and hard terminal constraints. Convergence of the algorithm, for linear in the weights neural networks, is proved through a novel idea by showing that the training algorithm is a contraction mapping. Once trained, the developed neurocontroller is capable of solving this class of optimal control problems for different initial conditions, different final times, and different terminal constraint surfaces providing some mild conditions hold. Three examples are provided and the numerical results demonstrate the versatility and the potential of the developed technique.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Fixed-final-time optimal control"}, {"@_fa": "true", "$": "Terminal state constraint"}, {"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Adaptive critics"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608013001871", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608013001871", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84882796772", "scopus-eid": "2-s2.0-84882796772", "pubmed-id": "23954546", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84882796772", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20130719", "$": "2013-07-19"}}}}}