{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608011002656", "dc:identifier": "doi:10.1016/j.neunet.2011.10.003", "eid": "1-s2.0-S0893608011002656", "prism:doi": "10.1016/j.neunet.2011.10.003", "pii": "S0893-6080(11)00265-6", "dc:title": "C-Mantec: A novel constructive neural network algorithm incorporating competition between neurons ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "26", "prism:startingPage": "130", "prism:endingPage": "140", "prism:pageRange": "130-140", "dc:format": "application/json", "prism:coverDate": "2012-02-29", "prism:coverDisplayDate": "February 2012", "prism:copyright": "Copyright \u00a9 2011 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Subirats, Jos\u00e9 L."}, {"@_fa": "true", "$": "Franco, Leonardo"}, {"@_fa": "true", "$": "Jerez, Jos\u00e9 M."}], "dc:description": "\n               Abstract\n               \n                  C-Mantec is a novel neural network constructive algorithm that combines competition between neurons with a stable modified perceptron learning rule. The neuron learning is governed by the thermal perceptron rule that ensures stability of the acquired knowledge while the architecture grows and while the neurons compete for new incoming information. Competition makes it possible that even after new units have been added to the network, existing neurons still can learn if the incoming information is similar to their stored knowledge, and this constitutes a major difference with existing constructing algorithms. The new algorithm is tested on two different sets of benchmark problems: a Boolean function set used in logic circuit design and a well studied set of real world problems. Both sets were used to analyze the size of the constructed architectures and the generalization ability obtained and to compare the results with those from other standard and well known classification algorithms. The problem of overfitting is also analyzed, and a new built-in method to avoid its effects is devised and successfully applied within an active learning paradigm that filter noisy examples. The results show that the new algorithm generates very compact neural architectures with state-of-the-art generalization capabilities.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Constructive neural network"}, {"@_fa": "true", "$": "Incremental learning"}, {"@_fa": "true", "$": "Active learning"}, {"@_fa": "true", "$": "Generalization"}, {"@_fa": "true", "$": "Feed-forward network"}, {"@_fa": "true", "$": "Overfitting"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608011002656", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608011002656", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84855953161", "scopus-eid": "2-s2.0-84855953161", "pubmed-id": "22075034", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84855953161", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20111018", "$": "2011-10-18"}}}}}