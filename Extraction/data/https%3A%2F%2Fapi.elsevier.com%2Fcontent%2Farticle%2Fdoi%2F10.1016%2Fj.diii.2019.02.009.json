{"scopus-eid": "2-s2.0-85063346345", "originalText": "serial JL 280852 291210 291703 291901 291926 31 Diagnostic and Interventional Imaging DIAGNOSTICINTERVENTIONALIMAGING 2019-03-27 2019-03-27 2019-04-04 2019-04-04 2019-04-04T11:55:42 1-s2.0-S2211568419300592 S2211-5684(19)30059-2 S2211568419300592 10.1016/j.diii.2019.02.009 S300 S300.1 FULL-TEXT 1-s2.0-S2211568419X00042 2020-04-01T01:12:25.935756Z 0 0 20190401 20190430 2019 2019-03-27T15:26:00.989623Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast orcid primabst pubtype ref 2211-5684 22115684 true 100 100 4 4 Volume 100, Issue 4 6 227 233 227 233 201904 April 2019 2019-04-01 2019-04-30 2019 Original articles Computer developments article fla \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. DIAGNOSISFOCALLIVERLESIONSULTRASOUNDUSINGDEEPLEARNING SCHMAUCH B Materials and methods Preprocessing Feature extraction Supervised attention mechanism Implementation Results Discussion Human and animal rights Informed consent and patient details Funding Disclosure of interest References REINER 2012 3 6 B GABA 2002 1249 1255 D MACDONALD 2003 102 117 W BRUNO 2015 1668 1676 M MUENZEL 2012 8 18 D SUZUKI 2010 509 514 C LECUN 2015 436 444 Y RUSSAKOVSKY 2015 211 252 O ZHU 2018 487 492 B YASAKA 2018 257 272 K HOSNY 2018 500 510 A PARK 2013 219 226 H CHOI 2018 H MACHINELEARNINGFORRAPIDASSESSMENTOUTCOMESULTRASOUNDSCREENINGSURVEILLANCEPROGRAMINPATIENTSRISKFORHEPATOCELLULARCARCINOMA LIU 2017 149 X ABAJIAN 2012 1543 1552 A LARSON 2018 313 322 D HOLZINGER 2017 A NEEDBUILDEXPLAINABLEAISYSTEMSFORMEDICALDOMAIN AUBE 2017 455 468 C SCHMAUCHX2019X227 SCHMAUCHX2019X227X233 SCHMAUCHX2019X227XB SCHMAUCHX2019X227X233XB Full 2020-04-01T00:01:35Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ 2020-04-04T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ 2020-04-04T00:00:00.000Z \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. This article is made available under the Elsevier license. item S2211-5684(19)30059-2 S2211568419300592 1-s2.0-S2211568419300592 10.1016/j.diii.2019.02.009 280852 2019-06-12T00:54:15.571525Z 2019-04-01 2019-04-30 1-s2.0-S2211568419300592-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/MAIN/application/pdf/16cfd5fb72e0cb51c7a252a7b16520e6/main.pdf main.pdf pdf true 1049414 MAIN 7 1-s2.0-S2211568419300592-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/PREVIEW/image/png/cca9175e9a5890ede1420e462aee4133/main_1.png main_1.png png 42876 849 656 IMAGE-WEB-PDF 1 1-s2.0-S2211568419300592-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr1/THUMBNAIL/image/gif/a7d98a76449c93fe611c7afa225b83f7/gr1.sml gr1 gr1.sml sml 12867 74 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300592-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr2/THUMBNAIL/image/gif/2cd7020f64e8bdb94c083b62658213e3/gr2.sml gr2 gr2.sml sml 17641 75 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300592-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr3/THUMBNAIL/image/gif/0f1018b859055230d1f1999aed7e1511/gr3.sml gr3 gr3.sml sml 9832 97 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300592-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr4/THUMBNAIL/image/gif/b0a00a10246fe2ca52d46bb5e8772c51/gr4.sml gr4 gr4.sml sml 14342 75 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300592-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr5/THUMBNAIL/image/gif/b2fb25fea5e6ab22ca55d0a82d33c7df/gr5.sml gr5 gr5.sml sml 9676 68 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300592-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr1/DOWNSAMPLED/image/jpeg/e29bc5e9c757ca3feafdab0fbd5a3363/gr1.jpg gr1 gr1.jpg jpg 37054 185 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300592-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr2/DOWNSAMPLED/image/jpeg/58240918261f636ca8b0b08cc0b4e495/gr2.jpg gr2 gr2.jpg jpg 49037 186 546 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300592-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr3/DOWNSAMPLED/image/jpeg/7b2dee79a745a8ece806b207cf9af1ee/gr3.jpg gr3 gr3.jpg jpg 33156 242 549 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300592-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr4/DOWNSAMPLED/image/jpeg/4f0d8aa33f84177d49ce18e4612b1d02/gr4.jpg gr4 gr4.jpg jpg 34226 186 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300592-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr5/DOWNSAMPLED/image/jpeg/330ddf97dc76e68b0bb6571eba22b1e0/gr5.jpg gr5 gr5.jpg jpg 50785 227 727 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300592-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr1/HIGHRES/image/jpeg/05a3e265471c078ca027051d5e36fe84/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 122551 492 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300592-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr2/HIGHRES/image/jpeg/1bf1cc4a3d089ca7c2591c88784d492d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 169767 494 1452 IMAGE-HIGH-RES 1-s2.0-S2211568419300592-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr3/HIGHRES/image/jpeg/6e9017199ac73799c6a4f649cab65a95/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 165086 1073 2431 IMAGE-HIGH-RES 1-s2.0-S2211568419300592-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr4/HIGHRES/image/jpeg/a1a5232194cf2e3d5d931c9d509fe2a6/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 107526 495 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300592-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/gr5/HIGHRES/image/jpeg/e99a80e5754b51d321d51be4d85a7a11/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 273990 1003 3219 IMAGE-HIGH-RES 1-s2.0-S2211568419300592-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/STRIPIN/image/gif/765bb7bc6f3fcce51f2ccd08f582e30e/si1.gif si1 si1.gif gif 403 31 89 ALTIMG 1-s2.0-S2211568419300592-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/STRIPIN/image/gif/8ab7fd9b7dbdb0a077c3bf9a01fafc46/si2.gif si2 si2.gif gif 584 42 121 ALTIMG 1-s2.0-S2211568419300592-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/STRIPIN/image/gif/579f6ce850d81f44dabc53b047e8d37f/si3.gif si3 si3.gif gif 568 42 100 ALTIMG 1-s2.0-S2211568419300592-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/STRIPIN/image/gif/fdf06592d1a51373f099a4ac423f3c13/si4.gif si4 si4.gif gif 342 30 47 ALTIMG 1-s2.0-S2211568419300592-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300592/STRIPIN/image/gif/f4e843cbaa62a5812856a83a13394ab6/si5.gif si5 si5.gif gif 1860 72 270 ALTIMG 1-s2.0-S2211568419300592-am.pdf am am.pdf pdf 335755 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:104DBGHNDF5/MAIN/application/pdf/f0f540a6371b0ccbca26b3dddbcfbada/am.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/egi:104DBGHNDF5/MAIN/application/pdf/f0f540a6371b0ccbca26b3dddbcfbada/am.pdf DIII 1168 S2211-5684(19)30059-2 10.1016/j.diii.2019.02.009 Soci show\u00e9t show\u00e9 fran\u00e7aises de radiologie Figure 1 Ultrasound images of the liver from the training set before (A) and after (B) intensity rescaling based on the observation that all abdominal tissue should share a common pixel intensity profile in all liver ultrasound images. The region used for computing the scaling factor is outlined in red on A. Figure 2 Ultrasound images of focal liver lesions from the training set. Bounding boxes annotations are superimposed on the original image for an angioma (a) and multiple metastasis (b). Those annotations were generated a radiologist using dedicated tool. Figure 3 Architecture of the neural network. Each image of size 240\u00d7345 is fed into the ResNet50 Neural Network which produces 2048 images of size 8\u00d711. They are then fed to the upper branch also called the \u201cattention block\u201d of the algorithm that learns to detect anomalies in the image. The latter are also fed to a second branch that averages features maps over the selected areas. Finally, the 2048 features are fitted to a logistic regression that outputs a score ranging from 0 to 1 for each category of breast lesion. This score can be interpreted as the probability of presence of such lesion in the image. Figure 4 Figures show two examples of attention maps generated by the model for a homogeneous liver without lesions (a) and one with a biliary cyst (b). In (a), the model indiscriminately covers a large part of the liver, whereas, in (b), the model focuses its attention on a restricted area containing the lesion. Figure 5 Diagrams show receiver operating characteristic (ROC) curves for focal liver lesion (FLL) detection (a), diagnosis of malignant FLL (b), and correct classification into one out five categories (c) obtained with one model on 122 images. Table 1 Number of samples provided by the organizers of the data challenge for each category. Table 1 Lesion Type Training set Test set* Homogeneous Liver 258 (70%) Unknown Angioma 17 (4.6%) Unknown Metastasis 48 (13%) Unknown HCC 6 (1.6%) Unknown Cyst 30 (8.2%) Unknown HNF 8 (2.2%) Unknown Total 367 (100%) 177 The test set was used by the data challenge organizers to evaluate the algorithms. The data challenge organizers did not indicate the lesion types present in the test set of liver ultrasound images. HCC indicates hepatocellular carcinoma. FNH indicates focal nodular hyperplasia Table 2. Results of the different models for the three classification tasks. Table 2 Dimensions Extractor \u03bb AUC lesion AUC mal. AUC type 240\u00d7345 ResNet 50 1 0.913 (0.027) 0.930 (0.040) 0.895 (0.060) 480\u00d7690 ResNet 50 1 0.906 (0.029) 0.910 (0.044) 0.910 (0.060) 240\u00d7345 DenseNet 121 0.25, 1 0.912 (0.021) 0.917 (0.044) 0.900 (0.062) \u2013 Ensemble \u2013 0.935 (0.022) 0.942 (0.044) 0.916 (0.058) Scores were computed as the mean of three shuffled three-fold cross-validation on the training set. Nine experiments in which 245/367 images (66.8%) of the training set were randomly used to train our algorithm and the scores were computed from the results obtained on the remaining 122/367 images (33.2%). AUC indicates area under the receiver operator characteristic curve. Data are presented as mean. Numbersin parentheses are standard deviation. In bold we highlighted the best scores for each task (detection of lesion/malignancy and lesion type). We see that the best score is achieved by the Ensemble model. Table 3 Area under the ROC curve (AUC) scores by lesion type. Table 3 Class AUC Angioma 0.898 (0.067) Metastasis 0.886 (0.052) HCC 0.931 (0.072) Cyst 0.954 (0.017) HNF 0.909 (0.084) Average 0.916 (0.058) AUC indicates area under the receiver operator characteristic curve. HCC indicates hepatocellular carcinoma. FNH indicates focal nodular hyperplasia. Data are presented as means. Numbers in parentheses are standard deviations. Scores were computed as the mean of three shuffled three-fold cross-validations on the training set. Nine experiments in which 245/367 images (66.8%) of the training set were randomly used to train our algorithm and the scores were computed from the results obtained on the remaining 122/367 images (33.2%). Original article Computer developments Diagnosis of focal liver lesions from ultrasound using deep learning B. Schmauch a P. Herent a P. Jehanno a \u204e paul.jehanno@owkin.com O. Dehaene b C. Saillard a C. Aub\u00e9 c A. Luciani d N. Lassau e f S. J\u00e9gou a a Owkin Inc, Research and Development Laboratory, 75, rue de Turbigo, 75003 Paris, France Owkin Inc, Research and Development Laboratory 75, rue de Turbigo Paris 75003 France b \u00c9cole Centrale d\u2019Electronique (ECE), 75015 Paris, France \u00c9cole Centrale d\u2019Electronique (ECE) Paris 75015 France c Radiology Department, CHU Angers, 49933 Angers, France Radiology Department, CHU Angers Angers 49933 France d Radiology Department, AP\u2013HP, H\u00f4pitaux Universitaires Henri-Mondor, 94010 Creteil, France Radiology Department, AP\u2013HP, H\u00f4pitaux Universitaires Henri-Mondor Creteil 94010 France e Radiology Department, Institut Gustave Roussy, 94805 Villejuif, France Radiology Department, Institut Gustave Roussy Villejuif 94805 France f IR4M, UMR8081CNRS, Universit\u00e9 Paris-Sud, Universit\u00e9 Paris-Saclay, 94805 Villejuif, France IR4M, UMR8081CNRS, Universit\u00e9 Paris-Sud, Universit\u00e9 Paris-Saclay Villejuif 94805 France \u204e Corresponding author. Abstract Purpose The purpose of this study was to create an algorithm that simultaneously detects and characterizes (benign vs. malignant) focal liver lesion (FLL) using deep learning. Materials and methods We trained our algorithm on a dataset proposed during a data challenge organized at the 2018 Journ\u00e9es Francophones de Radiologie. The dataset was composed of 367 two-dimensional ultrasound images from 367 individual livers, captured at various institutions. The algorithm was guided using an attention mechanism with annotations made by a radiologist. The algorithm was then tested on a new data set from 177 patients. Results The models reached mean ROC-AUC scores of 0.935 for FLL detection and 0.916 for FLL characterization over three shuffled three-fold cross-validations performed with the training data. On the new dataset of 177 patients, our models reached a weighted mean ROC-AUC scores of 0.891 for seven different tasks. Conclusion This study that uses a supervised-attention mechanism focused on FLL detection and characterization from liver ultrasound images. This method could prove to be highly relevant for medical imaging once validated on a larger independent cohort. Keywords Artificial intelligence Deep learning Focal liver lesions Ultrasound Radiology The number of imaging examinations in modern medicine is steadily increasing, along with the complexity of interpretation and demands on providers for access to healthcare data, collaborative decision making, and quality accountability measures [1]. Radiologists are exposed to decision fatigue, due to known susceptibility factors including prolonged shifts, sleep deprivation, and performance of high-volume and high-complexity tasks [2,3]. Decision fatigue can lead to medical errors, with missed, incorrect, or delayed diagnoses estimated up to 10\u201315% in radiology [4]. The two most frequent causes of errors are missed findings (42% of recounted errors) and satisfaction of search [4]. Moreover, interpretations by radiologists are prone to high intra- and inter-individual variability [5,6]. Artificial intelligence shows promise in many applications in radiology. Deep learning is a subtype of machine learning, called \u201cdeep\u201d because of digital architecture that uses a large number of layers of artificial neurons, called neural networks [7]. In computer vision, most deep learning papers use convolutional neural networks (CNN), following its success in data challenges in outperforming standard computer vision algorithms [8]. The application of deep learning in radiology is potentially vast and could revolutionize each step of the medical-imaging pipeline [9\u201311]. Few studies have focused on liver ultrasound and deep learning. Park et al. focused on the classification of fibrosis [12], Choi et al. reported work on the classification of LI-RADS based on radiological reports of ultrasound monitoring for hepatocellular carcinoma [13], and Liu et al. showed promising results on the diagnosis of cirrhosis based on hepatic capsule morphology [14]. Human performance for characterizing focal liver lesion (FLL) is limited with an area under the ROC curve (AUC) between 0.72 and 0.74 [12]. The purpose of this study was to create an algorithm that simultaneously detects and characterizes (benign vs. malignant) FLL using deep learning. Materials and methods Preprocessing The dataset was provided during a public challenge during the 2018 Journ\u00e9es Francophones de Radiologie in Paris, France. Despite the standardization already performed by the challenge organizers, the data were highly heterogeneous in terms of size and luminosity. First, the images were cropped to maximally remove the black borders and standardize the aspect ratio. Furthermore, we performed a normalization based on the observation that the upper part of the considered images consisted mostly of abdominal tissues, which share a common pattern. Thus, the intensity peak of this region should vary little between images. However, the image acquisition conditions varied, with the intensity peak values ranging from 30 to 110 units. Consequently, we rescaled every image to shift this peak to a common value of 70 units (corresponding to the average value of the dataset). We selected the top 20% of pixels from the image, excluding those that were black, and estimated the peak intensity by taking the median value (m) of this selection. Then, we applied Eq. (1) to every pixel x ij (Fig. 1 ). (1) x i j \u2190 x i j \u00d7 70 m Finally, we resized every image to 240\u00d7345 or 480\u00d7690. This choice had little impact on the performance of the models. In the description of the model below, we considered the first case. Feature extraction To extract features from images, we used a 50-layer residual neural network (ResNet50), pretrained on the ImageNet dataset, from which we removed the last two layers [15]. This network was designed for color images. Thus, we had to copy each grayscale image three times in order to simulate red, green and blue channels. For an input image of size 3\u00d7240\u00d7345, the network produced a feature map with a dimension of 2048\u00d78\u00d711. A first simple approach consisted of averaging this representation over the spatial dimensions using Eq. (2). (2) x k = 1 8 \u00d7 11 \u2211 i , j x k i j This gave a feature vector of 2048 for each image, which was fed to a unique densely connected layer with seven neurons, one for each classification task (detection and characterization). The main drawback of this approach was that it gave the same importance to each pixel of the image, including regions of little interest such as the tissues surrounding liver. Supervised attention mechanism One of the main difficulties of this task is that liver lesions vary widely in appearances and size. We facilitated learning by decomposing classification into two steps: \u2022 detection of abnormalities on the ultrasound image; \u2022 classification of these lesions. These two steps were simultaneously performed by two branches of the same model. For the first, we created and used additional labels for localization. These labels consisted of bounding boxes surrounding the lesions. These annotations did not require precise characterization. Thus, they were rapidly performed by a 5th-year resident in radiology (P.H.) (Fig. 2 ). For each image, we generated a binary mask of the same size, indicating the presence or absence of lesions. We reduced the size of this mask to match the output dimensions of the ResNet (i.e., 8\u00d711 pixels for an input image with spatial dimensions of 240\u00d7345). The localization module was a single 1\u00d71 convolution, applied to the output of the ResNet. This transformed the 2048\u00d78\u00d711 representation into a single image with the dimensions 8\u00d711, to which we applied a sigmoid function to generate a prediction between 0 and 1. This module was trained to reproduce the binary mask generated from the annotations. Then, this local prediction was used to guide the main module, responsible for determining the presence of FLL in the image level and their characterization. More precisely, we used the local prediction to compute a weighted average of the final feature map, shown below in Eq. (3), in which p i,j is the local prediction for pixel (i, j). If the localization module predicted a uniform probability of a FLL over the entire image, the formula was equivalent to the spatial average of the simple model, whereas, when the module predicted the presence of a lesion in a single pixel with high confidence, only the feature vector extracted from this pixel was used for the final prediction. (3) x k = \u2211 i , j p i , j x k i j \u2211 i , j p i , j The final prediction was performed by a densely connected layer with seven neurons, one for each prediction: FLL detection, malignancy, angioma, hepatocellular carcinoma, focal nodular hyperplasia, cyst, or metastasis. The architecture of the model is shown in Fig. 3 . Furthermore, this attention mechanism allowed interpretation of the model's predictions. To do so, we resized the 8\u00d711 attention map p i j \u2211 i , j p i , j , and to the original image dimensions (i.e., 240\u00d7345). We superposed this map over the image to see the areas considered by the model to make its decision (Fig. 4 ). In particular, this allowed us to identify reasons why the model may have failed. Implementation Given the relatively small number of images available for training, we performed data augmentation to avoid overfitting. We applied random transformations, such as cropping, modifications of the aspect ratio, small translations and rotations. Counter-intuitively, applying random horizontal flips to images during training slightly improved performance, even though images generated in this manner are not realistic, as the liver is not a symmetric organ. This may be due to the fact that our model focuses on parts of the image, and the mirror image of a lesion is still a plausible lesion. We also applied mixup with a coefficient of 0.2 [16]. Our models were simultaneously trained on the three tasks evaluated in this challenge (lesion detection, malignancy and lesion classification). This multitask setting contributed to limiting overfitting. However, these tasks were not necessarily learned at the same pace. Thus, we saved three copies of the weights, chosen depending on the performance of the model on a validation set during cross-validation. For example, when the model reached its best AUC for lesion detection, we saved the first copy of its weights which was used only for this task. We used stochastic gradient descent with Nesterov momentum to train the models. The results were highly variable due to the small amount of data. We thus performed three-fold cross-validation, which we repeated for three different splits of the data, to account for this variability. We repeated nine experiences during which we selected randomly 245 images (out of the 367 images of the training set) to train our neural network, and estimated its performance by computing an AUC over the 122 images left. We then computed the mean scores over those nine different experiences to evaluate our model before executing it on the test set provided by the organizers of the challenge. Results The details of the dataset are provided in Table 1 . We performed experiments with several variations of the model, making the following changes: \u2022 replacement of the ResNet used for feature extraction with a 121-layer DenseNet [17]; \u2022 modification of the attention focus: this was achieved by applying a factor \u03bb to the local prediction before applying the sigmoid function: the smaller this factor, the more diffuse the attention, whereas larger values focused the attention more on a single pixel. In particular, making \u03bb =0 amounts to suppressing attention, whereas making \u03bb =1 provides no modification to the original formula. It is also possible to simultaneously use different scales; \u2022 use of higher resolution images: 480\u00d7690 instead of 240\u00d7345. These models demonstrated similar performance, although calculating the average of their predictions improved ROC-AUC scores during cross-validation. The average ROC-AUC scores achieved by our three best architectures for the repeated cross-validation (i.e., three times three-fold) as well as the result of the ensemble (obtained by averaging predictions of the three models) are shown in Table 2 . The AUC for FLL detection was computed over the entire dataset, whereas the AUCs for diagnosing malignancy and lesion characterization were evaluated on the subset of images in which a lesion was actually present (109/367 [29.7%] images in the training data). The same ensemble achieved a weighted AUC of 0.891 on the challenge test set, using the metric defined in Eq. (1). Detailed scores, calculated using Eq. (4) below, are shown in Table 3 and corresponding ROC curves are provided in Fig. 5 . (4) S c o r e = 0.5 \u00d7 A U C l e s i o n + 0.3 \u00d7 A U C b e n i g n m a l i g n a n t + 0.2 \u00d7 \u2211 l e s i o n t y p e s A U C l e s i o n t y p e The attention mechanism of our model allowed interpretation of the model's predictions using heat maps (Fig. 4), which helped us identifying false-positive findings. For example, on a subset of ultrasound images, the algorithm misclassified blood vessels as cysts. Discussion This study is the first to assess the performance of automatic detection of FLL from ultrasound images, and yielded promising results given the relatively small amount of data. The training dataset consisted of only 367 images, with some lesion types poorly represented such as HCC. Despite these limitations, cross-validation yielded good performance. It is likely that the use of larger databases would further increase the accuracy of the model. The supervised attention model was beneficial for two reasons: first, providing better labels in the image, such as where the model focused its attention for prediction, improved interpretability of the results. Indeed, the heatmaps made it possible to better understand how the model performed and why misclassifications occurred. For example, a common mistake was confusion between a blood vessel (portal vein or hepatic vein) and a cyst, which suggests areas for improvement in this area by teaching the model to detect normal physiological structures to avoid confusion with potential lesions. Second, annotations by the radiologist increased the performance of the models with a 0.05 increase in AUC compared to the simple approach. Annotating datasets is a time-consuming task using classical research tools not amenable to a radiological workflow. One of the challenges in deep-learning is to build powerful tools for radiologists, data scientists and patients. Abajian et al. have already identified this key point [18]. In their study, they underlined that annotations made by radiologists such as measurements are present in the DCOM files but are currently insufficiently used [18]. Here, we developed a tool that enabled rapid labeling for better performance, creating bounding box instead of well-defined segmentation, which permitted annotation of the entire dataset in approximately half an hour. The use of heatmaps was beneficial in assessing clinical relevance of the algorithm prediction. In other studies, this method has helped to provide more confident predictions for detecting pneumonia on chest X-rays by revealing the source of infection in images, and predicting bone age, showing growth-plate cartilage to be relevant for prediction [19,20]. More generally, heatmaps are among the methods used in the research field of algorithm interpretability and are particularly relevant for computer vision. In other words, this method allows one to \u201csee what the algorithm sees\u201d and could be implemented in the radiological workflow [21]. A limitation of the dataset used in this study was that the labels \u201chepatocellular carcinoma\u201d or \u201cmetastasis\u201d do not make much sense in clinical practice, as it is difficult to characterize such FLL with ultrasound only [22]. In general, liver ultrasound is a first line examination that needs further examinations for further characterization. However, characterization of some benign FLL, such as cavernous hemangioma or biliary cyst is possible with ultrasound when these FLL display typical features. In conclusion, the validation of our algorithm on an independent dataset is an essential step in demonstrating the generalizability of the model, but the promising results appear to corroborate the relevance of applying deep-learning models to ultrasound images. Human and animal rights The authors declare that the work described has been carried out in accordance with the Declaration of Helsinki of the World Medical Association revised in 2013 for experiments involving humans as well as in accordance with the EU Directive 2010/63/EU for animal experiments. Informed consent and patient details The authors declare that this report does not contain any personal information that could lead to the identification of the patient(s). The authors declare that they obtained a written informed consent from the patients and/or volunteers included in the article. The authors also confirm that the personal details of the patients and/or volunteers have been removed. Funding This work did not receive any grant from funding agencies in the public, commercial, or not-for-profit sectors. Disclosure of interest Paul Herent is a part-time consultant at Owkin. Benoit Schmauch, Simon J\u00e9gou, Paul Jehanno and Charlie Saillard are employees at Owkin. Olivier Dehaene was a data scientist intern at Owkin at the time the Data Challenge of JFR 2018 occurred and is now a full-time employee at Owkin. C. Aub\u00e9, A. Luciani, N. Lassau declare that they have no competing interest References [1] B.I. Reiner E. Krupinski The insidious problem of fatigue in medical imaging practice J Digit Imaging 25 2012 3 6 [2] D.M. Gaba S.K. Howard Fatigue among clinicians and the safety of patients N Engl J Med 347 2002 1249 1255 [3] W. MacDonald The impact of job demands and workload on stress and fatigue Aust Psychol 38 2003 102 117 [4] M.A. Bruno E.A. Walker H.H. Abujudeh Understanding and confronting our mistakes: the epidemiology of error in radiology and strategies for error reduction Radiographics 35 2015 1668 1676 [5] D. Muenzel H.P. Engels M. Bruegel V. Kehl E.J. Rummeny S. Metz Intra- and inter-observer variability in measurement of target lesions: implication on response evaluation according to RECIST 1.1 Radiol Oncol 46 2012 8 18 [6] C. Suzuki M.R. Torkzad H. Jacobsson G. Astrom A. Sundin T. Hatschek Interobserver and intraobserver variability in the response evaluation of cancer therapy according to RECIST and WHO-criteria Acta Oncol 49 2010 509 514 [7] Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 2015 436 444 [8] O. Russakovsky J. Deng H. Su J. Krause S. Satheesh S. Ma ImageNet large scale visual Recognition Challenge Int J Comput Sci 115 2015 211 252 [9] B. Zhu J.Z. Liu S.F. Cauley B.R. Rosen M.S. Rosen Image reconstruction by domain-transform manifold learning Nature 555 2018 487 492 [10] K. Yasaka H. Akai A. Kunimatsu S. Kiryu O. Abe Deep learning with convolutional neural network in radiology Jpn J Radiol 36 2018 257 272 [11] A. Hosny C. Parmar J. Quackenbush L.H. Schwartz H. Aerts Artificial intelligence in radiology Nat Rev Cancer 18 2018 500 510 [12] H. Park J.Y. Park D.Y. Kim S.H. Ahn C.Y. Chon K.H. Han Characterization of focal liver masses using acoustic radiation force impulse elastography World J Gastroenterol 19 2013 219 226 [13] H. Choi I. Banerjee H. Sagreiya A. Kamaya D. Rubin T. Desser Machine learning for rapid assessment of outcomes of an ultrasound screening and surveillance program in patients at risk for hepatocellular carcinoma Presented at the 2018 Scientific Assembly and Annual Meeting of Radiological Society of North America; Chicago, IL 2018 [14] X. Liu J.L. Song S.H. Wang J.W. Zhao Y.Q. Chen Learning to diagnose cirrhosis with liver capsule guided ultrasound image classification Sensors 17 2017 149 [15] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit abs/1512.03385, 2016. http://arxiv.org/abs/1512.03385. [16] Zhang H, Cisse M, Dauphin YN, Lopez-Paz D. Mixup: beyond empirical risk minimization. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit abs/1710.09412, 2017. http://arxiv.org/abs/1710.09412. [17] Huang G, Liu Z, van der Maaten L, Weinberger K. Densely Connected Convolutional Networks. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit abs/1608.06993, 2016. http://arxiv.org/abs/1608.06993. [18] A. Abajian M. Levy D. Rubin Informatics in radiology improving clinical work flow through an AIM Database: a sample web-based lesion tracking application Radiographics 32 2012 1543 1552 [19] Rajpurkar P, Irvin J, Zhu K, Yang B, Mehta H, Duan T, et al. CheXNet: radiologist-level pneumonia detection on chest X-rays with deep learning. ArXiv Preprint, 2017, ArXiv :1711.05225. [20] D.B. Larson M.C. Chen M.P. Lungren S.S. Halabi N.V. Stence C.P. Langlotz Performance of a deep-learning neural network model in assessing skeletal maturity on pediatric hand radiographs Radiology 287 2018 313 322 [21] A. Holzinger C. Biemann C. Pattichis B.D. Kell What do we need to build explainable AI systems for the medical domain? 2017 [ArXiv Preprint ArXiv :1712.09923] [22] C. Aub\u00e9 P. Bazeries J. Lebigot V. Cartier J. Boursier Liver fibrosis, cirrhosis, and cirrhosis-related nodules: imaging diagnosis and surveillance Diagn Interv Imaging 98 2017 455 468", "scopus-id": "85063346345", "pubmed-id": "30926443", "coredata": {"eid": "1-s2.0-S2211568419300592", "dc:description": "Abstract Purpose The purpose of this study was to create an algorithm that simultaneously detects and characterizes (benign vs. malignant) focal liver lesion (FLL) using deep learning. Materials and methods We trained our algorithm on a dataset proposed during a data challenge organized at the 2018 Journ\u00e9es Francophones de Radiologie. The dataset was composed of 367 two-dimensional ultrasound images from 367 individual livers, captured at various institutions. The algorithm was guided using an attention mechanism with annotations made by a radiologist. The algorithm was then tested on a new data set from 177 patients. Results The models reached mean ROC-AUC scores of 0.935 for FLL detection and 0.916 for FLL characterization over three shuffled three-fold cross-validations performed with the training data. On the new dataset of 177 patients, our models reached a weighted mean ROC-AUC scores of 0.891 for seven different tasks. Conclusion This study that uses a supervised-attention mechanism focused on FLL detection and characterization from liver ultrasound images. This method could prove to be highly relevant for medical imaging once validated on a larger independent cohort.", "openArchiveArticle": "true", "prism:coverDate": "2019-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S2211568419300592", "dc:creator": [{"@_fa": "true", "$": "Schmauch, B."}, {"@_fa": "true", "$": "Herent, P."}, {"@_fa": "true", "$": "Jehanno, P."}, {"@_fa": "true", "$": "Dehaene, O."}, {"@_fa": "true", "$": "Saillard, C."}, {"@_fa": "true", "$": "Aub\u00e9, C."}, {"@_fa": "true", "$": "Luciani, A."}, {"@_fa": "true", "$": "Lassau, N."}, {"@_fa": "true", "$": "J\u00e9gou, S."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S2211568419300592"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S2211568419300592"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S2211-5684(19)30059-2", "prism:volume": "100", "prism:publisher": "Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "dc:title": "Diagnosis of focal liver lesions from ultrasound using deep learning", "prism:copyright": "\u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "openaccess": "1", "prism:issn": "22115684", "prism:issueIdentifier": "4", "dcterms:subject": [{"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Deep learning"}, {"@_fa": "true", "$": "Focal liver lesions"}, {"@_fa": "true", "$": "Ultrasound"}, {"@_fa": "true", "$": "Radiology"}], "openaccessArticle": "true", "prism:publicationName": "Diagnostic and Interventional Imaging", "prism:number": "4", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "227-233", "prism:endingPage": "233", "pubType": "Original article Computer developments", "prism:coverDisplayDate": "April 2019", "prism:doi": "10.1016/j.diii.2019.02.009", "prism:startingPage": "227", "dc:identifier": "doi:10.1016/j.diii.2019.02.009", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "74", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "12867", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "75", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "17641", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "97", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9832", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "75", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14342", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "68", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9676", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "185", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "37054", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "186", "@width": "546", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "49037", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "242", "@width": "549", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33156", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "186", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "34226", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "227", "@width": "727", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "50785", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "492", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "122551", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "494", "@width": "1452", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "169767", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1073", "@width": "2431", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "165086", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "495", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "107526", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1003", "@width": "3219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "273990", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "31", "@width": "89", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "403", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "121", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "584", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "100", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "568", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "47", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "342", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "72", "@width": "270", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1860", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300592-am.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "AAM-PDF", "@size": "335755", "@ref": "am", "@mimetype": "application/pdf"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85063346345"}}