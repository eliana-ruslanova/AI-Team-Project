{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S089360800900046X", "dc:identifier": "doi:10.1016/j.neunet.2009.03.018", "eid": "1-s2.0-S089360800900046X", "prism:doi": "10.1016/j.neunet.2009.03.018", "pii": "S0893-6080(09)00046-X", "dc:title": "Adaptive learning via selectionism and Bayesianism, Part I: Connection between the two ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2009 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "22", "prism:issueIdentifier": "3", "prism:startingPage": "220", "prism:endingPage": "228", "prism:pageRange": "220-228", "prism:number": "3", "dc:format": "application/json", "prism:coverDate": "2009-04-30", "prism:coverDisplayDate": "April 2009", "prism:copyright": "Copyright \u00a9 2009 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "Goal-Directed Neural Systems", "dc:creator": [{"@_fa": "true", "$": "Zhang, Jun"}], "dc:description": "\n               Abstract\n               \n                  According to the selection-by-consequence characterization of operant learning, individual animals/species increase or decrease their future probability of action choices based on the consequence (i.e.,\u00a0reward or punishment) of the currently selected action (the so-called \u201cLaw of Effect\u201d). Under Bayesianism, on the other hand, evidence is evaluated based on likelihood functions so that action probability is modified from a priori to a posteriori according to the Bayes formula. Viewed as hypothesis testing, a selectionist framework attributes evidence exclusively to the selected, focal hypothesis, whereas a Bayesian framework distributes across all hypotheses the support from a piece of evidence. Here, an intimate connection between the two theoretical frameworks is revealed. Specifically, it is proven that when individuals modify their action choices based on the selectionist\u2019s Law of Effect, the learning population, on the ensemble level, evolves according to a Bayesian-like dynamics. The learning equation of the linear operator model [Bush, R. R., & Mosteller, F. (1955). Stochastic models for learning, New York: John Wiley and Sons], under ensemble averaging, yields the class of predictive reinforcement learning models (e.g.,\u00a0[Busemeyer, J. R., & Myung, I. J. (1992). An adaptive approach to human decision making: Learning theory, decision theory, and human performance. Journal of Experimental Psychology: General, 121, 177\u2013194; Montague, P. R., Dayan, P., & Sejnowski, T. J. (1996). A framework for mesencephalic dopamine systems based on predictive Hebbian learning. Journal of Neuroscience, 16, 1936\u20131947]).\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Operant conditioning"}, {"@_fa": "true", "$": "Law of effect"}, {"@_fa": "true", "$": "Linear operator model"}, {"@_fa": "true", "$": "Stochastic learning automata"}, {"@_fa": "true", "$": "Bayesian update"}, {"@_fa": "true", "$": "Gibbs\u2013Boltzman distribution"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S089360800900046X", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S089360800900046X", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "67349238173", "scopus-eid": "2-s2.0-67349238173", "pubmed-id": "19386469", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/67349238173", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20090405", "$": "2009-04-05"}}}}}