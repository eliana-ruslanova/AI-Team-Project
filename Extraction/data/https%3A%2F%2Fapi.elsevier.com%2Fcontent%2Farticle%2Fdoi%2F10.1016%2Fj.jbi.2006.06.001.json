{"scopus-eid": "2-s2.0-33847616032", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2006-06-07 2006-06-07 2010-10-07T10:23:12 1-s2.0-S1532046406000621 S1532-0464(06)00062-1 S1532046406000621 10.1016/j.jbi.2006.06.001 S300 S300.1 FULL-TEXT 1-s2.0-S1532046407X00352 2015-05-15T06:30:58.184067-04:00 0 0 20070401 20070430 2007 2006-06-07T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 40 40 2 2 Volume 40, Issue 2 10 150 159 150 159 200704 April 2007 2007-04-01 2007-04-30 2007 article fla Copyright \u00a9 2006 Elsevier Inc. All rights reserved. USINGMEDLINEAKNOWLEDGESOURCEFORDISAMBIGUATINGABBREVIATIONSACRONYMSINFULLTEXTBIOMEDICALJOURNALARTICLES YU H 1 Introduction 2 Related work 3 Methods 3.1 Text data 3.2 Preprocessing 3.3 Abbreviation-full form dictionary 3.4 Full-form normalization 3.5 Training and testing sets 3.6 Machine learning 3.7 Cross-validation 3.8 Evaluation metrics and gold standard 4 Results 5 Discussion Acknowledgment Appendix A References FEDERIUK 1999 292 296 C RIMER 1998 888 889 M YU 2002 262 272 H PUSTEJOVSKY 2001 371 375 J PARK 2001 396 407 J SCHWARTZ 2002 A YOSHIDA 2000 169 175 M FUKUDA 1998 707 718 K YU 2003 322 330 H LIU 2001 249 261 H HUMPHREYS 1993 170 177 B RZHETSKY 2004 43 53 A PORTER 1980 130 137 M SINGHAL 1996 21 29 A SIGIR1996 PIVOTEDDOCUMENTLENGTHNORMALIZATION MCCRAY 1998 353 360 A WILBUR 2000 918 922 W PLATT 1999 J FASTTRAININGSUPPORTVECTORMACHINESUSINGSEQUENTIALMINIMALOPTIMIZATION AYER 1954 641 647 M WITTEN 1999 I MANAGINGGIGABYTESCOMPRESSINGINDEXINGDOCUMENTSIMAGES SALTON 1989 G AUTOMATICTEXTPROCESSINGTRANSFORMATIONANALYSISRETRIEVALINFORMATIONBYCOMPUTER BAEZAYATES 1999 R FASTERAPPROXIMATESTRINGMATCHING YUX2007X150 YUX2007X150X159 YUX2007X150XH YUX2007X150X159XH 2013-08-22T00:00:26Z Full http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window ElsevierBranded item S1532-0464(06)00062-1 S1532046406000621 1-s2.0-S1532046406000621 10.1016/j.jbi.2006.06.001 272371 2010-11-08T20:10:04.306434-05:00 2007-04-01 2007-04-30 1-s2.0-S1532046406000621-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/MAIN/application/pdf/d618cda59c39854bbb20f121582327a1/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/MAIN/application/pdf/d618cda59c39854bbb20f121582327a1/main.pdf main.pdf pdf true 242464 MAIN 10 1-s2.0-S1532046406000621-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/PREVIEW/image/png/c97e7a299d7b7eae094d858717b83676/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/PREVIEW/image/png/c97e7a299d7b7eae094d858717b83676/main_1.png main_1.png png 65068 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046406000621-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si9 si9.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/012b373be5dd256d0d73c6abadd33656/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/012b373be5dd256d0d73c6abadd33656/si7.gif si8 si8.gif gif 420 20 51 ALTIMG 1-s2.0-S1532046406000621-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/012b373be5dd256d0d73c6abadd33656/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/012b373be5dd256d0d73c6abadd33656/si7.gif si7 si7.gif gif 420 20 51 ALTIMG 1-s2.0-S1532046406000621-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif si6 si6.gif gif 452 18 55 ALTIMG 1-s2.0-S1532046406000621-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/5138aba5afd9badae0b585745dfee22c/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/5138aba5afd9badae0b585745dfee22c/si5.gif si5 si5.gif gif 1306 40 217 ALTIMG 1-s2.0-S1532046406000621-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/8f0870439abefbebc616c1db4310946f/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/8f0870439abefbebc616c1db4310946f/si4.gif si4 si4.gif gif 1272 17 309 ALTIMG 1-s2.0-S1532046406000621-si37.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/27312e6298d15b835ae0300a2beff892/si37.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/27312e6298d15b835ae0300a2beff892/si37.gif si37 si37.gif gif 1405 36 263 ALTIMG 1-s2.0-S1532046406000621-si36.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/50ced37c08fcd0f8a4d5df6c40b8adaf/si36.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/50ced37c08fcd0f8a4d5df6c40b8adaf/si36.gif si36 si36.gif gif 445 23 45 ALTIMG 1-s2.0-S1532046406000621-si35.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/2b6fe225f11129727c1770e18acf3891/si35.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/2b6fe225f11129727c1770e18acf3891/si35.gif si35 si35.gif gif 1400 41 261 ALTIMG 1-s2.0-S1532046406000621-si34.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif si34 si34.gif gif 452 18 55 ALTIMG 1-s2.0-S1532046406000621-si33.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si33 si33.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si32.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si32 si32.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si31.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si31 si31.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si30.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif si30 si30.gif gif 452 18 55 ALTIMG 1-s2.0-S1532046406000621-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/53da2ba925288b8551d64934728b2cfe/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/53da2ba925288b8551d64934728b2cfe/si3.gif si3 si3.gif gif 1147 17 253 ALTIMG 1-s2.0-S1532046406000621-si29.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si29 si29.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si28.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si28 si28.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si27.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si27 si27.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si26.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si26 si26.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si25.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si25 si25.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si24.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si24 si24.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si23.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si23 si23.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si22.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si22 si22.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si21.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si21 si21.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si20 si20.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/72635cd10b8807e2eb9348dcfe9824e0/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/72635cd10b8807e2eb9348dcfe9824e0/si2.gif si2 si2.gif gif 1168 17 275 ALTIMG 1-s2.0-S1532046406000621-si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si19 si19.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si18 si18.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si17 si17.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si16 si16.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si15 si15.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si14 si14.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/558b7c05b73ad322e181b8ab573bc1e1/si31.gif si13 si13.gif gif 261 16 24 ALTIMG 1-s2.0-S1532046406000621-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/a81aa84a4cb6c5089b54be86d75aedca/si24.gif si12 si12.gif gif 610 18 99 ALTIMG 1-s2.0-S1532046406000621-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/ab2039bfad663e6e2b48741f62c38860/si25.gif si11 si11.gif gif 578 18 83 ALTIMG 1-s2.0-S1532046406000621-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/0d27d2012e56249cd6972616563dbc89/si6.gif si10 si10.gif gif 452 18 55 ALTIMG 1-s2.0-S1532046406000621-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/STRIPIN/image/gif/04d4810aa3c1d7dadf51f20d19ed2d4e/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/STRIPIN/image/gif/04d4810aa3c1d7dadf51f20d19ed2d4e/si1.gif si1 si1.gif gif 1108 17 279 ALTIMG 1-s2.0-S1532046406000621-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/gr1/DOWNSAMPLED/image/jpeg/f8eeeb89d81c489eea2e3333d507ed27/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/gr1/DOWNSAMPLED/image/jpeg/f8eeeb89d81c489eea2e3333d507ed27/gr1.jpg gr1 gr1.jpg jpg 24587 283 359 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000621-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/gr1/THUMBNAIL/image/gif/f853271148834820323b1796f7c7d553/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/gr1/THUMBNAIL/image/gif/f853271148834820323b1796f7c7d553/gr1.sml gr1 gr1.sml sml 2373 93 118 IMAGE-THUMBNAIL 1-s2.0-S1532046406000621-gr2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/gr2/DOWNSAMPLED/image/gif/5deb669a3b7529361402a7d18b64b482/gr2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/gr2/DOWNSAMPLED/image/gif/5deb669a3b7529361402a7d18b64b482/gr2.gif gr2 gr2.gif gif 6914 159 260 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000621-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000621/gr2/THUMBNAIL/image/gif/4db6f4deb409cd70b6301e3617eaa9c7/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000621/gr2/THUMBNAIL/image/gif/4db6f4deb409cd70b6301e3617eaa9c7/gr2.sml gr2 gr2.sml sml 2080 76 125 IMAGE-THUMBNAIL YJBIN 1293 S1532-0464(06)00062-1 10.1016/j.jbi.2006.06.001 Elsevier Inc. Fig. 1 Distribution (from eleven million MEDLINE records) of the numbers of abbreviations paired with different numbers of full forms. Fig. 2 A na\u00efve Bayes sample prediction for the abbreviation CTP. 1377599 is the abstract pmid; 0\u20131 represents predicted probabilities of the full forms (e.g., c termin peptid and citrat transport protein); note that the full forms are stemmed. Synonymous full forms are listed as one cluster, to which the probability is assigned. The full form(s) with the highest predicted probability is selected as the predicted full form(s) for an abbreviation. Table 1 Cross-validation results of full-form prediction in the MEDLINE abstracts Machine learning Coverage/precision Micro (std) Macro Na\u00efve Bayes W/O normalization 0.79 (0.18)/0.87 (0.07) 0.78/0.89 W. normalization 0.83 (0.16)/0.92 (0.04) 0.83/0.93 SVMs W/O normalization 0.78 (0.18)/0.91 (0.06) 0.77/0.92 W. normalization 0.83 (0.16)/0.94 (0.03) 0.82/0.95 The learning features included single words and contiguous word pairs without punctuation or stop words. We removed from the sentences the full forms that correspond to the abbreviations. \u201cW. normalization,\u201d full-form normalization; \u201cW/O normalization,\u201d without full-form normalization. Table 2 Prediction of the full forms of abbreviations in full-text articles Machine learning Coverage Precision JBC JCI JBC JCI Na\u00efve Bayes 0.79 0.84 0.86 0.90 SVMs 0.91 0.88 0.89 0.92 JBC, Journal of Biological Chemistry. JCI, Journal of Clinical Investigation. Using MEDLINE as a knowledge source for disambiguating abbreviations and acronyms in full-text biomedical journal articles Hong Yu a \u204e hongyu@uwm.edu Won Kim b Vasileios Hatzivassiloglou c W. John Wilbur b a Department of Health Sciences, University of Wisconsin-Milwaukee, Milwaukee, WI 53211, USA b National Center for Biotechnology Information, Bethesda, MD 20894, USA c Department of Computer Science, University of Texas at Dallas, TX 75083, USA \u204e Corresponding author. Present address: Department of Health Sciences, University of Wisconsin-Milwaukee, 2400 E. Hartford Avenue, PO Box 413, Milwaukee, WI 53211, USA. Fax: +1 414 229 2206. Abstract Biomedical abbreviations and acronyms are widely used in biomedical literature. Since many of them represent important content in biomedical literature, information retrieval and extraction benefits from identifying the meanings of those terms. On the other hand, many abbreviations and acronyms are ambiguous, it would be important to map them to their full forms, which ultimately represent the meanings of the abbreviations. In this study, we present a semi-supervised method that applies MEDLINE as a knowledge source for disambiguating abbreviations and acronyms in full-text biomedical journal articles. We first automatically generated from the MEDLINE abstracts a dictionary of abbreviation-full pairs based on a rule-based system that maps abbreviations to full forms when full forms are defined in the abstracts. We then trained on the MEDLINE abstracts and predicted the full forms of abbreviations in full-text journal articles by applying supervised machine-learning algorithms in a semi-supervised fashion. We report up to 92% prediction precision and up to 91% coverage. Keywords Word-sense disambiguation Machine learning MEDLINE Full-text 1 Introduction Abbreviations and acronyms 1 To simplify the description, we will use \u201cabbreviation\u201d to indicate both abbreviations and acronyms in the following texts. 1 are widely used in the biomedical literature. The names of many clinical diseases and procedures, and of common entities such as genes and proteins, have widely used abbreviations. Many abbreviations represent important concepts in the biomedical literature [1]. Because many abbreviations are ambiguous, for example, the abbreviation CAT denotes chloramphenicol acetyl transferase, computer-aided testing, computer-automated tomography, or choline acetyltransferase [2] depending on the context in which the abbreviation appears, information extraction and retrieval may benefit from identifying the actual meaning of an abbreviation. Since the full form of an abbreviation usually represents the meaning of the abbreviation, if we were to correctly map an abbreviation to its intended full form, we would equivalently identify the meaning of the abbreviation. We distinguish between two types of abbreviation occurrences. First, abbreviations may be disambiguated (\u201cdefined\u201d) near their occurrence in the text. This typically is achieved by linking the abbreviation and the intended full form with a linguistic construction such as a parenthetical expression, apposition, or use of words such as \u201ci.e.,\u201d \u201cthat is,\u201d and equivalent expressions. The second type of abbreviation appears without the intended full form nearby. This second type of abbreviation is more prevalent and harder to disambiguate. In our earlier study, we have analyzed ten randomly selected biomedical full-text articles and found that 75% of a total of 358 abbreviations in these articles were never defined [3]. In this paper, we build upon our earlier work which identifies with high precision and recall defined abbreviations (i.e., cases where the abbreviations and intended full forms are explicitly linked) [3]. Using these simpler cases as training data, we postulate that the features occurring near abbreviations of this kind will have a similar distribution to the corresponding features for the same abbreviation when it is used in the same sense, i.e., to represent the same (hidden in that case) full form. This assumption leads us to apply supervised learning methods to what is essentially a classification problem, since defined abbreviations and their full forms can be extracted automatically. Our method does not depend upon a knowledge source for sense listing. We extracted senses automatically from the MEDLINE abstracts. Since we are able to obtain the data for training automatically, the system resembles a semi-supervised approach in the economy of effort required to accomplish the task. We develop approaches for disambiguating biomedical abbreviations in full-text journal articles. A unique challenge in disambiguating abbreviations in full-text articles is the difficulty in obtaining training sets directly from the articles. Though full-text biomedical journal articles are increasingly available online, their number is still small compared to the number of the MEDLINE abstracts. The approach that first maps an abbreviation to its full form and subsequently assigns the article to the training corpus will face the challenge of data sparseness since some abbreviations may be defined only in few articles. It is unlikely that the available full-text journal articles themselves would be sufficient for training. The data sparseness will also cause a problem in generating an exhaustive biomedical abbreviation-full form list automatically from full-text articles. To overcome the data sparseness of full-text articles, in this study, we explore the use of multiple corpora for obtaining the necessary knowledge during training. We trained on the larger number of the MEDLINE abstracts and predicted on full-text journal articles directly. We report results on predicting biomedical abbreviations on two leading full-text journals. 2 Related work There are a number of systems that have been developed to map abbreviations to full forms when the intended full forms are explicitly defined in the articles. Examples include [3\u201312] that reported {recall, precision} range from {73%, 84%} to {96%, 98%} [13]. Note that system-wide performance comparison is difficult due to a lack of common standard. Methods for mapping abbreviations to full forms falls into two broad categories: either abbreviations are linked to full forms when the full forms appear nearby in the text (e.g., in parentheses) according to a set of rules or patterns [3,11,12,14\u201316], or a statistical disambiguation method chooses one of the full forms for an abbreviation based on the context (nearby words) the abbreviation occurs in [4,16,17]. We have earlier developed AbbRE [3] that was specifically developed in the biomedical domain to map abbreviations to full forms when the full forms are explicitly defined in biomedical full-text articles. AbbRE operates through a set of manually annotated rules assigning matches between letters in the abbreviations and words in the full form. These rules include: \u2022 The first letter of an abbreviation matches the first letter of the meaningful word of the full form. \u2022 The abbreviation matches the first letter of each word in the full form. \u2022 The abbreviation letters match consecutive letters of a word in the full form. \u2022 The abbreviation letter matches a middle letter of a word in the full form if the first letter of the word matches the abbreviation. AbbRE was evaluated in full-text biomedical articles and found to have 70% recall and 95% precision. Fewer systems, on the other hand, have been developed for full form recognition of abbreviations of which intended full forms are not explicitly defined. Pustejovsky and colleagues [4] applied a vector space model, a typical information retrieval technique, for acronym sense disambiguation. Given an article in which the abbreviation is ambiguous, the task is to use the article to retrieve a similar article in which the same abbreviation has a defined full form; such full form is then predicted to be the intended full form of the abbreviation to be disambiguated. Pustejovsky and colleagues [4], however, evaluated their method with only one abbreviation with four distinct meanings that appears in a small set of testing abstracts (42 abstracts). [18] and [19] applied word-sense disambiguation methods for disambiguating medical abbreviations in medical notes. Both methods obtained the senses of abbreviations from the knowledge source Unified Medical Language System (UMLS), a biomedical knowledge source that incorporates a small set of abbreviations and their possible full forms [20]. Both applied surrounding words as features for disambiguation. Liu and colleagues [18] applied machine-learning approaches including na\u00efve Bayes and decision lists and reported 92\u201397% precision on disambiguating 12 medical abbreviations. Pakhomov [17] applied statistical maximum entropy techniques and reported 89% accuracy on disambiguating six medical abbreviations. In this study, we present a word-sense disambiguation model for identifying the full forms of biomedical abbreviations in the full-text articles. Unlike previous models ([18] and [19]), we do not depend on an existing knowledge source for sense listing. We generated a sense dictionary automatically from the MEDLINE corpus. We also propose a full-form-normalization method to normalize variations among full forms. Variations among full forms are abundant in the biomedical literature. For example, one sense of ITP includes varied full forms: immune allergic thrombocytopenia, immune mediated thrombocytopenia, immune thrombocytopenia, and immunothrombocytopenia. It is important to recognize those variations so that the abstracts or the full-text documents that incorporate the same sense for a given abbreviation can be clustered into one group for training. We developed an algorithm that identifies the variations among full forms. We have found that such normalization has significantly enhanced the performance for sense disambiguation. Neither [18] nor [19] mentioned the full-form-variation problem. Finally, we present the first attempt in the field to train on the MEDLINE abstracts to obtain the necessary knowledge and then apply the knowledge to predict the sense labels of ambiguous terms that appear in the full-text journal articles. The disambiguation task is very important because full-text journal articles are increasingly available online. Additionally, we report a high accuracy for disambiguation, which has made our algorithms feasible to apply for real use. 3 Methods We describe the corpora we used, the dictionary we obtained automatically, the training and test sets we used, our machine-learning methods, and our evaluation procedure and metrics. 3.1 Text data We used two corpora in our study. One consists of eleven million MEDLINE records (1966\u20132001), freely distributed by NLM, which contain the same number of titles and over six million abstracts (note that not all MEDLINE records contain abstracts). The MEDLINE records were used as a resource for generating the abbreviation-full form dictionary and were used for the training sets for disambiguating the abbreviations in full-text journal articles. The MEDLINE records were also used as both training and testing for cross-validation. The journal corpus consists of 40,933 (1995\u20132001) full-text articles of The Journal of Biological Chemistry (JBC) and 3530 (1996\u20132001) full-text articles of The Journal of Clinical Investigation (JCI). Both journal collections were a part of the GeneWays collection [21]. The journal articles were used to generate disambiguation test sets and reference standards for evaluation. 3.2 Preprocessing We converted the format of MEDLINE records from XML to regular text files and the full-text biomedical journal articles from HTML to regular text files. We lowered the case and removed hyphens: a hyphen was removed when it was used to break up a line; a hyphen was replaced with a space when it was used to connect multiple words. We treated a title as an individual sentence. We then parsed the abstracts into sentences based on a set of handcrafted rules. The most challenging task for splitting a sentence in the biomedical domain is the ability to identify abbreviations, which are frequently used in the biomedical texts. We have summarized a set of heuristic rules for identifying an abbreviation and have incorporated the rules to parse abstracts into sentences. Specifically, we consider a word as an abbreviation if it matches to a list of common biomedical abbreviations, or the word is a one-letter word (a\u2013z, but not \u201cI\u201d), or the word has no vowels and no digits, or the word contains a period and has no digits. We have evaluated the sentence splitter with a total of 114 MEDLINE abstracts that have been manually split into sentences and have found a performance of 92.2% precision and 94.5% recall. The performance is a little better than MXTerminator [22], which achieved 91.9% precision with 93.4% recall. 3.3 Abbreviation-full form dictionary We applied AbbRE to 11 million MEDLINE records (1966\u20132001) to generate a knowledge source of abbreviations and full forms. To make our knowledge source exhaustive, we assigned an arbitrary full form (denoted as XXX) to each abbreviation to represent other full forms that either could not be captured by AbbRE or did not appear in the MEDLINE records. 3.4 Full-form normalization Term variations are abundant in the biomedical domain. In particular, one meaning of an abbreviation frequently comes with different full forms that incorporate variations in word forms that derive from the same root, or variations in the formation of compound forms, or variable additions to the basic words that are being abbreviated. It is important to normalize the full forms that represent the same sense, otherwise, the full forms would represent \u201cdistinguished senses\u201d that only introduce noise for the training and testing. We carry out three processing steps to normalize full form variations: I. In the first step Porter\u2019s stemming algorithm [23] is applied to all the full forms found for a single abbreviation. Those full forms that stem to the same string are identified as synonyms. For example, the abbreviation \u201cscid\u201d has multiple variations of full forms including \u201csevere combined immunodeficiencies\u201d \u201csevere combined immunodeficiency\u201d \u201csevere combined immunodeficient\u201d \u201cseverely combined immunodeficient\u201d \u201csevere combine immunodeficient\u201d After we apply the Porter\u2019s algorithm to those full forms, we obtain the single form \u201csever combin immunodefici\u201d; this allows us to conclude that they are variations. II. If two full forms do not produce the same stemmed form, we then pass two tests to measure the similarity of two full forms: a high similarity indicates that they represent the same full form. The first test is based on the Trigram Matching Algorithm (for additional details see the Appendix A). We apply the Trigram Matching Algorithm [24] in its cosine form with IDF global and log(TF+1) local weights. Here TF is the count of the trigram being weighted within a phrase. This algorithm when applied to two phrases produces a score in the range 0\u20131 as a rating of how similar the two phrases are. We find that when the stemmed versions of two phrases represent synonyms and both phrases are significantly longer than the abbreviation, then they generally achieve a Trigram similarity score of 0.5 or greater. However, if one of the phrases is short, the threshold may be set higher. This has led us to an empirical formula for a threshold below which we do not consider phrases as candidates for synonymy. If len is the length of the shorter of two phrases and len abr is the length of the abbreviation then the score must be at or above a threshold of 0.5+0.5 exp (\u2212len\u22121+2len abr ). Again for the abbreviation \u201cscid\u201d the strings \u201cb icr scid scid\u201d and \u201cscid fc\u201d achieve a trigram similarity score of 0.651908. The relatively high score is a known problem with the cosine similarity calculation applied to short documents (in our case short strings). It requires a correction [25] and our approach is to use the above formula to produce a threshold that is higher for shorter strings. In this case the threshold is 1.0 and excludes the pair of strings from further consideration for synonym status. III. While the processing under II eliminates the bulk of phrase pairs that are not synonyms, we still find it necessary to apply an additional test to eliminate further non-synonymous phrase pairs. In this we look for a consistent assignment of the abbreviation letters to the two phrases being considered. If there is a consistent assignment the phrase pair is accepted as a valid synonym pair, otherwise it is rejected. To make this approach work we define a model for an abbreviation as an identification of where the letters in the abbreviation were taken from the phrase. Some models are more reasonable than others and this leads to optimal models. If two phrases have optimal models that are consistent, those phrases pass the consistency test for acceptance as synonyms. If either fails to have a model or if they simply fail to have a pair of consistent optimal models they fail the consistency test. We proceed to the detailed definition of models, optimal models, and consistent models. Let abr denote an abbreviation of length len abr and let phr1 denote a corresponding full form of length len1. Then a model for phr1 is an integer array mod1 of length len abr taking values between 0 and len1 and satisfying 0 \u2a7d i < j < len abr \u21d2 mod 1 [ i ] < mod 1 [ j ] 0 \u2a7d i < len abr \u21d2 abr [ i ] = phr 1 [ mod 1 [ i ] ] Thus a model is just an identification of the letters of the abbreviation with the same letters in the phrase and in the same order. A model is one way in which the abbreviation could have been constructed. Of course not all models are of equal quality or likelihood as an explanation for the abbreviation. This leads us to score a model as to its quality. Each letter assignment of a model is scored. The score for mod1[i] is 1 if mod1[i] is 0 or phr1[mod1[i]\u22121] is other than an alphabetic character (usually a space, or possibly a dash or number, etc.). Otherwise the score is 0. The score for a model is the sum of the scores for each of the characters it maps. We will refer to a model with the highest possible score as an optimal model. Any phrase that possesses a model will have one or more optimal models. As an example, consider the abbreviation \u201cacd\u201d and the stemmed full form \u201cacid citrat and dextro.\u201d A model will be an assignment of the letters of the abbreviation to letters in the full form that occur in the full form in the same order as in the abbreviation. Thus \u201cacid citrat and dextro\u201d would denote a model with score 2 as does \u201cacid citrat and dextro.\u201d Neither of these models is optimal because there is a model \u201cacid citrat and dextro,\u201d which achieves a score of 3 and is in fact optimal. It is readily apparent that the optimal model represents the origin of the abbreviation. Thus one sees the importance of optimal models. Now suppose mod1 is an optimal model for phr1 and mod2 is an optimal model for phr2, both based on the abbreviation abr. In order to consider these two models consistent we require that certain conditions be satisfied at each letter of the abbreviation. For any i, let e1[i] denote the number of characters from phr1[mod1[i]] to the next non-alphabetic character, to the end of phr1, or to phr1[mod1[i +1]] (if it is defined), whichever is the smallest. In like manner, define e2[i] for phr2[mod2[i]]. If 0\u2a7d i < len abr , then we know that phr1[mod1[i]] and phr2[mod2[i]] are the same. We require that the agreement phr 1 [ mod 1 [ i ] + j ] = phr 2 [ mod 2 [ i ] + j ] continues for 0< j \u2a7dmin(e1[i], e2[i]). Additionally we require that min ( e 1 [ i ] , e 2 [ i ] ) \u2a7d 2 \u21d2 max ( e 1 [ i ] , e 2 [ i ] ) \u2a7d 2 . This latter condition is imposed because very short segments tend to be highly ambiguous and should only be allowed to identify with other highly ambiguous segments. If these conditions are satisfied at each i, then we say mod1 and mod2 are consistent. Again consider abbreviation \u201cacd\u201d and consider the two stemmed full forms \u201cacid citrat dextro formula a\u201d and \u201cacid sodium citrat dextro.\u201d These full forms have optimal models \u201cacid citrat dextro formula a\u201d and \u201cacid sodium citrat dextro\u201d and it is readily apparent that when corresponding marked letters are compared the parts of words following these marked letters are the same and the models are therefore consistent and the full forms are thus considered synonyms. 3.5 Training and testing sets To train our system and evaluate our methods, we randomly selected 60 abbreviations from the LRABR file of the Unified Medical Language System SPECIALIST lexicon [26]. LRABR consists of over 10,000 common biomedical abbreviations. For each abbreviation, we extracted from the MEDLINE collection any record that contains any of the 60 abbreviations with definitions that could be detected by AbbRE. The total number of the MEDLINE abstracts that defined any of the 60 abbreviations is 46,860. This collection of the MEDLINE records was used for training. The full-text journal articles were used for testing. The Journal of Biological Chemistry has special rules for listing abbreviation-full form pairs. It requires authors to list abbreviation-full forms at the end section of an article if the abbreviations were used in the article. We included an article in the testing set if the article contains any of the 60 abbreviations in its abbreviation-full form list. We found 989 such articles. The Journal of Clinical Investigation, on the other hand, does not have the section for listing the full forms of abbreviations that appear in the articles. We therefore included for testing those articles that contain any of the 60 abbreviations with definitions that could be detected by AbbRE. We found, however, only 64 JCI articles that defined 24 abbreviations out of the 60 we selected. We assume that once an abbreviation is defined, the meaning or the full form of the abbreviation remains the same throughout the rest of the MEDLINE abstract and full-text article. This is the principle of \u201cone sense per discourse\u201d [27]. We grouped all the sentences that contained the abbreviation (we removed the full form) in an abstract or article and those sentences were used for the learning. Note that our training sets were heterogeneous in the sense that the size of the training sets is different among different abbreviations and among different full forms because some abbreviations are more frequently defined and some full forms are more frequently used. To allocate the training sets to represent the unseen full form XXX, we assigned any full form to be XXX if the abbreviation appeared together with the full form in less than six abstracts. The number six was chosen empirically. 3.6 Machine learning We have applied two supervised machine-learning methods, namely, na\u00efve Bayesian and support vector machines (SVMs); both are widely used for text categorization problems. The na\u00efve Bayesian algorithm is based on the assumption that attributes are distributed independently within the classes to be learned. Thus each attribute, in this case a term, can be weighted separately based on its distribution in the training set. If t represents a term and p + represents the probability it appears in any document in the positive set and p \u2212 the probability that it occurs in any document in the negative set, then the weight of that term is given by the formula weight of t = log p + ( 1 - p - ) p - ( 1 - p + ) . For binary classification problems it is convenient to represent the contributions of terms by their weights. One scores documents in the test set by summing the weights of the terms they contain. For details the reader may consult [28\u201331]. We have found that performance is improved with this learner if we remove from the scoring any terms with weights less than 1.0 and we refer to this as feature selection (weight>1.0). Such feature selection (weight>1.0) is used implicitly in all the applications of na\u00efve Bayes in this study. Support vector machines (SVMs) learn the linear hyperplane that separates a set of positive examples from a set of negative examples with maximum margin (the margin is defined by the distance from the hyperplane to the nearest of the positive and negative examples). Support vector machines are important because they have yielded improved performance over na\u00efve Bayes in a number of learning environments [28]. We use Platt\u2019s [32] sequential minimal optimization method of training support vector machines. We have followed Platt in taking the error tolerance eps to be 0.01. When the algorithm comes within eps of satisfying the Karush\u2013Kuhn\u2013Tucker conditions it halts with the resultant solution. There is a second parameter C, which is an upper bound on the individual Lagrange multipliers in the problem. Its interpretation is a penalty factor which penalizes the objective function which one seeks to maximize. For each data point that violates the margin the penalty is C times the distance of the violation. Values of 1.0, 0.5, 0.1, and 0.05 for C have been tested [28] and all gave close to the same results with 0.1 and 0.05 as the best and essentially equivalent. The value 0.01 gave a decreased performance by 0.5%. In the work reported here we use a C of 0.1. In order to obtain efficiency in SVM training we pruned the set of features by using a chi-square criterion. Each feature is assigned the chi-square values coming from the contingency table relating relevant set versus non-relevant set and feature-present versus feature-absent. We retain the features with a chi-square value greater than 3.84. The learning features in both na\u00efve Bayes and SVMs applications included single words and contiguous word pairs without punctuation or stop words [28]. We removed from the sentences the full forms that correspond to the abbreviations. We trained on the entire MEDLINE training sets and predicted the full forms of 60 abbreviations among full-text journal articles. 3.7 Cross-validation To describe our methods of cross-validation and prediction, we use the abbreviation abl as an example. Let A T + ( abl ) represent all the abstracts that contain abbreviation abl and one of its full forms. There are several full forms of abl, such as agaricus bisporus lectin, and abetalipoproteinemia. Let us suppose there are M full forms for abl and denote these full forms by the list { ff i } i = 1 M . Because na\u00efve Bayesian and Support Vector Machine-learning methods require some reasonable number of examples on which to train, we require a full form to occur at least six times in association with its abbreviation to include it on the list { ff i } i = 1 M . If the frequency of a full form was less than six, we put it into the category XXX. We treat XXX as a miscellaneous category and also include it on the list. The number of abstracts belonging to the category XXX must also be at least six. If this condition could not be satisfied the abbreviation was excluded from consideration. Given a full form ff k , the set A T + ( abl ) divides naturally into the two sets, A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) , that, respectively, contain the full form ff k and that do not. Training involves learning to distinguish A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) . All of our training involves cross-validation. We generally perform n-fold cross-validation for some small n. This involves randomly dividing the sets A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) each into n disjoint and as nearly equal pieces as possible. The attempt at equal division here is to prevent the pathological situation when some training set has no good abstracts or no bad abstracts and one cannot learn. This is potentially a problem when there are few good abstracts. Then in n cross-validation rounds we learn the difference between good (associated with the full form) and bad (not associated with the full form) from the training set and we then apply that learning to produce scores for the abstracts in the test set or sets. This is done for each of the full forms. Now our objective is to assign each abstract to some full form. Because there are many full forms and we cannot directly compare the scores produced based on the learning for different full forms, we must convert the scores to probability predictions that a given full form is correct for a particular document. Such probabilities can be directly compared across multiple predictions. This leads us to approach the problem of prediction in two stages: Stage I. We perform 10-fold cross-validation on A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) . All the scores on the test data from all 10 rounds are combined and the Pool Adjacent Violators (PAV) Algorithm [33,34] is applied to learn the relationship between score and probability of having ff k as the correct full form. In some cases the number of documents having ff k as their full form are too few to allow for 10-fold cross-validation. In those cases we still do at least 4-fold cross-validation to learn the relationship between score and probability. This is guaranteed by the requirement that at least six documents have ff k as their full form. Stage II. We perform 3-fold cross-validation on A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) . We chose 3-fold cross-validation rather than the more common 10-fold cross-validation because for many k the sets A T + ( abl , ff k ) will have only six abstracts which can be naturally divided into four for training and two for testing. In each of the three rounds the learning on the training data is used in two ways: (A) The test documents in A T + ( abl , ff k ) and A T + ( abl , \u223c ff k ) are scored and the scores are converted to probabilities that ff k is the correct full form. (B) All the journal articles are scored and the scores are converted to probabilities that ff k is the correct full form. When the 3-fold cross-validation is completed all the members of A T + ( abl ) will have received a probability predicting their likelihood of having ff k as their correct full form exactly once. All the journal articles that incorporate the abbreviation abl will have received a probability predicting their likelihood of having ff k as their correct full form three times. These three probabilities are averaged to yield a single probability. What we have just described in two stages applies to a single full form for the abbreviation abl. This same procedure is followed for each of the M full forms. The end result is a probability vector predicting the likelihood that an abstract belongs to the full form ff k for each of the M full forms. This applies to all the abstracts in A T + ( abl ) and the journal articles. Finally, we assign as our prediction to each abstract or full-text article that full form with highest probability. 3.8 Evaluation metrics and gold standard Since we introduced XXX to represent unseen full forms, we evaluated by coverage (i.e., the percentage of all predictions that predict non-XXX) and precision (i.e., the number of correctly predicted full forms divided by the total number of predicted full forms). The coverage value indicates the capability of our system in predicting non-XXX full forms. For cross-validation performed in the MEDLINE abstracts, we reported macro-precision (i.e., the average precision among all predictions). We also reported micro-precision (i.e., the precision that is the average precision of all abbreviations) and the standard deviation (std). Because the prediction instances in full-text journal articles are much smaller than the MEDLINE abstracts, we reported only the results of macro-precision. We generated the prediction reference standard automatically. The reference standard for the journal JBC consisted of those pre-defined abbreviation-full form pairs that were listed at the end of the articles. The reference standard for the journal JCI consisted of abbreviation-full form pairs captured by AbbRE. 4 Results We generated a total of 574,327 unique pairs of abbreviation and associated full form from 11 million MEDLINE records. Fig. 1 shows the distribution (from eleven million MEDLINE records) of the numbers of abbreviations paired with different numbers of full forms. The abbreviations \u201cor\u201d and \u201cca\u201d corresponded to the largest numbers (>600) of different full forms (e.g., \u201couabain-resistant,\u201d \u201corienting response\u201d and \u201ccalcium,\u201d \u201ccatecholamine\u201d and \u201ccommon enterobacterial antigen,\u201d respectively). We measured the numbers of abbreviation-full form pairs that occur at different frequencies throughout MEDLINE. The average number of full forms for any abbreviation was 9.6\u00b13.7. From the MEDLINE records, we obtained a total of 567 unique abbreviation-full form pairs for the 60 abbreviations we evaluated. The average number of full forms for the 60 abbreviations was 9.44\u00b114.45. The average number of abstracts that were used for training was 781\u00b1807. We calculated a lower bound for performance from the assignment of each abbreviation to its most frequent full form. The average lower bound was a precision of 0.41\u00b10.17. Fig. 2 shows a na\u00efve Bayes sample prediction output. The results of cross-validation are listed in Table 1 in which we report the coverage and precision with and without our full-form normalization. We applied bootstrapping analyses and the results (at \u03b1 =0.05 level) show that na\u00efve Bayes and SVM performed equally in all cases of cross-validation. Our results show an enhanced performance in both coverage (an up to 5% increase) and precision (an up to 5% increase) when we normalized the full-form. The results of prediction on full-text articles using normalization are listed in Table 2 . 5 Discussion We have developed and evaluated a context-based semi-supervised approach that uses a combination of rules and supervised machine-learning algorithms for predicting the full forms of abbreviations in full-text biomedical journal articles. Our approach is novel in that it is the first model that resolves the problem of abbreviation ambiguity in full-text journal articles. Our approach is also semi-supervised in that we automatically generated the training sets and the dictionary of abbreviation-full forms. We combined our statistical methods with a rule-based model that uses the MEDLINE records. We also trained from the MEDLINE data and applied the resulting classifiers to a somewhat different text form, full-text journal articles. We have performed a large scale evaluation and obtained approximately 90% coverage and precision for predicting the full forms of abbreviations in biomedical journal full-text articles. The results show that our approaches resulted in a coverage of 0.77\u20130.91 and a precision of 0.86\u20130.95 in predicting the full forms of ambiguous abbreviations with an average of 10 senses in both MEDLINE abstracts and full-text journal articles. Note that most other systems disambiguated many fewer senses (typically two senses). Our results also showed little differences between micro-precision and macro-precision; the results indicate that the approaches were robust and effective in predicting all abbreviations with different training sizes and numbers of senses. Our results show that both coverage and precision values are higher in full-text journal articles than in the MEDLINE abstracts. The results could be explained by the facts that the full-text articles tended to be longer than the MEDLINE abstracts and therefore the surrounding context that could be used for disambiguation was larger than the abstracts. The performance differences could also be the results of the evaluation biases. For example, the abbreviations evaluated in full-text articles were either the common abbreviations (i.e., for JBC, we selected the abbreviations if they were listed by the journal) or the abbreviations (in JCI) that could be captured by our AbbRE system. Our results show that our strategy of applying the MEDLINE records for training is important. While the number of abstracts that could be used for training 60 randomly chosen biomedical abbreviations were over 46,000, there were only about 1000 full-text journal articles (i.e., 989 JBC and 64 JCI) that could be used for the training. A small training set would almost certainly lead to poor performance. Our results indicate that MEDLINE is efficient as a knowledge source for disambiguating abbreviations in the full-text articles. We empirically evaluated the assumptions in our study. We generated an abbreviation-full form dictionary automatically from the MEDLINE abstracts based on the assumption that abbreviations were usually defined in some of the abstracts, especially when the full form was first used in the literature. Even if an abbreviation was not defined in some abstracts, it may be defined in other abstracts due to information redundancy. We also assumed that the majority of abbreviation-full form pairs that were defined could be captured by our automatic system AbbRE. Our results of high coverage empirically validated all the assumptions above. Our results show that full-form normalization has significantly enhanced the performance in full-form prediction in the MEDLINE abstracts. Our good prediction coverage indicates that a majority of abbreviation-full forms in the MEDLINE abstracts can be captured by AbbRE. Note that the reported recall of AbbRE was 74% and was lower than the coverage we found in our disambiguation. This was due to the fact that AbbRE was sensitive to how the full forms were defined. For example, AbbRE may miss some abbreviation-full form pairs when the full forms were not defined with parentheses. The higher coverage of our disambiguation suggests that even though AbbRE may miss some abbreviation-full form pairs in some abstracts or articles, it can capture the same abbreviation-full form pairs in other articles where the abbreviation-full form pairs are expressed in a suitable format recognized by AbbRE. Therefore, our results suggest that the actual recall of AbbRE in generating the abbreviation-full form dictionary from the MEDLINE records is higher than 74%. Acknowledgment We thank two anonymous reviewers for valuable comments. Appendix A The Trigram Matching Algorithm [24] is a computational algorithm used to compare two phrases to see how similar they are. It produces a number between zero and one that denotes the level of similarity with a one denoting identity and a zero no relationships at all or stated in another way no features in common. We describe the algorithm in three steps. The first step is to assign features to any phrase. As an example suppose we wish to treat the phrase p1=\u201cDNA sequence selectivity.\u201d Then we process this string (phrase) by lowering case to \u201cdna sequence selectivity.\u201d Next the individual words of the phrase are each broken into all contiguous trigrams of letters. This produces the list: \u201cdna,\u201d \u201cseq,\u201d \u201cequ,\u201d \u201cque,\u201d \u201cuen,\u201d \u201cenc,\u201d \u201cnce,\u201d \u201csel,\u201d \u201cele,\u201d \u201clec,\u201d \u201cect,\u201d \u201ccti,\u201d \u201ctiv,\u201d \u201civi,\u201d \u201cvit,\u201d \u201city.\u201d Finally, special features are added to mark the beginnings of words in the phrase: \u201cdna!\u201d, \u201cseq!\u201d, \u201csel!\u201d, \u201cd#,\u201d \u201cs#,\u201d \u201cs#,\u201d and to bridge between consecutive words: \u201cd s\u201d and \u201cs s.\u201d Note that the bridging features are not the last letter of one word the space between and the first letter of the next word, but rather the first letter of a word, the space between, and then the first letter of the next word. This is because first letters of words are much more important and characteristic of the words than are the last letters. Now suppose we are given a collection PH of N phrases that we wish to process and let F be the set of all the features that come from all the phrases in PH when they are processed just as we processed p1 above. The second step in the processing is to represent each phrase in PH by a vector of numbers. The vectors are very high dimensional and have one dimension for each possible feature that occurs in any phrase in PH. That is there is a dimension for each feature f \u2208 F. Let v p1 denote the vector associated with the phrase p1. Then we may write v p1 =(v f ) f\u2208F . In other words for each possible feature f, v p1 has a number v f which represents how f relates to the phrase p1. This number v f is made up of two parts, a local factor and a global factor. This is given by the equation v f = log ( Ct ( f in p 1 ) + 1 ) \u00d7 log N n f . Here the local factor is log(Ct(f in p1)+1) and Ct(f in p1) is just the count of the number of times f appears in p1. The global factor is log ( N n f ) where N is the number of phrases in the whole collection PH and n f is the number of phrases in the collection that contain the feature f at least once. If a feature f does not occur in a phrase then the count of the feature in that phrase is zero and the local part of the weight is zero and forces the vector to have a zero component v f for that feature. Thus, for any given phrase only a small number of the features will have a nonzero component in the vector representing that phrase. For example p1 has a nonzero component for \u201cdna\u201d or \u201cseq,\u201d but the component for \u201cstd,\u201d \u201crna,\u201d or \u201cwnt\u201d would be zero. The third or final step in the algorithm is to use the vector representation of two phrases, say p1 and p2, to compute how related those two phrases are. This computation uses the cosine formula based on the vector representations of the phrases. similarity ( p 1 , p 2 ) = v p 1 \u00b7 v p 2 v p 1 \u00b7 v p 1 v p 2 \u00b7 v p 2 In this formula v p1 \u00b7 v p2 is the vector dot product. The resulting similarity is always a number between 0 and 1 inclusive and the more similar the phrases the closer to 1. For more information regarding the cosine formula and its use in similarity calculations we refer the reader to [35\u201337]. References [1] C.S. Federiuk The effect of abbreviations on MEDLINE searching Acad Emerg Med 6 4 1999 292 296 [2] M. Rimer M. O\u2019Connell BioABACUS: a database of abbreviations and acronyms in biotechnology and computer science Bioinformatics 14 10 1998 888 889 [3] H. Yu G. Hripcsak C. Friedman Mapping abbreviations to full forms in biomedical articles J Am Med Inform Assoc 9 3 2002 262 272 [4] J. Pustejovsky J. Castano B. Cochran M. Kotecki M. Morrell Automatic extraction of acronym-meaning pairs from MEDLINE databases Medinfo 2001 2001 371 375 [5] Adar E. S-RAD: a simple and robust abbreviation dictionary: HP Laboratories Technical Report; 2002. [6] J.C. Park H.S. Kim J.J. Kim Bidirectional incremental parsing for automatic pathway identification with combinatory categorial grammar Pac Symp Biocomput 2001 396 407 [7] A.S. Schwartz M.A. Hearst A simple algorithm for identifying abbreviation definitions in biomedical text Pac Symp Biocomput 2002 to appear [8] Chang JT, Schutze H, Altman RB. Creating an online dictionary of abbreviations from MEDLINE. JAMIA to appear. [9] Yeates S, Bainbridge D, Witten IH. Using compression to identify acronyms in text. Data compression conference, 2000; 2000. [10] M. Yoshida K. Fukuda T. Takagi PNAD-CSS: a workbench for constructing a protein name abbreviation dictionary Bioinformatics 16 2 2000 169 175 [11] Hisamitsu T, Niwa Y. Extraction of useful terms from parenthetical expression by using simple rules and statistical measures. CompuTerm98; 1998; Montreal, Canada; 1998. p. 36\u201342. [12] Bowden PR, Eventt L, Halsted P. Automatic arconym acquistion in a knowledge extraction program. ComputTerm98; 1998; Montreal, Ontario; 1998. [13] K. Fukuda A. Tamura T. Tsunoda T. Takagi Toward information extraction: identifying protein names from biological papers Pac Symp Biocomput 1998 707 718 [14] Liu H, Frideman C. Mining terminological knowledge in large biomedical corpora. Pac Symp Biocomput, 2003; 2003. [15] H. Yu V. Hatzivassiloglou A. Rzhetsky W.J. Wilbur Automatically identifying gene/protein terms in MEDLINE abstracts J Biomed Inform 35 5\u20136 2003 322 330 [16] Pustejovsky J, Castano J, Sauri R, Rumshisky A, Zhang J, Luo W. Medstract: creating large-scale information servers for biomedical libraries. ACL2002 Workshop on Natural Language Processing in the Biomedical Domain, 2002; 2002. [17] Pakhomov S. Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical text. In: The 40th annual meeting of the association for computational linguistics; 2002; Philadelphia, Pennsylvania, USA; 2002. [18] H. Liu Y.A. Lussier C. Friedman Disambiguating ambiguous biomedical terms in biomedical narrative text: an unsupervised method J Biomed Inform 34 4 2001 249 261 [19] Pakhomov S. Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical text. In: 40th annual meeting of the association for computational linguistics; 2002; Philadelphia, Pennsylvania, USA; 2002. p. 160\u20137. [20] B.L. Humphreys D.A. Lindberg The UMLS project: making the conceptual connection between users and the information they need Bull Med Libr Assoc 81 2 1993 170 177 [21] A. Rzhetsky I. Iossifov T. Koike M. Krauthammer P. Kra M. Morris GeneWays: a system for extracting, analyzing, visualizing, and integrating molecular pathway data J Biomed Inform 37 1 2004 43 53 [22] Ratnaparkhi A. Maximum entropy models for natural language ambiguity resolution [Ph.D. Dissertation]: University of Pennsylvania; 1998. [23] M.F. Porter An algorithm for suffix stripping Program 14 3 1980 130 137 [24] Wilbur WJ, Kim W. Flexible phrase based query handling algorithms. In: Aversa E, Manley C, editors. In: Proceedings of the ASIST 2001 annual meeting, 2001; Washington, DC; 2001. p. 438\u201349. [25] A. Singhal C. Buckley M. Mitra Pivoted document length normalization SIGIR, 1996 1996 ACM Press Zurich 21 29 [26] A.T. McCray The nature of lexical knowledge Methods Inf Med 37 4\u20135 1998 353 360 [27] Yarowsky D. Unsupervised word sense disambiguation rivaling supervised methods. In: Proceeding of the 33rd annual meeting of the association for computational linguistics, 1995; Cambridge; 1995. p. 189\u201396. [28] W.J. Wilbur Boosting naive Bayesian learning on a large subset of MEDLINE Proc AMIA Symp 2000 918 922 [29] Lewis D. Naive (Bayes) at forty: the independence assumption in information retrieval. ECML, 1998; 1998. p. 4\u201315. [30] Langley P, Iba W, Thompson K. An analysis of Bayesian classifiers. In: Tenth national conference on artificial intelligence; 1992, San Jose: AAAI press; 1992. p. 223\u20138. [31] Langley P, Sage S. Induction of selective Bayesian classifiers. In: Kaufmann WM, editor. Tenth conference on uncertainty in artificial intelligence; 1994; Seattle; 1994. p. 918\u201322. [32] J. Platt Fast training of support vector machines using sequential minimal optimization 1999 The MIT Press [33] M. Ayer H.D. Brunk G.M. Ewing W.T. Reid E. Silverman An empirical distribution function for sampling with incomplete information Ann Meth Stat 26 1954 641 647 [34] Hardle W. Smoothing techniques: with implementation in S.: New York: Spring-Verlag; 1991. [35] I. Witten A. Moffat T. Bell Managing gigabytes: compressing and indexing documents and images 1999 Morgan Kaufmann Publishers [36] G. Salton Automatic text processing: the transformation, analysis, and retrieval of information by computer 1989 Addison-Wesley Longman Publishing Co., Inc. Boston, MA, USA [37] R. Baeza-Yates Faster approximate string matching 1999 Spinger-Verlag, New York, Inc. New York, USA", "scopus-id": "33847616032", "pubmed-id": "16843731", "coredata": {"eid": "1-s2.0-S1532046406000621", "dc:description": "Abstract Biomedical abbreviations and acronyms are widely used in biomedical literature. Since many of them represent important content in biomedical literature, information retrieval and extraction benefits from identifying the meanings of those terms. On the other hand, many abbreviations and acronyms are ambiguous, it would be important to map them to their full forms, which ultimately represent the meanings of the abbreviations. In this study, we present a semi-supervised method that applies MEDLINE as a knowledge source for disambiguating abbreviations and acronyms in full-text biomedical journal articles. We first automatically generated from the MEDLINE abstracts a dictionary of abbreviation-full pairs based on a rule-based system that maps abbreviations to full forms when full forms are defined in the abstracts. We then trained on the MEDLINE abstracts and predicted the full forms of abbreviations in full-text journal articles by applying supervised machine-learning algorithms in a semi-supervised fashion. We report up to 92% prediction precision and up to 91% coverage.", "openArchiveArticle": "true", "prism:coverDate": "2007-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046406000621", "dc:creator": [{"@_fa": "true", "$": "Yu, Hong"}, {"@_fa": "true", "$": "Kim, Won"}, {"@_fa": "true", "$": "Hatzivassiloglou, Vasileios"}, {"@_fa": "true", "$": "Wilbur, W. John"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046406000621"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046406000621"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(06)00062-1", "prism:volume": "40", "prism:publisher": "Elsevier Inc.", "dc:title": "Using MEDLINE as a knowledge source for disambiguating abbreviations and acronyms in full-text biomedical journal articles", "prism:copyright": "Copyright \u00a9 2006 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Word-sense disambiguation"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "MEDLINE"}, {"@_fa": "true", "$": "Full-text"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "150-159", "prism:endingPage": "159", "prism:coverDisplayDate": "April 2007", "prism:doi": "10.1016/j.jbi.2006.06.001", "prism:startingPage": "150", "dc:identifier": "doi:10.1016/j.jbi.2006.06.001", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "51", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "420", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "51", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "420", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "55", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "452", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "40", "@width": "217", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1306", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "309", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1272", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "36", "@width": "263", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si37.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1405", "@ref": "si37", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "23", "@width": "45", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si36.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "445", "@ref": "si36", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "41", "@width": "261", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si35.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1400", "@ref": "si35", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "55", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si34.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "452", "@ref": "si34", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si33.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si33", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si32.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si32", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si31.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si31", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "55", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si30.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "452", "@ref": "si30", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "253", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1147", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si29.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si29", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si28.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si28", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si27.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si27", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si26.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si26", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si25.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si25", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si24.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si24", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si23.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si23", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si22.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si22", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si21.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si21", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "275", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1168", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "261", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "99", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "610", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "578", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "55", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "452", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "279", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1108", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "283", "@width": "359", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24587", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "118", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2373", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "159", "@width": "260", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-gr2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "6914", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "76", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000621-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2080", "@ref": "gr2", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33847616032"}}