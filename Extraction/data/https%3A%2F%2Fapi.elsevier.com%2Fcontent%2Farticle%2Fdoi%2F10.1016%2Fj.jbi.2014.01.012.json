{"scopus-eid": "2-s2.0-84902545076", "originalText": "serial JL 272371 291210 291682 291870 291901 31 90 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2014-02-04 2014-02-04 2014-10-03T20:16:27 1-s2.0-S1532046414000148 S1532-0464(14)00014-8 S1532046414000148 10.1016/j.jbi.2014.01.012 S300 S300.2 FULL-TEXT 1-s2.0-S1532046414X00031 2015-05-15T06:30:58.629321-04:00 0 0 20140601 20140630 2014 2014-02-04T00:00:00Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref specialabst 1532-0464 15320464 UNLIMITED NONE true 49 49 C Volume 49 16 148 158 148 158 201406 June 2014 2014-06-01 2014-06-30 2014 Original Research Articles article fla Copyright \u00a9 2014 The Authors. Published by Elsevier Inc. AUTOMATICRECOGNITIONDISORDERSFINDINGSPHARMACEUTICALSBODYSTRUCTURESCLINICALTEXTANNOTATIONMACHINELEARNINGSTUDY SKEPPSTEDT M 1 Introduction 2 Related research 3 Methods 3.1 Corpus annotation 3.2 Feature selection and evaluation 3.3 Final evaluation on held-out data 3.4 Error analysis 4 Results 4.1 Corpus annotation 4.2 Feature selection and evaluation 4.3 Final evaluation on held-out data 4.4 Error analysis 5 Discussion 5.1 Application of NER methods on Swedish clinical text 5.2 Division into the two categories Disorder and Finding 5.3 Limitations 5.4 Future directions 6 Conclusion Authors\u2019 contributions Acknowledgments References MEYSTRE 2008 128 144 S CHAPMAN 2005 31 40 W ROQUE 2011 F ERIKSSON 2013 R JURAFSKY 2008 D SPEECHLANGUAGEPROCESSINGINTRODUCTIONNATURALLANGUAGEPROCESSINGCOMPUTATIONALLINGUISTICSSPEECHRECOGNITION CAO 2005 106 110 H DEBRUIJN 2011 557 562 B JIANG 2011 M SAVOVA 2010 507 513 G ALBRIGHT 2013 922 930 D CHAPMAN 2008 107 113 W ROBERTS 2009 950 966 A WANG 2009 18 26 Y PROCEEDINGSACLIJCNLP2009STUDENTRESEARCHWORKSHOPACLSTUDENT09 ANNOTATINGRECOGNISINGNAMEDENTITIESINCLINICALNOTES OGREN 2008 3143 3149 P PROCEEDINGSSIXTHINTERNATIONALLANGUAGERESOURCESEVALUATIONLREC08 CONSTRUCTINGEVALUATIONCORPORAFORAUTOMATEDCLINICALNAMEDENTITYRECOGNITION UZUNER 2010 514 518 O UZUNER 2010 519 523 O PATRICK 2010 524 527 J UZUNER 2011 552 556 O ROBERTS 2008 2974 2979 A PROCEEDINGSSIXTHINTERNATIONALCONFERENCELANGUAGERESOURCESEVALUATIONLREC08 COMBININGTERMINOLOGYRESOURCESSTATISTICALMETHODSFORENTITYRECOGNITIONEVALUATION DOAN 2010 528 531 S DOAN 2012 36 S LAFFERTY 2001 282 289 J PROCEEDINGS18THINTERNATIONALCONFERENCEMACHINELEARNING CONDITIONALRANDOMFIELDSPROBABILISTICMODELSFORSEGMENTINGLABELINGSEQUENCEDATA OGREN 2006 273 275 P PROCEEDINGS2006CONFERENCENORTHAMERICANCHAPTERASSOCIATIONFORCOMPUTATIONALLINGUISTICSHUMANLANGUAGETECHNOLOGY KNOWTATORAPROTEGEPLUGINFORANNOTATEDCORPUSCONSTRUCTION ARTSTEIN 2008 555 596 R HRIPCSAK 2005 296 298 G SKEPPSTEDT 2012 1250 1257 M PROCEEDINGSEIGHTINTERNATIONALCONFERENCELANGUAGERESOURCESEVALUATIONLREC12 RULEBASEDENTITYRECOGNITIONCOVERAGESNOMEDCTINSWEDISHCLINICALTEXT CARLBERGER 1999 815 832 J CAMPBELL 2007 M MEDICALSTATISTICSATEXTBOOKFORHEALTHSCIENCES SKEPPSTEDTX2014X148 SKEPPSTEDTX2014X148X158 SKEPPSTEDTX2014X148XM SKEPPSTEDTX2014X148X158XM Full 2014-02-04T12:25:00Z Author http://creativecommons.org/licenses/by-nc-nd/3.0/ item S1532-0464(14)00014-8 S1532046414000148 1-s2.0-S1532046414000148 10.1016/j.jbi.2014.01.012 272371 2014-10-05T01:55:14.533078-04:00 2014-06-01 2014-06-30 UNLIMITED NONE 1-s2.0-S1532046414000148-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/MAIN/application/pdf/bc00732118eb0338498048a99d154a27/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/MAIN/application/pdf/bc00732118eb0338498048a99d154a27/main.pdf main.pdf pdf true 562674 MAIN 11 1-s2.0-S1532046414000148-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/PREVIEW/image/png/89f1ce57621e7cef7f72fcaa4cfe3bcd/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/PREVIEW/image/png/89f1ce57621e7cef7f72fcaa4cfe3bcd/main_1.png main_1.png png 55782 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046414000148-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr2/HIGHRES/image/jpeg/aa68355d73042da7ae1e020439471356/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr2/HIGHRES/image/jpeg/aa68355d73042da7ae1e020439471356/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 190083 1402 1459 IMAGE-HIGH-RES 1-s2.0-S1532046414000148-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr1/HIGHRES/image/jpeg/f74b90f92a9189f5c8fc6e1669ca2595/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr1/HIGHRES/image/jpeg/f74b90f92a9189f5c8fc6e1669ca2595/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 220753 767 2559 IMAGE-HIGH-RES 1-s2.0-S1532046414000148-fx1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/fx1/HIGHRES/image/jpeg/cbdb1179a06dcbc0b84f39993e8595be/fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/fx1/HIGHRES/image/jpeg/cbdb1179a06dcbc0b84f39993e8595be/fx1_lrg.jpg fx1 true fx1_lrg.jpg jpg 164032 805 2213 IMAGE-HIGH-RES 1-s2.0-S1532046414000148-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr2/DOWNSAMPLED/image/jpeg/7f5eb6e741326b7034c6b75e7a29ab2c/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr2/DOWNSAMPLED/image/jpeg/7f5eb6e741326b7034c6b75e7a29ab2c/gr2.jpg gr2 gr2.jpg jpg 22356 316 329 IMAGE-DOWNSAMPLED 1-s2.0-S1532046414000148-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr1/DOWNSAMPLED/image/jpeg/b17a3994c098498f74e386f4650e62b5/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr1/DOWNSAMPLED/image/jpeg/b17a3994c098498f74e386f4650e62b5/gr1.jpg gr1 gr1.jpg jpg 22603 173 578 IMAGE-DOWNSAMPLED 1-s2.0-S1532046414000148-fx1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/fx1/DOWNSAMPLED/image/jpeg/979874b6b5d0f3fb2b5e0d8196ef8f6e/fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/fx1/DOWNSAMPLED/image/jpeg/979874b6b5d0f3fb2b5e0d8196ef8f6e/fx1.jpg fx1 true fx1.jpg jpg 24855 182 500 IMAGE-DOWNSAMPLED 1-s2.0-S1532046414000148-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr2/THUMBNAIL/image/gif/ec3655bc09318fe0d1c5dda8de4f0235/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr2/THUMBNAIL/image/gif/ec3655bc09318fe0d1c5dda8de4f0235/gr2.sml gr2 gr2.sml sml 4652 163 170 IMAGE-THUMBNAIL 1-s2.0-S1532046414000148-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/gr1/THUMBNAIL/image/gif/860349ac74c76697ae9747ee87f7ccae/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/gr1/THUMBNAIL/image/gif/860349ac74c76697ae9747ee87f7ccae/gr1.sml gr1 gr1.sml sml 2808 66 219 IMAGE-THUMBNAIL 1-s2.0-S1532046414000148-fx1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046414000148/fx1/THUMBNAIL/image/gif/efd1fe809832e0f3102ff44e97e36778/fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046414000148/fx1/THUMBNAIL/image/gif/efd1fe809832e0f3102ff44e97e36778/fx1.sml fx1 true fx1.sml sml 3366 80 219 IMAGE-THUMBNAIL YJBIN 2117 S1532-0464(14)00014-8 10.1016/j.jbi.2014.01.012 The Authors Fig. 1 Used features shown in a constructed example sentence. Each row contains one token and each column contains feature values corresponding to that token. The information marked with boldface is used for predicting the label for the word chestpain. Chestpain (br\u00f6stsm\u00e4rta) is here shown as one compound word in English to mimic the Swedish equivalent. Two separate columns are used for the compound constituents. The last column, Category, shows the IOB-encodings of the annotated entities, which are the desired output labels that the CRF model aims at learning. Fig. 2 Comparison to a number of previous clinical NER studies. Results from the same studies are connected with a line: a solid line for the present study and dashed lines for previous studies. The first column from the left shows the result of three rule-base studies, and the second column shows the result of two machine learning studies for which the number of used training instances were not reported. The rest of the diagram shows the result of a number of machine learning studies for which the number of training instances were reported. The entity names of the present study are used for denoting comparable entity types in previous studies. Table 1 Inter-annotator agreement in previous annotation studies. The data shown is the inter-annotator agreement (F-score), types of clinical text and annotation classes. The entity types Clinical Condition, Condition and Finding are fairly equivalent, including what is referred to as the category Medical Problem or Disorder+Finding in our study. Annotation class F-score Chapman et al. [16]; Emergency department reports Clinical Condition 0.92 CLinical E-Science Framework [17]; (1) Narratives: To GP, Discharge letter and Case note. (2) Imaging reports. (3) Histopathology reports Condition 0.81, 0.77, 0.67 Drug or Device 0.84, 0.32, 0.59 Locus 0.78, 0.75, 0.71 Wang [10]; Intensive care service progress notes Finding 0.91 Substance 0.95 Body 0.85 Ogren et al. [19]; Outpatient notes, discharge summaries, inpatient service notes Disorder 0.76 i2b2/VA challenge on concepts \u2026 [23]; Discharge summaries, some progress notes Medical Problem, Test and Treatment Not given i2b2 medication challenge [21]; Discharge summaries Medication Name 0.86\u20130.91 Previous Swedish study [13]; Discharge summaries Disease and Drug Not given MiPACQ [15]; Clinical narratives UMLS semantic groups 0.70 Table 2 NER results for previous studies. n is the number of training instances. Corpus Entity category n Precision Recall CLinical E-Science Framework Roberts et al. [24]; SVM (10-fold) Condition 739 0.82 0.65 Drug or Device 272 0.83 0.59 Locus 490 0.80 0.62 Wang Wang [18]; CRF (10-fold) Finding 4741 0.83 0.79 Substance 2249 0.92 0.89 Body 735 0.72 0.64 Wang and Patrick [10]; Combining CRF, SVM and ME (10-fold) Finding 4741 0.84 0.82 Substance 2249 0.92 0.88 Body 735 0.76 0.66 i2b2/VA challenge on concepts\u2026 Jiang et al. [12]; Combining 4 CRF models (Separate evaluation set) Medical Problem 11,968 0.87 0.84 de Bruijn et al. [11]; Semi-Markov HMM (Separate evaluation set) Medical concepts (including Problem) 27,837 0.87 0.84 i2b2 Medication challenge Patrick and Li [22]; CRF Medication Names \u2013 0.91 0.86 Doan et al. [27]; Terminology matching Medication Names \u2013 0.85 0.87 Doan et al. [26]; CRF, SVM, terminology Medication Names \u2013 0.94 0.90 Previous Swedish study Kokkinakis and Thurin [13]; Terminology matching Disease \u2013 0.98 0.87 Drug \u2013 0.95 0.93 Ogren et al. Savova et al. [14]; Terminology matching Disorder \u2013 0.80 0.65 Table 3 Data used for measuring inter annotator agreement by main annotator (PH1), other physician (PH2) and computational linguist (CL). 90% of this data was also used for constructing the Final Evaluation subset. Entity types is the number of types of entities that were annotated. Annotator PH1 PH2 CL Entity category Annotated entities (Entity types) Annotated entities (Entity types) Annotated entities (Entity types) Disorder 766 (354) 329 (174) 355 (214) Finding 1327 (715) 631 (466) 686 (461) Pharmaceuticals 636 (249) 282 (119) 262 (143) Body Structure 275 (112) 117 (67) 101 (65) Table 4 Number of annotated entities in Development subset and Final Evaluation subset. Entity types is the number of types of entities that were annotated. Data set: Development Final evaluation Entity category Annotated entities (Entity types) Annotated entities Disorder 1317 (607) 681 Finding 2540 (1353) 1282 Pharmaceuticals 959 (350) 580 Body Structure 497 (197) 253 Tokens in corpus 45,482 25,370 Table 5 Inter-annotator agreement scores. PH2 shows the inter-annotator agreement between PH1 and PH2. CL shows the inter-annotator agreement between PH1 and CL. Disorder+Finding shows the results when merging the two classes Disorder and Finding into one class, and \u2212num. val. means the agreement when removing numerical values from annotations for the class Finding. The average results were also measured. PH2 CL Average F-score F-score F-score Disorder 0.77 0.80 0.79 Finding 0.58 0.73 0.66 Pharmaceutical Drug 0.88 0.92 0.90 Body Structure 0.80 0.80 0.80 Disorder+Finding 0.72 0.84 0.78 Finding \u2212 num. val. 0.61 0.73 0.67 Disorder+Finding \u2212 num. val. 0.75 0.84 0.80 Table 6 The average results for all four categories, compared to the average results for terminology match (macro average over the four classes Disorder, Finding, Pharmaceutical Drug and Body Structure). The results for removing one feature type at a time, as well as for using the inflected tokens instead of the lemmatised forms, are also shown. Evaluated configuration Precision Recall F-score Baseline: Terminology match 0.700 0.582 0.623 Best average CRF settings 0.832 0.759 0.794 Without lemmatisation 0.823 0.741 0.780 Best settings \u2013 current lemma 0.758 0.633 0.688 Best settings \u2013 previous lemma 0.826 0.758 0.790 Best settings \u2013 part-of-speech tagging 0.830 0.751 0.788 Best settings \u2013 terminology match 0.848 0.688 0.759 Best settings \u2013 compound splitting 0.837 0.742 0.786 Best settings \u2013 orthographics 0.832 0.755 0.791 Table 7 The results for the category Disorder for the CRF model settings giving the best average results, compared to the Disorder results for the baseline obtained by terminology matching. The results for removing one feature type at a time, as well as for using the inflected tokens instead of the lemmatised forms, are also shown. Evaluated configuration Precision Recall F-score Baseline: Terminology match 0.677 0.574 0.621 Best average CRF settings 0.853 0.762 0.805 Without lemmatisation 0.841 0.743 0.789 Best settings \u2013 current lemma 0.739 0.640 0.686 Best settings \u2013 previous lemma 0.852 0.765 0.806 Best settings \u2013 part-of-speech tagging 0.858 0.763 0.808 Best settings \u2013 terminology match 0.884 0.658 0.755 Best settings \u2013 compound splitting 0.860 0.746 0.799 Best settings \u2013 orthographics 0.855 0.762 0.806 Table 8 The results for the category Finding for the CRF model settings giving the best average results, compared to the Finding results for the baseline obtained by terminology matching. The results for removing one feature type at a time, as well as for using the inflected tokens instead of the lemmatised forms, are also shown. Evaluated configuration Precision Recall F-score Baseline: Terminology match 0.574 0.312 0.404 Best average CRF settings 0.737 0.636 0.683 Without lemmatisation 0.720 0.608 0.659 Best settings \u2013 current lemma 0.610 0.401 0.484 Best settings \u2013 previous lemma 0.734 0.633 0.680 Best settings \u2013 part-of-speech tagging 0.736 0.615 0.670 Best settings \u2013 terminology match 0.735 0.611 0.668 Best settings \u2013 compound splitting 0.737 0.615 0.671 Best settings \u2013 orthographics 0.736 0.632 0.680 Table 9 The results for the category Drug for the CRF model settings giving the best average results, compared to the Drug results for the baseline obtained by terminology matching. The results for removing one feature type at a time, as well as for using the inflected tokens instead of the lemmatised forms, are also shown. Evaluated configuration Precision Recall F-score Baseline: Terminology match 0.918 0.702 0.795 Best average CRF settings 0.900 0.832 0.865 Without lemmatisation 0.898 0.822 0.858 Best settings \u2013 current lemma 0.879 0.782 0.828 Best settings \u2013 previous lemma 0.906 0.818 0.860 Best settings \u2013 part-of-speech tagging 0.902 0.827 0.863 Best settings \u2013 terminology match 0.891 0.795 0.840 Best settings \u2013 compound splitting 0.904 0.810 0.855 Best settings \u2013 orthographics 0.902 0.821 0.860 Table 10 The results for the category Body Structure for the CRF model settings giving the best average results, compared to the Body Structure results for the baseline obtained by terminology matching. The results for removing one feature type at a time, as well as for using the inflected tokens instead of the lemmatised forms, are also shown. Evaluated configuration Precision Recall F-score Baseline: Terminology match 0.618 0.739 0.673 Best average CRF settings 0.836 0.808 0.822 Without lemmatisation 0.832 0.794 0.812 Best settings \u2013 current lemma 0.805 0.711 0.755 Best settings \u2013 previous lemma 0.812 0.816 0.814 Best settings \u2013 part-of-speech tagging 0.824 0.798 0.811 Best settings \u2013 terminology match 0.881 0.688 0.773 Best settings \u2013 compound splitting 0.849 0.796 0.821 Best settings \u2013 orthographics 0.836 0.804 0.819 Table 11 The final results of the NER model. The model was trained on the Development subset, using the best features, and thereafter evaluated on the Final Evaluation subset. Finding+Disorder shows the result for the two categories Finding and Disorder merged into one category. A 95% confidence interval is calculated [43, pp. 91\u201392, 94\u201396]. Precision Recall F-score Disorder 0.80 (\u00b10.03) 0.82 (\u00b10.03) 0.81 Finding 0.72 (\u00b10.03) 0.65 (\u00b10.03) 0.69 Drug 0.95 (\u00b10.02) 0.83 (\u00b10.03) 0.88 Body Structure 0.88 (\u00b10.04) 0.82 (\u00b10.05) 0.85 Disorder+Finding 0.80 (\u00b10.02) 0.76 (\u00b10.02) 0.78 Table 12 False positives. The first four classes are confusions between the different entity types. Span error means that the model classified a longer span than what had been annotated. Manual annotation error is an evident case of a mistake made by the human annotator, whereas Borderline case is a case for which it is not evident if the model or the annotator is correct. Abbreviation is an abbreviated word as a false positive. Compound word with one relevant stem is a compound word that does not belong to the category, but which includes a stem belonging to the category or that is a common stem in words belonging to that category. Location is a physical location, such as the abbreviated word infection for department of infection, and the remaining false positives were classified as Incorrect, no other reason. nm stands for not measured. Error type Classified by the model as: Disorder Finding Drug Body Structure Annotated as disorder \u2013 23% 5% 14% Annotated as finding 48% \u2013 7% 15% Annotated as drug 1% 3% \u2013 1% Annotated as body structure 1% 1% 0% \u2013 Manual annotation error 8% nm 49% 56% Span error 12% 13% 0% 1% Borderline case 2% nm 3% 3% Incorrect: Abbreviation 4% nm 11% 0% Incorrect: Compound word with one relevant stem 11% nm 8% 7% Incorrect: Location 5% nm 0% 0% Incorrect: No other reason 9% nm 17% 1% Total number of false positives (classification instances) 138 389 75 71 Table 13 False negatives. The first four classes are confusions between the different entity types. Span error means that the model classified a shorter span than what had been annotated. Manual annotation error is an evident case of a mistake made by the human annotator, whereas Borderline case is a case for which it is not evident if the model or the annotator is correct. Abbreviation is an abbreviated word that was not recognised, Compound word is a compound word that was not recognised and Compound word with an abbreviation is a compound word for which one of its constituents is a compound. Lemmatisation failed is when the word was inflected and for which a probable reason for it not being detected is that the lemmatiser failed to lemmatise it correctly, Misspelling or spelling variant is when an alternative spelling of a term was used or when a term was misspelled, Jargon is when a non-standard version of a term was used and the remaining false negatives were classified as Incorrect, no other reason. nm stands for not measured. Error type Annotated as: Disorder Finding Drug Body Structure Incorrectly classified as disorder \u2013 9% 4% 2% Incorrectly classified as finding 31% \u2013 8% 9% Incorrectly classified as drug 1% 1% \u2013 0% Incorrectly classified as body structure 3% 1% 1% \u2013 Manual annotation error 4% nm 0% 2% Span error 10% 18% 5% 5% Borderline case 3% nm 3% 0% Incorrect: Abbreviation 5% nm 7% 2% Incorrect: Compound word 7% nm 35% 27% Incorrect: Compound word with an abbreviation 7% nm 8% 0% Incorrect: Lemmatisation failed 6% nm 2% 6% Incorrect: Misspelling or spelling variant 5% nm 2% 2% Incorrect: Jargon 8% nm 3% 1% Incorrect: No other reason 9% nm 22% 43% Total number of false negatives (annotation instances) 292 808 155 93 Automatic recognition of disorders, findings, pharmaceuticals and body structures from clinical text: An annotation and machine learning study Maria Skeppstedt a \u204e mariask@dsv.su.se Maria Kvist a b c Gunnar H. Nilsson d Hercules Dalianis a a Department of Computer and Systems Sciences (DSV), Stockholm University, Forum 100, SE-164 40 Kista, Sweden Department of Computer and Systems Sciences (DSV) Stockholm University Forum 100 SE-164 40 Kista Sweden b Department of Clinical Immunology and Transfusion Medicine, Karolinska University Hospital, Stockholm, Sweden Department of Clinical Immunology and Transfusion Medicine Karolinska University Hospital Stockholm Sweden c Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institutet, Sweden Department of Learning, Informatics, Management and Ethics (LIME) Karolinska Institutet Sweden d Department of Neurobiology, Care Sciences and Society, Karolinska Institutet, Stockholm, Sweden Department of Neurobiology Care Sciences and Society Karolinska Institutet Stockholm Sweden \u204e Corresponding author. Fax: +46 (0)8 703 90 25. Graphical abstract Highlights \u2022 Disorders, Findings, Drugs and Body parts were annotated in Swedish clinical text. \u2022 A conditional random fields model was trained to recognise the annotated entity types. \u2022 English clinical entity recognition approaches were also suitable for Swedish. \u2022 Disorder and Finding are more granular categories than those used in most other studies. \u2022 Results for these two separate categories show that this division is meaningful. Abstract Automatic recognition of clinical entities in the narrative text of health records is useful for constructing applications for documentation of patient care, as well as for secondary usage in the form of medical knowledge extraction. There are a number of named entity recognition studies on English clinical text, but less work has been carried out on clinical text in other languages. This study was performed on Swedish health records, and focused on four entities that are highly relevant for constructing a patient overview and for medical hypothesis generation, namely the entities: Disorder, Finding, Pharmaceutical Drug and Body Structure. The study had two aims: to explore how well named entity recognition methods previously applied to English clinical text perform on similar texts written in Swedish; and to evaluate whether it is meaningful to divide the more general category Medical Problem, which has been used in a number of previous studies, into the two more granular entities, Disorder and Finding. Clinical notes from a Swedish internal medicine emergency unit were annotated for the four selected entity categories, and the inter-annotator agreement between two pairs of annotators was measured, resulting in an average F-score of 0.79 for Disorder, 0.66 for Finding, 0.90 for Pharmaceutical Drug and 0.80 for Body Structure. A subset of the developed corpus was thereafter used for finding suitable features for training a conditional random fields model. Finally, a new model was trained on this subset, using the best features and settings, and its ability to generalise to held-out data was evaluated. This final model obtained an F-score of 0.81 for Disorder, 0.69 for Finding, 0.88 for Pharmaceutical Drug, 0.85 for Body Structure and 0.78 for the combined category Disorder+Finding. The obtained results, which are in line with or slightly lower than those for similar studies on English clinical text, many of them conducted using a larger training data set, show that the approaches used for English are also suitable for Swedish clinical text. However, a small proportion of the errors made by the model are less likely to occur in English text, showing that results might be improved by further tailoring the system to clinical Swedish. The entity recognition results for the individual entities Disorder and Finding show that it is meaningful to separate the general category Medical Problem into these two more granular entity types, e.g. for knowledge mining of co-morbidity relations and disorder-finding relations. Keywords Named entity recognition Corpora development Clinical text processing Disorder Finding Swedish 1 Introduction Electronic health records contain valuable information in the form of symptom descriptions, documentation of examinations, diagnostic reasoning and motivations for treatment decisions. Automatic extraction of this information makes it possible to improve applications for patient care documentation, and enables secondary usage of the information in the form of medical knowledge mining. While a subset of the health record information, e.g. medication lists and diagnosis coding, is documented in a structured format, much important information is only available as free text [1]. An automatic summary of the free text part is therefore called for, providing health personnel with the possibility of forming a quick overview of the patient [2,3]. The information contained in health records can also be used for clinical text mining, i.e. to generate new medical knowledge from a large corpus of electronic health records. Syndromic surveillance [4], comorbidity studies [5] and automatic detection of adverse drug reactions [6] are examples of clinical text mining applications. An important component in information extraction from health record text is named entity recognition (NER) of relevant entities mentioned in the text, i.e. the automatic detection of spans of text referring to entities of certain semantic categories [7]. This study focuses on recognition of four entity categories that are particularly relevant for constructing a patient overview as well as for studies of co-morbidity [5,8], disorder and finding co-occurrences [9] and adverse drug reactions [6], namely the categories: Disorder, Finding, Pharmaceutical Drug and Body Structure. There are previous studies on the recognition of clinical entities in English text, but very few studies have been carried out on clinical text written in other languages. The present study was performed on Swedish clinical text, and although both Swedish and English are Germanic languages, NER in Swedish poses additional challenges, as Swedish is more inflective and compounding of words occurs frequently. In addition, medical terminological resources are less extensive for Swedish than for English. Moreover, previous annotation and NER studies have typically combined the two more granular entity categories Disorder and Finding into one more general category, e.g. called Condition or Medical Problem [10\u201312], or have focused only on the entity category Disorder [13,14]. To the best of our knowledge, there is only one previous corpus [15] in which the categories Disorder and Finding are annotated as two separate entity categories. The study describing the creation of this corpus does not, however, investigate the effect of this more granular division. The present study therefore has two specific research questions: \u2022 To what extent is it possible to use the NER methods, which have been successful for English clinical texts, on health record texts written in Swedish? \u2022 To what extent is it possible to separate the more general entity category Medical Problem into the two more granular entity categories Disorder and Finding? 2 Related research There are several studies that describe the creation of corpora annotated for named entities and that measure inter-annotator agreement scores between annotators (Table 1 ). There are also a number of studies in which the created corpora are used for training and/or evaluating NER systems (Table 2 ). The annotation study by Chapman et al. [16] showed that detailed guidelines for annotating Clinical Conditions resulted in a substantially higher F-score than less detailed, but no significant differences in inter-annotator agreement between pairs of physicians and between physicians and lay people were found (but lay people required more training and had a lower ability to retain their annotation skills over time). Within the CLinical E-Science Framework, Roberts et al. [17] observed a higher F-score for lay people (a biologist/linguist and a computational linguist) than for a clinician, when measuring agreement to a constructed consensus set containing annotations for Condition (symptom and diagnosis), Drug or Device and Locus (e.g. anatomical structure or location). Wang [18] measured the inter-annotator agreement between two computational linguists annotating the categories Finding (corresponding to Medical Problem), Substance and Body, while Ogren et al. [19] measured the average agreement between four clinical data retrieval experts for annotating identical spans of text denoting the category Disorder. For the i2b2 medication challenge [20,21], inter-annotator agreement was calculated for annotations of Medication Names on pre-annotated data. No statistically significant differences were observed between pairs of NLP community annotators, pairs of expert annotators, or pairs of experts annotating raw text. One participating group [22] annotated an additional subset of the development data provided for the challenge. Pre-annotation was also applied when annotating the MiPACQ corpus [15] for entity categories including Disorder, Anatomy, Sign or Symptom, and Chemical and Drug. The created corpora have been used as training and evaluation data for machine learning-based NER systems and as evaluation data for rule- and terminology-based systems. An SVM (support vector machine) with uneven margins was trained on a subset of the CLinical E-Science Framework corpus [24], and two studies have been performed on the corpus created by Wang; one using the CRF (conditional random fields) package CRF++ [18] and one combining output from CRF++ with an SVM and an ME (maximum entropy) classifier [10]. All but one of the best performing systems in the i2b2/VA challenge on concepts, assertions, and relations used CRF for concept recognition [23,25]. The best performing system (by de Bruijn et al. [11]) instead used semi-Markov HMM. The second best (by Jiang et al. [12]) found that CRF (CRF++) outperformed SVM, and also managed to improve the results with a rule-based post-processing module. In the i2b2 medication challenge, on the other hand, which included the identification of Medication Names, a majority of the ten top-ranked systems were rule-based [20]. The best performing system (by Patrick and Li [22]) did, however, use CRF++, while the second best (by Doan et al. [26]) was built on terminology matching and a spell checker developed for drug names. This rule-based system was later employed by Doan et al. [27] in an ensemble classifier, together with an SVM and a CRF++ system. On the Ogren et al. [19] corpus, a terminology-based method for recognising disorders by matching to SNOMED CT has been evaluated [14,28], and there is also a terminology-based study for recognising diseases and drugs in Swedish discharge summaries, described by Kokkinakis and Thurin [13], in which the MeSH terminology was used. Typical features used for training the machine learning models were the tokens (sometimes in a stemmed form), orthographics (e.g. number, word, capitalisation), prefixes and suffixes, part-of-speech information, as well as the output of terminology matching, which had a large positive effect in many studies (e.g. [10,18]). Most studies used features extracted from the current and the two preceding and two following tokens, while Roberts et al. [24] used a window size of \u00b11. The best performing system in the i2b2/VA concepts challenge used a very large feature set with a window size of \u00b14, also including character n-grams, word bi/tri/quad-grams and skip-n-grams, as well as sentence, section and document features (e.g. sentence and document length and section headings). In addition, features from semi-supervised learning methods were incorporated, in the form of hierarchical word clusters constructed on unlabelled data [11]. 3 Methods A corpus was first annotated and evaluated. Thereafter, suitable features for the NER task were evaluated and a model was trained using these features and evaluated on held-out data. Finally, an error analysis was carried out. The IOB-encoding [7, pp. 763\u2013764] of the annotated entities was used, as exemplified in Fig. 1 . As machine learning algorithm, the CRF (conditional random fields) [29] implementation CRF++ [30] was chosen, which has been used in many previous clinical NER studies. CRF++ was used as linear chain CRF, in which each output variable is dependent on the previous and subsequent output variable. 3.1 Corpus annotation To study clinical narratives with a variety of disorders, free text sections from clinical notes from an internal medicine emergency unit (from Karolinska University Hospital) were compiled into a corpus. 1 This research has been approved by the Regional Ethical Review Board in Stockholm (Etikpr\u00f6vningsn\u00e4mnden i Stockholm), permission number 2012/834-31/5. 1 Texts with the Assessment sub-heading were chosen as these contain reasoning about findings as well as diagnostic speculations, which means that they contain many mentions of disorders and findings and are particularly suited for a study of these two entity categories. Assessment fields are also interesting in that they form a prototypical example of the difficulties associated with conducting text processing on clinical text, as they are written using a highly telegraphic language, containing many abbreviations and few full sentences. The compiled corpus consisted of 1,148 randomly selected Assessment fields that were extracted from the health record database Stockholm EPR Corpus [31], which contains patient records written in Swedish from the years 2006 to 2008. The definition of the four used annotated entity categories can be summarised as follows: (1) A Disorder is a disease or abnormal condition that is not momentary and that has an underlying pathological process. (2) A Finding is a symptom reported by the patient, an observation made by the physician or the result of a medical examination of the patient. This includes non-pathological findings with medical relevance. 2 The definition for Disorders and Findings is a summary of the definition in the SNOMED CT Style Guide [32]. 2 (3) A Pharmaceutical Drug is a medical drug that is either mentioned with a generic name or trade name, or with other expressions denoting drugs, e.g. drugs expressed by their effect, such as painkiller or sleeping pill. Narcotic drugs used outside of medical care were excluded. (4) A Body Structure is an anatomically defined body part, excluding body fluids and expressions indicating positions on the body. Annotation guidelines were developed by a senior physician with previous experience of annotating clinical text (PH1) and a computational linguist without previous annotation experience (CL). A test annotation of 664 Assessment fields (not included in the 1,148 fields compiled for the final corpus) was performed by PH1, for annotator training as well as for development of the annotation guidelines. Whenever there was an issue not covered by the annotation guidelines, it was discussed and the guidelines were updated accordingly. At first, there were many modifications of the guidelines, but the need for modifications gradually decreased. The final version of the guidelines were reviewed by a second physician, also with previous experience of annotating clinical text (PH2). The following are the most important points of the guidelines 3 The complete guidelines are available at http://dsv.su.se/health/guidelines/. 3 : The shortest possible expression that still fully describes the entity was annotated. Modifiers that, for example, describe severity were therefore excluded, while modifiers describing the type of an entity were included. In the example: The patient experiences a strong stabbing pain in left knee, 4 In Swedish: Patienten k\u00e4nner en kraftigt huggande sm\u00e4rta i v\u00e4nster kn\u00e4. 4 the words strong and left were therefore not annotated, whereas stabbing pain was annotated as a Finding, and knee as a Body Structure. All mentions of any of the four selected classes were annotated, regardless of whether, for example, a Disorder was referred to with an abbreviation or acronym or in a negated or a speculative context, or the person experiencing the Finding was someone other than the patient. The guidelines also included rules for handling the frequent occurrence of compound words. Compound words were not split up into substrings, and therefore, for example, diabetes in diabetesclinic 5 In Swedish: diabetesklinik. 5 was not annotated as a Disorder, whereas the word heartdisease 6 In Swedish: hj\u00e4rtsjukdom. 6 which is a compound denoting a Disorder, was annotated as such. A compound including a treatment with a pharmaceutical drug was, however, classified as belonging to the entity category Pharmaceutical Drug. The definition of Finding was broader than the definition used in other annotation studies, e.g. i2b2 [25], and more closely followed the definition in SNOMED CT, as also non-pathological, medically relevant findings were included. The guidelines did, however, agree with the i2b2 guidelines [25] in that only findings that were explicitly stated (e.g. high blood pressure) were included, whereas test measures (e.g. blood pressure 145/95) were not annotated. PH1 had the role as the main annotator, annotating all notes included in the study. A subset of the notes were independently annotated by PH2, and yet another subset was independently annotated by CL (Table 3 ). To become familiar with the annotation task, PH2 and CL carried out a test annotation on 50 notes. Neither these test annotations, nor the texts annotated by PH1 in the guideline development phase, were included in the constructed corpus. The annotation tool Knowtator [33], a plug-in to Prot\u00e9g\u00e9, was used for all annotations. The doubly annotated notes were used for measuring inter-annotator agreement (and thereby the reliability of the annotations [34]), as well as for constructing a reference standard to use in the final evaluation. Inter-annotator agreement was measured in terms of F-score, as e.g. the frequently used inter-annotator agreement measure kappa cannot be applied on this task that lacks well-defined negative cases [35]. Disagreements were: entities annotated by only one of the annotators, differences in choice of entity category, and differences in the length of annotated text spans. Out of a large subset (25,370 tokens) of the doubly annotated notes shown in Table 3, a sub-corpus was created to use in the evaluation phase of the machine learning study (the Final Evaluation subset). This sub-corpus was compiled by PH1 who resolved each conflicting annotation in the doubly annotated data. A program for presenting and resolving annotations was developed, which presented pairs of conflicting annotations on a sentence level, without revealing who had produced which annotation. PH1 could thereby select one of the presented annotations without knowing who had produced it, thereby minimising bias. The rest of the annotated corpus (the Development subset), was used for feature selection and as training data for the final model. The properties of each of the two subsets are presented in Table 4 . 3.2 Feature selection and evaluation The most frequently used features among previous studies on English clinical text were evaluated, including terminology matching. MeSH, ICD-10 and SNOMED CT are available in Swedish, as well as FASS, which includes a list of pharmaceutical drugs used in Sweden [36]. One of the challenges of clinical NER in Swedish is, however, that medical terminologies are less extensive for Swedish than for English, as e.g. SNOMED CT only contains the preferred term for each concept and lacks synonyms. We have previously developed terminology-based systems for Swedish clinical text, detecting the entities Disorder, Finding and Body Structure [37] as well as Pharmaceutical Drug [38]. For the present study, these systems were extended by adding a more extensive version of MeSH [39] as well as a vocabulary list of general Swedish, extracted from the Swedish Parole corpus, containing non-medical language compiled from e.g. newspaper texts and fiction [40]. All semantic classes in SNOMED CT and MeSH were used as feature values, and tokens matching the FASS vocabulary [36] were given the feature value Pharmaceutical Drug. Tokens matching (the descriptions of) ICD-10 codes in chapter 1\u201317 and 19 (except codes T36\u2013T62.9, which list substances) were assigned the feature value ICD-10-disorder and tokens matching codes in chapter 18 (listing symptoms and clinical findings) were assigned the value ICD-10-finding. Tokens not found in any of the medical resources, but in the vocabulary from Parole, were assigned the feature value Parole, and tokens not found in any terminology were given the value Unknown. A token matching several semantic classes was assigned feature value according to the priority stated in the annotation guidelines (e.g. the SNOMED CT category Qualifier having the highest priority, followed by Body Structure, Disorder, Finding and Pharmaceutical). As described in the previous terminology matching study [37], the SNOMED CT list of body structures was stop word filtered. Another particular challenge for NER in Swedish text compared to English text is the frequent occurrence of compound words. Compound word splitting was therefore added as an additional feature not used in previous clinical NER studies. We implemented a simple compound splitting, dividing words into a maximum of two word constituents if at least one of the constituents was found in one of the vocabulary lists and the other constituent was found when applying fuzzy matching. The fuzzy matching, which used a maximum string distance of one, was motivated by the fact that Swedish compounds are sometimes constructed with an \u2018s\u2019 binding the two constituents, and sometimes constructed by the removal or change of the last vowel in the first constituent. The compound splitting favoured two equally long constituents over one short and one long constituent. The minimum length of a constituent was set to four letters. Compound splitting was applied on tokens not found in any of the vocabulary lists used. If it was possible to divide a token, the constituents as well as the semantic classes found by the terminology matching for the constituents were used as features. A third challenge is that Swedish is more inflective than English, increasing the importance of morphologic normalisation. The Swedish lemmatiser Granska [41] was therefore applied on the texts to obtain lemma forms of tokens to use as features for the machine learning, as well as for being able to use normalised forms for the terminology match. Granska was also used for obtaining part-of-speech information. The following features were added one by one, in the following order: (1) The current token. (2) The lemma form of the current token. (3) The lemma form of surrounding tokens. (4) Part-of-speech of the current token and surrounding tokens. (5) The output of the terminology-based system for the current token and surrounding tokens. (6) Compound splitting for the current token. (7) Orthographic features, i.e. initial upper case, all upper case or no upper case for the current token (Fig. 1). The use of an increasingly larger window size was evaluated for features (2)\u2013(5), as well as fuzzy terminology matching. Features and window sizes that lead to an improved result were retained, whereas the others were not used. For the best feature combination, the CRF++ regularisation hyper-parameter for balancing between under-and over-fitting, was also varied. For all experiments, L2-regularisation was used. We chose to use 30-fold cross validation on the Development subset for selecting features. This is more time-consuming than using the more standard approach of 10-fold cross validation, but has the advantage that a larger proportion of the Development subset is used as training data in each fold, thereby more closely resembling the situation that is being optimised for, that the entire Development subset is available as training data. As the features were added incrementally, with each feature improving the results being retained, figures from this evaluation could not determine how much each individual feature contributed to the results. Therefore, models were constructed, in which one feature type at a time was removed while retaining all other features. 30-Fold cross validation was used also for the evaluation of these models, which were constructed with the best settings and all the best features, except the feature type whose contribution was to be evaluated. 3.3 Final evaluation on held-out data In order to obtain the final results, we evaluated how well a model trained using the best features would perform in a deployment setting. A CRF model was therefore trained on the entire Development subset using the best features (i.e. the features for which best results were achieved in the feature selection process), and this model was then evaluated on the previously unused Final Evaluation subset. 3.4 Error analysis As a final step, an error analysis was carried out. The error analysis was performed on NER classifications obtained when applying 30-fold cross-validation with the best features on the Development subset. Using the Development subset means that it is also possible to treat the Final Evaluation subset as unseen data in future studies. A manual error analysis was carried out by PH1 for the classes Disorder, Drug and Body Structure, while only errors due to incorrect span and to confusion between classes were measured for Finding (due to the large number of instances for this class). 4 Results The results consist of inter-annotator agreement scores for the annotated corpus, as well as results for the feature selection and for the NER model evaluated on held-out data. The results were measured using precision, recall and F-score, all calculated with the CoNLL 2000 script [42]. 4.1 Corpus annotation Inter-annotator agreement between the physicians (PH1 and PH2), between the main physician annotator and the computational linguist (PH1 and CL), and the average results are shown in Table 5 for the four categories, as well as for Disorder and Finding merged into one class. Annotations for the entity Pharmaceutical Drug had the highest agreement for both pairs of annotators, and there was also a relatively high agreement for annotations of Body Structure and Disorder, while the agreement for Finding was lower. Finding was also the only category for which there was a large difference between the two pairs of annotators, with higher agreement between PH1 and CL than between PH1 and PH2. When automatically removing all annotated Findings containing a number for PH2 and CL (to make them better comply with the guidelines), the agreement between the two physicians increased from 0.58 to 0.61, but still remained lower than the agreement between PH1 and CL. In the Development subset, there were a total of 27 unique entity types sometimes annotated as a Disorder and sometimes as a Finding: this was 3% of the total number of unique entity types among annotated Disorders and Findings. Among these, some were equally frequently annotated as a Disorder and as a Finding (e.g. stress and muscle pain), whereas some more often were classified as a Finding (e.g. dizziness/vertigo and tachycardia). In accordance with the annotation guidelines, these entities were annotated as a Finding when they were symptoms of another disorder, and as a Disorder when they were the main medical problem described in the Assessment field. 4.2 Feature selection and evaluation Feature selection was performed through 30-fold cross-validation on the Development subset, and by selecting features that maximised the macro-averaged F-score over the four entity categories. The following features and window sizes gave the best average results: (1) Lemma forms for the current token and the previous token. (2) Part-of-speech for the current token, the following token and the two previous tokens. (3) Terminology match for the current token and the previous token. (4) The compound splitting features for the current token. (5) Orthographic features for the current token. The features used when obtaining the best results are illustrated with boldface in Fig. 1. Fuzzy terminology matching, using a Levenshtein distance of one, as well as larger window sizes for lemma, part-of-speech and terminology match, were evaluated, which, however, gave slightly lower results. The best average results using the selected features are shown in boldface in Table 6 , while the boldfaced figures in Tables 7\u201310 show the results for each individual category. Results were high above the baseline for all categories, with the smallest difference for Pharmaceutical Drug, which had the best baseline. Tables 6\u201310 also show the extent to which each individual feature type contributed to the best results, by presenting results when one feature type at a time was removed, while all other features were retained. That is, e.g. Best settings \u2013 Orthographics shows the results when all the best features except orthographics were used. Without lemmatisation shows the results when using the current and previous token instead of the current and previous lemma. The evaluation showed that the current lemma was the most important feature for all categories, while terminology matching was the second most important feature, except for the category Finding, for which lemmatisation was more important. All other features played a minor role. Results are presented for a CRF++ hyper-parameter of 6, for which the best results were achieved when giving the parameter integer values between 1 and 11 (with the best average F-score of 0.794 at 6 and the lowest F-score of 0.779 at 1). 4.3 Final evaluation on held-out data The best parameter and feature settings were also used when training a model on the entire Development subset. The results when evaluating this model on the held-out Final Evaluation subset are shown in Table 11 . All categories were recognised with an F-score that is in line with the average inter-annotator agreement scores, and the final results were also very close to those achieved on the Development subset during feature selection, which shows that the results obtained during development generalise well on unseen data. 4.4 Error analysis The results of the error analysis are shown in Tables 12 and 13 . Confusions between entity categories (especially between Disorder and Finding), manual annotation errors, an incorrect span and borderline cases (i.e. when it was not evident whether the CRF classifier or the human annotator was correct) were identified error types among false positives as well as among false negatives. An entity span that was longer than the annotated span was counted as a false positive, whereas a too short span was classified as a false negative. Abbreviations were also a source of both false positives and negatives, as were compound words. A compound word that was incorrectly classified as belonging to an entity category had either as one of its constituents a word belonging to that category (e.g. lyme-disease-test 7 In Swedish: borreliaprov. 7 and liver-values 8 In Swedish: leverv\u00e4rden. 8 ), or had as one of its constituents a word that frequently occurred in compound words of this category (e.g. COPD-treatment, 9 In Swedish: KOL-behandling. 9 which was misclassified as a Drug because of the constituent treatment). Compound words were also frequent among false negatives, both compounds of full-length words and compounds with an abbreviation as one of its constituents. Other false negatives were inflected words, which the lemmatiser had failed to lemmatise, misspellings or spelling variants and entities expressed with jargon or non-standard language. Among the false negatives for the category Drug grouped into Incorrect: No other reason were expressions for groups of medicines, for instance Cortisone. 5 Discussion A comparison between the final results of the created NER model and results of previous clinical NER studies answers the research question of whether previous approaches for English are applicable on Swedish clinical text. The achieved NER results, as well as an analysis of the confusion between categories for different annotators and for the NER classifier, also answer the research question as to what extent it is possible to separate the category Medical Problem into the two more granular entity categories Disorder and Finding. 5.1 Application of NER methods on Swedish clinical text Apart from the fact that guidelines for handling compound words are needed, entity annotation of Swedish clinical text does not pose any evident challenges additional to those posed by entity annotation of English text. It is, therefore, not surprising that there are no systematic differences between the inter-annotator agreement figures reported here and the results from previous English studies. Our reported inter-annotator agreement for Pharmaceutical Drug (average F-score 0.90) was better than the average agreements for the category Drug or Device reported by Roberts et al. (F-scores 0.84, 0.32 and 0.59 for three different document types) but slightly lower than the agreement reported by Wang and Patrick for the comparable category Substance (F-score 0.95). Also for Body Structure (average F-score 0.80), the agreement presented here was slightly higher than the figures reported for the category Locus by Roberts et al. (F-scores 0.78, 0.75 and 0.71), but lower than the agreement for Body presented by Wang and Patrick (F-score 0.85). For the combined category Disorder+Finding, our agreement (F-score 0.78, or 0.80 when numerical values were removed) was lower than the agreement for the corresponding categories Clinical Condition reported by Chapman et al. (F-score 0.92) and Finding reported by Wang and Patrick (F-score 0.91). The agreement figures for Condition, reported by Roberts et al. (F-scores 0.81, 0.77, 0.67), however, closely match or are lower than our agreement for Disorder+Finding. Also our inter-annotator agreement for the separate category Disorder (F-score of 0.79) was slightly higher than the agreement reported by Ogren et al. for Disorder (F-score 0.76). Finally, it can be noted that the similarity in inter-annotator agreement figures between the pairs of physicians, and the physician versus the computational linguist, is also in accordance with results from previous studies [16,17]. For NER of the annotated entities on the other hand, differences between English and Swedish and between available terminological resources might affect the results. Fig. 2 gives an overview of NER results from a number of previous studies and compares them to the results achieved here. The results achieved in these previous studies vary, which e.g. could be attributed to variations in the exact definitions of entities, to the type of clinical text that is studied, as well as to the size of the used training data. The best results among these studies for recognising the categories Disease and Drug were achieved by Kokkinakis and Thurin [13] with a rule- and terminology-based method using a restricted vocabulary list. These results, which are better than the results obtained here, can probably be partly explained by their study being carried out on discharge summaries, which are the opposite in terms of style to e.g. Assessment fields. The other rule- and terminology-based system evaluated on discharge summaries (by Doan et al. [26]) obtained slightly lower results for Medication Names than we obtained for the entity Pharmaceutical Drug, and the rule- and terminology-based system evaluated on the corpus created by Ogren et al. (which contains several different clinical text types [14,28]) achieved lower results for Disorder than what was obtained here. There is also a machine learning study by Patrick and Li [22], obtaining results for Medication Names that are very similar to the results we achieved for Pharmaceutical Drug, despite the fact that the study by Patrick and Li was conducted solely on discharge summaries. Doan et al. [26] showed, however, that better results can be achieved using the same corpus; results that exceed those achieved here. Also the i2b2 2010 challenge corpus consists to a large extent of discharge summaries, and the entity Medical Problem was recognised by Jiang et al. [12] with higher precision and recall than we obtained for the comparable category combination Disorder+Finding. The fact that the i2b2 2010 challenge corpus contains a considerably larger set of annotated entities probably had additional positive effects on the results. The two English NER studies that are most similar to the present study, in terms of number of available annotated entities and also in terms of clinical text types, were conducted by Wang and Patrick [10] and by Roberts et al. [24]; Wang and Patrick using intensive care service progress notes and Roberts et al. using a number of different clinical text types. For entity categories similar to Disorder+Finding and Pharmaceutical Drug, Wang and Patrick had a somewhat larger set of annotated entities and also achieved somewhat better results than presented here, while the opposite holds true for Roberts et al., who used a smaller set of annotated data and achieved lower results. For Body Structure, however, both Wang and Patrick and Roberts et al. present lower results than those achieved here, for Wang and Patrick possibly since nested annotations were allowed, making the Body entity more difficult to recognise. It can also be noted that, similar to previous studies, terminology matching proved to be an important feature, while in contrast to most previous studies, a small window size gave the best results. That a small window size was most optimal might be explained by the training data not being large enough for the number of features that is generated when many feature types with a large number of possible values are included. In summary, the results presented here are in general in line with or slightly lower than results for presented previous studies. This difference is, however, probably not primarily caused by the study being conducted on Swedish texts, but could be attributed to the fact that we used a wide definition of entities (as non-pathological findings and pharmaceuticals expressed in general terms were included), a clinical text type for which a highly telegraphic language is used and in which a large variety of disorders and findings occur, and that we had a smaller number of entities in our training data compared to some previous studies. The error analysis shows, however, that some of the identified errors belong to error types that are unlikely to occur in English text. Some of the false negatives caused by entities being expressed with jargon, spelling variants and abbreviations might have been avoided with more extensive terminology resources, which are available for English. There were also some errors caused by the lemmatiser failing to lemmatise inflected words, which indicates that an adaption of the lemmatiser to the medical domain is needed. Compound words were frequent among false negatives, including compounds with an abbreviation as one of its constituents. Compound splitting improved the average recall by 1.7 percentage points, but the implemented compound splitter was dependent on finding a constituent in either the Parole corpus or in a medical terminology, which had the effect that compounds containing abbreviations or medical terms not included in a terminology were not split. Also the heuristics of having a minimum length of four letters for a constituent prevented compound splitting of words containing an abbreviation. However, compound splitting also reduced the average precision by 0.5 percentage points, indicating that an improved splitting is not enough, but has to be combined with, e.g. a larger training set. The false positives for compound words also show that the entity recognition task defined here might be somewhat more difficult than the task defined for previous English studies, as e.g. the word liver in liver-values, would be defined as a Body Structure in previous English studies, whereas we did not define constituents of compound words as belonging to an entity category. 5.2 Division into the two categories Disorder and Finding The inter-annotator agreement scores, as well as the results from the NER model, show that the two categories Disorder and Finding are more difficult to differentiate than the other categories. The agreement for the category Finding was low for both pairs of annotators, 0.58 for PH1-PH2 and 0.73 for PH1-CL, while the agreement for Disorder was higher with an F-score of 0.77 for PH1-PH2 and 0.80 for PH1-CL. Merging the two classes Disorder and Finding, resulted in higher agreement between the two physicians (F-score 0.72) than would be the case for a weighted average of the two classes (F-score 0.65). For the physician and the computational linguist, the result of merging the classes had an even larger effect, with an F-score of 0.84. These figures show that disagreements between annotators were often with regard to which of these two categories to use. However, in the Development subset, annotated by the main annotator, only 3% of the unique entity types among annotated Disorders and Findings were annotated as Disorders in some contexts and as Findings in other contexts, which supports the meaningfulness of this more granular division. The error analysis of the NER model shows that in many of the cases for which the system failed to detect a Disorder, it had instead classified the entity as a Finding (31% of the false negatives for Disorder), and vice versa (9% of the false negatives for Finding). Whether it is meaningful to divide Medical Problem into the two more granular categories Disorder and Finding is dependent on the intended application of the NER system. An automatically generated high-level patient summary might for instance place high demands on recall of the category Disorder. This means that recognised entities classified as belonging to the category Finding (or entities for which the model is uncertain of whether to classify it as a Finding or a Disorder) also ought to be included in the summary, thereby boosting recall for included Disorders. The results achieved, i.e. recognising the category Disorder with a precision of 80% and Finding with a precision of 72% are, however, likely to be high enough for it to be meaningful to make a distinction between these categories when mining for new medical knowledge (e.g. since known co-morbidities have been successfully extracted from structured data that contains inaccuracies [8,44]). Having access to a NER system that distinguishes between disorders and findings makes it possible to separately mine for co-morbidity relations and disorder-finding relations. 5.3 Limitations The inter-annotator agreement shows how reliable the evaluation data is for measuring the performance of the evaluated system, and gives an indication of the difficulty of recognising the four entities, especially of their relative difficulty. The agreement scores cannot, however, be used as an absolute upper ceiling for the performance of the NER system, since the system mimics the behaviour of one single annotator, which might be easier than agreeing on how to annotate given the annotation guidelines. Another limitation concerns the involvement of the authors of this paper in the annotation and feature experiments. PH1 and PH2 might have been biased towards producing annotations which they suspected would be easier for the NER system to detect. However, as neither of these two annotators were involved in the development of the NER system, it is unlikely that they knew enough of the system for this possible bias to have any large effects on the results. CL, on the other hand, who annotated half of the evaluation data, was the main person responsible for the NER experiments. However, as only PH1 was involved in the construction of the final version of the evaluation data, the risk that CL biased the evaluation data is likely to be a minor one. 5.4 Future directions Future work includes further improving the NER system (e.g. by further adapting it to clinical Swedish by improved compound splitting and lemmatisation) as well as adapting the system to other clinical text types. As the relatively small size of the training data might have influenced the results, a possible future direction could be to provide more annotated data. However, since annotating data is costly, a more important contribution would be to study how the results can be improved, or how the constructed model can be applied to another clinical domain, with a minimum of additional data. This could, for instance, be achieved by using features from unsupervised methods [11] or by cleverly selecting the data to annotate by using active learning [45]. 6 Conclusion This study has shown that clinical NER methods previously applied on English are successful also on Swedish clinical text. The category Disorder was recognised with an F-score of 0.81; Finding with an F-score of 0.69; Drug with an F-score of 0.88; Body Structure with an F-score of 0.85; and the combination Disorder+Finding with an F-score of 0.78. These results are in line with, or slightly lower than, published results for similar studies on English texts. The slightly lower results achieved here could probably to a large extent be explained by the smaller size of the training data and by the fact that our study was conducted on clinical text extracted from Assessment fields and that wide definitions of the entity categories were used. A small proportion of the errors made by the NER system were, however, errors less likely to occur in an English text, such as the lemmatiser failing to lemmatise medical terms, errors caused by compounding and by compounds containing an abbreviated constituent. The smaller size of the available Swedish vocabularies might also have affected the results. The study has also shown that a distinction between the two more granular categories Disorder and Finding is sometimes difficult to make, but that the NER results for the two separate categories are high enough for this separation to be meaningful for some applications. A NER system separating disorders and findings could, for instance, be used for knowledge mining of co-morbidity relations and of disorder-finding relations. Authors\u2019 contributions MS was responsible for the overall design of the study and for the NER part, while MK was responsible for the annotation part. MK developed the annotation guidelines with assistance from MS, and GN revised them. MK was the main annotator, while MS and GN annotated subsets of the data. MK also carried out the error analysis. MS designed and carried out the NER experiments, with feedback from HD. HD drafted parts of the background, while MS drafted the rest of the manuscript with feedback from the other authors. All authors read and approved the final manuscript. Acknowledgments This work was partly supported by the Swedish Foundation for Strategic Research through the project High-Performance Data Mining for Drug Effect Detection (Ref. No. IIS11-0053) at Stockholm University, Sweden. It was also partly supported by V\u00e5rdal Foundation. We are very grateful to the reviewers for their many detailed and constructive comments, and we would like to thank Aron Henriksson and Magnus Ahltorp for fruitful discussions on the design of the study. References [1] S.M. Meystre G.K. Savova K.C. Kipper-Schuler J.F. Hurdle Extracting information from textual documents in the electronic health record: a review of recent research Yearb Med Inform 2008 128 144 [2] Hallett C, Power R, Scott D. Summarisation and visualisation of e-health data repositories. In: UK E-science all-hands meeting. Nottingham, UK. [3] Kvist M, Skeppstedt M, Velupillai S, Dalianis H. Modeling human comprehension of Swedish medical records for intelligent access and summarization systems \u2013 future vision, a physician\u2019s perspective. In: Proceedings of SHI 2011, scandinavian health informatics meeting. p. 31\u20135. [4] W.W. Chapman L.M. Christensen M.M. Wagner P.J. Haug O. Ivanov J.N. Dowling Classifying free-text triage chief complaints into syndromic categories with natural language processing Artif Intell Med 33 2005 31 40 [5] F.S. Roque P.B. Jensen H. Schmock M. Dalgaard M. Andreatta T. Hansen Using electronic patient records to discover disease correlations and stratify patient cohorts PLoS Comput Biol 7 2011 [6] R. Eriksson P.B. Jensen S. Frankild L.J. Jensen S. Brunak Dictionary construction and identification of possible adverse drug events in danish clinical narrative text J Am Med Inform Assoc 2013 [7] D. Jurafsky J.H. Martin 2nd ed. Speech and language processing: an introduction to natural language processing, computational linguistics and speech recognition 2008 Prentice Hall [8] Tanushi H, Dalianis H, Nilsson G. Calculating prevalence of comorbidity and comorbidity combinations with diabetes in hospital care in Sweden using a health care record database. In: LOUHI, third international workshop on health document text mining and information analysis. p. 59\u201365. [9] H. Cao M. Markatou G.B. Melton M.F. Chiang G. Hripcsak Mining a clinical data warehouse to discover disease-finding associations using co-occurrence statistics AMIA Annu Symp Proc 2005 106 110 [10] Wang Y, Patrick J. Cascading classifiers for named entity recognition in clinical notes. In: Proceedings of the workshop on biomedical information extraction. p. 42\u20139. [11] B. de Bruijn C. Cherry S. Kiritchenko J.D. Martin X. Zhu Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010 J Am Med Inform Assoc 18 2011 557 562 [12] M. Jiang Y. Chen M. Liu S.T. Rosenbloom S. Mani J.C. Denny A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries J Am Med Inform Assoc 2011 [13] Kokkinakis D, Thurin A. Identification of entity references in hospital discharge letters. In: Proceedings of the 16th nordic conference of computational linguistics (NODALIDA). Estonia. p. 329\u201332. [14] G.K. Savova J.J. Masanz P.V. Ogren J. Zheng S. Sohn K.C. Kipper-Schuler Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications J Am Med Inform Assoc 17 2010 507 513 [15] D. Albright A. Lanfranchi A. Fredriksen W.F. Styler 4th C. Warner J.D. Hwang Towards comprehensive syntactic and semantic annotations of the clinical narrative J Am Med Inform Assoc 20 2013 922 930 10.1136/amiajnl-2012-001317 Epub 2013 Jan 25 [16] W.W. Chapman J.N. Dowling G. Hripcsak Evaluation of training with an annotation schema for manual annotation of clinical conditions from emergency department reports Int J Med Inform 77 2008 107 113 Epub 2007 Feb 20 [17] A. Roberts R. Gaizauskas M. Hepple G. Demetriou Y. Guo I. Roberts Building a semantically annotated corpus of clinical texts J Biomed Inform 42 2009 950 966 [18] Y. Wang Annotating and recognising named entities in clinical notes Proceedings of the ACL-IJCNLP 2009 student research workshop, ACLstudent \u201909 2009 Association for Computational Linguistics Stroudsburg (PA), USA 18 26 [19] P. Ogren G. Savova C. Chute Constructing evaluation corpora for automated clinical named entity recognition Proceedings of the sixth international language resources and evaluation (LREC\u201908) 2008 European Language Resources Association (ELRA) Marrakech, Morocco 3143 3149 [20] O. Uzuner I. Solti E. Cadag Extracting medication information from clinical text J Am Med Inform Assoc 17 2010 514 518 [21] \u00d6. Uzuner I. Solti F. Xia E. Cadag Community annotation experiment for ground truth generation for the i2b2 medication challenge J Am Med Inform Assoc 17 2010 519 523 [22] J. Patrick M. Li High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge J Am Med Inform Assoc 17 2010 524 527 [23] \u00d6. Uzuner B. South S. Shen S. DuVall 2010 i2b2/va Challenge on concepts, assertions, and relations in clinical text J Am Med Inform Assoc 18 2011 552 556 [24] A. Roberts R. Gaizasukas M. Hepple Y. Guo Combining terminology resources and statistical methods for entity recognition: an evaluation Proceedings of the sixth international conference on language resources and evaluation (LREC\u201908) 2008 European Language Resources Association (ELRA) Marrakech, Morocco 2974 2979 [25] i2b2/VA, 2010 i2b2/va challenge evaluation, concept annotation guidelines; 2010 <https://www.i2b2.org/NLP/Relations/assets/Concept%20Annotation%20Guideline.pdf>. [26] S. Doan L. Bastarache S. Klimkowski J.C. Denny H. Xu Integrating existing natural language processing tools for medication extraction from discharge summaries J Am Med Inform Assoc 17 2010 528 531 [27] S. Doan N. Collier H. Xu H.D. Pham M.P. Tu Recognition of medication information from discharge summaries using ensembles of classifiers BMC Med Inform Dec Mak 12 2012 36 [28] Kipper-Schuler K, Kaggal V, Masanz JJ, Ogren PV, Savova GK. System evaluation on a named entity corpus from clinical notes. In: Proceedings of the sixth international conference on language resources and evaluation (LREC\u201908). Marrakech, Morocco. p. 3007\u201311. [29] J. Lafferty A. McCallum F. Pereira Conditional random fields: probabilistic models for segmenting and labeling sequence data Proceedings of the 18th international conference on machine learning 2001 Morgan Kaufman San Francisco, CA 282 289 [30] Kudo T. CRF++: yet another CRF toolkit; 2012 <http://crfpp.sourceforge.net/> [accessed 15.06.12]. [31] Dalianis H, Hassel M, Velupillai S. The Stockholm EPR corpus \u2013 characteristics and some initial findings. In: 14th International symposium for health information management research on proceedings of ISHIMR 2009, evaluation and implementation of e-health and health information initiatives: international perspectives. Kalmar, Sweden. p. 243\u20139. [32] International Health Terminology Standards Development Organisation, IHTSDO, SNOMED CT style guide: clinical findings; 2008 <Http://www.ihtsdo.org> [accessed 24.01.11]. [33] P.V. Ogren Knowtator: a prot\u00e9g\u00e9 plug-in for annotated corpus construction Proceedings of the 2006 conference of the North American chapter of the association for computational linguistics on human language technology 2006 Association for Computational Linguistics Morristown (NJ), USA 273 275 [34] R. Artstein M. Poesio Inter-coder agreement for computational linguistics Comput Linguist 34 2008 555 596 [35] G. Hripcsak A.S. Rothschild Technical brief: agreement, the f-measure, and reliability in information retrieval J Am Med Inform Assoc 12 2005 296 298 [36] FASS. Fass.se; 2012 <http://www.fass.se> [accessed 27.08.12]. [37] M. Skeppstedt M. Kvist H. Dalianis Rule-based entity recognition and coverage of SNOMED CT in Swedish clinical text N. Calzolari K. Choukri T. Declerck M.U. Do\u011fan B. Maegaard J. Mariani Proceedings of the eight international conference on language resources and evaluation (LREC\u201912) 2012 European Language Resources Association (ELRA) Istanbul, Turkey 1250 1257 [38] ul Muntaha S, Skeppstedt M, Kvist M, Dalianis H. Entity recognition of pharmaceutical drugs in Swedish clinical text. In: Proceedings of SLTC 2012 the fourth swedish language technology conference. p. 77\u20138. [39] Karolinska Institutet, Hur man anv\u00e4nder den svenska MeSHen (In Swedish, translated as: How to use the Swedish MeSH); 2012 <http://mesh.kib.ki.se/swemesh/manual_se.html> [accessed 10.03.12]. [40] Gellerstam M, Cederholm Y, Rasmark T. The bank of Swedish. In: The 2nd international conference on language resources and evaluation, LREC 2000. Athens, Greece. p. 329\u201333. [41] J. Carlberger V. Kann Implementing an efficient part-of-speech tagger Softw\u2013Pract Exper 29 1999 815 832 [42] CoNLL-2000; 2000 <http://www.cnts.ua.ac.be/conll2000/chunking/>. [43] M.J. Campbell D. Machin S.J. Walters Medical statistics: a textbook for the health sciences 4th ed. 2007 Wiley Chichester [44] Socialstyrelsen. Diagnosgranskningar utf\u00f6rda i Sverige 1997\u20132005 samt r\u0227d inf\u00f6r granskning; 2006. [45] Settles B, Craven M. An analysis of active learning strategies for sequence labeling tasks. In: Proceedings of the conference on empirical methods in natural language processing, EMNLP \u201908. Stroudsburg (PA), USA: Association for Computational Linguistics; 2008. p. 1070\u20139.", "scopus-id": "84902545076", "pubmed-id": "24508177", "coredata": {"eid": "1-s2.0-S1532046414000148", "dc:description": "Abstract Automatic recognition of clinical entities in the narrative text of health records is useful for constructing applications for documentation of patient care, as well as for secondary usage in the form of medical knowledge extraction. There are a number of named entity recognition studies on English clinical text, but less work has been carried out on clinical text in other languages. This study was performed on Swedish health records, and focused on four entities that are highly relevant for constructing a patient overview and for medical hypothesis generation, namely the entities: Disorder, Finding, Pharmaceutical Drug and Body Structure. The study had two aims: to explore how well named entity recognition methods previously applied to English clinical text perform on similar texts written in Swedish; and to evaluate whether it is meaningful to divide the more general category Medical Problem, which has been used in a number of previous studies, into the two more granular entities, Disorder and Finding. Clinical notes from a Swedish internal medicine emergency unit were annotated for the four selected entity categories, and the inter-annotator agreement between two pairs of annotators was measured, resulting in an average F-score of 0.79 for Disorder, 0.66 for Finding, 0.90 for Pharmaceutical Drug and 0.80 for Body Structure. A subset of the developed corpus was thereafter used for finding suitable features for training a conditional random fields model. Finally, a new model was trained on this subset, using the best features and settings, and its ability to generalise to held-out data was evaluated. This final model obtained an F-score of 0.81 for Disorder, 0.69 for Finding, 0.88 for Pharmaceutical Drug, 0.85 for Body Structure and 0.78 for the combined category Disorder+Finding. The obtained results, which are in line with or slightly lower than those for similar studies on English clinical text, many of them conducted using a larger training data set, show that the approaches used for English are also suitable for Swedish clinical text. However, a small proportion of the errors made by the model are less likely to occur in English text, showing that results might be improved by further tailoring the system to clinical Swedish. The entity recognition results for the individual entities Disorder and Finding show that it is meaningful to separate the general category Medical Problem into these two more granular entity types, e.g. for knowledge mining of co-morbidity relations and disorder-finding relations.", "openArchiveArticle": "false", "prism:coverDate": "2014-06-30", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/3.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046414000148", "dc:creator": [{"@_fa": "true", "$": "Skeppstedt, Maria"}, {"@_fa": "true", "$": "Kvist, Maria"}, {"@_fa": "true", "$": "Nilsson, Gunnar H."}, {"@_fa": "true", "$": "Dalianis, Hercules"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046414000148"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046414000148"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(14)00014-8", "prism:volume": "49", "prism:publisher": "The Authors. Published by Elsevier Inc.", "dc:title": "Automatic recognition of disorders, findings, pharmaceuticals and body structures from clinical text: An annotation and machine learning study", "prism:copyright": "Copyright \u00a9 2014 The Authors. Published by Elsevier Inc.", "openaccess": "1", "prism:issn": "15320464", "dcterms:subject": [{"@_fa": "true", "$": "Named entity recognition"}, {"@_fa": "true", "$": "Corpora development"}, {"@_fa": "true", "$": "Clinical text processing"}, {"@_fa": "true", "$": "Disorder"}, {"@_fa": "true", "$": "Finding"}, {"@_fa": "true", "$": "Swedish"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "openaccessSponsorType": "Author", "prism:pageRange": "148-158", "prism:endingPage": "158", "prism:coverDisplayDate": "June 2014", "prism:doi": "10.1016/j.jbi.2014.01.012", "prism:startingPage": "148", "dc:identifier": "doi:10.1016/j.jbi.2014.01.012", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "high", "@height": "1402", "@width": "1459", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "190083", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "767", "@width": "2559", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "220753", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "805", "@width": "2213", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-fx1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "164032", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "316", "@width": "329", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "22356", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "173", "@width": "578", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "22603", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "182", "@width": "500", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-fx1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24855", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "163", "@width": "170", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4652", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "66", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2808", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "80", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046414000148-fx1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3366", "@ref": "fx1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84902545076"}}