{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608008000026", "dc:identifier": "doi:10.1016/j.neunet.2007.12.046", "eid": "1-s2.0-S0893608008000026", "prism:doi": "10.1016/j.neunet.2007.12.046", "pii": "S0893-6080(08)00002-6", "dc:title": "Boosting random subspace method ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "21", "prism:issueIdentifier": "9", "prism:startingPage": "1344", "prism:endingPage": "1362", "prism:pageRange": "1344-1362", "prism:number": "9", "dc:format": "application/json", "prism:coverDate": "2008-11-30", "prism:coverDisplayDate": "November 2008", "prism:copyright": "Copyright \u00a9 2008 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Garc\u00eda-Pedrajas, Nicol\u00e1s"}, {"@_fa": "true", "$": "Ortiz-Boyer, Domingo"}], "dc:description": "\n               Abstract\n               \n                  In this paper we propose a boosting approach to random subspace method (RSM) to achieve an improved performance and avoid some of the major drawbacks of RSM. RSM is a successful method for classification. However, the random selection of inputs, its source of success, can also be a major problem. For several problems some of the selected subspaces may lack the discriminant ability to separate the different classes. These subspaces produce poor classifiers that harm the performance of the ensemble. Additionally, boosting RSM would also be an interesting approach for improving its performance.\n                  Nevertheless, the application of the two methods together, boosting and RSM, achieves poor results, worse than the results of each method separately. In this work, we propose a new approach for combining RSM and boosting. Instead of obtaining random subspaces, we search subspaces that optimize the weighted classification error given by the boosting algorithm, and then the new classifier added to the ensemble is trained using the obtained subspace. An additional advantage of the proposed methodology is that it can be used with any classifier, including those, such as \n                        k\n                      nearest neighbor classifiers, that cannot use boosting methods easily.\n                  The proposed approach is compared with standard AdaBoost and RSM showing an improved performance on a large set of 45 problems from the UCI Machine Learning Repository. An additional study of the effect of noise on the labels of the training instances shows that the less aggressive versions of the proposed methodology are more robust than AdaBoost in the presence of noise.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Random subspace method"}, {"@_fa": "true", "$": "Boosting"}, {"@_fa": "true", "$": "Classification"}, {"@_fa": "true", "$": "Ensemble of classifiers"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608008000026", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608008000026", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "54449084620", "scopus-eid": "2-s2.0-54449084620", "pubmed-id": "18272334", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/54449084620", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20080106", "$": "2008-01-06"}}}}}