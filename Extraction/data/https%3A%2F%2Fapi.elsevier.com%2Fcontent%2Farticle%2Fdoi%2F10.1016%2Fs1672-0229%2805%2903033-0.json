{"scopus-eid": "2-s2.0-33744992230", "originalText": "serial JL 273534 291210 291856 31 90 Genomics, Proteomics & Bioinformatics GENOMICSPROTEOMICSBIOINFORMATICS 2016-11-28 2016-11-28 2016-11-28 2016-11-28 2016-11-22T09:53:09 1-s2.0-S1672022905030330 S1672-0229(05)03033-0 S1672022905030330 10.1016/S1672-0229(05)03033-0 S300 S300.1 FULL-TEXT 1-s2.0-S1672022905X03047 2016-11-28T16:10:19.412081-05:00 0 0 20050101 20051231 2005 2016-11-28T13:12:47.75838Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor primabst pubtype ref 1672-0229 16720229 false 3 3 4 4 Volume 3, Issue 4 8 238 241 238 241 2005 2005 2005-01-01 2005-12-31 2005 article fla Copyright \u00a9 2005 Beijing Institute of Genomics, the Chinese Academy of Sciences and the Genetics Society of China. Production and hosting by Elsevier B.V. CONSTRUCTINGSUPPORTVECTORMACHINEENSEMBLESFORCANCERCLASSIFICATIONBASEDPROTEOMICPROFILING MAO Y Introduction Algorithm Results and Discussion Conclusion Acknowledgements References VALENTINI 2004 461 466 G BERTONI 2005 535 539 A KUNCHEVA 2002 245 258 L WINDEATT 2005 21 36 T ZHOU 2002 239 263 Z GUYON 2002 389 422 I MAO 2005 160 171 Y MAO 2005 961 973 Y MAO 2006 65 72 Y VAPNIK 1999 V NATURESTATISTICALLEARNINGTHEORY LILIEN 2003 925 946 R MAOX2005X238 MAOX2005X238X241 MAOX2005X238XY MAOX2005X238X241XY Full 2016-11-11T10:49:51Z FundingBody Beijing Institute of Genomics, Chinese Academy of Sciences http://creativecommons.org/licenses/by-nc-nd/4.0/ OA-Window item S1672-0229(05)03033-0 S1672022905030330 1-s2.0-S1672022905030330 10.1016/S1672-0229(05)03033-0 273534 2016-11-28T16:10:19.412081-05:00 2005-01-01 2005-12-31 1-s2.0-S1672022905030330-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/MAIN/application/pdf/d3bff4f52d56c0d9b858a908548c18cc/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/MAIN/application/pdf/d3bff4f52d56c0d9b858a908548c18cc/main.pdf main.pdf pdf true 458875 MAIN 4 1-s2.0-S1672022905030330-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/PREVIEW/image/png/a7b8e37d8512d38204d4b37514ba5f5c/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/PREVIEW/image/png/a7b8e37d8512d38204d4b37514ba5f5c/main_1.png main_1.png png 63397 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1672022905030330-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/gr1/THUMBNAIL/image/gif/c3b006a23db6a34c1e7b28a70d545278/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/gr1/THUMBNAIL/image/gif/c3b006a23db6a34c1e7b28a70d545278/gr1.sml gr1 gr1.sml sml 14691 115 219 IMAGE-THUMBNAIL 1-s2.0-S1672022905030330-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/gr2/THUMBNAIL/image/gif/37c6eed4aa53bcc988eeed07ec366e24/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/gr2/THUMBNAIL/image/gif/37c6eed4aa53bcc988eeed07ec366e24/gr2.sml gr2 gr2.sml sml 15117 120 219 IMAGE-THUMBNAIL 1-s2.0-S1672022905030330-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/gr1/DOWNSAMPLED/image/jpeg/6c3b800d4f4a09d84a3d090e6ec94d8b/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/gr1/DOWNSAMPLED/image/jpeg/6c3b800d4f4a09d84a3d090e6ec94d8b/gr1.jpg gr1 gr1.jpg jpg 72993 394 752 IMAGE-DOWNSAMPLED 1-s2.0-S1672022905030330-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/gr2/DOWNSAMPLED/image/jpeg/dd1fbe0f48ce50382d01b9d126f39299/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/gr2/DOWNSAMPLED/image/jpeg/dd1fbe0f48ce50382d01b9d126f39299/gr2.jpg gr2 gr2.jpg jpg 77315 412 752 IMAGE-DOWNSAMPLED 1-s2.0-S1672022905030330-si0001.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/8aeb405e2f5fa59bc9da03cf3da59a62/si0001.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/8aeb405e2f5fa59bc9da03cf3da59a62/si0001.gif si0001 si0001.gif gif 1818 24 222 ALTIMG 1-s2.0-S1672022905030330-si0002.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/e18ccafd6755b34b2d257c6c634a82c7/si0002.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/e18ccafd6755b34b2d257c6c634a82c7/si0002.gif si0002 si0002.gif gif 1690 30 256 ALTIMG 1-s2.0-S1672022905030330-si0003.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/b33a275ff48a95fe6be90dcd6fdc718c/si0003.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/b33a275ff48a95fe6be90dcd6fdc718c/si0003.gif si0003 si0003.gif gif 1338 30 152 ALTIMG 1-s2.0-S1672022905030330-si0004.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/4eddf4e0d9f81f4c8fe268e6eb8ca4bc/si0004.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/4eddf4e0d9f81f4c8fe268e6eb8ca4bc/si0004.gif si0004 si0004.gif gif 462 26 228 ALTIMG 1-s2.0-S1672022905030330-si0005.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/481c8578f93e1ced9848a276cbcfe01b/si0005.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/481c8578f93e1ced9848a276cbcfe01b/si0005.gif si0005 si0005.gif gif 123 26 21 ALTIMG 1-s2.0-S1672022905030330-si0006.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/663a64fb9d2e6a676ffa515c53905215/si0006.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/663a64fb9d2e6a676ffa515c53905215/si0006.gif si0006 si0006.gif gif 120 26 21 ALTIMG 1-s2.0-S1672022905030330-si0007.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/2df7d2d73d73688f34f7724a3d34e447/si0007.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/2df7d2d73d73688f34f7724a3d34e447/si0007.gif si0007 si0007.gif gif 120 26 21 ALTIMG 1-s2.0-S1672022905030330-si0008.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1672022905030330/STRIPIN/image/gif/e0e45a89eeafc57bac0aa62693aa527c/si0008.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1672022905030330/STRIPIN/image/gif/e0e45a89eeafc57bac0aa62693aa527c/si0008.gif si0008 si0008.gif gif 117 26 21 ALTIMG GPB 30033 S1672-0229(05)03033-0 10.1016/S1672-0229(05)03033-0 Beijing Institute of Genomics, the Chinese Academy of Sciences and the Genetics Society of China \u00a9 2005 Beijing Institute of Genomics, the Chinese Academy of Sciences and the Genetics Society of China. Fig. 1 Performance comparisons of Majority Voting and double-layer fusion strategy with ranked classifiers on training set and test set from Dataset 1. A. Classification analysis on training set. The optimal ensemble size is indicated by 10-fold cross-validation. B. Classification analysis on test set. The performance of ensemble with optimal ensemble size is indicated. Fig. 1 Fig. 2 Performance comparisons of Majority Voting and double-layer fusion strategy with ranked classifiers on training set and test set from Dataset 2. A. Classification analysis on training set. The optimal ensemble size is indicated by 10-fold cross-validation. B. Classification analysis on test set. The performance of ensemble with optimal ensemble size is indicated. Fig. 2 Article Constructing Support Vector Machine Ensembles for Cancer Classification Based on Proteomic Profiling Yong Mao 1 * ymao@iipc.zju.edu.cn Xiao-Bo Zhou 2 Dao-Ying Pi 1 You-Xian Sun 1 1 National Laboratory of Industrial Control Technology, Institute of Modern Control Engineering, Zhejiang University, Hangzhou 310027, China National Laboratory of Industrial Control Technology Institute of Modern Control Engineering Zhejiang University Hangzhou 310027 China 2 Harvard Center for Neurodegeneration and Repair, Harvard Medical School and Brigham and Women\u2019s Hospital, Harvard Medical School, Harvard University, Boston, MA 02115, USA Harvard Center for Neurodegeneration and Repair Harvard Medical School and Brigham and Women\u2019s Hospital Harvard Medical School Harvard University Boston MA 02115 USA * Corresponding author. In this study, we present a constructive algorithm for training cooperative support vector machine ensembles (CSVMEs). CSVME combines ensemble architecture design with cooperative training for individual SVMs in ensembles. Unlike most previous studies on training ensembles, CSVME puts emphasis on both accuracy and collaboration among individual SVMs in an ensemble. A group of SVMs selected on the basis of recursive classifier elimination is used in CSVME, and the number of the individual SVMs selected to construct CSVME is determined by 10-fold cross-validation. This kind of SVME has been tested on two ovarian cancer datasets previously obtained by proteomic mass spectrometry. By combining several individual SVMs, the proposed method achieves better performance than the SVME of all base SVMs. Key words support vector machine ensemble (SVME) design constructive approach proteomic profiling cancer diagnosis Introduction Biomarker expression data are usually characterized by a small number of sample vectors of high dimension, which makes it very difficult to be treated with many kinds of single classifiers. Up to now, a possible approach to reduce the dimensionality consists in applying straightforward statistical feature selection operation. Ensemble methods based on re-sampling technique are addressed to solve problems arising from small samples and biological variability of the data (1). Ensembles consisting of a certain number of single classifiers outperform a single classifier greatly in terms of classification accuracy (1, 2). In recent studies (3\u20135), the generation performance of an ensemble classifier mainly depends on its base learners\u2019 classification accuracy and relativity. Therefore, how to choose a group of single classifiers to compose a high-powered ensemble is a hot topic in this field. In the present study, we propose a constructive ensemble algorithm based on double-layer hierarchical fusion strategy, that is, the outputs of all base learners are combined together by a specific single classifier. Upon the fusion strategy, the support vector machine recursive feature elimination (SVM-RFE) method (6) is adopted to produce a rank of base learners by their contributions to the upper-layer decision machine. The first several base learners in this rank are selected and the ensemble size (or the number of selected base learners) is given by 10-fold cross-validation. Ensembles constructed by our method not only have relatively simpler structures but also have better performance than those constructed by bagging. This algorithm has been tested on two ovarian cancer datasets previously obtained by proteomic mass spectrometry (MS). Algorithm The training set of biomarker expression is represented as Gtr = {(x i ; y i )| i = 1, 2,\u2026, l}, where x i \u2208 R d is a d-dimensional vector, in which every dimension corresponds to the expression data from a specific biomarker, and y i \u2208 {\u22121, +1} represents the class label, that is, which class the sample belongs to. A total of K replicate training sets {Gtr bootstrap\u2212k \u2502k = 1, 2,\u2026, K} are produced independently by bootstrap technique. Each replicate training set is used to train a certain SVM; the base learners used to constitute the lower layer of ensemble will be selected from these K SVMs. A new SVM is trained to fuse the output of these K SVMs, and these (K+1) SVMs form a hierarchical structure. Using f k to be the decision function of the k th SVM in the lower layer and F to be the decision function of the SVM in the upper layer, the final decision value of a given sample x i is determined as: (1) D ( x i ) = F ( f 1 ( x i ) , f 1 ( x i ) , \u2026 , f K ( x i ) ) The experimental results from previous studies (3, 4) indicate that most gain of ensemble\u2019s performance comes from an optimal combination of several specific base learners. In our method, the optimal combination is realized by the SVM in the upper layer, where the focus is how to select these specific base learners. According to Equation (1), f 1(x i ), f 1(x i ),\u2026, f K (x i ) are regarded as the K features or inputs of the upper-layer decision machine. Therefore, to choose a group of base learners for constructing ensembles means to choose a group of most discriminative features for the upper-layer decision machine. The SVM-RFE method proposed by Guyon et al. (6) has shown sound performance on bio-feature selection in bioinformatics (7, 8) and key variable identification in chemical industrial process (9). In the present study, this method is adopted to rank the lower-layer decision machines according to their importance to the upper-layer decision machine. In brief, RFE is a circulation procedure for eliminating features by a criterion. It consists of three steps: (1) train the classifier; (2) compute the ranking criterion; (3) remove the features with the smallest ranking scores. The ranking criterion is relative to the realization of classifier, that is to say, RFE is a wrapper algorithm. When the linear kernel SVM f ( x ) = \u3008 w , x \u3009 + b = \u2211 i = 1 l a i y i < x i , x > + b is used as a classifier in RFE, the contribution of each feature to the discriminative function, J(i), lies on its weight value, namely J(i) = (w i )2, where w = ( w i ) = \u2211 i = 1 l a i y i x i , and the decision coefficients a = (a i ) and b are obtained by training of SVM (10). SVM is retrained after each elimination operation, because a feature of medium-low importance may be promoted by removing a correlated feature. Finally, 10-fold cross-validation is used to determine the number of classifiers in ensemble. Results and Discussion The linear kernel SVM was used throughout our experiments. To avoid the noise resulted from the oversize number of features, the Fisher criterion score F ( j ) = ( \u03bc j + \u2212 \u03bc j \u2212 ) 2 / ( ( \u03c3 j + ) 2 + ( \u03c3 j \u2212 ) 2 ) was used to preselect 100 biomarkers, where \u03bc j + and \u03bc j \u2212 denote the mean value of the j th biomarker for Classes 1 and 2, respectively, while \u03c3 j + and \u03c3 j \u2212 denote the standard deviation of the j th biomarker for Classes 1 and 2, respectively. In bootstrapping, the size of each resampled dataset was set as 50% of the original training set. The initialized number of base learners in the lower layer of ensemble was set as 100. Two ovarian cancer datasets from the surface-enhanced laser desorption ionization time-of-flight (SELDI-TOF) experiments by MS (11) were analyzed in this study. Dataset 1 was prepared manually, consisting of 200 samples that were separated into a training set and a test set. Each set has 100 samples, including 50 ovarian cancer samples and 50 control (normal) samples. Dataset 2 was prepared with a robotic instrument, consisting of 162 ovarian cancer samples and 91 control samples. Its training set was made up of 60 cancer samples and 40 control samples drawn out statistically, and the test set consisted of the remaining samples. Both two original datasets had 15,154 bio-features in total. Each feature corresponded to the relative intensity of a certain kind of ionized proteomic molecule with specific m/z value. To prove the effectiveness of classifier selection, Majority Voting and double-layer fusion strategy were used as the decision method respectively on the classifier ranking results to test the ensemble accuracy. The classification accuracy analysis on Dataset 1 is shown in Figure 1. The ensemble size indicated by 10-fold cross-validation is 7, and these seven base learners are selected by SVM-RFE. The positive predictive values are 100% on the training set and 96% on the test set, with the ensemble constructed by these seven classifiers. In other cases, the classification accuracy is lower. According to Figure 1, the ensemble constructed by all base learners cannot achieve a lower error rate whether it is fused by Majority Voting or double-layer fusion strategy. The similar result achieved on Dataset 2 is shown in Figure 2. The ensemble size indicated by 10-fold cross-validation is 3. The positive predictive values are 100% on both the training set and the test set, with the ensemble constructed by these three classifiers. In other cases, the classification accuracy is lower. According to Figure 2, it is also concluded that the ensemble constructed by all base learners cannot achieve a lower error rate whether it is fused by Majority Voting or double-layer fusion strategy. By all the above analysis, it can be seen that Majority Voting and double-layer fusion strategy can depress the error rate of cancer diagnosis resulted from single SVM. In most cases, ensembles constructed by double-layer fusion strategy with classifier selection achieve better performance and simpler structure. Conclusion We have presented a constructive algorithm for training cooperative support vector machine ensembles (CSVMEs). CSVME combines ensemble architecture design with cooperative training for individual SVMs in ensembles and puts emphasis on both base learner\u2019s accuracy and collaboration among individual SVMs. This kind of SVME has been tested on two ovarian cancer datasets previously obtained by proteomic MS. By combining several optimally selected individual SVMs, the proposed method achieves better performance and simpler structure than the SVME of all base SVMs. In addition, CSVME performs better than a single SVM trained on the whole training set. Acknowledgements This work was partly supported by the National Natural Science Foundation of China (No. 60574019 and 60474045), the National Basic Research Program (973 Program) of China (No. 2002CB312200), the Key Technologies R&D Program of Zhejiang Province (No. 2005C21087), the Academician Foundation of Zhejiang Province (No. 2005A1001-13), and the Center for Bioinformatics Program Grant of Harvard Center of Neurodegeneration and Repair, Harvard Medical School, Boston, USA. References 1. G. Valentini Cancer recognition with bagged ensembles of support vector machines Neurocomputing 56 2004 461 466 2. A. Bertoni Bio-molecular cancer prediction with random subspace ensembles of support vector machines Neurocomputing 63 2005 535 539 3. L.I. Kuncheva An experimental study on diversity for bagging and boosting with linear classifiers Information Fusion 3 2002 245 258 4. T. Windeatt Diversity measures for multiple classifier system analysis and design Information Fusion 6 2005 21 36 5. Z.H. Zhou Ensembling neural networks: many could be better than all Arti. Intell. 137 2002 239 263 6. I. Guyon Gene selection for cancer classification using support vector machines Mach. Learn. 46 2002 389 422 7. Y. Mao Multiclass cancer classification by using fuzzy support vector machine and binary decision tree with gene selection J. Biomed. Biotechnol. 2005 2005 160 171 8. Y. Mao Parameters selection in gene selection using Gaussian kernel support vector machines by genetic algorithm J. Zhejiang Univ. Sci. B 6 2005 961 973 9. Y. Mao Accelerated recursive feature elimination based on support vector machine for key variable identification Chin. J. Chem. Eng. 14 2006 65 72 10. V.N. Vapnik The Nature of Statistical Learning Theory 1999 Springer New York, USA 11. R.H. Lilien Probabilistic disease classification of expression-dependent proteomic data from mass spectrometry of human serum J. Comput. Biol. 10 2003 925 946", "scopus-id": "33744992230", "pubmed-id": "16689692", "coredata": {"eid": "1-s2.0-S1672022905030330", "dc:description": "In this study, we present a constructive algorithm for training cooperative support vector machine ensembles (CSVMEs). CSVME combines ensemble architecture design with cooperative training for individual SVMs in ensembles. Unlike most previous studies on training ensembles, CSVME puts emphasis on both accuracy and collaboration among individual SVMs in an ensemble. A group of SVMs selected on the basis of recursive classifier elimination is used in CSVME, and the number of the individual SVMs selected to construct CSVME is determined by 10-fold cross-validation. This kind of SVME has been tested on two ovarian cancer datasets previously obtained by proteomic mass spectrometry. By combining several individual SVMs, the proposed method achieves better performance than the SVME of all base SVMs.", "openArchiveArticle": "false", "prism:coverDate": "2005-12-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1672022905030330", "dc:creator": [{"@_fa": "true", "$": "Mao, Yong"}, {"@_fa": "true", "$": "Zhou, Xiao-Bo"}, {"@_fa": "true", "$": "Pi, Dao-Ying"}, {"@_fa": "true", "$": "Sun, You-Xian"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1672022905030330"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1672022905030330"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1672-0229(05)03033-0", "prism:volume": "3", "prism:publisher": "Beijing Institute of Genomics, the Chinese Academy of Sciences and the Genetics Society of China. Production and hosting by Elsevier B.V.", "dc:title": "Constructing Support Vector Machine Ensembles for Cancer Classification Based on Proteomic Profiling", "prism:copyright": "Copyright \u00a9 2005 Beijing Institute of Genomics, the Chinese Academy of Sciences and the Genetics Society of China. Production and hosting by Elsevier B.V.", "openaccess": "1", "prism:issn": "16720229", "prism:issueIdentifier": "4", "dcterms:subject": [{"@_fa": "true", "$": "support vector machine ensemble (SVME) design"}, {"@_fa": "true", "$": "constructive approach"}, {"@_fa": "true", "$": "proteomic profiling"}, {"@_fa": "true", "$": "cancer diagnosis"}], "openaccessArticle": "true", "prism:publicationName": "Genomics, Proteomics & Bioinformatics", "prism:number": "4", "openaccessSponsorType": "FundingBody", "prism:pageRange": "238-241", "prism:endingPage": "241", "pubType": "Article", "prism:coverDisplayDate": "2005", "prism:doi": "10.1016/S1672-0229(05)03033-0", "prism:startingPage": "238", "dc:identifier": "doi:10.1016/S1672-0229(05)03033-0", "openaccessSponsorName": "Beijing Institute of Genomics, Chinese Academy of Sciences"}, "objects": {"object": [{"@category": "thumbnail", "@height": "115", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14691", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "120", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "15117", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "394", "@width": "752", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "72993", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "412", "@width": "752", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "77315", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "24", "@width": "222", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0001.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1818", "@ref": "si0001", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "256", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0002.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1690", "@ref": "si0002", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "152", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0003.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1338", "@ref": "si0003", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "26", "@width": "228", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0004.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "462", "@ref": "si0004", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "26", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0005.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "123", "@ref": "si0005", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "26", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0006.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "120", "@ref": "si0006", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "26", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0007.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "120", "@ref": "si0007", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "26", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1672022905030330-si0008.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "117", "@ref": "si0008", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33744992230"}}