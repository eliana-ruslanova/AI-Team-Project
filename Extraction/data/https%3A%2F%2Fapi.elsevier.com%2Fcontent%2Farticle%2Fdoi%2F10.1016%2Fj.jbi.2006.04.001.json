{"scopus-eid": "2-s2.0-33847677992", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2006-04-20 2006-04-20 2010-10-07T10:23:12 1-s2.0-S1532046406000463 S1532-0464(06)00046-3 S1532046406000463 10.1016/j.jbi.2006.04.001 S300 S300.1 FULL-TEXT 1-s2.0-S1532046407X00352 2015-05-15T06:30:58.184067-04:00 0 0 20070401 20070430 2007 2006-04-20T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 1532-0464 15320464 40 40 2 2 Volume 40, Issue 2 8 131 138 131 138 200704 April 2007 2007-04-01 2007-04-30 2007 article fla Copyright \u00a9 2006 Elsevier Inc. All rights reserved. DIRECTCLASSIFICATIONHIGHDIMENSIONALDATAINLOWDIMENSIONALPROJECTEDFEATURESPACESCOMPARISONSEVERALCLASSIFICATIONMETHODOLOGIES SOMORJAI R 1 Introduction 2 The RDP mapping 3 Classifiers in the RDP 3.1 Statistical classifiers 3.2 Classifiers specific to the RDP mapping 3.2.1 Classification in a plane defined by two reference axes 3.2.2 Transvariation-based classification 4 Results 5 Discussion 5.1 Characterization of the dataset via the RDP classification 6 Future developments 6.1 Neighborhood-based dataset augmentation 6.2 K-centroid-based dataset augmentation 6.3 Approximate distance clustering/classification (ADC) 7 Conclusion References LEAN 2002 71 111 C SOMORJAI 2002 97 102 R VLAHOU 2001 1491 1502 A LI 2002 1296 1304 J PETRICOIN 2002 572 577 E ALON 1999 6745 6750 U KHAN 2001 1 10 J IBRAHIM 2002 88 99 J POMEROY 2002 436 442 S SOMORJAI 2003 1484 1491 R SOMORJAI 2004 366 379 R LEE 1977 288 292 R FRIEDMAN 1974 881 889 J HUBER 1985 435 475 P PEKALSKA 2002 943 956 E PEKALSKA 2001 159 160 E FARAGO 1993 957 962 A ANDERSON 1962 420 431 T GINI 1959 C TRANSVARIAZIONE MONTANARI 2004 71 88 A PACLIK 2003 237 244 P PELTONEN 2005 68 83 J COWEN 1997 319 331 L PRIEBE 2001 404 413 C METCHEV 2002 73 83 S NIKULIN 1998 209 217 A SOMORJAIX2007X131 SOMORJAIX2007X131X138 SOMORJAIX2007X131XR SOMORJAIX2007X131X138XR Full http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window 2013-08-22T00:00:26Z ElsevierBranded item S1532-0464(06)00046-3 S1532046406000463 1-s2.0-S1532046406000463 10.1016/j.jbi.2006.04.001 272371 2010-11-08T20:10:04.338966-05:00 2007-04-01 2007-04-30 1-s2.0-S1532046406000463-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/MAIN/application/pdf/a6f259f37be5fbc4156b9a0c3d719e6c/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/MAIN/application/pdf/a6f259f37be5fbc4156b9a0c3d719e6c/main.pdf main.pdf pdf true 894592 MAIN 8 1-s2.0-S1532046406000463-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/PREVIEW/image/png/f7d70096f0591281f07e06bc04b3af53/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/PREVIEW/image/png/f7d70096f0591281f07e06bc04b3af53/main_1.png main_1.png png 67238 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046406000463-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/e77410f83479e039488323f7610265af/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/e77410f83479e039488323f7610265af/si8.gif si8 si8.gif gif 2317 52 410 ALTIMG 1-s2.0-S1532046406000463-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/2afaeb1e1e98c65a9176d8be5ad1dea8/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/2afaeb1e1e98c65a9176d8be5ad1dea8/si7.gif si7 si7.gif gif 1606 20 392 ALTIMG 1-s2.0-S1532046406000463-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/78b0ed16d85a8028159f8d4f17e87222/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/78b0ed16d85a8028159f8d4f17e87222/si1.gif si1 si1.gif gif 1071 19 271 ALTIMG 1-s2.0-S1532046406000463-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/54ba25ddabd85479e925e14d956d4d12/si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/54ba25ddabd85479e925e14d956d4d12/si9.gif si9 si9.gif gif 657 18 130 ALTIMG 1-s2.0-S1532046406000463-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/60c767174a202b8693eec81168176e16/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/60c767174a202b8693eec81168176e16/si6.gif si6 si6.gif gif 313 25 27 ALTIMG 1-s2.0-S1532046406000463-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/0830a6c96804eedbcd9b5c67189787dd/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/0830a6c96804eedbcd9b5c67189787dd/si5.gif si5 si5.gif gif 296 22 27 ALTIMG 1-s2.0-S1532046406000463-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/0ba04d14f4f1c683c7e93b80c7ddfd43/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/0ba04d14f4f1c683c7e93b80c7ddfd43/si4.gif si4 si4.gif gif 812 24 162 ALTIMG 1-s2.0-S1532046406000463-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/f41d7e595ff82a00f2c9d4075dd32324/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/f41d7e595ff82a00f2c9d4075dd32324/si3.gif si3 si3.gif gif 1267 42 186 ALTIMG 1-s2.0-S1532046406000463-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/91352316a6579bd57a9b2947bfcc23cf/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/91352316a6579bd57a9b2947bfcc23cf/si2.gif si2 si2.gif gif 836 21 199 ALTIMG 1-s2.0-S1532046406000463-si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/cf9e8a07327cfda612a3ffee2c5ac54f/si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/cf9e8a07327cfda612a3ffee2c5ac54f/si18.gif si18 si18.gif gif 344 14 51 ALTIMG 1-s2.0-S1532046406000463-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/7d8f5de9b123df3261ff605095e008e7/si17.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/7d8f5de9b123df3261ff605095e008e7/si17.gif si17 si17.gif gif 208 13 13 ALTIMG 1-s2.0-S1532046406000463-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/ad0a386b2166e6f6cd10afb86eafda10/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/ad0a386b2166e6f6cd10afb86eafda10/si16.gif si16 si16.gif gif 596 18 116 ALTIMG 1-s2.0-S1532046406000463-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/bea557e91289286341801836e23be0a7/si15.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/bea557e91289286341801836e23be0a7/si15.gif si15 si15.gif gif 283 18 30 ALTIMG 1-s2.0-S1532046406000463-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/1fb6d016384728a8e50640321988cda6/si14.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/1fb6d016384728a8e50640321988cda6/si14.gif si14 si14.gif gif 250 17 18 ALTIMG 1-s2.0-S1532046406000463-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/c173d3203eaa8a0f83dd81e2fdc92791/si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/c173d3203eaa8a0f83dd81e2fdc92791/si13.gif si13 si13.gif gif 371 17 39 ALTIMG 1-s2.0-S1532046406000463-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/5319d170b881916ff7de34ad223bc8a4/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/5319d170b881916ff7de34ad223bc8a4/si12.gif si12 si12.gif gif 347 18 36 ALTIMG 1-s2.0-S1532046406000463-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/bdbec43674cc31cf60125d1bbe4c3433/si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/bdbec43674cc31cf60125d1bbe4c3433/si11.gif si11 si11.gif gif 1376 65 206 ALTIMG 1-s2.0-S1532046406000463-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/STRIPIN/image/gif/f84ca5555b16b05f3d253d04860a3bfb/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/STRIPIN/image/gif/f84ca5555b16b05f3d253d04860a3bfb/si10.gif si10 si10.gif gif 1180 47 200 ALTIMG 1-s2.0-S1532046406000463-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr5/DOWNSAMPLED/image/jpeg/edb81d8bec2046212312d8cf94b1717f/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr5/DOWNSAMPLED/image/jpeg/edb81d8bec2046212312d8cf94b1717f/gr5.jpg gr5 gr5.jpg jpg 80045 844 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr5/THUMBNAIL/image/gif/b011ff382dcd5871b7393d2a5dc5a435/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr5/THUMBNAIL/image/gif/b011ff382dcd5871b7393d2a5dc5a435/gr5.sml gr5 gr5.sml sml 1957 93 41 IMAGE-THUMBNAIL 1-s2.0-S1532046406000463-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr3/DOWNSAMPLED/image/jpeg/7755915e3963ba4bc7f89bc9b221f4e8/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr3/DOWNSAMPLED/image/jpeg/7755915e3963ba4bc7f89bc9b221f4e8/gr3.jpg gr3 gr3.jpg jpg 54298 336 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr3/THUMBNAIL/image/gif/b324de5136947add6778ebc0896efcb4/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr3/THUMBNAIL/image/gif/b324de5136947add6778ebc0896efcb4/gr3.sml gr3 gr3.sml sml 3624 94 104 IMAGE-THUMBNAIL 1-s2.0-S1532046406000463-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr4/DOWNSAMPLED/image/jpeg/8f3d9c2ae07a81ba23d38ecb447f56db/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr4/DOWNSAMPLED/image/jpeg/8f3d9c2ae07a81ba23d38ecb447f56db/gr4.jpg gr4 gr4.jpg jpg 59518 433 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr4/THUMBNAIL/image/gif/4dd6fe24a0d67cb937fbbb1bd9c28822/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr4/THUMBNAIL/image/gif/4dd6fe24a0d67cb937fbbb1bd9c28822/gr4.sml gr4 gr4.sml sml 3038 93 80 IMAGE-THUMBNAIL 1-s2.0-S1532046406000463-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr2/DOWNSAMPLED/image/jpeg/b38af6ae5dc7d182bc3926885c7b719c/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr2/DOWNSAMPLED/image/jpeg/b38af6ae5dc7d182bc3926885c7b719c/gr2.jpg gr2 gr2.jpg jpg 40525 233 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr2/THUMBNAIL/image/gif/b4df7106cc170acbcff3ccfe0b7eaea8/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr2/THUMBNAIL/image/gif/b4df7106cc170acbcff3ccfe0b7eaea8/gr2.sml gr2 gr2.sml sml 3247 78 125 IMAGE-THUMBNAIL 1-s2.0-S1532046406000463-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr6/DOWNSAMPLED/image/jpeg/b9af295c2342b4d6d3d028c05554e377/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr6/DOWNSAMPLED/image/jpeg/b9af295c2342b4d6d3d028c05554e377/gr6.jpg gr6 gr6.jpg jpg 61783 358 369 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr6/THUMBNAIL/image/gif/42263bea4c36e2b795620000dd84db80/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr6/THUMBNAIL/image/gif/42263bea4c36e2b795620000dd84db80/gr6.sml gr6 gr6.sml sml 3639 93 96 IMAGE-THUMBNAIL 1-s2.0-S1532046406000463-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr1/DOWNSAMPLED/image/jpeg/736e8b566c408f239dfb7807cd614e5b/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr1/DOWNSAMPLED/image/jpeg/736e8b566c408f239dfb7807cd614e5b/gr1.jpg gr1 gr1.jpg jpg 68205 197 654 IMAGE-DOWNSAMPLED 1-s2.0-S1532046406000463-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046406000463/gr1/THUMBNAIL/image/gif/5d7162695e673b5fb611363acb10c602/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046406000463/gr1/THUMBNAIL/image/gif/5d7162695e673b5fb611363acb10c602/gr1.sml gr1 gr1.sml sml 2804 38 125 IMAGE-THUMBNAIL YJBIN 1290 S1532-0464(06)00046-3 10.1016/j.jbi.2006.04.001 Elsevier Inc. Fig. 1 Influence of the Anderson\u2013Bahadur parameter \u03b1 on class distribution and classification in the RDP. (Left) \u03b1 =0.0, \u03b2 =0.57, \u03b3 =\u22126.1\u00b0, reference pair (29,123); (middle) \u03b1 =1.44, \u03b2 =0.43, \u03b3 =11.0\u00b0, reference pair (44,102); (right) \u03b1 =2.0, \u03b2 =0.39, \u03b3 =9.0\u00b0, reference pair (52,102). Fig. 2 Two-class (prostate cancer vs. healthy) mass spectra, originally 15,154-dimensional, reduced to 5 features. RDP mapping with training (TS) and validation (VS) sets. TS (red and blue disks), VS (yellow and turquoise triangles). Classes (both TS and VS) separate perfectly when mapped to the reference axis defined by reference points 1 (class 1) and 66 (class 2). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.) Fig. 3 The classes are separable only in the RDP (dashed line represents the separating LDA line \u201chyperplane\u201d). Both reference points (41 and 46) derive from the TS of class 1. Fig. 4 As in Fig. 3. Both reference points (78 and 83) derive from the TS of class 2. Fig. 5 (a) Classification of mass spectra of ovarian cancer using two reference axis pairs (83\u201371) and (71\u201374), after feature space reduction to three from the original, 15,154 dimensions. The angle between the two axes is 31.53\u00b0, the value \u03b1 for the Anderson\u2013Bahadur parameter is 1.06. (b) Classification of mass spectra of ovarian cancer using two reference axes (160\u2013178) and (214\u2013257). Fig. 6 Displaying the improvement of the LDA separating \u201chyper surface\u201d by rotating it by 11.0\u00b0. Direct classification of high-dimensional data in low-dimensional projected feature spaces\u2014Comparison of several classification methodologies R.L. Somorjai \u204e Ray.Somorjai@nrc-cnrc.gc.ca B. Dolenko M. Mandelzweig Institute for Biodiagnostics, National Research Council Canada, 435 Ellice Avenue, Winnipeg, Man., Canada R3B 1Y6 \u204e Corresponding author. Fax: +1 204 984 5472. Abstract Previously, we introduced a distance (similarity)-based mapping for the visualization of high-dimensional patterns and their relative relationships. The mapping preserves exactly the original distances from all points to any two reference patterns in a special two-dimensional coordinate system, the relative distance plane (RDP). We extend the RDP mapping\u2019s applicability from visualization to classification. Several of the classifiers use the RDP directly. These include the standard linear discriminant analysis (LDA), nearest neighbor classifiers, and a transvariation probabilities-based classification method that is natural in the RDP. Several reference directions can also be combined to create new coordinate systems in which arbitrary classifiers can be developed. We obtain increased confidence in the classification results by cycling through all possible reference pairs and computing a misclassification-based weighted accuracy. The classification results on several high-dimensional biomedical datasets are compared. Keywords RDP mapping Relative distance plane mapping Direct classification Comparison of classifiers Transvariation 1 Introduction The principal goal of acquiring biomedical spectra of biofluids and tissues [1,2], mass spectra, e.g., from proteomics [3\u20135], etc., and microarray expression profiles [6\u20139], etc. is to non-invasively discriminate diseases and disease states. Biomedical data are characterized by relatively few patterns (N = O (10)\u2013O (100)) that are initially presented in a very high-dimensional feature space (L = O (1000)\u2013O (10000)). These two characteristics lead to the two curses afflicting any analysis of such high-dimensional data: the curse of dimensionality and the curse of dataset sparsity [10]. Any analysis of such data requires special considerations. One inevitable corollary of dealing with high-dimensional feature spaces is that visualization of such data is an essential first step of any exploratory data analysis. This requires dimensionality-reducing mapping/projection from L to 1\u20133 dimensions. Several such dimensionality reduction mappings exist. They all attempt to preserve all pattern-to-pattern distances, necessarily only approximately. However, when the ultimate aim of visualization/projection is group discrimination, we only need to preserve class or cluster separability. With this perspective in mind, we recently [11] proposed a simple, yet exact visualization approach that maps any high-dimensional pattern to a special plane we call the relative distance plane (RDP). The mapping can be used either as an exploratory tool, or as a confirmatory one, if class labels are available for members of the different classes comprising the dataset. The mapping preserves class separability. Here, we extend its applicability to explore direct classification in the RDP (two dimensions), or even in one dimension, i.e., with respect to a particular reference line/axis, and compare its efficacy with those of other one- and two-dimensional classifiers. 2 The RDP mapping For a two-class problem, assume N k L-dimensional samples in class k, k =1,2. The total number of samples is N = N 1 + N 2. The \u201cpatterns\u201d (samples) of the N-sample dataset in the L-dimensional feature space are X j =[X j1,\u2026, X jL ], j =1,\u2026, N. When augmented by the two class centroids, C 1, C 2, this gives a Q \u00d7 L data matrix, Q = N +2. The RDP mapping is a distance (dissimilarity)-based, intrinsically nonlinear projection. It only requires a single computation of a distance matrix D =[D jk ], j, k =1,\u2026, N in some user-selected metric or dissimilarity measure. (The approach is equally applicable when only a matrix D is available.) This is computationally quite feasible for the number of samples (N = O (100)) we generally encounter in biomedical applications. Our method is based on the fact that the three distances between any three points in the original L-space are exactly preserved when displayed in any 2D coordinate system (V, T). Such triangulation mapping method has indeed been suggested [12], but never exploited either as a visualization tool [11] or, in particular, for direct classification, as we propose here. The RDP mapping consists of the following steps: 1. Choose some distance (or dissimilarity) measure and compute the corresponding Q \u00d7 Q distance matrix. 2. Select any two patterns R 1 (\u2261X j ) and R 2 (\u2261X k ) in the original L-dimensional space as a reference pair; the distance D (R 1, R 2)\u2261 D 12 has already been computed in Step 1. The line through R 1 and R 2 defines a reference axis or reference line onto which one can further project the data from the RDP. 3. For each pattern X m , m \u2260 j, k, denote its distances to the reference patterns by D 1m \u2261 D (X m , R 1) and D 2m \u2261 D (X m , R 2). The Euclidean (V, T) coordinates (V m , T m ) in the RDP for all points X m , m =1,2,\u2026, Q \u22122, m \u2260 j, k, are (1a) V m \u2261 V [ X m ] = ( D 12 2 + D 1 m 2 - D 2 m 2 ) / 2 D 12 , (1b) T m \u2261 T [ X m ] = ( D 1 m 2 - V m 2 ) 1 / 2 . (Setting R 1 (V m )=0, R 2 (V m )=1 scales D 12 to 1 and simplifies V m , T m .) Note that from the three given distances it is equally easy to compute the (V, T) coordinates in another distance metric. Because it is distance-based, the RDP display is independent of the metric in which we eventually express the (V, T) coordinates, which however, will be different for different metrics. For instance, in the L 1 norm, the expressions equivalent to Eqs. (1a) and (1b) are even simpler: T m = ( D 1 m - D 2 m + D 12 ) / 2 , V m = ( D 1 m + D 2 m - D 12 ) / 2 . Project all points in the RDP onto the current V (horizontal) axis, and create a histogram. (If the two reference points are the class centroids, this projection is the RDP representation of the LDA-provided reference line in the original, L-dimensional space.) If the histogram is multimodal, or at least skewed, then the R 1 \u2212 R 2 axis provides a \u201cpotentially interesting\u201d line traversing the original L-space. If the current R 1 \u2212 R 2 axis is not \u201cinteresting\u201d, all other pairs of points (R j , R k ) of the N pair = Q (Q \u22121)/2 set are available for inspection and testing. When projecting the patterns onto some reference axis, our approach is a discretized, optimization-free version of Projection Pursuit [13,14], confined however to the N pair lines through existing pattern pairs. (Note however, that once displayed in the RDP, maximally discriminatory reference lines not traversing any original pattern pairs might suggest themselves.) The distance in the original L-space of point X m from any reference axis is T m . Thus, we can display all points of the dataset, without distorting their original distances to the two reference patterns. It is in principle possible (confirmed in practice) that in the RDP, two reference points from the same class would separate classes better. We shall also explore this possibility. Current interest [15\u201317,22,23] in direct dissimilarity-based classification [18] provides an important link, because the RDP mapping would not only convert high-dimensional feature vectors into distances (dissimilarities) prior to classification, but also allow the conversion of raw dissimilarities into low-dimensional coordinate representations for subsequent classification. In the following, we place RDP-based classifiers in the context of standard classification methodology, confined however to the one or two dimensions created by the RDP mapping. 3 Classifiers in the RDP There are several reasons to develop a (possibly nonlinear) classifier directly in the RDP. Because the samples in the RDP are two-dimensional, one can simply select a set of s points (V k , T k ), k =1,\u2026, s, that optimally separate the two classes in the RDP, and fit to them some piecewise linear or even smooth function y = f (V, T). Although high-dimensional distance relationships are not preserved in general, except relative to the two reference points, classification of new exemplars might only be ambiguous if they were to fall \u201cclose\u201d to the separating curve f (V, T). What is \u201cclose\u201d is, of course, data- and metric-dependent. 3.1 Statistical classifiers We assume that some exploratory analysis of the original L-dimensional feature space has already been carried out via the RDP mapping. Based on this exploration, if at all possible (or necessary), reduce the dataset from the original L features to a set of M optimal features (2< M \u226a L), e.g., by a wrapper-based feature reduction approach (see e.g., [28]). Then the following is a reasonable procedure in the RDP (with some abuse of notation, bold fonts below denote both the original L-dimensional vectors and two-dimensional vectors or matrices in the RDP): Step 1: Assuming two classes, consider the subset of all N C \u2261(N 1 +1)(N 2 +1) possible reference pairs (including the class centroids C 1, C 2) whose components belong to different classes. This set is { R 1 i \u2261 X i ( 1 ) , R 2 j \u2261 X j ( 2 ) } , i =1,\u2026, N 1 +1, j =1,\u2026, N 2 +1, where X i ( 1 ) denotes sample i of class 1, X j ( 2 ) sample j of class 2. Cycle through this subset, and for each pair (R 1i , R 2j ) map the remaining patterns. Then develop classifiers for the mapped patterns, one for each reference pair. Thus, we can explore the dataset in detail, from N C different \u201cperspectives\u201d. Step 2: For each reference pair/axis (R 1i , R 2j ), a simple generalization of the Nearest Mean Classifier (NMC) is (2) f GNMC ( x | R 1 i , R 2 j ; \u03b2 ) = [ x - \u03b2 R 1 i + ( 1 - \u03b2 ) R 2 j ] t ( R 1 i - R 2 j ) The pattern to be classified is x. The superscript \u201ct\u201d denotes the transpose. The pair (R 1i , R 2j ) extends and generalizes the class means (C 1, C 2). The parameter \u03b2 (0\u2a7d \u03b2 \u2a7d1) controls the position of the separating \u201chyperplane\u201d (line in the RDP) that is perpendicular to the R 1i \u2212 R 2j axis and shifts from the standard halfway position (\u03b2 =1/2), and possibly further reduces the misclassification error. Note that this process extends the conventional, class-centroid-based approaches. When M (or L)< N, a generalized LDA-like discriminant function in the RDP is (3) F GLDA ( x | C \u2217 1 i , C \u2217 2 j ; \u03b1 , \u03b2 ) = [ x - \u03b2 C \u2217 1 i + ( 1 - \u03b2 ) C \u2217 2 j ] t \u2032 W ( \u03b1 ) - 1 ( C \u2217 1 i - C \u2217 2 j ) ; 0 \u2a7d \u03b1 \u2a7d 2 with mapping to the R 1i \u2212 R 2j axis. Here, C \u2217 1i and C \u2217 2j are the class centroids, calculated after excluding the above two reference points (\u201cleave-two-out\u201d). W (\u03b1)\u22121 is the inverse of the appropriately modified pooled covariance matrix, W (\u03b1)= \u03b1 p 1 W 1 +(2\u2212 \u03b1)p 2 W 2, W k is the covariance matrix of class k, k =1,2, p 1, p 2 =1\u2212 p 1, are the prior class probabilities, and \u03b1 is a parameter, 0\u2a7d \u03b1 \u2a7d2. W(1.0) gives the well-known Mahalanobis distance, used in standard LDA. For \u03b1 \u22601.0, we obtain the Anderson-Bahadur (AB) generalization of LDA [19]. In fact, we can optimize \u03b1 to equalize the misclassification probabilities P 1, P 2 for the two classes, or, for a given P 1, minimize P 2 or vice versa. [19]. Again, \u03b2 determines the position at which the parallel separating hyperplane crosses the reference axis, i.e., \u03b2 shifts the LDA hyperplane (shown as a dashed line); this hyperplane, however, is unlikely to be perpendicular to the axis, especially when both reference points derive from the same class. In Fig. 1 , we show the influence of the parameter \u03b1 on both the mapping distribution and on the subsequent classification. (The parameter \u03b3, to be discussed later, controls the rotation of the hyperplane.) The two extremes, \u03b1 =0.0 (reference pair 29,103) and \u03b1 =2.0 (reference pair 52,102) produce distinctly different distributions in the RDP. For \u03b1 =0.0, class 1 (training set: red disks, test set: yellow triangles) appears much more heterogeneous than for class 2 (blue disks and green triangles), whereas the reverse applies when \u03b1 =2.0. The optimal \u03b1 =1.44 (reference pair 44,102) provides the most balanced distributions for both classes. Of course, using the Anderson-Bahadur approach is not confined to the RDP. However, the visualizability of the consequences of changing \u03b1, combined with the additional flexibility provided by optimized and optimizable reference pairs, makes classification in the RDP particularly attractive and powerful. Whether the reference points derive from different classes or from the same class, two classification possibilities arise: (a) The additional mapping of the data points onto the reference axis (a 1D classification) is optimal, i.e., f GNMC (x|R 1i , R 2j ; \u03b2), is best for some \u03b2. (b) The classification is optimal only in the RDP, i.e. a 2D, possibly 2-parameter linear classifier, such as f GLDA (x|C \u2217 1i , C \u2217 2j ; \u03b1, \u03b2), or some more general nonlinear classifier will be needed. An example of the first possibility is shown in Fig. 2 . This is a mapping from five dimensions into the RDP. The optimal one-dimensional separation of training sets (TS; red & blue disks) and validation (test) sets (VS; yellow & turquoise triangles) is along the line joining the class centroid (1) of class 1 and sample # 66 from class 2. In Fig. 3 we present the situation when the two classes separate only in the RDP (dashed line represents the separating LDA \u201chyperplane\u201d). Both reference points (41 and 46, enclosed in a red ellipse for emphasis) derive from the TS of class 1. Similarly, in Fig. 4 , both reference points (78 and 83) belong to class 2. Nevertheless, for both cases a simple LDA separates the classes. 3.2 Classifiers specific to the RDP mapping In addition to the basically standard statistical classifiers discussed above, there are two useful possibilities for classification in the RDP. The first one involves the reference axes. 3.2.1 Classification in a plane defined by two reference axes For N (= N 1 + N 2) M-dimensional two-class patterns, there are, when the two centroids are included, at least N C =(N 1 +1)(N 2 +1) possible reference axes in the RDP, if the reference pairs consist of two patterns that derive from different classes. (This number increases significantly if the reference axes are allowed to belong to the same class.) Thus, selecting any two reference axes A j and A k , and using the corresponding V j (X m ) and V k (X m ) values as the two coordinates for sample X m , one can display, and classify all patterns in this (generally oblique) coordinate system. The choice of the pair A j and A k determines the accuracy of the classifier. Choosing A j and A k such that they are individually good classifier directions is more than likely optimal. However, (near-) orthogonality of A j and A k does not seem to be required. In fact, based on the tests we have conducted on different datasets, the angles distended by the best A j \u2212 A k pairs are generally not orthogonal and, at least for the datasets tested, tend to be in the 20\u201370\u00b0 range. We demonstrate classification in the plane formed by two reference axes in Section 4. Extension to higher dimensions is immediate. 3.2.2 Transvariation-based classification The other possibility for classification, called transvariation, is quite natural when the RDP mapping is further confined to projecting into the single dimension defined by any reference axis. Two groups, g 1 and g 2, transvariate on a variable y with respect to their corresponding average (median) values C y1 and C y2 if the sign of some of the N 1 N 2 differences y i1 \u2212 y j2 (i =1,\u2026, N 1, j =1,\u2026, N 2), is opposite to that of C y1 \u2212 C y2. Any difference satisfying this condition is called a \u201ctransvariation\u201d [20]. Of the several possible measures of transvariation, the transvariation probability seems to be particularly useful for classification [21]. It is defined as (4a) p 12 trans = 2 t 12 / N 1 N 2 , (4b) t 12 = \u2211 j = 1 N 1 \u2211 k = 1 N 2 \u03b7 ( y j 1 , y k 2 ) Cy 1 < Cy 2 t 12 is the number of transvariations between the two groups, with (4c) \u03b7 ( a , b ) m a < m b = 0 , if a < b , 1 / 2 , if a = b , 1 , if a > b . This is the most robust (least affected by outliers) of the three transvariation-based measures because it uses only counts. In practice, the high-dimensional data are first projected onto some line traversing the original feature space, and p 12 trans is calculated with respect to that line. For transvariation-based classification in the RDP, the variable y is the (horizontal) coordinate V (R i , R j ), Eq. (4b) with respect to which transvariation probability is assessed, and the two medians would be computed for and from the one-dimensional projected data. Thus, we have N 1 N 2 possible one-dimensional classification results. In Figs. 1, 2 and 6, any vertical line between the red margin lines could indicate the transvariation result. A transvariation-based classifier provides more information about misclassification than mere accuracy: It also probes the extent by which the misclassified samples are \u201cburied\u201d in the wrong class, unlike a statistical classifier, such as GNMC (Eq. (2)), which assesses classification error simply by counting the misclassified samples. Dissimilarity-Based Classification (DBC) [15] makes contact with RDP-based mappings. It advocates using k-member reference sets, sampled only from one of the classes, to create k new features that are the distances (or dissimilarities) between the remaining N \u2212 k samples and members of the set. Then conventional classifiers are developed using such features. The authors show that these classifiers may have smaller classification errors than k-nearest-neighbour classifiers. The sequential extension of the RDP mapping to a (k \u22121)-simplex (for visualizability purposes naturally constrained to k =2 or 3) could also produce k distance-derived features for classification. Appropriately scaled, the k features could also be the distances, although analytical expressions [extending Eqs. (1a) and (1b)] are also derivable, at least for k =3. 4 Results We demonstrate the above-described RDP classification options/capabilities on high-dimensional, but sparse biomedical datasets. In Figs. 2\u20134, we show the mapping of a two-class prostate cancer vs. healthy dataset to the RDP. We first reduced the original, 15,154-feature (the measured mass/charge ratios) proteomic mass spectra to five features via a wrapper-based feature selection, using LDA. The red disks represent the training set (TS), the yellow triangles the validation set (VS) of the prostate cancer exemplars. Similarly, the blue disks and turquoise triangles correspond to the TS and VS of the healthy class. Fig. 2 shows that the two classes (both TS and VS) separate perfectly when mapped down to the reference axis defined by reference points 1 (class 1) and 66 (class 2). (Note that the optimal reference axis traverses only the class 1 centroid, but not that of class 2.) This perfect separation is both with respect to the one-dimensional transvariation classifier and with respect to LDA (dashed line). Although it was not really necessary, we optimized the two parameters in Eq. (3), obtaining \u03b1 =1.44, \u03b2 =0.52. We have implemented in the RDP [11], both the one- and two-dimensional Kolmogorov\u2013Smirnov (K\u2013S) tests, and Kuiper\u2019s test. For the mapping in Fig. 2 we compute 1.00, with p =1.1\u00d710\u221218 for the 1D K\u2013S, 0.89, p =1.8\u00d710\u221217 for Kuiper\u2019s and 0.93, p =6.5\u00d710\u221214 for the 2D K\u2013S. The significance level of the observed K\u2013S statistics is indicated by p (as the probability of rejection of the null hypothesis that the two distributions are the same). Figs. 3 and 4 present examples when the classes separate only in the RDP. For Fig. 3, with reference pair (41,46), both from the TS of class 1, the tests give 0.95, p =5.7\u00d710\u22127 (1D K\u2013S), 0.93, p =1.0\u00d710\u221215 (Kuiper\u2019s) and 0.86, p =4.0\u00d710\u221212 (2D K\u2013S). The relatively large (0.95 and 0.93) values for the 1D tests reflect the fact that the 2 two-class histograms overlap only relatively weakly. For Fig. 4, with reference pair (78,83), both from the TS of class 2, we compute 0.57, p =2.2\u00d710\u22126 (1D K\u2013S), 0.55, p =5.6\u00d710\u22125 (Kuiper\u2019s) and 0.92, p =2.0\u00d710\u22123 (2D K\u2013S). As expected, the 1D K\u2013S and Kuiper\u2019s tests now have smaller values, \u223c0.5, suggesting considerable overlap, whereas the 2D K\u2013S test is still indicating good separation (0.92). This reflects the particular two-dimensional distribution of these data points and the heuristic, Monte Carlo-based simulations of the maximum cumulative difference between the two distributions over the (V\u2013T) plane. For more details, consult [27]. We demonstrate a classification possibility in the plane defined by a pair of reference axes. The dataset is mass spectra of ovarian cancer vs. normal subjects, with a 15,154-dimensional initial feature space. The two RDP reference axes chosen traverse data point pairs 83\u201371 and 71\u201374. In Fig. 5 a, we display the mapping onto these two axes. Fig. 5b shows another pair, 160\u2013178 and 214\u2013257, also demonstrating that any choice of the two axes is legitimate as long as 3 of the 4 reference points differ. Note that for N total samples there are N total (N total +1)/2\u22121 unique pairs of reference axes that can be tested for classification accuracy, a large number even for limited N total. This provides great flexibility for assessing classification accuracy. 5 Discussion 5.1 Characterization of the dataset via the RDP classification A useful categorization of the N C pairs is their ability to separate the two classes in the RDP. For any given pair, we can assess this by optimizing Eq. (2) with respect to \u03b2. Note that this categorization still gives the worst-case classification scenario, because it assumes mapping onto the reference axis (a one-dimensional classification). Thus, there will be cases when the classes are not separable in one (the V) dimension, but will be in the RDP (e.g., by an oblique line relative to the reference axis, or generally by a specific, arbitrary curve.) This second, more general option produces an LDA-type classifier in the two-dimensional RDP. Some of the possible ambiguity can be resolved by cycling through all N C reference pairs, and for every pair, assigning the new exemplar X new to one of the two classes. Then the reliability of a class assignment can be assessed by the size of the fraction 0\u2a7d F (X new)\u2a7d1 of N C reference pairs for which X new was assigned to class 1, say. The closer F (X new)>1/2 to 1, the more likely that the assignment to class 1 is correct. Instead of this majority voting, one could assess assignment reliability by computing some function (e.g., the mean or the median) of the N C probabilities P k (X new), k =1,2,\u2026, N C. Consider the case when the two classes are separable without misclassification. In particular, for a given metric, let n 0 \u2a7d N C be the number of reference pairs that perfectly separate the two classes in RDP (the \u201cperfect pairs\u201d). Then the fraction F = n 0/N C is an overall measure of how easy it is to separate the two classes under consideration. Thus, if via some feature reduction method one found several, equally accurate classifiers in an M-dimensional (2< M < L) feature space, ties could be broken among the M-feature classifiers by comparing misclassification errors when going from the M-space to the RDP. An example is provided for the prostate cancer dataset (Fig. 2), for which two error-free classifiers were found in a 5-dimensional reduced feature space, with no overlap of the five discriminating features [11]. However, mapping from these two distinct feature sets to the RDP clearly showed a better generalization potential of one of these. RDP mapping preserves exactly pattern distances only to the two reference points (e.g., the two class centroids). Furthermore, in general, the patterns\u2019 RDP displays will not correspond to the best possible classification achievable in the original L-space. That is because any projection into a lower dimensional space leads to a loss of degrees of freedom. Consequently, the number of misclassifications in the current RDP will likely be an upper bound to that in any dimension greater than two. This may be mitigated by the flexibility to choose any pair of reference points (e.g., two putative or otherwise identified \u201csupport vectors\u201d) to recompute a new RDP mapping and possibly improve classification accuracy. The examples presented clearly support this notion. 6 Future developments The sample size N = N 1 + N 2 of O (10)\u2013O (100) per class, typical for biomedical data, implies that the RDP mapping, viewed as a discretized version of projection pursuit, may not explore the original, high-dimensional feature space sufficiently thoroughly to identify the optimal reference direction(s). However, we can create new reference pairs, hence reference directions, via several procedures: 6.1 Neighborhood-based dataset augmentation Consider the following K-nearest neighbor approach. 1. Select sample X i, i =1,\u2026, N. Choose an integer k \u2a7e1, k =1,\u2026, K. This will determine the size of the neighborhood of X i , consisting of its K nearest neighbors. 2. Generate a uniformly distributed random number on each of the K lines joining X i and one of its K neighbors. Their positions define the K new, surrogate patterns, X i \u2217 ( k ) . Observe that if the original pattern was either misclassified or is in or near the overlap region between the two classes, the class label of the new pattern X i \u2217 will likely be assigned to the class it is closer to and not to the class of X i . This will avoid creating surrogate patterns only in the convex hull of the class to which X i belongs. By cycling through the N original samples, we create KN new ones. 6.2 K-centroid-based dataset augmentation This is an alternate approach to augment limited-sized datasets. The procedure is simple: 1. Choose an odd integer K \u2a7e3. 2. Select either all C K N subsets of size K, or if this set is too large, a random subset of predetermined size, M, with a reasonable range, e.g., C K N > M \u2a7e KN . 3. Calculate the M centroids of the K-pattern subsets. These form the new, surrogate samples, augmenting the original N patterns to (M +1)N. 4. To get the new class labels, compute the K nearest neighbors for each of the M centroids. Because members of the K-pattern subsets may belong to either or both classes, we assign the centroids to the class whose nearest neighbors are in the majority. 6.3 Approximate distance clustering/classification (ADC) ADC is another distance-based nonlinear projection method [24\u201326]. The ADC method is based on nearest-neighbour notions and uses a family of randomized projections from the set I in L-space to lines (i.e., to 1D), indexed by w subsets of observations W i \u2282 I (witness set), i =1,\u2026, w, with cardinality |W i|= k < N, where k is the number of samples in each of the w witness sets. When used for classification, one selects the W i s randomly from only one of the classes. Computing the distances of all other patterns in the data to the various witness set members and recording the minimum values produces the one-dimensional mapping of interest. In practice, a classifier is developed for each of the w (w odd) witness sets, and from these the r \u2a7d w best crossvalidated classification results are averaged [25]. The philosophies of the ADC and RDP mappings are different. However, it is possible in principle that in the RDP, two reference points from the same class would separate classes better. (In the ADC context, k =2.) Figs. 3 and 4 confirm this in practice. One of our future plans is to compare the relative efficacies of the ADC- and RDP-based classifiers. We also plan on exploring a new crossvalidation approach, suggested by, and particularly suited to the RDP. This involves the \u201cleave-two-out\u201d option referred to earlier. The two left-out samples are the reference point pairs. There are N (N \u22121)/2 of these, rather than the N samples in the standard leave-one-out crossvalidation. One can develop classifiers based on the remaining N\u22122 samples and assign the two left-out samples. This approach would probe and assess more thoroughly the classification accuracy inherent in the dataset. Another extension involves an optimal rotation of the 2D class-separating line by some angle \u03b3. Positive \u03b3 rotates this LDA line counterclockwise, negative \u03b3 rotates it clockwise. The pivot point lies along the line joining the two class centroids (computed after excluding the two reference points). An example is provided by the ovarian cancer mass spectra, for which better separation could have been found by rotating the LDA line (Fig. 6 ). Thus, while originally there are two optimizable parameters in the generalized LDA equation (Eq. (3)), this can be increased to three by also optimizing \u03b3. Note that these three parameters can be optimized sequentially. As an example, for Fig. 1, the best parameters are \u03b2 =0.57, \u03b3 =\u22126.1\u00b0 (\u03b1 =0.0), \u03b2 =0.43, \u03b3 =11.0\u00b0 (\u03b1 =1.44), and \u03b2 =0.39, \u03b3 =9.0\u00b0 (\u03b1 =2.0). 7 Conclusion We introduced the notion of direct classification of L-dimensional patterns (L arbitrarily large) in two and/or one dimensions, and outlined and demonstrated its feasibility and power, using several high-dimensional biomedical datasets. We demonstrated the utility of a statistical classifier, LDA, and Gini\u2019s transvariation method, natural for the RDP mapping. Several possible extensions were also sketched, including a potentially very powerful combination of pairs of reference axes derived from the RDP mapping. In future publications, we shall report on a more detailed comparative set of experiments on the different possible classifiers in the RDP, or derived from the RDP mapping. References [1] C.L. Lean R.L. Somorjai I.C.P. Smith P. Russell C.E. Mountford Accurate diagnosis and prognosis of human cancers by proton MRS and a three stage classification strategy Annu Rep NMR Spectrosc 48 2002 71 111 [2] R.L. Somorjai B. Dolenko A. Nikulin P. Nickerson D. Rush A. Shaw Distinguishing normal from rejecting renal allografts: application of a three-stage classification strategy MR and IR spectra of urine Vib Spectrosc 28 2002 97 102 [3] A. Vlahou P.F. Schellhammer S. Mendrinos K. Patel F.I. Kondylis L. Gong Development of a novel proteomic approach for the detection of transitional cell carcinoma of the bladder in urine Am J Pathol 158 2001 1491 1502 [4] J. Li Zh. Zhang J. Rosenzweig Y.Y. Wang D.W. Chan Proteomics and bioinformatics approaches for identification of serum biomarkers to detect breast cancer Clin Chem 48 2002 1296 1304 [5] E.F. Petricoin III A.M. Ardekani B.A. Hitt P.J. Levine V.A. Fusaro S.M. Steinberg Use of proteomic patterns in serum to identify ovarian cancer Lancet 359 2002 572 577 [6] U. Alon N. Barkai D.A. Notterman K. Gish S. Ybarra D. Mack Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays Proc Natl Acad Sci USA 96 1999 6745 6750 [7] J. Khan J.S. Wei M. Ringn\u00e9r H. Saal M. Ladanyi F. Westermann Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks Nat Med 7 2001 1 10 [8] J.G. Ibrahim M.-H. Chen R.J. Gray Bayesian models for gene expression with DNA microarray data J Am Stat Assoc\u2014Application and Case Studies 97 2002 88 99 [9] S.L. Pomeroy P. Tamayo M. Gaasenbeek L.M. Sturla M. Angelo M.E. McLaughlin Prediction of central nervous system embryonal tumour outcome based on gene expression Nature 415 2002 436 442 [10] R.L. Somorjai B. Dolenko R. Baumgartner Class prediction and discovery using gene microarray and proteomics mass spectroscopy data: curses, caveats, cautions Bioinformatics 19 2003 1484 1491 [11] R.L. Somorjai A. Demko M. Mandelzweig B. Dolenko A.E. Nikulin R. Baumgartner Mapping high-dimensional data onto a relative distance plane\u2014A novel, exact method for visualizing and characterizing high-dimensional patterns J Biomed Inform 37 2004 366 379 [12] R.C.T. Lee J.R. Slagle H. Blum A triangulation method for the sequential mapping of points from N-space to two-space IEEE Trans Comput 1977 288 292 [13] J. Friedman J.W. Tukey A projection pursuit algorithm for exploratory data analysis IEEE Trans Computing C-23 1974 881 889 [14] P.J. Huber Projection pursuit Ann Stat 13 1985 435 475 [15] E. Pekalska R.P.W. Duin Dissimilarity representations allow for building good classifiers Pattern Recognit Lett 23 2002 943 956 [16] E. Pekalska R.P.W. Duin Automatic pattern recognition by similarity representations Electron Lett 37 2001 159 160 [17] Pekalska E, Duin RPW. Classifiers for dissimilarity-based pattern recognition. In Sanfeliu A, Villanueva JJ, Vanrell M, Alquezar R, Eklundh JO, Aloimonos Y, editors. Proceedings of the 15th international conference on pattern recognition, ICPR 2000, vol. 2, Los Alamitos, CA, USA, 2000. p. 12\u20136. [18] A. Farag\u00f3 T. Linder G. Lugosi Fast nearest-neighbor search in dissimilarity spaces IEEE Trans Pattern Anal Mach Intell 15 9 1993 957 962 [19] T.W. Anderson R.R. Bahadur Classification into two multivariate normal distributions with different covariance matrices Ann Math Statist 33 1962 420 431 [20] C. Gini Transvariazione 1959 Libreria Goliardica Roma [21] A. Montanari Linear discriminant analysis and transvariation J Classification 21 2004 71 88 [22] P. Paclik R.P.W. Duin Dissimilarity-based classification of spectra: computational issues Real-Time Imaging 9 2003 237 244 [23] J. Peltonen S. Kaski Discriminative components of data IEEE Trans Neural Netw 16 1 2005 68 83 [24] L.J. Cowen C.E. Priebe Randomized nonlinear projections uncover high-dimensional structure Adv Appl Math 19 1997 319 331 [25] Cannon AH, Cowen LJ, Priebe CE. Approximate distance classification. In: Proceedings of the 1998 symposium on the interface between computer science and statistics. vol. 30, no. (1), 1999. p. 544\u20139. [26] C.E. Priebe Olfactory classification via interpoint distance analysis IEEE Trans Pattern Anal Mach Intell 23 2001 404 413 [27] S.A. Metchev J.E. Grindlay A two-dimensional Kolmogorov\u2013Smirnov test for crowded field source detection: ROSAT sources in NGC 6397 Mon Not R Astron Soc 335 2002 73 83 [28] A.E. Nikulin B. Dolenko T. Bezabeh R.L. Somorjai Near-optimal region selection for feature space reduction: novel preprocessing methods for classifying MR spectra NMR Biomed 11 1998 209 217", "scopus-id": "33847677992", "pubmed-id": "16765098", "coredata": {"eid": "1-s2.0-S1532046406000463", "dc:description": "Abstract Previously, we introduced a distance (similarity)-based mapping for the visualization of high-dimensional patterns and their relative relationships. The mapping preserves exactly the original distances from all points to any two reference patterns in a special two-dimensional coordinate system, the relative distance plane (RDP). We extend the RDP mapping\u2019s applicability from visualization to classification. Several of the classifiers use the RDP directly. These include the standard linear discriminant analysis (LDA), nearest neighbor classifiers, and a transvariation probabilities-based classification method that is natural in the RDP. Several reference directions can also be combined to create new coordinate systems in which arbitrary classifiers can be developed. We obtain increased confidence in the classification results by cycling through all possible reference pairs and computing a misclassification-based weighted accuracy. The classification results on several high-dimensional biomedical datasets are compared.", "openArchiveArticle": "true", "prism:coverDate": "2007-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046406000463", "dc:creator": [{"@_fa": "true", "$": "Somorjai, R.L."}, {"@_fa": "true", "$": "Dolenko, B."}, {"@_fa": "true", "$": "Mandelzweig, M."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046406000463"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046406000463"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(06)00046-3", "prism:volume": "40", "prism:publisher": "Elsevier Inc.", "dc:title": "Direct classification of high-dimensional data in low-dimensional projected feature spaces\u2014Comparison of several classification methodologies", "prism:copyright": "Copyright \u00a9 2006 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "RDP mapping"}, {"@_fa": "true", "$": "Relative distance plane mapping"}, {"@_fa": "true", "$": "Direct classification"}, {"@_fa": "true", "$": "Comparison of classifiers"}, {"@_fa": "true", "$": "Transvariation"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "131-138", "prism:endingPage": "138", "prism:coverDisplayDate": "April 2007", "prism:doi": "10.1016/j.jbi.2006.04.001", "prism:startingPage": "131", "dc:identifier": "doi:10.1016/j.jbi.2006.04.001", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "52", "@width": "410", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2317", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "392", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1606", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "271", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1071", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "130", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "657", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "25", "@width": "27", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "313", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "22", "@width": "27", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "296", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "24", "@width": "162", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "812", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "186", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1267", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "199", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "836", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "51", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "344", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "208", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "116", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "596", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "30", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "283", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "18", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "250", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "39", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "371", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "36", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "347", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "65", "@width": "206", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1376", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "47", "@width": "200", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1180", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "844", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "80045", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "41", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1957", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "336", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "54298", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "94", "@width": "104", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3624", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "433", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "59518", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "80", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3038", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "233", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "40525", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "78", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3247", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "358", "@width": "369", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "61783", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "96", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3639", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "197", "@width": "654", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "68205", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "38", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046406000463-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2804", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/33847677992"}}