{"scopus-eid": "2-s2.0-84907963420", "originalText": "serial JL 272195 291210 291735 291737 31 80 Neuron NEURON 2014-10-01 2014-10-01 2017-10-04T21:34:03 1-s2.0-S0896627314007934 S0896-6273(14)00793-4 S0896627314007934 10.1016/j.neuron.2014.09.005 S300 S300.5 FULL-TEXT 1-s2.0-S0896627314X0019X 2017-10-04T16:59:01.627675-04:00 0 0 20141001 2014 2014-10-01T16:30:58.682769Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authlast footnotes primabst pubtype ref teaserabst 0896-6273 08966273 true 84 84 1 1 Volume 84, Issue 1 7 18 31 18 31 20141001 1 October 2014 2014-10-01 2014 Perspectives article rev Copyright \u00a9 2014 Elsevier Inc. All rights reserved. TOWARDASCIENCECOMPUTATIONALETHOLOGY ANDERSON D Main Text Introduction The Impact of Automated Measurement: Lessons from Fly Social Behavior Anatomy of a Computational Ethology System Generalization Discovery A Language to Describe Behavior Outlook Author Contributions Acknowledgments References ALBRECHT 2011 599 605 D ANDERSON 2014 187 200 D ANONYMOUS 2007 463 ARAKAWA 2014 127 134 T ASAHINA 2014 221 235 K BARSHALOM 1987 Y TRACKINGDATAASSOCIATION BATH 2014 756 762 D BELONGIE 2005 S WORKSHOPMEASURINGBEHAVIOR MONITORINGANIMALBEHAVIORINSMARTVIVARIUM BENTLEY 1978 35 59 D BORST 2009 R36 R47 A BRANSON 2009 451 457 K BROWN 2013 791 796 A BUI 2013 191 204 T BUTTON 2013 365 376 K CARD 2008 341 353 G CHEN 2002 5664 5668 S COIFMAN 1998 271 288 B COLLETT 2000 245 259 M CRAWLEY 1982 235 247 J DANKERT 2009 297 303 H DECHAUMONT 2012 410 417 F DELVECCHIO 2003 2085 2098 D DELL 2014 417 428 A DESLAND 2014 7 14 F DICKINSON 2000 100 106 M DICKSON 2008 904 909 B DIERICK 2007 2712 2718 H DOLLAR 2005 65 72 P IEEEINTERNATIONALWORKSHOPVISUALSURVEILLANCEPERFORMANCEEVALUATIONTRACKINGSURVEILLANCE BEHAVIORRECOGNITIONVIASPARSESPATIOTEMPORALFEATURES EGELHAAF 1988 351 358 M FARRELL 2013 6 20 M FENG 2004 115 Z FONTAINE 2006 3716 3719 E FONTAINE 2008 1305 1316 E FONTAINE 2009 1307 1323 E FRYE 2004 729 736 M GOMES 2010 R GOMEZMARIN 2012 e41642 A GONCALVES 2004 143 170 L GOTZ 1987 35 46 K GREEN 2012 266 271 J HALL 1994 1702 1714 J HAMADA 2008 217 220 F HAMBLEN 1986 249 291 M HARTLEY 2003 R MULTIPLEVIEWGEOMETRYINCOMPUTERVISION HARVEY 2009 941 946 C HASHEMI 2014 935686 J HEISENBERG 1984 M VISIONINGENETICSMICROBEHAVIOR HOYER 2008 159 167 S HUANG 2008 153 164 K INAGAKI 2014 325 332 H INSEL 1991 149 152 T IYENGAR 2012 306 316 A JHUANG 2010 68 H KABRA 2013 64 67 M KARBOWSKI 2008 253 276 J KATO 2014 616 628 S KERR 2008 195 205 J KHAN 2005 1805 1819 Z KITAMOTO 2001 81 92 T KLAPOETKE 2014 338 346 N KOHLHOFF 2011 755 760 K LAND 1974 331 357 M LEVITIS 2009 103 110 D LIN 2013 1499 1508 J LUO 2008 634 660 L LUO 2010 4261 4272 L METZ 1983 643 658 H MIZUTANI 2003 604 A MOORMAN 2011 377 385 S MULLER 1988 5287 5290 M NITABACH 2008 R84 R93 M NOLDUS 2001 398 414 L OHAYON 2013 10 19 S PAN 2010 1345 1359 S PEREZESCUDERO 2014 743 748 A PHAM 2009 323 326 J RABINER 1986 4 16 L REICHARDT 1976 311 375 W REID 2012 209 217 R REISER 2008 127 139 M RIHEL 2010 281 294 J SHOTTON 2013 116 124 J SIEGEL 1979 3430 3434 R SILASI 2013 123 G SIMONETTA 2007 273 280 S SIMPSON 2009 79 143 J SOKOLOWSKI 2001 879 890 M SPINK 2001 731 744 A STRAW 2011 395 409 A SWIERCZEK 2011 592 598 N TATAROGLU 2014 140 150 O TECOTT 2004 462 466 L TINBERGEN 1951 N STUDYINSTINCT TINBERGEN 1963 410 433 N TINBERGEN 1950 1 39 N TSAI 2012 e34784 H VEERARAGHAVAN 2008 463 476 A VOGELSTEIN 2014 386 392 J VONPHILIPSBORN 2011 509 522 A WEHRHAHN 1982 123 130 C WEISSBROD 2013 2018 A WILLIAMS 2004 1 30 H WOLF 2002 11035 11044 F YAMAMOTO 2013 681 692 D YEMINI 2013 877 879 E YIZHAR 2011 9 34 O ZABALA 2012 1344 1350 F ANDERSONX2014X18 ANDERSONX2014X18X31 ANDERSONX2014X18XD ANDERSONX2014X18X31XD Full 2015-10-01T00:21:25Z ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window item S0896-6273(14)00793-4 S0896627314007934 1-s2.0-S0896627314007934 10.1016/j.neuron.2014.09.005 272195 2017-10-04T16:59:01.627675-04:00 2014-10-01 1-s2.0-S0896627314007934-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/MAIN/application/pdf/18fdd1ae838cb36c7b602f93f0a2ad33/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/MAIN/application/pdf/18fdd1ae838cb36c7b602f93f0a2ad33/main.pdf main.pdf pdf true 3184805 MAIN 14 1-s2.0-S0896627314007934-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/PREVIEW/image/png/f7432782fdb4cbb3a097217be59d745b/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/PREVIEW/image/png/f7432782fdb4cbb3a097217be59d745b/main_1.png main_1.png png 69447 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0896627314007934-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr1/THUMBNAIL/image/gif/0e35e08351b130a2334a57397be3923c/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr1/THUMBNAIL/image/gif/0e35e08351b130a2334a57397be3923c/gr1.sml gr1 gr1.sml sml 32676 149 219 IMAGE-THUMBNAIL 1-s2.0-S0896627314007934-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr2/THUMBNAIL/image/gif/c4dab01fa3e2a8edab887dc0d5120fd1/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr2/THUMBNAIL/image/gif/c4dab01fa3e2a8edab887dc0d5120fd1/gr2.sml gr2 gr2.sml sml 22129 164 206 IMAGE-THUMBNAIL 1-s2.0-S0896627314007934-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr3/THUMBNAIL/image/gif/e975d9f7746f353d06a17ba8b0c57bca/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr3/THUMBNAIL/image/gif/e975d9f7746f353d06a17ba8b0c57bca/gr3.sml gr3 gr3.sml sml 17477 114 219 IMAGE-THUMBNAIL 1-s2.0-S0896627314007934-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr4/THUMBNAIL/image/gif/b00e832fd44460aefaaf34a9011a217a/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr4/THUMBNAIL/image/gif/b00e832fd44460aefaaf34a9011a217a/gr4.sml gr4 gr4.sml sml 27897 164 182 IMAGE-THUMBNAIL 1-s2.0-S0896627314007934-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr5/THUMBNAIL/image/gif/023bad70e22138bf63e5598e7d15b07d/gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr5/THUMBNAIL/image/gif/023bad70e22138bf63e5598e7d15b07d/gr5.sml gr5 gr5.sml sml 30258 159 219 IMAGE-THUMBNAIL 1-s2.0-S0896627314007934-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr1/DOWNSAMPLED/image/jpeg/ae0563a1cef9faf1242a60d7075e769d/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr1/DOWNSAMPLED/image/jpeg/ae0563a1cef9faf1242a60d7075e769d/gr1.jpg gr1 gr1.jpg jpg 151888 445 655 IMAGE-DOWNSAMPLED 1-s2.0-S0896627314007934-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr2/DOWNSAMPLED/image/jpeg/b4c7302f1c25a9c24a3a6bc1a14f733a/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr2/DOWNSAMPLED/image/jpeg/b4c7302f1c25a9c24a3a6bc1a14f733a/gr2.jpg gr2 gr2.jpg jpg 105961 521 656 IMAGE-DOWNSAMPLED 1-s2.0-S0896627314007934-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr3/DOWNSAMPLED/image/jpeg/1458219f0191430b28be50630dc7dbb0/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr3/DOWNSAMPLED/image/jpeg/1458219f0191430b28be50630dc7dbb0/gr3.jpg gr3 gr3.jpg jpg 53397 264 505 IMAGE-DOWNSAMPLED 1-s2.0-S0896627314007934-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr4/DOWNSAMPLED/image/jpeg/6ce1847a8aca4627a6dcaf650fdabe6a/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr4/DOWNSAMPLED/image/jpeg/6ce1847a8aca4627a6dcaf650fdabe6a/gr4.jpg gr4 gr4.jpg jpg 159214 591 656 IMAGE-DOWNSAMPLED 1-s2.0-S0896627314007934-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr5/DOWNSAMPLED/image/jpeg/0dac15460a39c523ab76cc333465652b/gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr5/DOWNSAMPLED/image/jpeg/0dac15460a39c523ab76cc333465652b/gr5.jpg gr5 gr5.jpg jpg 156358 559 769 IMAGE-DOWNSAMPLED 1-s2.0-S0896627314007934-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr1/HIGHRES/image/jpeg/0087ab8038bc481414d04ae4ee5de20f/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr1/HIGHRES/image/jpeg/0087ab8038bc481414d04ae4ee5de20f/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 1550351 1970 2899 IMAGE-HIGH-RES 1-s2.0-S0896627314007934-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr2/HIGHRES/image/jpeg/04c97f9852c5750db6b3a878c7d3d3e6/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr2/HIGHRES/image/jpeg/04c97f9852c5750db6b3a878c7d3d3e6/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 699747 2308 2905 IMAGE-HIGH-RES 1-s2.0-S0896627314007934-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr3/HIGHRES/image/jpeg/f7cd6cd06ff81a042b10305978a69842/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr3/HIGHRES/image/jpeg/f7cd6cd06ff81a042b10305978a69842/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 327022 1168 2238 IMAGE-HIGH-RES 1-s2.0-S0896627314007934-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr4/HIGHRES/image/jpeg/95602518ae2eaf386eaa1538b897fe05/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr4/HIGHRES/image/jpeg/95602518ae2eaf386eaa1538b897fe05/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 941533 2615 2905 IMAGE-HIGH-RES 1-s2.0-S0896627314007934-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627314007934/gr5/HIGHRES/image/jpeg/449b908609430adab4b869a075122e94/gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627314007934/gr5/HIGHRES/image/jpeg/449b908609430adab4b869a075122e94/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 1124444 2475 3404 IMAGE-HIGH-RES NEURON 12307 S0896-6273(14)00793-4 10.1016/j.neuron.2014.09.005 Elsevier Inc. Figure 1 Mapping, Monitoring, and Manipulating Specific Neuron Populations in Drosophila Using the GAL4/UAS Binary System (A) Upper: parental fly lines that contain a promoter-GAL4 or UAS-effector transgene (Table 1) are crossed to yield progeny that express GAL4, and therefore the UAS-effector, in a subpopulation of neurons defined by the specificity of the promoter (B and C). Lower: the effector can be an indicator to monitor the activity of the neurons, or ion channels or other genes to activate or silence the neurons. Reproduced with modifications by permission from Borst (2009). (B and C) A fluorescent marker allows visualization of neurons identified by different promoter-GAL4 lines. Arrows indicate cell bodies (green dots); arrowheads indicate nerve fibers. Figure 2 Drosophila Courtship Behavior Sequence of human observer-defined actions in courtship behavior. The actions may vary in their duration and the length of the interval between them. The sequence is not necessarily irreversible. The \u201ccourtship index\u201d (CI) is defined as the total amount of time that a fly spends in any of these behaviors. From Sokolowski (2001), with permission. Figure 3 Summary of Steps in the Automated Analysis of Social Behavior Each of the four steps (detection, tracking, action detection, and behavior analysis) requires validation by comparison to manually scored ground truth. The ethogram illustrates different behaviors performed during male-male and male-female social interactions. From Dankert et al., 2009, with permission. See also Table 3. Figure 4 Ethograms Based on Machine Vision Analysis of Multiple Flies (A\u2013D) Eight different behaviors (A) were automatically scored from video recordings of 20 flies walking in an arena. (B) Two minute trajectory of a single male fly detected among 20 in the arena. (C) Upper: raster plot for behaviors exhibited during the trajectory in (B); lower: translational and angular velocities superimposed on a 30 s segment of the raster plot. (D) Behavioral \u201cvectors\u201d for female, male, and fru1/fru1 mutant male flies. Each column represents a single fly and each row a single behavior. Numbers at top refer to experiment and number of flies tracked. From Branson et al., 2009, with permission. Figure 5 Unsupervised Behavioral Phenotyping of Large-Scale Neuronal Activation Screen in Drosophila Larvae (A) Larvae from each of 1,050 different GAL4 lines expressing channelrhodopsin-2 in 2\u201315 neurons/line (37,780 animals tested in total) were photostimulated and video recorded. (B) The contours of each larva from each video frame were extracted (streaming), and each of eight different time-varying features that characterize the shape and motion of each animal in each frame was extracted (sketching). (C) Unsupervised machine learning was used to identify and cluster different behavioral phenotypes (\u201cbehaviotypes\u201d), based on the sketches. (D) Behaviotypes associated with each GAL4 line were identified. (E) Actual tree clustering 29 different behaviotypes identified in the screen. Post hoc human labels were applied following retrospective analysis of videos associated with each cluster; clusters below third level could not be discriminated by human observers. (F) Matrix illustrating the different behaviotypes (columns) exhibited by each of approximately 450 \u201chits\u201d from the screen (GAL4 lines that were statistically significantly different from negative controls). Red circle identifies rare case of one behavior type (#16) produced by activation of just a few lines. From Vogelstein et al., 2014, with permission. Table 1 Genetic Manipulation of Neural Circuit Activity Promoter A segment of non-protein-coding DNA that contains information to cause (\u201cdrive\u201d) expression of a gene in a particular cell or cell type. \u201cPromoter\u201d is often misused as shorthand for an \u201cenhancer,\u201d which is technically the more precise and correct term. Effector A gene that encodes a protein whose activity influences some aspect of cellular function. In the case of a neuron, this could be an ion channel, neurotransmitter transporter, receptor, etc. It can also encode a fluorescent protein that serves to reveal a cell\u2019s anatomy (Figures 1B and 1C). Effectors can come from diverse species, e.g., jellyfish, algae, or bacteria. Binary system A general means of using a given promoter to express a given effector in a specific cell type, in order to functionally manipulate (e.g., excite or silence) that cell. The approach is \u201cbinary\u201d in the sense that it typically comprises two inhibit distinct components, which must be genetically combined to achieve the desired manipulation. Binary systems are flexible, because they permit different combinations of promoters and effectors, either to perform the same functional manipulation (e.g., neuronal silencing) in two different cell types or different functional manipulations (e.g., silencing and activation) in the same cell type. The \u201cGAL4/UAS\u201d system is a binary system often used in Drosophila to manipulate different classes of neurons (Figure 1A). Conditionality A property that permits an effector to be turned on or off in a controlled manner, by manipulating parameters that influence protein function. Examples of such variables include temperature, light, drugs, or mechanical pressure. Conditional manipulations can be used to control the time at which a particular effector is turned on or off in a given cell type. Those parameters must have no independent influence on the behavior or interest. Not all effectors are conditional. Thermogenetic Temperature-dependent conditional control, e.g., of an ion channel or enzyme. Optogenetic Light-dependent conditional control, e.g., of an ion channel or enzyme. Pharmacogenetic Drug-dependent conditional effector control. Sometimes called \u201cchemogenetic.\u201d Table 2 Machine Vision and Machine Learning Machine vision Discipline concerned with enabling machines to \u201csee,\u201d similarly to biological organisms that use their eyes to measure properties of their environment. Machine vision systems consist of one or more cameras that are connected to a computer. Images collected by the camera(s) are transferred to the computer, where appropriate calculations are performed to extract the relevant information, e.g., the position, shape, and identity of objects in the scene. This information is then made available to the user or employed to control equipment such as an autonomous vehicle. A challenge in machine vision is computing invariant descriptors from images, i.e., descriptors of image content (e.g., the color of a surface, the shape of an object\u2019s boundary) that are invariant with respect to irrelevant variations in the environment, such as the distribution and intensity of lighting. Machine Learning Discipline concerned with enabling machines to \u201clearn,\u201d instead of being programmed explicitly, to perform appropriate functions. Learning takes place with training from data and from human experts. In the case of computational ethology, a computer may be tasked with analyzing video and detecting specific animal actions, as specified by a human expert. For simple actions (e.g., \u201cwalking\u201d), the behavior of the machine may be programmed explicitly by the expert in the form of rules (e.g., \u201cif the fly moves faster than 0.1 mm/s, then it is walking\u201d); in this case no machine learning is needed. For more complex actions (e.g., lunging) it is difficult for a human to program explicit rules to detect the action reliably. However, the expert will be able to provide examples of the action to be detected, in the form of short snippets of video where the action takes place (positive training examples) and short snippets where it does not (negative training examples). Machine learning algorithms will make use of the training examples and produce an appropriate \u201caction classifier.\u201d The performance of the action classifier may be assessed by testing it on new examples that were not used for training. Machine vision and machine learning researchers collaborate in designing vision systems that can learn from example. Supervised learning A classifier is trained to recognize specific patterns that are defined by a human expert. The expert provides the learning algorithm with a set of labeled patterns. The classifier is iteratively trained and tested, and more training examples may be added, until it achieves satisfactory detection of all true positive events (\u201crecall\u201d) and minimal contamination with false positive events (\u201cprecision\u201d). Unsupervised learning A classifier is trained to detect and discriminate different patterns from unlabeled data, i.e., without information regarding how many and which patterns to look for. Unsupervised learning may lead to the discovery of previously unknown actions. Unsupervised learning is possible when the data cluster naturally, e.g., some actions may involve high-velocity motion and some may be carried out at low velocity. Table 3 A Glossary for Computational Ethology Detection Revealing the presence of an animal in a video frame and measuring its position. This requires algorithms for distinguishing the animal from the background. Pose The posture of an animal in three-dimensional space, including its length, orientation, and the angles of its body, limbs, or other appendages. Typically, first the animal is detected and then its pose is estimated by algorithms that analyze the image near the detected position. Tracking Tracing an animal\u2019s position and pose as a function of time in order to obtain a space-time trajectory from which parameters such as velocity, turning rate, and direction of approach may be computed. This requires algorithms for associating the position of an animal from one video frame to the next. Tracking is challenging when the animal moves quickly, when detection is missed in a few frames, and when the position is estimated incorrectly. These difficulties are more frequent when several animals are tracked simultaneously, since when animals interact they frequently come into contact and block each other from camera view (occlusion). Action classifier An algorithm using a set of rules to recognize a particular behavioral action (e.g., rearing, sniffing, wing extension) and discriminate episodes in which it occurs from those in which it does not. This includes determining the start and end points of the action (to the resolution of a single video frame). Classifiers may be hand crafted by experts or may be trained using machine learning algorithms. Classifiers are trained and tested using separate, \u201cground truth\u201d data sets (see below), which contain information about tracking and pose. They can be trained in a supervised or unsupervised manner (see above). Ground truth Segments of video in which an animal\u2019s position, pose, and/or actions have been manually annotated, on a frame-by-frame basis, by one or more experts. Ground truth provides not only data for training classifiers, but also represent the \u201cgold standard\u201d for evaluating the performance of a tracker, pose estimator, or classifier. Table 4 A Language for Behavior Moveme The simplest meaningful pattern associated with a behavior. Typically involves a short, ballistic trajectory described by a verb, such as a turn, a step, or a wing extension, which cannot be further decomposed. It is analogous to a \u201cphoneme\u201d in language. Action A combination of movemes that always occurs in the same stereotypical sequence and that is also described by a verb. Examples of actions include \u201cwalk\u201d (step + step + step), \u201cassess threat\u201d (stop + stand up on hind legs + sniff), \u201ceat\u201d (open mouth + bite + chew + swallow), etc. In language, it would be analogous to a word or to an idiomatic expression. Activity A species-characteristic concatenation of actions and movemes whose structure is typical, ranging from stereotyped to variable. Variability can be observed both in the structure or dynamics of the individual actions that comprise an activity, as well as in the timing and/or sequence of the actions. Examples of activities include courtship, aggression, nest-building, parenting, etc. Ethogram A representation of the different actions that occur during an activity or activities, which indicates the frequency or probability with which each action is followed by another action (either the same or a different one). Ethograms have traditionally been computed manually, by generating a \u201ctransition matrix\u201d composed of all the actions that are observed and the number of times each action is followed by another, given action, averaged over an observation period. Computer algorithms that detect and represent activities use hidden Markov models or other stochastic, time-series models to construct what is essentially a time-varying ethogram. Hidden Markov model A Markov model is a stochastic model of a time series, where knowledge of the item X(t) (the \u201cstate\u201d of the series at time t) makes the following samples independent of previous samples. For instance, if the state of the model is the action performed by a fly, the observation that a fly is lunging at time t makes any previous action irrelevant in predicting future actions (Markov property). This is clearly a simplification in most circumstances; however, biologists find Markov models informative when drawn in the form of ethograms, and engineers find them simple and useful in many circumstances. In a hidden Markov model (HMM), the state is not observable; instead, observations Y(t) are dependent on the \u201chidden\u201d state X(t). For example, the state might be an \u201cemotion state\u201d of the fly (Anderson and Adolphs, 2014), and we may only be able to observe a fly\u2019s action, which depends on the emotion. Mathematical manipulations permit estimating the hidden state of an HMM from the visible observations. Perspective Toward a Science of Computational Ethology David J. Anderson 1 2 1 \u2217 wuwei@caltech.edu Pietro Perona 3 1 \u2217\u2217 perona@caltech.edu 1 Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, CA 91125, USA 2 Howard Hughes Medical Institute, California Institute of Technology, Pasadena, CA 91125, USA 3 Division of Engineering, California Institute of Technology, Pasadena, CA 91125, USA \u2217 Corresponding author \u2217\u2217 Corresponding author 1 Co-senior author The new field of \u201cComputational Ethology\u201d is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area. The new field of \u201cComputational Ethology\u201d is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. In this Perspective, Anderson and Perona explore the opportunities and long-term directions of research in this area. Main Text Nothing enters our minds or determines our actions which is not directly or indirectly a response to stimuli beating upon our sense organs from without. Owing to the similarity of our construction and the sameness of our environment, we respond in like manner to similar stimuli, and from the concordance of our reactions, understanding is born.\u2014Nikola Tesla (\u201cA Machine to End War\u201d) Contempt for simple observation is a lethal trait in any science.\u2014Niko Tinbergen (\u201cOn Aims and Methods of Ethology\u201d) Introduction Understanding how the brain works is one of the most fascinating and challenging problems facing 21st century science. Brain function involves sensory perception, emotion, cognition, learning and memory, and motor control. All of these functions serve ultimately to control an animal\u2019s behavior. Behavior allows individuals and groups of animals to adapt to a changing environment on a timescale much faster than evolution by natural selection. It is that adaptive function, the \u201cselection for flexibility,\u201d that has driven the rapid evolution of brains across phylogeny. Accordingly, if we want to understand how the brain works, it is essential to consider its functions in the context of behavior (Dickinson et al., 2000; Frye and Dickinson, 2004). That means we need ways of quantitatively and accurately measuring behavior, in all its richness and complexity. The biological study of animal behavior, including its phenomenological, causal, ontogenetic, and evolutionary aspects, is a discipline known as ethology (Tinbergen, 1951, 1963). Ethology has a rich tradition, going back to the work of Konrad Lorenz, Karl von Frisch, and Niko Tinbergen, who received the 1973 Nobel Prize for their fundamental contributions. While ethology is a multifaceted discipline, at its core is the description and characterization of behavior, typically of intact freely moving animals in their natural environment. At its inception, such descriptions were primarily qualitative in nature. Over the last 40\u201350 years, however, the analysis of behavior has become increasingly quantitative. For example, while watching an animal behave, a human observer, rather than simply writing down a description of what he or she sees, will score incidents of specific behaviors defined according to certain generally accepted criteria. This produces a numerical, rather than a purely written, description of the behavior or behaviors being observed. While such quantification was at first typically performed using the traditional pencil, paper and stopwatch, advances in technology have facilitated this laborious process. There are now computer-assisted video analysis tools that allow user-defined behaviors to be scored with a keystroke, frame-by-frame. From such measurements, an observer can compute various statistics, such as the frequency of a given behavior, the latency to initiate a behavior, the duration of a behavioral episode, and the relative proportion of different behaviors or activities. Such measurements can be compared with simultaneous electrophysiological recordings of neuronal activity, permitting quantitative correlations between behavior and neuronal spiking activity. Behavior has also been quantified using instrumental methods such as audio recording (Insel and Winslow, 1991; Williams, 2004). In the field of birdsong research, for example, sonograms derived from such recordings have been a key quantitative readout for experimental manipulations of the underlying brain circuitry (Moorman et al., 2011). In the field of chronobiology, measurements of locomotor activity in Drosophila based on IR beam breaks (Hamblen et al., 1986; Tataroglu and Emery, 2014) have been essential to the identification of genes and circuits that control circadian rhythms (Nitabach and Taghert, 2008). Quantification of turning behavior during flight using torque sensors (Reichardt and Poggio, 1976; Heisenberg and Wolf, 1984) or wing-beat detectors (G\u00f6tz, 1987) has been valuable for studying orientation control and optomotor responses in Drosophila. The ability to correlate neuronal activity with quantitative analysis of behavior has been essential to the development of the field of neuroethology and has led to the development of important concepts such as the \u201ccommand neuron\u201d (Bentley and Konishi, 1978). Neuroscience is in the midst of a revolution fueled by spectacular new technologies for mapping, monitoring, and manipulating neural activity based on genetic targeting of specific neuron subtypes (Luo et al., 2008) (Figure 1 and Table 1 ). These methods afford the ability to move beyond correlation to establishing causal relationships between neural circuit activity and behavior. New tools, such as optogenetics (Yizhar et al., 2011) and pharmacogenetics (Farrell and Roth, 2013), connectomics (Reid, 2012), and optical imaging of neuronal activity (Kerr and Denk, 2008), are transforming our ability to understand how neural circuits control sensory perception, cognitive processes, internal brain states, and behavior (http://www.nih.gov/science/brain/2025/index.htm). Exploiting this transformative technology is, however, critically dependent on the ability to assess quantitatively, and with a high degree of spatiotemporal precision, the behavioral consequences of neural circuit manipulations. However, the technology for measuring behavior has not kept pace with the rapid development of these new methods; manual scoring of behavior is (with notable exceptions described below) still the dominant approach in the field. This has hampered progress in both understanding the neural circuit control of ethologically relevant behaviors and in using behavior as a \u201cread-out\u201d for manipulations aimed at uncovering fundamental principles of neural circuit function. Reliance on human observation to score behavior imposes a number of limitations on data acquisition and analysis. These include: It is slow. Human observer-based measurements of behavior, even when computer assisted, are extremely slow and labor intensive. (Annotating a video recording of an animal\u2019s behavior typically takes a human observer 3\u00d7 the length of the video.) This limits throughput and therefore reduces the number of different experiments that can be performed, as well as sample size, thereby limiting statistical power and the reliability of results (Button et al., 2013). It is imprecise and subjective. The scoring of behaviors by human observers is subjective, difficult to standardize, and therefore often inconsistent between different observers (Levitis et al., 2009). In our experience, two human observers will only agree 70% of the time in their analysis of the same data set. This makes it difficult to achieve consistency and reproducibility in behavioral analysis, both within and between laboratories. It is low-dimensional. Even a single behavior, as will be discussed below, can be deconstructed into progressively finer components, whose granularity is ultimately limited by the structure and function of the animal\u2019s motor system (Anonymous, 2007). While techniques exist for fine-scale analysis of motor behavior (Bui et al., 2013), measuring even a single behavior at multiple spatial and temporal scales simultaneously is currently challenging. Furthermore, due to the laborious nature of manual scoring, the number of different behaviors that can be measured concurrently is relatively small. It is limited by the properties of the human visual system. Some aspects of this ability, such as distinguishing two animals even when they occlude each other, or recognizing a behavioral pattern that involves rapid and variable but nevertheless typical actions (e.g., aggression), have not yet been surpassed by computers. However, computers may be able to spot new patterns that a human observer may have missed, due to inattention (see above), ascertainment bias, or timescales that exceed working memory. Simply put, humans are best at identifying and measuring behaviors that they expect to see. They are less good at discovering new ones. It is limited by human language. The identification or classification of actions and activities by human observers cannot always be captured by formal verbal definitions. Some features of the operational definition of a behavior may therefore be difficult to communicate, even if they seem intuitively obvious to a given observer. This makes it difficult to train new observers to score behaviors in a manner that replicates the original criteria developed by the trainer. It is dull. Spending hours and hours each day sitting in front of a video monitor is mind-numbingly boring, so an observer\u2019s attention can easily drift. This not only increases the likelihood of error, but also the chance that interesting new behaviors will be missed. It also diverts effort from more creative uses of the human brain. We argue here that a new interdisciplinary field, made possible by advances in mathematics, engineering, and computer science, is in a position to overcome these difficulties. This emerging field, which involves collaborations between physical scientists from these disciplines and biologists, has the potential to revolutionize the way in which we measure and model behavior. We call this field \u201ccomputational ethology\u201d (CE), to emphasize both its roots in the study of natural behavior in freely moving animals and the application of modern quantitative tools for measuring, describing, and analyzing behavior. It exploits recent advances in machine learning and machine vision (Table 2 ) to automatically identify and quantify instances of known, observer-defined behaviors, as well as to discover potentially new behaviors. One might wonder about the need of prepending \u201ccomputational\u201d to any field of science\u2014it is obvious that computational tools are useful and will be gradually introduced to almost any discipline. However, we feel that in the case of ethology, computational tools have the potential of transforming the study of behavior. With the new technology available for neural circuit analysis, this field should improve the ability to move beyond correlations to establish causal relationships between molecular and cellular mechanisms, circuit-level computations, and behavior. Here we describe the promise of CE and the challenges it faces to grow into a mature discipline. The Impact of Automated Measurement: Lessons from Fly Social Behavior Automating the measurement of behavior offers, in principle, solutions to many of the problems outlined above. Before delving into the details of how such measurements are achieved, however, it is useful to see how this approach has already impacted a particular field. The study of social behavior in Drosophila melanogaster, an area in which the authors have worked (Dankert et al., 2009), provides such an illustrative example. Social behaviors in the vinegar fly include both dyadic (pairwise) interactions such as male-female courtship and male-male aggression, as well as higher-dimensional interactions within large (>10) groups of flies. Innate social behaviors such as mating and fighting have, in particular, attracted attention because they offer the possibility of understanding how such behaviors are encoded in DNA (Dickson, 2008). Increasing the dimensionality of behavioral analysis. Courtship in Drosophila is a complex process involving a series of discrete, sterotypic actions that progress in an ordered, but temporally variable, sequence (reviewed in Dickson, 2008; Yamamoto and Koganezawa, 2013) (Figure 2 ). Because of the labor involved in manually scoring each of these actions individually, courtship behavior has typically been quantified by means of a \u201ccourtship index\u201d (CI) (Hall, 1994), which reflects the amount of time the fly spends engaged in any courtship-related behavior. Such a metric, however, conflates multiple courtship-related actions, such as chasing, licking, singing, or copulation, and gives them equal weight. The use of such a combined index can, therefore, confound the comparison of results between experiments, since similar CI values may reflect different underlying behaviors: a male fly that spends most of his time chasing a female could receive the same CI as a fly that spent the same amount of time singing. Recent studies have developed technology to automatically measure individual courtship-related behaviors (Dankert et al., 2009; von Philipsborn et al., 2011; Tsai and Huang, 2012; Asahina et al., 2014), using machine vision and machine learning-based approaches (see Figure 3 and Table 3 ). In time, one would ideally like to measure automatically multiple elements of the courtship behavioral repertoire with equal precision and accuracy, in order to be able to quantify the distribution, proportion, and relationship of each of these different actions over the course of an experiment, a sort of time-evolving ethogram. Increasing the throughput of behavioral analysis. Automated analysis increases not only the dimensionality of behavioral measurements, but also its accuracy, consistency, objectivity, and\u2014perhaps most importantly\u2014experimental throughput. Until recently, Drosophila social behavior was (and largely continues to be) manually scored. This has limited analysis to dyadic interactions, simply because keeping track of 10\u201320 flies in an arena is virtually impossible for a human observer. However, recent studies have developed video tracking systems to measure the individual trajectories of dozens of flies simultaneously in an arena (Branson et al., 2009) (Figure 4 ). Analysis of these data has led to the discovery of new behaviors: for example, it was found that female flies in such groups occur within < 2 body lengths of each other much less frequently than expected by chance, suggesting that they actively avoid proximity (Branson et al., 2009; Zabala et al., 2012). More recent efforts have allowed multiorganism tracking while identifying and discriminating individuals (P\u00e9rez-Escudero et al., 2014). Even in the case of dyadic interactions such as courtship and aggression, the laboriousness of manual scoring has been a limiting factor in several respects. First, it makes it very difficult to perform high-throughput, unbiased \u201cforward\u201d genetic screens, for which Drosophila is an ideal system. Such screens are the best way that fundamentally new and unexpected discoveries can be made about genes or neurons that regulate behavior. Audio recordings have been used to conduct a systematic screen for neurons that control male courtship song (von Philipsborn et al., 2011), illustrating the power of such an approach. But automated measurements of additional aspects of courtship behavior (von Philipsborn et al., 2011; Tsai and Huang, 2012; Asahina et al., 2014), as well as of other social behaviors, such as aggression (Dankert et al., 2009), could greatly increase the applicability of this approach. Automated analysis also affords greater depth, scope, rigor of experimental design, and statistical power in \u201clow-throughput\u201d hypothesis-driven research. It allows more hypotheses to be tested, more variables to be explored, and more controls to be included and enables larger sample sizes to achieve greater statistical power (Button et al., 2013). A recent study of aggression in Drosophila (Asahina et al., 2014), which utilized automated behavioral analysis (Dankert et al., 2009), illustrates the magnitude of this benefit. The authors used temperature-dependent manipulations of neuronal activity (Simpson, 2009) (Table 1 and Figure 1) to identify a small cluster of neurons that controls aggression. Such manipulations require a variety of controls (up to five per experiment) to exclude artifactual effects of transgenes or temperature on behavior. This, in turn, requires correction for multiple comparisons when performing statistical analysis. Because of the high variance in the data, large sample sizes (e.g., n = 25 per experimental or control condition) are required for sufficient statistical power (Button et al., 2013). Thus, quantifying behavior in a typical experiment with associated controls would require about 150 hr of manual scoring of aggressive behaviors (Chen et al., 2002; Dierick, 2007); the aggregate experiments in the paper would have required 2,000\u20133,000 hr of scoring (5 months of 40 hr/week of scoring). Such a study would, therefore, simply not have been feasible without automated measurement of behavior. Automated analysis facilitates closed-loop experiments. Machine vision-based automated tracking of freely moving animals offers another advantage: real-time analysis of behavior. This allows on-line measurements of the animal\u2019s position, speed or other behavioral statistics to be fed back into a system for controlling the activity of specific neurons, in a closed-loop manner. This can reveal features of the causal relationship between neuronal activity and behavior in a manner that would not otherwise be apparent. In a recent study, for example, online video tracking was used to target an IR laser to moving flies during courtship behavior to activate or inhibit the activity of previously identified (von Philipsborn et al., 2011) courtship neurons using thermogenetic effectors such as dTrpA1 (Hamada et al., 2008) and shibirets (Kitamoto, 2001; Bath et al., 2014) (Table 1). Using this approach, the authors were able to control neuronal activity with an unprecedented degree of spatial and temporal resolution, uncovering features of courtship behavior that had not previously been identified, such as persistent male \u201csinging\u201d following transient activation of a particular subset of neurons (see also Inagaki et al., 2014). This method also permits, in principle, bimodal manipulation of different neuronal populations in the fly brain, using a combination of thermogenetic and red-shifted optogenetic effectors recently adapted to flies (Lin et al., 2013; Inagaki et al., 2014; Klapoetke et al., 2014). This approach should expand the applications of the \u201cgenetic toolkit\u201d in Drosophila and facilitate the analysis of functional pathways controlling courtship and other social behaviors in Drosophila. The foregoing examples all involve the quantification of behaviors that have been previously identified by biologists. The current practice is to observe a behavior and then to develop a method to automatically detect the same behavior in a manner that agrees with the human observer. However, as we discuss later, computational ethology offers the possibility of discovering new behaviors that have been missed by human observers and may soon allow us to describe and measure the complete behavioral repertoire of an animal. This in turn would enable researchers to correlate virtually any aspect of behavior with genetic, pharmacological, and neurophysiological manipulations and measurements, rather than prejudging which aspects of behavior should be measured. In order to see how this could be achieved, it is necessary to understand the process by which behavioral \u201cclassifiers\u201d are developed using machine vision and machine learning techniques. Anatomy of a Computational Ethology System It has been exciting to observe, during the past decade, increasing efforts directed at automating the measurement of behavior. Both academic open-source and commercially available hardware/software packages now allow investigators to carry out measurements such as tracking the trajectories of rodents in an arena (de Chaumont et al., 2012; Gomez-Marin et al., 2012; Ohayon et al., 2013) and measuring freezing in fear conditioning experiments (Spink et al., 2001; Noldus et al., 2001; Pham et al., 2009). Such tools are valuable to the pharmaceutical industry, because they permit high-throughput screening of drugs for neurological or neuropsychiatric disorders (Tecott and Nestler, 2004). Due to the intrinsic difficulty of building automated systems, most such efforts have focused on measuring a limited but well-defined repertoire of behaviors (e.g., see Dankert et al., 2009; Pham et al., 2009). Machines or software that are created to measure a predefined set of actions, however, will be unable to reveal new ones, or new ways in which known actions may be performed. One long-term objective of computational ethology ought to be measuring an animal\u2019s behavior in its entirety (a discipline that has been dubbed \u201cethomics\u201d [Branson et al., 2009]), including behaviors that were not foreseen by the experimentalist. What are the components of an automated system for measuring behavior? First of all, sensors are needed. While instrumentation to detect movement (acclerometers, IR beam-breaks, reflectors), audio signals (microphones), and other physical behavioral measures have been used for some time, advances in machine vision and machine learning have led to an increased emphasis on video recording as a primary sensing modality. Cameras afford high spatial and temporal resolution and can, in principle, access most aspects of behavior without constraining the animal\u2019s movements (Feng et al., 2004; Belongie et al., 2005; Fontaine et al., 2008). Arrangements including multiple cameras (Straw et al., 2011) and depth sensors (Shotton et al., 2013) facilitate the measurement of position and motion in 3D. Infrared cameras and lighting are used to sense the behavior of nocturnal animals, such as mice. The mundane business of acquiring and storing large volumes of video is, nevertheless, nontrivial and complicated by the need to calibrate (Hartley and Zisserman, 2003) and to synchronize all the sensors involved (Straw et al., 2011), as well as having to handle and store large volumes of data, which may reach terabytes for a single experiment (Ohayon et al., 2013). Alongside sensors, it is often important to make use of actuators in order to modify the environment dynamically and thus elicit or modify interesting behavior; this is traditionally done by hand (Tinbergen and Perdeck, 1950). Automated and robotic systems have recently made their appearance (Reiser and Dickinson, 2008; Harvey et al., 2009; Albrecht and Bargmann, 2011), including closed-loop systems (Zabala et al., 2012; Bath et al., 2014), where the behavior of the animal determines the machine\u2019s response, and vice-versa. Software systems for measuring behavior from video recordings are composed of three distinct modules: tracking, action classification, and behavior analysis (Figure 3 and Table 3). Tracking means computing trajectories (Bar-Shalom, 1987). First of all, animals are detected, i.e., identified and distinguished from background in each frame of the video, and their position is measured. Additional parameters, such as the orientation of the body, limbs, and other appendages, may be measured as well; this richer characterization of the configuration of the body is often called \u201cpose.\u201d Finally, the position and pose of each individual animal is concatenated frame by frame to obtain a trajectory describing its motion through time (Wehrhahn et al., 1982; Noldus et al., 2001; Spink et al., 2001; Branson and Belongie, 2005; Straw et al., 2011; Ohayon et al., 2013; P\u00e9rez-Escudero et al., 2014). Automated tracking has been developed and applied to multiple model organisms, including C. elegans (Feng et al., 2004; Fontaine et al., 2006; Simonetta and Golombek, 2007; Swierczek et al., 2011), Musca (Wehrhahn et al., 1982), Drosophila (Wolf et al., 2002; Card and Dickinson, 2008; Branson et al., 2009; Dankert et al., 2009; Fontaine et al., 2009; Kohlhoff et al., 2011; Gomez-Marin et al., 2012; Iyengar et al., 2012; Tsai and Huang, 2012), ants (Khan et al., 2005), bees (Veeraraghavan et al., 2008), zebrafish (Fontaine et al., 2008; Rihel et al., 2010; Green et al., 2012), and mice (Crawley et al., 1982; Ohayon et al., 2013; Silasi et al., 2013; Weissbrod et al., 2013; Arakawa et al., 2014; Desland et al., 2014). In fact, the proliferation of automatic video tracking software has created its own problems, due to a lack of standardization. Tracking software that generalizes across multiple settings and organisms is an important step toward resolving this issue (P\u00e9rez-Escudero et al., 2014). The next module is action classification, i.e., identifying specific intervals of time when an \u201caction,\u201d i.e., a user-defined, relatively simple, and ethologically or ecologically relevant pattern of motion, is performed. These patterns (e.g., grooming, walking, or courtship song in flies) are detected by classifiers: computer algorithms that are able to detect instances of a given action in a video recording and to discriminate those instances from periods where the action does not occur. Action classifiers are trained from expert-provided \u201cground truth\u201d examples, i.e., \u201cpositive\u201d video clips where the action takes place and \u201cnegative\u201d clips where it is not happening. From these labeled training examples, a machine learning algorithm can \u201ctrain the classifier,\u201d i.e., develop a set of rules by which the classifier can recognize the desired actions (Doll\u00e1r et al., 2005; Dankert et al., 2009; Branson et al., 2009; Jhuang et al., 2010; Burgos-Artizzu et al., 2012; de Chaumont et al., 2012; Kabra et al., 2013). Action classifiers may be trained in two ways: \u201csupervised\u201d and \u201cunsupervised.\u201d A supervised classifier is one that is trained to detect specific actions, specified via annotated training examples (see above), that are already recognized and established by biologists, e.g., a \u201clunge\u201d or a \u201cwing extension\u201d in Drosophila (Hoyer et al., 2008; Dankert et al., 2009; Iyengar et al., 2012; Tsai and Huang, 2012; Asahina et al., 2014). An unsupervised classifier is one in which the operator makes no assumptions about what kind(s) of actions and behaviors are occurring, but simply provides the learning algorithm with representative videos without annotations. Using statistical criteria, the algorithm then develops its own set of classifiers by which it decomposes the animals\u2019 behavior into units or episodes. A biologist is then free to determine whether any of those units correspond to \u201cbehaviors\u201d that he/she could recognize. Initial efforts to apply such unsupervised approaches have recently been made in Drosophila (Berman et al., 2013; Vogelstein et al., 2014) (Figure 5 ) and in C. elegans (Yemini et al., 2013). The last module, behavior analysis, has the ultimate goal of estimating \u201cactivities,\u201d i.e., large-scale behavioral patterns (aggression, courtship) assembled from different actions. At a minimum, such an analysis consists of computing an ethogram (Chen et al., 2002; Dankert et al., 2009), which describes the frequency of each action and the probability that a given action is followed by another. A more sophisticated level of analysis involves developing models of how animals make decisions and control their actions based on their internal state and on external stimuli (e.g., see Karbowski et al., 2008; Luo et al., 2010). Each one of these modules presents considerable technical and conceptual difficulties. Tracking (of multiple animals) is made difficult by camouflage, occlusion (one animal blocking another, hiding behind an object, or leaving the field of view of the camera), motions that are too fast or too minute for the camera to follow at a given frame rate and a given spatial resolution, and trying to reconstruct three-dimensional body plans and motions based on observations made using two-dimensional sensors. These somewhat mundane difficulties may be addressed by the use of more than two cameras or of complementary technology such as depth sensors. However, there are more subtle and formidable challenges: generalization and discovery. These are currently exciting subjects of research for machine learning and machine vision researchers, and computational ethology will benefit greatly from these efforts. Generalization Currently, trackers, action, and behavior classifiers do not generalize well from one preparation to another (but see P\u00e9rez-Escudero et al., 2014; Kabra et al., 2013). They are designed specifcally for a given animal, a given environment, and a given set of actions (Simonetta and Golombek, 2007; Huang et al., 2008; Dankert et al., 2009; Branson et al., 2009). Therefore, for each preparation a considerable investment of time and effort is required on the part of biologists, who have to label extensively by hand both the pose and the actions of a number of animals in order to provide the system with sufficient and diverse training examples. Furthermore, considerable manual effort is required to validate and retrain the classifier until it reaches an adequate level of performance (e.g., see Asahina et al., 2014). This process must be repeated each time the system design changes or new actions are scored. Future systems will be much more adaptable and intelligent: they will be trainable quickly and interactively to track new animals and detect new actions in a variety of experimental settings. A first step in this direction was taken in the design of the Janelia Automatic Animal Behavior Annotator (JAABA) package developed by Branson and colleagues (Kabra et al., 2013). Furthermore, by using recently developed \u201ctransfer learning\u201d techniques (Pan and Yang, 2010), an action learned from one animal (e.g., \u201cchasing\u201d in Drosophila) will prepare the system for detecting similar actions in other animals (e.g., \u201cchasing\u201d in mouse). Discovery Human observers are sometimes able to spot novel patterns, e.g., a new behavior, or a different way in which a known action is performed, provided that they are sufficiently observant and attentive. Automated systems that have the same ability to make serendipitous discoveries would be of great value. For example, screening thousands of mutant genotypes by video recording of freely moving animals may reveal new behaviors (Brown et al., 2013; Vogelstein et al., 2014), which conventionally trained machine vision systems (i.e., supervised classifiers) are bound to either miss or to misclassify. Classifiers developed using unsupervised learning can be used to circumvent this problem and produce testable hypotheses on the existence of new behavioral phenotypes among the different genotypes screened (Brown et al., 2013; Vogelstein et al., 2014). Such techniques are reasonably well understood in the simple scenarios that are used by theoreticians to develop proof-of-principle experiments (Gomes et al., 2010). However, their application is notoriously difficult in real-life scenarios, especially, as in the case of behavior, when the data are high-dimensional and behavioral phenomena are highly variable. A handful of pioneering studies in the past two years have started the exploration of this topic (Brown et al., 2013; Berman et al., 2013; Vogelstein et al., 2014). A Language to Describe Behavior How should we describe behavior? As ethology becomes more quantitative and computational, mathematical models that attempt to account for the details of phenomena appear alongside earlier qualitative verbal descriptions. We can identify three types of description/model: phenomenological, mechanistic, and functional/evolutionary. One may think of these three descriptions as answering the \u201cwhat,\u201d \u201chow,\u201d and \u201cwhy\u201d questions. A phenomenological description is typically the first step of analysis. However, understanding of behavior must include both a mechanistic and functional account. These are all related, but it is important to distinguish between them. The phenomenological description accounts for the form of the phenomenon, e.g., that a \u201cchase,\u201d whether it is carried out by a fly or a mouse, consists of walking fast a short distance behind another individual for at least a certain amount of time. The phenomenological description is what a scientist would provide in order to characterize the action and communicate its nature to a colleague, and it is the signal that the \u201caction classifiers\u201d described above will use to detect and classify the action. Since behavioral phenomena take place at multiple scales of resolution in time and space, qualitatively different statistical and geometrical models are needed to describe the phenomenology of behavior (Table 4 ). We use the term \u201cmoveme\u201d (Bregler and Malik, 1998; Del Vecchio et al., 2002, 2003; Goncalves et al., 2004) (in analogy to \u201cphoneme\u201d) to refer to the simplest meaningful pattern: a short, mostly ballistic, trajectory that has a verb associated with it, e.g., \u201cturn,\u201d \u201cstep,\u201d \u201cextend wing,\u201d and cannot be further decomposed. We call \u201caction\u201d the composition of movemes that occur always in the same stereotypical sequence, e.g., \u201cwalk\u201d (step + step + step), \u201csing\u201d (extend wing + hold wing out + retract wing) (Siegel and Hall, 1979), and \u201clunge\u201d (stop + raise on hind legs + lunge forward + stop) (Hoyer et al., 2008), and we call \u201cactivity\u201d a concatenation of actions and movemes whose structure is typical and yet variable, e.g., \u201ccourtship\u201d or \u201cfighting\u201d (Chen et al., 2002). While movemes may be detected by frame-by-frame pattern recognition systems, detecting actions may require more sophisticated techniques to account for their composite nature (Eyjolfsdottir et al., 2014). Detecting and representing activities requires stochastic time-series models, such as Markov models (MM) and hidden Markov models (HMM) (Rabiner and Juang, 1986), which are commonly depicted as \u201cethograms\u201d by biologists (e.g., see Yamato et al., 1992; Metz et al., 1983; Chen et al., 2002) (Table 4). The mechanistic description relates behavior to the underlying neural mechanisms that produce it, i.e., to circuits composed of sensors, integrators, and actuators, and their relationship to the interaction of the animal\u2019s body with the environment. For example, \u201cchase\u201d may be described as being triggered by visual motion, guided by visual mechanisms that respond to dark dots that, in turn, control pattern generators in the motor system (Land and Collett, 1974; Egelhaaf et al., 1988). Complex trajectories may have simple explanations once one understands the mechanisms that are involved, e.g., path integration in ants (M\u00fcller and Wehner, 1988; Collett and Collett, 2000), a bit like gravitation, provides a more satisfactory and simpler explanation of the planets\u2019 trajectories, as observed from earth, than does Ptolemy\u2019s theory of epicycles. The functional/evolutionary level of description focuses on the goals or objectives of the organism under a given set of environmental and/or internal conditions. It models behavior as a means to achieve these goals, taking into account physical and information-processing constraints, but also abstracting from the specifics of the implementation. For example, the trajectory of a dragonfly chasing a prey object may be optimized to reduce the probability of its detection by the prey, and this may account for its characteristic shape (Mizutani et al., 2003). The functional/evolutionary explanation is particularly important because it has the potential of identifying common aspects of behavior across species. This in turn can be used to formulate new questions about whether the circuit-level implementation of such common aspects is conserved or not. The complexity of behavior makes representation and modeling a challenge. First of all, behavior is a mixture of discrete and continuous components (e.g., \u201cdecisions\u201d and \u201ctrajectories\u201d). Its temporal scale ranges from the simplest actions taking place in a fraction of a second (a glance, a step) to elaborate patterns lasting for minutes (courtship) or even hours and days (dominance). Furthermore, the number of individuals, objects, and locations involved in a behavior ranges in complexity from the simple act of taking a step to the multiagent, multilocation, and multiepoch social behavior of a large colony of rodents. Current modeling techniques allow us to focus on only one aspect of behavior at a time. However, as computational ethology evolves, we will seek to understand how the different components of behavior are tied together, e.g., how the instantaneous position and orientation of a fly affects the view it has of its nearest conspecific, and how this, in turn, may trigger a chase, and what effect this has on the outcome of courtship. For this type of analysis, we will need to develop more complete models of behavior that can represent all aspects of a given scene. This will require simultaneous representations at multiple timescales, discrete and continuous descriptions (actions, trajectories), multiple frames of reference (world, object-centered, partner-centered, self-centered), goals (eat, mate) as well as attributes (safe, virgin), as well as describe causal relationships. Given these complexities, it is not surprising that a general, computationally sound approach to describing behavior using conventional descriptors has not yet emerged, since it is unlikely to be manageable \u201cby hand\u201d (as, for instance, ethograms are). Nevertheless, important initial steps in this direction have already been taken (Berman et al., 2013; Karbowski et al., 2008; Kato et al., 2014; Luo et al., 2010). Further progress will be enabled by modern computers and software and by researchers who can seamlessly transition from biology to computation and back. Outlook Computational ethology offers the promise of revolutionary progress in a number of areas of neuroscience. The first is being able to combine neurophysiological recordings or optical imaging of neuronal activity, and functional manipulation of specific neurons, together with both broad and fine-grained descriptions of behavior (e.g., see Kato et al., 2014). Second, the ability to carry out rich unbiased high-throughput genetic or cellular screens will take us closer to understanding the link between genes, neural networks, and behavior (Brown et al., 2013; Vogelstein et al., 2014). These approaches will provide a wealth of detailed information allowing neuroscientists and ethologists to develop a deeper understanding of how behavior is controlled by the brain. The study of social behaviors will be particularly advanced (Branson et al., 2009; Dankert et al., 2009; Burgos-Artizzu et al., 2012; Iyengar et al., 2012; Eyjolfsdottir et al., 2014). Ultimately, the study of human psychiatric and neurological disorders may benefit from this technology, allowing, for example, the development of novel diagnostic tests for disorders such as autism (Hashemi et al., 2014). There are, of course, many applications of computational ethology beyond neuroscience (Coifman et al., 1998; Shotton et al., 2013; Dell et al., 2014). As this brief (and necessarily superficial) survey indicates, CE is an expanding, fast-moving, and exciting new area of science, at the boundary between biology and engineering, where tool building, computational theory, and biological discovery are progressing hand in hand. However, many challenges remain. Behavior is among the richest and most complex phenomena that machine vision and machine learning researchers are tackling today. Progress in CE will likely become a large driver of research in these two fields. Conversely, major new insights in machine learning and machine vision are needed in order to achieve the full potential of CE in furthering neuroscience research. Lastly, and more importantly, the development of computational tools will enable researchers to study the phenomenon and causes of behavior more in detail, more completely, and more systematically. The path ahead is exciting and rich in promise, but the endeavor is a complex intellectual and technological enterprise that will take large doses of both inspiration and perspiration to make progress and deliver on its promises. It will also require that funding agencies recognize the importance of this cutting-edge interdisciplinary field. Currently, support for research focused on the analysis of behavior per se appears as if it is being phased out, in favor of neural circuit-oriented research. While research on neural circuits is undeniably important and an exciting new frontier, our ability to pose meaningful questions about how these circuits function is, ultimately, limited by our ability to identify and measure behavioral readouts of circuit activity. Particularly in laboratory-bred model organisms, the repertoire of measurable behaviors is not especially diverse, leading to a great deal of redundant research focused on a relatively small number of behavioral paradigms. The application of CE methods, and especially unsupervised machine learning approaches, should produce a more complete and thorough characterization of known behaviors and may lead to the discovery of previously unknown behaviors, thereby diversifying the opportunities for new lines of research. One of the biggest challenges facing CE is recruiting to the field and training a new generation of scientists who are equally conversant in machine learning and machine vision as in biology. Engineers and mathematicians are not trained to understand the nature of the questions that biologists ask, as well as the strategy that is followed by biologists to chip away at the important questions outlined above. Conversely, biologists are not routinely trained in computer science and in mathematics. Researchers in these disparate fields often do not even use compatible word-processing programs, as the coauthors of this Perspective discovered to their unending frustration. Thus, computational ethology will provide not only new scientific and technological advances, but opportunities for educating a new generation of interdisciplinary researchers as well. Author Contributions D.J.A. and P.P. jointly shaped the message and the structure of this paper and contributed to the introduction and conclusions sections. D.J.A. contributed writing regarding the biological applications of computational ethology. P.P. contributed writing regarding the engineering and mathematical aspects of computational ethology. Acknowledgments The authors are grateful to the Moore Foundation, to an ONR MURI grant, and to Caltech Provost Ed Stolper for funding their research in this area. Conversations with Michael Dickinson and Kristin Branson were influential in forming many of the ideas that shaped this contribution. D.J.A. is an Investigator of the Howard Hughes Medical Institute and an Allen Distinguished Investigator. This work was supported in part by a grant from Gerald Fischbach and the Simons Foundation. D.J.A. thanks P.P. for introducing him to the joys of LaTeX. References Albrecht and Bargmann, 2011 D.R. Albrecht C.I. Bargmann High-content behavioral analysis of Caenorhabditis elegans in precise spatiotemporal chemical environments Nat. Methods 8 2011 599 605 Anderson and Adolphs, 2014 D.J. Anderson R. Adolphs A framework for studying emotions across species Cell 157 2014 187 200 Anonymous, 2007 Anonymous Geneticist seeks engineer: must like flies and worms Nat. Methods 4 2007 463 Arakawa et al., 2014 T. Arakawa A. Tanave S. Ikeuchi A. Takahashi S. Kakihara S. Kimura H. Sugimoto N. Asada T. Shiroishi K. Tomihara A male-specific QTL for social interaction behavior in mice mapped with automated pattern detection by a hidden Markov model incorporated into newly developed freeware J. Neurosci. Methods 234 2014 127 134 Asahina et al., 2014 K. Asahina K. Watanabe B.J. Duistermars E. Hoopfer C.R. Gonz\u00e1lez E.A. Eyj\u00f3lfsd\u00f3ttir P. Perona D.J. Anderson Tachykinin-expressing neurons control male-specific aggressive arousal in Drosophila Cell 156 2014 221 235 Bar-Shalom, 1987 Y. Bar-Shalom Tracking and data association 1987 Academic Press Professional, Inc. San Diego Bath et al., 2014 D.E. Bath J.R. Stowers D. H\u00f6rmann A. Poehlmann B.J. Dickson A.D. Straw FlyMAD: rapid thermogenetic control of neuronal activity in freely walking Drosophila Nat. Methods 11 2014 756 762 Belongie et al., 2005 S. Belongie K. Branson P. Doll\u00e1r V. Rabaud Monitoring animal behavior in the smart vivarium Workshop on Measuring Behavior 2005 Wageningen The Netherlands Bentley and Konishi, 1978 D. Bentley M. Konishi Neural control of behavior Annu. Rev. Neurosci. 1 1978 35 59 Berman et al., 2013 Berman, G.J., Choi, D.M., Bialek, W., and Shaevitz, J.W. (2013). Mapping the stereotyped behaviour of freely-moving fruit flies. arXiv, arXiv:1310.4249, http://arxiv.org/abs/1310.4249. Borst, 2009 A. Borst Drosophila\u2019s view on insect vision Curr. Biol. 19 2009 R36 R47 Branson and Belongie, 2005 Branson, K., and Belongie, S. (2005). Tracking multiple mouse contours (without too many samples). In IEEE Computer Society Conference on Computer Vision and Pattern Recognition 1, 1039\u20131046. Branson et al., 2009 K. Branson A.A. Robie J. Bender P. Perona M.H. Dickinson High-throughput ethomics in large groups of Drosophila Nat. Methods 6 2009 451 457 Bregler and Malik, 1998 Bregler, C., and Malik, J. (1998). Tracking people with twists and exponential maps. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 8\u201315. Brown et al., 2013 A.E. Brown E.I. Yemini L.J. Grundy T. Jucikas W.R. Schafer A dictionary of behavioral motifs reveals clusters of genes affecting Caenorhabditis elegans locomotion Proc. Natl. Acad. Sci. USA 110 2013 791 796 Bui et al., 2013 T.V. Bui T. Akay O. Loubani T.S. Hnasko T.M. Jessell R.M. Brownstone Circuits for grasping: spinal dI3 interneurons mediate cutaneous control of motor behavior Neuron 78 2013 191 204 Burgos-Artizzu et al., 2012 Burgos-Artizzu, X.P., Doll\u00e1r, P., Lin, D., Anderson, D.J., and Perona, P. (2012). Social behavior recognition in continuous video. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1322\u20131329. Button et al., 2013 K.S. Button J.P. Ioannidis C. Mokrysz B.A. Nosek J. Flint E.S. Robinson M.R. Munaf\u00f2 Power failure: why small sample size undermines the reliability of neuroscience Nat. Rev. Neurosci. 14 2013 365 376 Card and Dickinson, 2008 G. Card M. Dickinson Performance trade-offs in the flight initiation of Drosophila J. Exp. Biol. 211 2008 341 353 Chen et al., 2002 S. Chen A.Y. Lee N.M. Bowens R. Huber E.A. Kravitz Fighting fruit flies: a model system for the study of aggression Proc. Natl. Acad. Sci. USA 99 2002 5664 5668 Coifman et al., 1998 B. Coifman D. Beymer P. McLauchlan J. Malik A real-time computer vision system for vehicle tracking and traffic surveillance Transportation Research Part C: Emerging Technologies 6 1998 271 288 Collett and Collett, 2000 M. Collett T.S. Collett How do insects use path integration for their navigation? Biol. Cybern. 83 2000 245 259 Crawley et al., 1982 J.N. Crawley S. Szara G.T. Pryor C.R. Creveling B.K. Bernard Development and evaluation of a computer-automated color TV tracking system for automatic recording of the social and exploratory behavior of small animals J. Neurosci. Methods 5 1982 235 247 Dankert et al., 2009 H. Dankert L. Wang E.D. Hoopfer D.J. Anderson P. Perona Automated monitoring and analysis of social behavior in Drosophila Nat. Methods 6 2009 297 303 de Chaumont et al., 2012 F. de Chaumont R.D. Coura P. Serreau A. Cressant J. Chabout S. Granon J.-C. Olivo-Marin Computerized video analysis of social interactions in mice Nat. Methods 9 2012 410 417 10.1038/nmeth.1924 Del Vecchio et al., 2002 Del Vecchio, D., Murray, R.M., and Perona, P. (2002). Primitives for human motion: A dynamical approach. In Procedures of IFAC World Congress, Barcelona, Spain. Del Vecchio et al., 2003 D. Del Vecchio R. Murray P. Perona Decomposition of human motion into dynamics based primitives with application to drawing tasks Automatica 39 2003 2085 2098 Dell et al., 2014 A.I. Dell J.A. Bender K. Branson I.D. Couzin G.G. de Polavieja L.P. Noldus A. P\u00e9rez-Escudero P. Perona A.D. Straw M. Wikelski U. Brose Automated image-based tracking and its application in ecology Trends Ecol. Evol. 29 2014 417 428 Desland et al., 2014 F.A. Desland A. Afzal Z. Warraich J. Mocco Manual versus Automated Rodent Behavioral Assessment: Comparing Efficacy and Ease of Bederson and Garcia Neurological Deficit Scores to an Open Field Video-Tracking System J Cent Nerv Syst Dis 6 2014 7 14 Dickinson et al., 2000 M.H. Dickinson C.T. Farley R.J. Full M.A. Koehl R. Kram S. Lehman How animals move: an integrative view Science 288 2000 100 106 Dickson, 2008 B.J. Dickson Wired for sex: the neurobiology of Drosophila mating decisions Science 322 2008 904 909 Dierick, 2007 H.A. Dierick A method for quantifying aggression in male Drosophila melanogaster Nat. Protoc. 2 2007 2712 2718 Doll\u00e1r et al., 2005 P. Doll\u00e1r V. Rabaud G. Cottrell S. Belongie Behavior recognition via sparse spatio-temporal features IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance 2005 65 72 Egelhaaf et al., 1988 M. Egelhaaf K. Hausen W. Reichardt C. Wehrhahn Visual course control in flies relies on neuronal computation of object and background motion Trends Neurosci. 11 1988 351 358 Eyjolfsdottir et al., 2014 Eyjolfsdottir, E.A., Branson, S., Branson, K., Burgos-Artizzu, X.P., Hoopfer, E.D., Schor, J., Anderson, D.J., and Perona, P. (2014). Detecting actions of social fruit flies. In Proceedings of the European Conference on Computer Vision (ECCV2014). Farrell and Roth, 2013 M.S. Farrell B.L. Roth Pharmacosynthetics: Reimagining the pharmacogenetic approach Brain Res. 1511 2013 6 20 Feng et al., 2004 Z. Feng C.J. Cronin J.H. Wittig Jr. P.W. Sternberg W.R. Schafer An imaging system for standardized quantitative analysis of C. elegans behavior BMC Bioinformatics 5 2004 115 Fontaine et al., 2006 E. Fontaine J. Burdick A. Barr Automated tracking of multiple C. elegans Conf Proc IEEE Eng Med Biol Soc. 2006 2006 3716 3719 Fontaine et al., 2008 E. Fontaine D. Lentink S. Kranenbarg U.K. M\u00fcller J.L. van Leeuwen A.H. Barr J.W. Burdick Automated visual tracking for studying the ontogeny of zebrafish swimming J. Exp. Biol. 211 2008 1305 1316 Fontaine et al., 2009 E.I. Fontaine F. Zabala M.H. Dickinson J.W. Burdick Wing and body motion during flight initiation in Drosophila revealed by automated visual tracking J. Exp. Biol. 212 2009 1307 1323 Frye and Dickinson, 2004 M.A. Frye M.H. Dickinson Closing the loop between neurobiology and flight behavior in Drosophila Curr. Opin. Neurobiol. 14 2004 729 736 Gomes et al., 2010 R. Gomes A. Krause P. Perona Discriminative clustering by regularized information maximization Advances in Neural Information Processing Systems 23 2010 Gomez-Marin et al., 2012 A. Gomez-Marin N. Partoune G.J. Stephens M. Louis B. Brembs Automated tracking of animal posture and movement during exploration and sensory orientation behaviors PLoS ONE 7 2012 e41642 Goncalves et al., 2004 L. Goncalves E. di Bernardo P. Perona Movemes for modeling biological motion perception Seeing, Thinking and Knowing Theory and Decision Library A 38 2004 143 170 G\u00f6tz, 1987 K.G. G\u00f6tz Course-control, metabolism and wing interference during ultralong thethered flight in melanogaster J. Exp. Biol. 128 1987 35 46 Green et al., 2012 J. Green C. Collins E.J. Kyzar M. Pham A. Roth S. Gaikwad J. Cachat A.M. Stewart S. Landsman F. Grieco Automated high-throughput neurophenotyping of zebrafish social behavior J. Neurosci. Methods 210 2012 266 271 Hall, 1994 J.C. Hall The mating of a fly Science 264 1994 1702 1714 Hamada et al., 2008 F.N. Hamada M. Rosenzweig K. Kang S.R. Pulver A. Ghezzi T.J. Jegla P.A. Garrity An internal thermal sensor controlling temperature preference in Drosophila Nature 454 2008 217 220 Hamblen et al., 1986 M. Hamblen W.A. Zehring C.P. Kyriacou P. Reddy Q. Yu D.A. Wheeler L.J. Zwiebel R.J. Konopka M. Rosbash J.C. Hall Germ-line transformation involving DNA from the period locus in Drosophila melanogaster: overlapping genomic fragments that restore circadian and ultradian rhythmicity to per0 and per- mutants J. Neurogenet. 3 1986 249 291 Hartley and Zisserman, 2003 R. Hartley A. Zisserman Multiple view geometry in computer vision 2003 Cambridge University Press Cambridge Harvey et al., 2009 C.D. Harvey F. Collman D.A. Dombeck D.W. Tank Intracellular dynamics of hippocampal place cells during virtual navigation Nature 461 2009 941 946 Hashemi et al., 2014 J. Hashemi M. Tepper T. Vallin Spina A. Esler V. Morellas N. Papanikolopoulos H. Egger G. Dawson G. Sapiro Computer vision tools for low-cost and noninvasive measurement of autism-related behaviors in infants Autism Res. Treat. 2014 2014 935686 Heisenberg and Wolf, 1984 M. Heisenberg R. Wolf Vision in: Genetics of Microbehavior 1984 Springer Verlag Berlin Hoyer et al., 2008 S.C. Hoyer A. Eckart A. Herrel T. Zars S.A. Fischer S.L. Hardie M. Heisenberg Octopamine in male aggression of Drosophila Curr. Biol. 18 2008 159 167 Huang et al., 2008 K.-M. Huang P. Cosman W.R. Schafer Automated detection and analysis of foraging behavior in Caenorhabditis elegans J. Neurosci. Methods 171 2008 153 164 Inagaki et al., 2014 H.K. Inagaki Y. Jung E.D. Hoopfer A.M. Wong N. Mishra J.Y. Lin R.Y. Tsien D.J. Anderson Optogenetic control of Drosophila using a red-shifted channelrhodopsin reveals experience-dependent influences on courtship Nat. Methods 11 2014 325 332 Insel and Winslow, 1991 T.R. Insel J.T. Winslow Central administration of oxytocin modulates the infant rat\u2019s response to social isolation Eur. J. Pharmacol. 203 1991 149 152 Iyengar et al., 2012 A. Iyengar J. Imoehl A. Ueda J. Nirschl C.-F. Wu Automated quantification of locomotion, social interaction, and mate preference in Drosophila mutants J. Neurogenet. 26 2012 306 316 Jhuang et al., 2010 H. Jhuang E. Garrote J. Mutch X. Yu V. Khilnani T. Poggio A.D. Steele T. Serre Automated home-cage behavioural phenotyping of mice Nat Commun 1 2010 68 Kabra et al., 2013 M. Kabra A.A. Robie M. Rivera-Alba S. Branson K. Branson JAABA: interactive machine learning for automatic annotation of animal behavior Nat. Methods 10 2013 64 67 Karbowski et al., 2008 J. Karbowski G. Schindelman C.J. Cronin A. Seah P.W. Sternberg Systems level circuit model of C. elegans undulatory locomotion: mathematical modeling and molecular genetics J. Comput. Neurosci. 24 2008 253 276 Kato et al., 2014 S. Kato Y. Xu C.E. Cho L.F. Abbott C.I. Bargmann Temporal responses of C. elegans chemosensory neurons are preserved in behavioral dynamics Neuron 81 2014 616 628 Kerr and Denk, 2008 J.N. Kerr W. Denk Imaging in vivo: watching the brain in action Nat. Rev. Neurosci. 9 2008 195 205 Khan et al., 2005 Z. Khan T. Balch F. Dellaert MCMC-based particle filtering for tracking a variable number of interacting targets IEEE Trans. Pattern Anal. Mach. Intell. 27 2005 1805 1819 Kitamoto, 2001 T. Kitamoto Conditional modification of behavior in Drosophila by targeted expression of a temperature-sensitive shibire allele in defined neurons J. Neurobiol. 47 2001 81 92 Klapoetke et al., 2014 N.C. Klapoetke Y. Murata S.S. Kim S.R. Pulver A. Birdsey-Benson Y.K. Cho T.K. Morimoto A.S. Chuong E.J. Carpenter Z. Tian Independent optical excitation of distinct neural populations Nat. Methods 11 2014 338 346 Kohlhoff et al., 2011 K.J. Kohlhoff T.R. Jahn D.A. Lomas C.M. Dobson D.C. Crowther M. Vendruscolo The iFly tracking system for an automated locomotor and behavioural analysis of Drosophila melanogaster Integr. Biol. 3 2011 755 760 Land and Collett, 1974 M.F. Land T. Collett Chasing behaviour of houseflies (fannia canicularis) J. Comp. Physiol. 89 1974 331 357 Levitis et al., 2009 D.A. Levitis W.Z. Lidicker Jr. G. Freund Behavioural biologists don\u2019t agree on what constitutes behaviour Anim. Behav. 78 2009 103 110 Lin et al., 2013 J.Y. Lin P.M. Knutsen A. Muller D. Kleinfeld R.Y. Tsien ReaChR: a red-shifted variant of channelrhodopsin enables deep transcranial optogenetic excitation Nat. Neurosci. 16 2013 1499 1508 Luo et al., 2008 L. Luo E.M. Callaway K. Svoboda Genetic dissection of neural circuits Neuron 57 2008 634 660 Luo et al., 2010 L. Luo M. Gershow M. Rosenzweig K. Kang C. Fang-Yen P.A. Garrity A.D. Samuel Navigational decision making in Drosophila thermotaxis J. Neurosci. 30 2010 4261 4272 Metz et al., 1983 H.A. Metz H. Dienske G. de Jonge F.A. Putters Continuous-time Markov chains as models for animal behaviour Bull. Math. Biol. 45 1983 643 658 Mizutani et al., 2003 A. Mizutani J.S. Chahl M.V. Srinivasan Insect behaviour: Motion camouflage in dragonflies Nature 423 2003 604 Moorman et al., 2011 S. Moorman C.V. Mello J.J. Bolhuis From songs to synapses: molecular mechanisms of birdsong memory. Molecular mechanisms of auditory learning in songbirds involve immediate early genes, including zenk and arc, the ERK/MAPK pathway and synapsins Bioessays 33 2011 377 385 M\u00fcller and Wehner, 1988 M. M\u00fcller R. Wehner Path integration in desert ants, Cataglyphis fortis Proc. Natl. Acad. Sci. USA 85 1988 5287 5290 Nitabach and Taghert, 2008 M.N. Nitabach P.H. Taghert Organization of the Drosophila circadian control circuit Curr. Biol. 18 2008 R84 R93 Noldus et al., 2001 L.P. Noldus A.J. Spink R.A. Tegelenbosch EthoVision: a versatile video tracking system for automation of behavioral experiments Behav. Res. Methods Instrum. Comput. 33 2001 398 414 Ohayon et al., 2013 S. Ohayon O. Avni A.L. Taylor P. Perona S.E. Roian Egnor Automated multi-day tracking of marked mice for the analysis of social behaviour J. Neurosci. Methods 219 2013 10 19 Pan and Yang, 2010 S.J. Pan Q. Yang A survey on transfer learning IEEE Transactions on Knowledge and Data Engineering 22 2010 1345 1359 P\u00e9rez-Escudero et al., 2014 A. P\u00e9rez-Escudero J. Vicente-Page R.C. Hinz S. Arganda G.G. de Polavieja idTracker: tracking individuals in a group by automatic identification of unmarked animals Nat. Methods 11 2014 743 748 Pham et al., 2009 J. Pham S.M. Cabrera C. Sanchis-Segura M.A. Wood Automated scoring of fear-related behavior using EthoVision software J. Neurosci. Methods 178 2009 323 326 Rabiner and Juang, 1986 L. Rabiner B. Juang An introduction to hidden Markov models IEEE ASSP Magazine January 1986 1986 4 16 Reichardt and Poggio, 1976 W. Reichardt T. Poggio Visual control of orientation behaviour in the fly. Part I. A quantitative analysis Q. Rev. Biophys. 9 1976 311 375 428\u2013438 Reid, 2012 R.C. Reid From functional architecture to functional connectomics Neuron 75 2012 209 217 Reiser and Dickinson, 2008 M.B. Reiser M.H. Dickinson A modular display system for insect behavioral neuroscience J. Neurosci. Methods 167 2008 127 139 Rihel et al., 2010 J. Rihel D.A. Prober A.F. Schier Monitoring sleep and arousal in zebrafish Methods Cell Biol. 100 2010 281 294 Shotton et al., 2013 J. Shotton T. Sharp A. Kipman A. Fitzgibbon M. Finocchio A. Blake M. Cook R. Moore Real-time human pose recognition in parts from single depth images Communications of the ACM 56 2013 116 124 Siegel and Hall, 1979 R.W. Siegel J.C. Hall Conditioned responses in courtship behavior of normal and mutant Drosophila Proc. Natl. Acad. Sci. USA 76 1979 3430 3434 Silasi et al., 2013 G. Silasi J.D. Boyd J. Ledue T.H. Murphy Improved methods for chronic light-based motor mapping in mice: automated movement tracking with accelerometers, and chronic EEG recording in a bilateral thin-skull preparation Front Neural Circuits 7 2013 123 Simonetta and Golombek, 2007 S.H. Simonetta D.A. Golombek An automated tracking system for Caenorhabditis elegans locomotor behavior and circadian studies application J. Neurosci. Methods 161 2007 273 280 Simpson, 2009 J.H. Simpson Mapping and manipulating neural circuits in the fly brain Adv. Genet. 65 2009 79 143 Sokolowski, 2001 M.B. Sokolowski Drosophila: Genetics meets behavior Nat Rev Genet. 2 2001 879 890 Spink et al., 2001 A.J. Spink R.A. Tegelenbosch M.O. Buma L.P. Noldus The EthoVision video tracking system\u2014a tool for behavioral phenotyping of transgenic mice Physiol. Behav. 73 2001 731 744 Straw et al., 2011 A.D. Straw K. Branson T.R. Neumann M.H. Dickinson Multi-camera real-time three-dimensional tracking of multiple flying animals J. R. Soc. Interface 8 2011 395 409 Swierczek et al., 2011 N.A. Swierczek A.C. Giles C.H. Rankin R.A. Kerr High-throughput behavioral analysis in C. elegans Nat. Methods 8 2011 592 598 Tataroglu and Emery, 2014 O. Tataroglu P. Emery Studying circadian rhythms in Drosophila melanogaster Methods 68 2014 140 150 Tecott and Nestler, 2004 L.H. Tecott E.J. Nestler Neurobehavioral assessment in the information age Nat. Neurosci. 7 2004 462 466 Tinbergen, 1951 N. Tinbergen The Study of Instinct 1951 Clarendon Press/Oxford University Press Oxford Tinbergen, 1963 N. Tinbergen On aims and methods of ethology Z. Tierpsychol. 20 1963 410 433 Tinbergen and Perdeck, 1950 N. Tinbergen A.C. Perdeck On the stimulus situation releasing the begging response in the newly hatched herring gull chick (larus argentatus argentatus pont.) Behaviour 3 1950 1 39 Tsai and Huang, 2012 H.-Y. Tsai Y.-W. Huang Image tracking study on courtship behavior of Drosophila PLoS ONE 7 2012 e34784 Veeraraghavan et al., 2008 A. Veeraraghavan R. Chellappa M. Srinivasan Shape-and-behavior encoded tracking of bee dances IEEE Trans. Pattern Anal. Mach. Intell. 30 2008 463 476 Vogelstein et al., 2014 J.T. Vogelstein Y. Park T. Ohyama R.A. Kerr J.W. Truman C.E. Priebe M. Zlatic Discovery of brainwide neural-behavioral maps via multiscale unsupervised structure learning Science 344 2014 386 392 von Philipsborn et al., 2011 A.C. von Philipsborn T. Liu J.Y. Yu C. Masser S.S. Bidaye B.J. Dickson Neuronal control of Drosophila courtship song Neuron 69 2011 509 522 Wehrhahn et al., 1982 C. Wehrhahn T. Poggio H. Buelthoff Tracking and chasing in houseflies (musca) Biol. Cybern. 45 1982 123 130 Weissbrod et al., 2013 A. Weissbrod A. Shapiro G. Vasserman L. Edry M. Dayan A. Yitzhaky L. Hertzberg O. Feinerman T. Kimchi Automated long-term tracking and social behavioural phenotyping of animal colonies within a semi-natural environment Nat Commun 4 2013 2018 Williams, 2004 H. Williams Birdsong and singing behavior Ann. N Y Acad. Sci. 1016 2004 1 30 Wolf et al., 2002 F.W. Wolf A.R. Rodan L.T.-Y. Tsai U. Heberlein High-resolution analysis of ethanol-induced locomotor stimulation in Drosophila J. Neurosci. 22 2002 11035 11044 Yamamoto and Koganezawa, 2013 D. Yamamoto M. Koganezawa Genes and circuits of courtship behaviour in Drosophila males Nat. Rev. Neurosci. 14 2013 681 692 Yamato et al., 1992 Yamato, J., Ohya, J., and Ishii, K. (1992). Recognizing human action in time-sequential images using hidden markov model. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 379\u2013385. Yemini et al., 2013 E. Yemini T. Jucikas L.J. Grundy A.E. Brown W.R. Schafer A database of Caenorhabditis elegans behavioral phenotypes Nat. Methods 10 2013 877 879 Yizhar et al., 2011 O. Yizhar L.E. Fenno T.J. Davidson M. Mogri K. Deisseroth Optogenetics in neural systems Neuron 71 2011 9 34 Zabala et al., 2012 F. Zabala P. Polidoro A. Robie K. Branson P. Perona M.H. Dickinson A simple strategy for detecting moving objects during locomotion revealed by animal-robot interactions Curr. Biol. 22 2012 1344 1350", "scopus-id": "84907963420", "pubmed-id": "25277452", "coredata": {"eid": "1-s2.0-S0896627314007934", "dc:description": "The new field of \u201cComputational Ethology\u201d is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.", "openArchiveArticle": "true", "prism:coverDate": "2014-10-01", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0896627314007934", "dc:creator": [{"@_fa": "true", "$": "Anderson, David J."}, {"@_fa": "true", "$": "Perona, Pietro"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0896627314007934"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0896627314007934"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0896-6273(14)00793-4", "prism:volume": "84", "prism:publisher": "Elsevier Inc.", "dc:title": "Toward a Science of Computational Ethology", "prism:copyright": "Copyright \u00a9 2014 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "08966273", "prism:issueIdentifier": "1", "openaccessArticle": "true", "prism:publicationName": "Neuron", "prism:number": "1", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "18-31", "prism:endingPage": "31", "pubType": "Perspective", "prism:coverDisplayDate": "1 October 2014", "prism:doi": "10.1016/j.neuron.2014.09.005", "prism:startingPage": "18", "dc:identifier": "doi:10.1016/j.neuron.2014.09.005", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "149", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "32676", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "206", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "22129", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "114", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "17477", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "182", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "27897", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "159", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "30258", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "445", "@width": "655", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "151888", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "521", "@width": "656", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "105961", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "264", "@width": "505", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "53397", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "591", "@width": "656", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "159214", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "559", "@width": "769", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "156358", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1970", "@width": "2899", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "1550351", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2308", "@width": "2905", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "699747", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1168", "@width": "2238", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "327022", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2615", "@width": "2905", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "941533", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2475", "@width": "3404", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627314007934-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "1124444", "@ref": "gr5", "@mimetype": "image/jpeg"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84907963420"}}