{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608011002577", "dc:identifier": "doi:10.1016/j.neunet.2011.09.005", "eid": "1-s2.0-S0893608011002577", "prism:doi": "10.1016/j.neunet.2011.09.005", "pii": "S0893-6080(11)00257-7", "dc:title": "Analysis and improvement of policy gradient estimation ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "26", "prism:startingPage": "118", "prism:endingPage": "129", "prism:pageRange": "118-129", "dc:format": "application/json", "prism:coverDate": "2012-02-29", "prism:coverDisplayDate": "February 2012", "prism:copyright": "Copyright \u00a9 2011 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Zhao, Tingting"}, {"@_fa": "true", "$": "Hachiya, Hirotaka"}, {"@_fa": "true", "$": "Niu, Gang"}, {"@_fa": "true", "$": "Sugiyama, Masashi"}], "dc:description": "\n               Abstract\n               \n                  \n                     Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We first prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Policy gradients"}, {"@_fa": "true", "$": "Policy gradients with parameter-based exploration"}, {"@_fa": "true", "$": "Variance reduction"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608011002577", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608011002577", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84855919433", "scopus-eid": "2-s2.0-84855919433", "pubmed-id": "22019189", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84855919433", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20111001", "$": "2011-10-01"}}}}}