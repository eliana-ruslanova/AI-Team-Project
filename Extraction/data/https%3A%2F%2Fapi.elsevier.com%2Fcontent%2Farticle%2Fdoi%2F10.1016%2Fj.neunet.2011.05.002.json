{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608011001444", "dc:identifier": "doi:10.1016/j.neunet.2011.05.002", "eid": "1-s2.0-S0893608011001444", "prism:doi": "10.1016/j.neunet.2011.05.002", "pii": "S0893-6080(11)00144-4", "dc:title": "An information-theoretic analysis of return maximization in reinforcement learning ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "24", "prism:issueIdentifier": "10", "prism:startingPage": "1074", "prism:endingPage": "1081", "prism:pageRange": "1074-1081", "prism:number": "10", "dc:format": "application/json", "prism:coverDate": "2011-12-31", "prism:coverDisplayDate": "December 2011", "prism:copyright": "Copyright \u00a9 2011 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Iwata, Kazunori"}], "dc:description": "\n               Abstract\n               \n                  We present a general analysis of return maximization in reinforcement learning. This analysis does not require assumptions of Markovianity, stationarity, and ergodicity for the stochastic sequential decision processes of reinforcement learning. Instead, our analysis assumes the asymptotic equipartition property fundamental to information theory, providing a substantially different view from that in the literature. As our main results, we show that return maximization is achieved by the overlap of typical and best sequence sets, and we present a class of stochastic sequential decision processes with the necessary condition for return maximization. We also describe several examples of best sequences in terms of return maximization in the class of stochastic sequential decision processes, which satisfy the necessary condition.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Stochastic sequential decision process"}, {"@_fa": "true", "$": "Information theory"}, {"@_fa": "true", "$": "Asymptotic equipartition property"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608011001444", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608011001444", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "80054785939", "scopus-eid": "2-s2.0-80054785939", "pubmed-id": "21665429", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/80054785939", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20110517", "$": "2011-05-17"}}}}}