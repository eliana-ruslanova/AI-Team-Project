{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608010000456", "dc:identifier": "doi:10.1016/j.neunet.2010.02.003", "eid": "1-s2.0-S0893608010000456", "prism:doi": "10.1016/j.neunet.2010.02.003", "pii": "S0893-6080(10)00045-6", "dc:title": "Meta-learning approach to neural network optimization ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2010 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "23", "prism:issueIdentifier": "4", "prism:startingPage": "568", "prism:endingPage": "582", "prism:pageRange": "568-582", "prism:number": "4", "dc:format": "application/json", "prism:coverDate": "2010-05-31", "prism:coverDisplayDate": "May 2010", "prism:copyright": "Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "The 18th International Conference on Artificial Neural Networks, ICANN 2008", "dc:creator": [{"@_fa": "true", "$": "Kord\u00edk, Pavel"}, {"@_fa": "true", "$": "Koutn\u00edk, Jan"}, {"@_fa": "true", "$": "Drchal, Jan"}, {"@_fa": "true", "$": "Kov\u00e1\u0159\u00edk, Oleg"}, {"@_fa": "true", "$": "\u010cepek, Miroslav"}, {"@_fa": "true", "$": "\u0160norek, Miroslav"}], "dc:description": "\n               Abstract\n               \n                  Optimization of neural network topology, weights and neuron transfer functions for given data set and problem is not an easy task. In this article, we focus primarily on building optimal feed-forward neural network classifier for i.i.d. data sets. We apply meta-learning principles to the neural network structure and function optimization. We show that diversity promotion, ensembling, self-organization and induction are beneficial for the problem. We combine several different neuron types trained by various optimization algorithms to build a supervised feed-forward neural network called Group of Adaptive Models Evolution (GAME). The approach was tested on a large number of benchmark data sets. The experiments show that the combination of different optimization algorithms in the network is the best choice when the performance is averaged over several real-world problems.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Neural network optimization"}, {"@_fa": "true", "$": "Ant colony"}, {"@_fa": "true", "$": "Evolutionary algorithms"}, {"@_fa": "true", "$": "Meta-learning"}, {"@_fa": "true", "$": "Meta-optimization"}, {"@_fa": "true", "$": "Optimization algorithm benchmark"}, {"@_fa": "true", "$": "Diversity promotion"}, {"@_fa": "true", "$": "Inductive modeling"}, {"@_fa": "true", "$": "Self-organization"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608010000456", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608010000456", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "77950296566", "scopus-eid": "2-s2.0-77950296566", "pubmed-id": "20227243", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/77950296566", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20100220", "$": "2010-02-20"}}}}}