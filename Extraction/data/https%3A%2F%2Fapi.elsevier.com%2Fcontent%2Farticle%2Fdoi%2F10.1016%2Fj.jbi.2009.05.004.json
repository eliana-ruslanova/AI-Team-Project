{"scopus-eid": "2-s2.0-70350569364", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2009-05-13 2009-05-13 2010-10-09T20:28:24 1-s2.0-S1532046409000768 S1532-0464(09)00076-8 S1532046409000768 10.1016/j.jbi.2009.05.004 S300 S300.1 FULL-TEXT 1-s2.0-S1532046409X00079 2015-05-15T06:30:58.184067-04:00 0 0 20091201 20091231 2009 2009-05-13T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 42 42 6 6 Volume 42, Issue 6 10 1046 1055 1046 1055 200912 December 2009 2009-12-01 2009-12-31 2009 article fla Copyright \u00a9 2009 Elsevier Inc. All rights reserved. TWOLEARNINGAPPROACHESFORPROTEINNAMEEXTRACTION TATAR S 1 Introduction 2 Related work 3 Methods 3.1 Hierarchical syntactic token types 3.2 Bigram language model for protein names 3.3 Learning rules for protein name extraction 4 Experimental evaluation 4.1 Corpora and methodology 4.2 Results and discussion 4.2.1 Comparison of methods 4.2.2 Hierarchy usage 4.2.3 Smoothing 4.2.4 Threshold factor 5 Conclusions and future work References ZHOU 2008 393 407 D TANABE 2002 1124 1132 L ZHOU 2004 1178 1190 G GRISHMAN 1997 10 27 R INTERNATIONALSUMMERSCHOOLINFORMATIONEXTRACTIONAMULTIDISCIPLINARYAPPROACHEMERGINGINFORMATIONTECHNOLOGYLECTURENOTESINCOMPUTERSCIENCE INFORMATIONEXTRACTIONTECHNIQUESCHALLENGES KRAUTHAMMER 2004 512 526 M KRAUTHAMMER 2000 245 252 M ALTSCHUL 1997 3389 33402 S TSURUOKA 2004 461 470 Y SCHUEMIE 2007 316 324 M KIM 2003 180 182 J FUKUDA 1998 707 718 K FRANZEN 2002 49 61 K DEMETRIOU 2003 135 143 G HUMPHREYS 2000 502 513 K SEKI 2003 71 77 K BUNESCU 2005 139 155 R DUDA 1973 R PATTERNCLASSIFICATIONSCENEANALYSIS BRILL 1995 543 565 E VAPNIK 1995 V NATURESTATISTICALLEARNINGTHEORY BERGER 1996 39 71 A TAKEUCHI 2005 125 137 K KOU 2005 266 273 Z MIKA 2004 241 247 S HOU 2004 448 460 W YU 2002 322 330 H SEKI 2005 723 743 K SHANNON 1948 379 423 C CICEKLI 2006 23 36 I GALE 1995 217 237 W TATARX2009X1046 TATARX2009X1046X1055 TATARX2009X1046XS TATARX2009X1046X1055XS Full 2014-11-20T07:55:51Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(09)00076-8 S1532046409000768 1-s2.0-S1532046409000768 10.1016/j.jbi.2009.05.004 272371 2010-11-01T14:20:58.023134-04:00 2009-12-01 2009-12-31 1-s2.0-S1532046409000768-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/MAIN/application/pdf/8c3e6a69abf0c1f4ceb0468ab4e130d3/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/MAIN/application/pdf/8c3e6a69abf0c1f4ceb0468ab4e130d3/main.pdf main.pdf pdf true 454336 MAIN 10 1-s2.0-S1532046409000768-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/PREVIEW/image/png/1d50ae08ed5430da75c5f2de9738f8a6/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/PREVIEW/image/png/1d50ae08ed5430da75c5f2de9738f8a6/main_1.png main_1.png png 85015 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046409000768-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/8c1dc6af4804686f3af088fa986ee9db/si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/8c1dc6af4804686f3af088fa986ee9db/si9.gif si9 si9.gif gif 2342 44 336 ALTIMG 1-s2.0-S1532046409000768-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/1753ef3097361740b123d1bb30366692/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/1753ef3097361740b123d1bb30366692/si8.gif si8 si8.gif gif 2438 45 344 ALTIMG 1-s2.0-S1532046409000768-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/39caf1e72d76facd9e616a024ba0ebe0/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/39caf1e72d76facd9e616a024ba0ebe0/si7.gif si7 si7.gif gif 2560 45 367 ALTIMG 1-s2.0-S1532046409000768-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/85590ebabc19cc70ed0780f1d880ba89/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/85590ebabc19cc70ed0780f1d880ba89/si6.gif si6 si6.gif gif 2650 72 429 ALTIMG 1-s2.0-S1532046409000768-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/54628a2626eeda246c3f7655856a325e/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/54628a2626eeda246c3f7655856a325e/si5.gif si5 si5.gif gif 569 17 139 ALTIMG 1-s2.0-S1532046409000768-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/29072c790a25f4dacd4644518b79d211/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/29072c790a25f4dacd4644518b79d211/si4.gif si4 si4.gif gif 383 17 63 ALTIMG 1-s2.0-S1532046409000768-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/553450687d6073fc8b9196893f578a7d/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/553450687d6073fc8b9196893f578a7d/si3.gif si3 si3.gif gif 490 17 91 ALTIMG 1-s2.0-S1532046409000768-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/36ab7a324895a7e24878dcc4ec6aca1e/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/36ab7a324895a7e24878dcc4ec6aca1e/si2.gif si2 si2.gif gif 1495 47 365 ALTIMG 1-s2.0-S1532046409000768-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/6f8e75cd790a5b7769d7cfe99ffaca1b/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/6f8e75cd790a5b7769d7cfe99ffaca1b/si12.gif si12 si12.gif gif 188 13 9 ALTIMG 1-s2.0-S1532046409000768-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/0a94a0498505ec8df892dcee802f8c2a/si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/0a94a0498505ec8df892dcee802f8c2a/si11.gif si11 si11.gif gif 310 16 53 ALTIMG 1-s2.0-S1532046409000768-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/a51265926fe08d0003f9ab5c1243670f/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/a51265926fe08d0003f9ab5c1243670f/si10.gif si10 si10.gif gif 11066 199 457 ALTIMG 1-s2.0-S1532046409000768-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/STRIPIN/image/gif/6505facb823d862324e501f85997b886/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/STRIPIN/image/gif/6505facb823d862324e501f85997b886/si1.gif si1 si1.gif gif 551 17 126 ALTIMG 1-s2.0-S1532046409000768-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr1/DOWNSAMPLED/image/jpeg/d939d1305bf01da4063ae5d737c3c958/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr1/DOWNSAMPLED/image/jpeg/d939d1305bf01da4063ae5d737c3c958/gr1.jpg gr1 gr1.jpg jpg 20331 112 574 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000768-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr1/THUMBNAIL/image/gif/60779805cdc08e95638f0958ccf91a0c/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr1/THUMBNAIL/image/gif/60779805cdc08e95638f0958ccf91a0c/gr1.sml gr1 gr1.sml sml 2587 43 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000768-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr2/DOWNSAMPLED/image/jpeg/b6dfdcf7859dd6c3db3e392e59339b28/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr2/DOWNSAMPLED/image/jpeg/b6dfdcf7859dd6c3db3e392e59339b28/gr2.jpg gr2 gr2.jpg jpg 15345 118 379 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000768-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr2/THUMBNAIL/image/gif/d3785f87b3c5971d9076ad5a88e93efd/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr2/THUMBNAIL/image/gif/d3785f87b3c5971d9076ad5a88e93efd/gr2.sml gr2 gr2.sml sml 3527 68 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000768-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr3/DOWNSAMPLED/image/jpeg/5dd85568758dec04f458f938f7e07178/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr3/DOWNSAMPLED/image/jpeg/5dd85568758dec04f458f938f7e07178/gr3.jpg gr3 gr3.jpg jpg 29739 167 533 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000768-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr3/THUMBNAIL/image/gif/6640c1b89fa0a682bdbeac6a3a4b2d28/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr3/THUMBNAIL/image/gif/6640c1b89fa0a682bdbeac6a3a4b2d28/gr3.sml gr3 gr3.sml sml 3309 68 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000768-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr4/DOWNSAMPLED/image/jpeg/d014fa880ca019b9afdb97b298a88155/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr4/DOWNSAMPLED/image/jpeg/d014fa880ca019b9afdb97b298a88155/gr4.jpg gr4 gr4.jpg jpg 33362 174 533 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000768-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr4/THUMBNAIL/image/gif/f8c7b1361fe5f073dbebc626f8bbd961/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr4/THUMBNAIL/image/gif/f8c7b1361fe5f073dbebc626f8bbd961/gr4.sml gr4 gr4.sml sml 3641 71 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000768-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr5/DOWNSAMPLED/image/jpeg/7447de008bd7c4afd117c3960a96b442/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr5/DOWNSAMPLED/image/jpeg/7447de008bd7c4afd117c3960a96b442/gr5.jpg gr5 gr5.jpg jpg 25530 207 333 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000768-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000768/gr5/THUMBNAIL/image/gif/9df5d0c7deb2493f58c86abe4a0ad19a/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000768/gr5/THUMBNAIL/image/gif/9df5d0c7deb2493f58c86abe4a0ad19a/gr5.sml gr5 gr5.sml sml 6470 136 219 IMAGE-THUMBNAIL YJBIN 1553 S1532-0464(09)00076-8 10.1016/j.jbi.2009.05.004 Elsevier Inc. Fig. 1 An example case for filtering. Fig. 2 An example rule generation. Fig. 3 The performance of Bigram method on the YAPEX dataset as the threshold parameter \u201c\u03c4\u201d changes. Fig. 4 The performance of Bigram method on the YAPEX dataset as the threshold parameter \u201cT 1\u201d changes. Fig. 5 The performance of automatic rule learning method on the YAPEX dataset as the threshold parameter changes. Table 1 Hierarchically categorized syntactic token types. Class Token type Pattern Length Single Roman Numeral I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII No restrictions Single Number [0\u20139]+ No restrictions Single Single Letter [a\u2013zA\u2013Z] 1 Single Greek Letter alpha|beta|gamma|delta|epsilon|theta|kappa|lambda|sigma|mu No restrictions Abbreviation Very Long Abbreviation [a\u2013zA\u2013Z]+([A\u2013Z][a\u2013z]* | [0\u20139]+) ([a\u2013zA\u2013Z]+| [0\u20139]+| [\u2019])* Length>12 Abbreviation Long Abbreviation [a\u2013zA\u2013Z]+([A\u2013Z][a\u2013z]* | [0\u20139]+) ([a\u2013zA\u2013Z]+| [0\u20139]+| [\u2019])* Length>7; length<13 Abbreviation Abbreviation [a\u2013zA\u2013Z]+([A\u2013Z][a\u2013z]* | [0\u20139]+) ([a\u2013zA\u2013Z]+| [0\u20139]+| [\u2019])* Length>3; length<7 Abbreviation Short Abbreviation [a\u2013zA\u2013Z]+([A\u2013Z][a\u2013z]* | [0\u20139]+) ([a\u2013zA\u2013Z]+| [0\u20139]+| [\u2019])* Length=3 Abbreviation Very Short Abbreviation [a\u2013zA\u2013Z]+([A\u2013Z][a\u2013z]* | [0\u20139]+) ([a\u2013zA\u2013Z]+| [0\u20139]+| [\u2019])* Length=2 Delimiter Frequent [.()\u2013/] No restrictions Delimiter Rare [:{}<>] No restrictions Delimiter Very Rare [%=;,+] No restrictions Regular Long Frequent Type-1 [a\u2013zA\u2013Z]+(ase|gen) Length>8 Regular Frequent Type-1 [a\u2013zA\u2013Z]+(ase|gen) Length<9 Regular Frequent Type-2 [a\u2013zA\u2013Z]+(in) No restrictions Regular Frequent Type-3 [a\u2013zA\u2013Z]+(al|um|ide) No restrictions Regular Lower Case [a\u2013z][a\u2013z\u2019]+ No restrictions Regular Long Proper Case [A\u2013Z][a\u2013z\u2019]+ Length>9 Regular Proper Case [A\u2013Z][a\u2013z\u2019]+ Length>3; length<10 Regular Short Proper Case [A\u2013Z][a\u2013z\u2019]+ Length<4 Other Other No specific pattern No restrictions Table 2 Quantitative results of the experiments performed. Corpus Cross validation Precision (%) Recall (%) F-score (%) Bigram YAPEX Yes 67.5 67.9 67.7 Bigram YAPEX No 63.3 71.8 67.3 Bigram GENIA Yes 61.4 73.2 66.8 Rule YAPEX Yes 64.4 59.4 61.8 Rule YAPEX No 63.1 60.1 61.6 Rule GENIA Yes 60.0 62.1 61.0 YAPEX YAPEX No 62.0 59.9 61.0 \u201cBigram\u201d denotes our first method based on Bigram language model, and \u201cRule\u201d denotes the developed rule learning method. The last row contains the performance values for the YAPEX project and cited from the project homepage (Protein halt i text, http://www.sics.se/humle/project/prothalt). The performance scores on the YAPEX dataset are recorded both with and without 10-fold cross validation, whereas the performance scores on the GENIA dataset are recorded only with 10-fold cross validation. We use the standard formula for precision, recall, and F-score calculation: precision=(true positives)/(true positives+false positives); recall=(true positives)/(true positives+false negatives); F-score=(2 * precision * recall)/(precision+recall). Table 3 Performence values for Bigram method with different token type sets (T 1 \u22120.0005, \u03c4 =0.09). Set Number of token/class types Hierarchy Precision (%) Recall (%) F-score (%) Base types 11 Token types No 63.8 12.7 21.2 Base types with hierarchy 11 Token types+5 token classes Yes 56.8 68.9 62.3 All types 21 Token types No 64.7 13.3 22.1 All types with hierarchy 21 Token types+5 token classes Yes 63.3 71.8 67.3 Table 4 Performence values for Bigram method with different calculation methods and different smoothing schemes (T 1 \u22120.0005, \u03c4 =0.09). Bigram calculation method Smoothing Precision (%) Recall (%) F-score (%) Eq. (1) No smoothing 17.3 15.3 16.2 Eq. (1) Good-turing 16.9 16.5 16.7 Eq. (1) Hierarchical token types 31.8 48.8 38.5 Eq. (2) No smoothing 64.7 13.3 22.1 Eq. (2) Good-turing 62.7 15.7 25.1 Eq. (2) Hierarchical token types 63.3 71.8 67.3 The first three rows contain the performance values obtained by using standard Bigram calculation method described in Eq. (1) and the last three shows the performance values obtained by using modified calculation method described in Eq. (2). We investigated the schemes where no smoothing performed on the observed probabilities (\u201cNo smoothing\u201d), good-turing [38] method employed for probability estimation (\u201cGood-Turing\u201d), and hierarchically categorized syntactic token types used (\u201cHierarchical Token Types\u201d). Two learning approaches for protein name extraction Serhan Tatar statar@cs.bilkent.edu.tr Ilyas Cicekli \u204e ilyas@cs.bilkent.edu.tr Department of Computer Engineering, Bilkent University, 06800 Bilkent, Ankara, Turkey \u204e Corresponding author. Fax: +90 312 266 4047. Abstract Protein name extraction, one of the basic tasks in automatic extraction of information from biological texts, remains challenging. In this paper, we explore the use of two different machine learning techniques and present the results of the conducted experiments. In the first method, Bigram language model is used to extract protein names. In the latter, we use an automatic rule learning method that can identify protein names located in the biological texts. In both cases, we generalize protein names by using hierarchically categorized syntactic token types. We conducted our experiments on two different datasets. Our first method based on Bigram language model achieved an F-score of 67.7% on the YAPEX dataset and 66.8% on the GENIA corpus. The developed rule learning method obtained 61.8% F-score value on the YAPEX dataset and 61.0% on the GENIA corpus. The results of the comparative experiments demonstrate that both techniques are applicable to the task of automatic protein name extraction, a prerequisite for the large-scale processing of biomedical literature. Keywords Statistical learning Bigram language model Rule learning Protein name extraction Information extraction 1 Introduction Biological knowledge, generated as a result of biological research, is currently stored in scientific publications which can be accessed via different knowledge sources storing vast amounts of information \u2013 Medline being a prominent example. Knowledge sources do not, however, feature a formal structure in which to access stored information, thus rendering information search, retrieval and processing especially tedious and time-consuming. This consequently results in a strong demand for automatized discovery and extraction of information. Our research focus is on protein name extraction from unstructured texts, a prerequisite for further information processing. Though a basic task of automatic extraction of information from biological texts, it remains challenging to perform. Information extraction performed at different levels can be viewed as a layered structure, which makes the different extraction tasks at different layers interdependent. In other words, because the output of a task on one layer becomes input to the next layer, the success of a former task impacts the success of a latter one. For example, accurate location of protein names in a text will then affect the finding of interactions between proteins [1]. Parallel to the continuous growth in the biological knowledge, the number of protein names is rapidly increasing. New names, however, are not necessarily recorded in terms of standard terminology, thus complicating the information extraction process. Moreover, authors often refer to already named proteins using new variations, reflecting the fact that standards, rules or conventions for naming proteins are not necessarily well-established. Because protein names are generally depictive, they include many common words describing them, which cause difficulty in distinguishing protein name boundaries from the general language text. Abbreviations are also frequently used, and the way in which these are formed is yet another source of difficulty (e.g. \u201cLRP6\u201d for \u201cLow density lipoprotein Receptor-related Protein 6\u201d, \u201cIL5\u201d for \u201cInterLeukin 5\u201d, \u201cRan\u201d for \u201cRAs-related Nuclear protein\u201d). Furthermore, protein names may overlap with other biomedical terms; such as genes, cell cultures or chemical compounds. To make things worse, exploiting contextual information is not very helpful in the extraction process due to the fact that protein names are usually multi-token and include symbols, common nouns, adjectives, adverbs, and even conjunctions, which makes it difficult to distinguish them from surrounding texts [2]. Zhou et al. [3] collected the reasons causing the difficulty in protein name extraction task under five main titles: descriptive naming conventions, conjunctions and disjunctions, non-standardized naming conventions, abbreviations, and cascaded constructions. Information extraction (IE) systems can be used as a solution for automatic extraction of protein names from unstructured biological texts. IE can be defined as the identification of selected types of entities, relations, facts or events in a set of unstructured text documents written in natural language. It is the process of analyzing unstructured texts and extracting the necessary information into a structured representation, or, simply put, the process of selective information structuring [4]. Considering all the IE systems developed so far for protein name extraction, which we will summarize in the following section, systems often rely on knowledge-source based and rule-based approaches. Since both approaches require (essentially manual) careful analysis of the biomedical domain, most systems require manual development of the resources (i.e. dictionaries, rules etc.) by human experts. Furthermore, given rapid changes in biomedical knowledge, keeping the resources used by these systems up-to-date with the changes in the domain is another issue which requires constant human intervention. These issues also make it difficult to adapt the developed systems to extract other entity types. This paper describes the use of supervised learning strategy to extract protein names from biological texts and presents the results of the conducted experiments. The principal goal is to develop IE systems that learn to extract protein names from examples and achieve acceptable extraction performance without using the resources requiring manual effort and time. Our approach is to start with a set of protein names collected from a training set, and generalize the initial set using a carefully designed strategy, while at the same time trying to minimize reduction in accuracy. In order to obtain accurate generalization and overcome the problem of data sparseness, we propose using hierarchically categorized syntactic token types. Many projects have so far used syntactic token types/patterns based on the surface features of the constituents for detecting protein names and their fragments. However, to the best of our knowledge, ours is the first study exploiting a hierarchy of syntactic token patterns. We have developed two novel methods for the extraction task. The first method follows a probabilistic approach for protein name identification. More specifically, we adapted the conceived hierarchy of syntactic token patterns concept to Bigram language model in order to address the problem of protein name extraction. In the second method, we developed an automatic rule learning algorithm to find protein names located in the biological texts. According to our rule generalization method, the learner first derives the unique match sequences, that is a sequence of similarities and differences between two strings, from the given protein names. The extraction rules are then generated by generalizing the differences in the derived unique match sequences using the hierarchical syntactic token types. The structure of the paper is as follows. The next section summarizes the previous research done in this field. The following section describes how we applied Bigram language model to protein name extraction problem and employed rule learning for the task of protein name identification. After presenting the studied methods, Section 4 shows the results of the experimental evaluation of the study. Finally, in the last section we discuss our conclusions, pointing towards future research. 2 Related work In terms of information extraction from biological texts, namely basic term identification to more complex relation extraction tasks, many IE projects have focused on dictionary-based, rule-based, machine learning, statistical, and hybrid approaches, each with its advantages and disadvantages. Krauthammer and Nenadic [5] have surveyed state-of-art approaches for addressing the protein name identification problem in the context of term identification in the biomedical literature. There have been several attempts to develop dictionary-based techniques [6\u201310] for finding protein names in biological texts. Dictionary-based methods use existing protein databases in order to locate protein occurrences in text. Tsuruoka and Tsujii [8,9] reported F =66.6% (F-score: harmonic average between accuracy and coverage) on GENIA corpus [11] by using approximate string searching techniques and expanding the dictionary in advance with a probabilistic variant generator. Krauthammer et al. [6] reported F =75% (considering partial matches as correct) on a set of two papers. With a decrease on the performance score, the proposed system achieved F =59.8% when only exact matches were considered as correct. Although their system fully matched 94.6% of the names marked by the evaluators and available in the BLAST database, it was able to fully match only 4.4% of names marked by the evaluators and not available in the database. The system attempts to perform approximate string matching after converting both dictionary entries and input texts into nucleotide sequence-like strings, which can be then compared by BLAST [7], a DNA and protein sequence comparison tool. More recently, Schuemie et al. [10] combined information from existing gene and protein databases and generated spelling variations according to rules for automatic generation of a comprehensive dictionary. Because of the name variations in referring to the same protein and the new protein names not yet found in the dictionaries, dictionary-based techniques are not totally effective on their own. Another major drawback of the dictionary-based methods is the need for regular dictionary updates. Rule-based approaches [12\u201316] assume that protein names occur in texts in certain patterns and these patterns can be discovered and expressed by a meta-language. Thus, they attempt to locate new names by exploiting the determined patterns, or rules. PROPER (PROtein Proper-noun phrase Extracting Rules) system [12] achieved F =96.7% on a set of 30 abstracts using simple lexical patterns and orthographic features. Franzen et al. [13] introduced the YAPEX system that combines lexical and syntactic knowledge, heuristic rules and a document-local dynamic dictionary. The YAPEX system reached a recall of 61.0% and a precision of 62.0% on a dataset which consists of 200 abstracts. Seki and Mostafa [16] reported F =63.7% on the same dataset using hand-crafted rules based on surface clues reflecting the characteristics of protein names and a protein name dictionary. The PASTA (Protein Active Site Template Acquisition) system [14,15], a pipeline of processing modules, performs protein name extraction in seven steps: text preprocessing, terminological processing, syntactic and semantic analysis, discourse interpretation, and template extraction. In the experiments, the PASTA system achieved F =85% on a set of 61 abstracts. Though rule-based systems have demonstrated remarkable performance and rules in such systems can be flexibly defined and extended as needed, rule development and management is the main issue in these systems, since manually analyzing biological texts and crafting rules require high human expertise and are often time-consuming. Moreover, domain specific rules constructed for a domain can not be easily reused for a new domain. Machine learning techniques are also widely used in order to perform protein name extraction from texts. Bunescu et al. [17] developed and evaluated several learning systems for locating protein names in Medline abstracts. They performed comparative experiments on various systems: RAPIER [18], boosted wrapper induction (BWI) [19], memory-based learning (MBL) [20], transformation-based learning (TBL) [21], support vector machines (SVMs) [22], and maximum entropy (Max-Ent) [23]. The Max-Ent method that uses a \u201cdictionary tagger\u201d, achieved the best result among the others with F =57.9% on the University of Texas, Austin dataset which consists of 748 abstracts. Earlier studies [3,17,24\u201331] in the IE community have shown that statistical techniques can be of service in performing protein name extraction tasks. Hidden Markov Models (HMMs), one of the successful statistical learning techniques, have been applied to different IE tasks [3,24,27,30]. Several studies [25,26,28] have concentrated on the use of support vector machines (SVMs). Seki and Mostafa [31] reported F =63.3% on the YAPEX dataset by employing a probabilistic model together with the surface clues specified for identifying protein names with an emphasis on finding name boundaries. By avoiding deep language analysis, they reduced both processing overhead and the number of probabilistic parameters to be estimated. Machine learning techniques, using training data for learning process, generate rules via generalization of examples instead of creating patterns by hand. Statistical techniques are also relatively easy to apply as long as appropriate models and training data are available. However, in order to achieve good coverage both need a large annotated corpus (as training data) whose preparation requires domain experts and is also time-consuming. Hybrid methods [2,28,27,32\u201334] combining diverse approaches and various resources are also applied to the protein name recognition task. Yu et al. [32] combined pattern-recognition and knowledge-based approaches to identify gene/protein terms in MEDLINE abstracts. Tanabe and Wilbur [2] proposed ABGene system which uses both statistical and knowledge-based strategies for finding gene and protein names. They adapted the Brill tagger [35] to protein and gene name recognition problem. NLProt [28], a system that combines a pre-processing dictionary and rule-based filtering step with several separately trained support vector machines to identify protein names in the MEDLINE abstracts, achieved F =75% on the YAPEX dataset. SemiCRF and DictHMM, two methods developed by Kou et al. [27], achieved levels of F =66.1% and F =51% on the YAPEX dataset respectively. In this paper, we propose two different learning methods. Both methods utilize supervised learning strategy. Contrary to many of the earlier studies, we did not use any resource requiring manual effort and time (e.g. dictionaries, hand-coded filtering rules and so on). Furthermore, we avoid deep syntactic/semantic analysis in order to reduce processing overhead. We focus our efforts on the pure learning performance of the extractors. The proposed methods in the paper exploit hierarchy of syntactic token patterns, a concept no previous study has examined, to generalize protein names accurately and address the problem of data sparseness. Moreover, we propose novel methods for both rule-learning and Bigram calculation. 3 Methods 3.1 Hierarchical syntactic token types In spite of the irregularities and the lack of common standards, and all the problems stated above, protein names exhibit a degree of regularity that becomes a basis for generalization. Protein names are almost always depictive. That is, the majority of the phrases used to name proteins include words reflecting the characteristics of the named protein (function, localization, physical properties etc.). Names are also constructed using a combination or abbreviation of such characteristics and often consist of multiple words [13]. The regularities observed in the syntactic structure of the words appear in protein names are also useful for detecting them. Two important criteria that determine the efficacy and the success of a protein name extractor are (1) the ability to recognize unseen protein names, and (2) the ability to precisely distinguish protein names from non-protein names. Both criteria require accurate generalization of the known protein names. Generalizing means to recognize the parts susceptible of being changed in new protein names, and represent them with generic placeholders [17]. In our study, we generalize protein names by using hierarchically categorized syntactic token types. In addition to accurate generalization, using hierarchically categorized syntactic token types will help overcome data sparseness problem that occurs because of the diversity of the language constructs and the insufficiency of the input data. When we consider all possible language constructs, it is not possible to observe most of the sequences during training of the language model. We determine a two level hierarchy of useful syntactic token types for representing the protein names. Prior to assigning appropriate hierarchical token types to tokens, the input text is segmented in sentences and tokenized. We followed the standard tokenization method which uses white-space and punctuation characters as delimiters except that we removed the apostrophe symbol (\u2019) from our delimiter set as we use the symbol in our token type patterns. Table 1 shows our hierarchy with token types and their patterns as well as the priority of the token types; the list order is the priority order of the tokens. 21 syntactic token types in Table 1 are categorized under the following five main classes: \u2022 Single: The tokens in this class are usually used to diversify a protein name. Single Letter (e.g. \u201cA\u201d, \u201cB\u201d), Number (e.g. \u201c1\u201d, \u201c2\u201d), Roman Numeral (e.g. \u201cI\u201d, \u201cV\u201d) and Greek Letter (e.g. \u201calpha\u201d, \u201cbeta\u201d) are the token types that are classified under this class. \u2022 Abbreviation: The tokens in this class are abbreviations, which consist of both alphabetic and numeric characters. There are five token types under this class, they are classified according to their length: Very Long Abbreviation (e.g. \u201cKKLSMYGVDLHKAKDL\u201d), Long Abbreviation (e.g. \u201cAcAFYHSK5OEQ\u201d), Abbreviation (e.g. \u201cCIN85\u201d), Short Abbreviation (e.g. \u201cAMP\u201d), and Very Short Abbreviation (e.g. \u201cAD\u201d). \u2022 Delimiter: According to their frequency in the protein names, there are three types of delimiters under this class: Frequent (e.g. \u201c-\u201d, \u201c/\u201d), Rare (e.g. \u201c:\u201d, \u201c<\u201d), Very Rare (e.g. \u201c%\u201d, \u201c=\u201d). \u2022 Regular: This class is the broadest class in our hierarchy and includes the tokens containing alphabetic characters only. The criteria used for classifying word types under this class are length, case and frequency. There are eight word types sorted under it: Frequent Type-1 (tokens suffixed with \u201case\u201d or \u201cgen\u201d and have less than nine characters, e.g. \u201ckinase\u201d), Long Frequent Type-1 (tokens suffixed with \u201case\u201d or \u201cgen\u201d and have more than eight characters, e.g. \u201cacetyltransferase\u201d), Frequent Type-2 (tokens suffixed with \u201cin\u201d, e.g. \u201cactin\u201d), Frequent Type-3 (tokens suffixed with \u201cal\u201d, \u201cum\u201d or \u201cide\u201d, e.g. \u201cantiserum\u201d), Lower Case (not of Frequent Types and consists of all lowercase letters, e.g. \u201cchemokine\u201d), Proper Case (not of Frequent Types, only first letter is uppercase and have length restrictions, e.g. \u201cAcademic\u201d), Short Proper Case (not of Frequent Types, only first letter is uppercase and have length restrictions, e.g. \u201cAla\u201d), and Long Proper Case (not of Frequent Types, only first letter is uppercase and have length restrictions, e.g. \u201cAccordingly\u201d). \u2022 Other: This class contains the tokens that cannot be grouped into the above classes. 3.2 Bigram language model for protein names In this first method, we use a Bigram language model, a special case of N-gram which is used in various areas of statistical natural language processing, along with the hierarchically categorized syntactic word types in order to identify protein names. Statistical language models date back to Shannon\u2019s work on information theory [36]. One of the basic aims of the statistical language models is to predict the probability of the next word, given the previous word sequence: P ( w i \u2223 w 1 , \u2026 w i - 1 ) . Given a Bigram language model, it is straightforward to compute the probability of a word sequence as follows: (1) P ( w 1 , w 2 , \u2026 , w i - 1 , w i ) = P ( w 1 ) P ( w 2 \u2223 w 1 ) \u2026 P ( w i \u2223 w i - 1 ) We used a modified version of Eq. (1) in order to apply the Bigram language model to the problem. Given a token wi , Psingle (wi ) denotes the unigram probability of token wi being a single-word protein name, Pfirst (wi ) denotes the unigram probability of token wi being the first word in a protein name, Plast (wi ) denotes the unigram probability of token wi being the last word in a protein name, and the conditional probability P in ( w i \u2223 w i - 1 ) represents the Bigram probability of the fragment \u3008 w i - 1 , w i \u3009 being a constituent in a protein name. After defining the unigram and Bigram probabilities, our Bigram language model calculates the likelihood of a given token sequence \u3008 w 1 , w 2 , \u2026 , w i - 1 , w i \u3009 being a protein name as in Eq. (2). (2) P prot ( w 1 , w 2 , \u2026 , w i - 1 , w i ) = P single ( w i ) if i = 1 P first ( w i ) P in ( w 2 \u2223 w 1 \u2026 P in ( w i \u2223 w i - 1 P last ( w ) if i > 1 Our first modification on Eq. (1) is calculating the unigram probabilities in three different categories according to their positions: (1) single for the single-word protein names, (2) first for the words in the first position in a multiple-word protein name, and (3) last for the words in the last position in a multiple-word protein name. The second modification is adding a new factor, the unigram probability of the last token being the last word in a protein name, to Eq. (1). The modification is based on the observation that the rightmost words of the protein names exhibits considerable degree of regularity and carry important information for extraction purposes. For example, the number of unique words appears in the last position in the protein names tagged in the YAPEX corpus is 110 1 Single protein names are not counted. 1 . This number is 175 for the words in the first position, and 150 for the words in middle positions, 258 for the words preceding a protein name and 287 for the words following a protein name. Moreover, about half of the words appear in the last position can be grouped into Single class in our hierarchy. As stated in Section 2, our system uses a hierarchy of syntactic token patterns to generalize the tokens and reduce the influence of the data sparseness problem. In the absence of a token probability (Psingle, Pfirst, Plast, Pin ), our system looks for a good substitute for the token probability to put into Eq. (2). Token type probabilities P(ti ) and token class probabilities P(ci ) are added to the model to substitute the token probabilities P(wi ). We simply substitute the token probability by the type probability, if the type probability is available. Otherwise, we use the token class probability for the substitution. Eqs. (3)\u2013(5) describes how our model assigns the unigram probabilities in the absence of the token probabilities. (3) P single ( w i ) = P single ( t i ) if P single ( t i ) available P single ( c i ) if P single ( t i ) not available (4) P first ( w i ) = P first ( t i ) if P first ( t i ) available P first ( c i ) if P first ( t i ) not available (5) P last ( w i ) = P last ( t i ) if P last ( t i ) available P last ( c i ) if P last ( t i ) not available Here, ti denotes token type of word wi , and ci denotes hierarchical class of word wi . There are two alternatives for substituting the unigram probabilities: either type probabilities or class probabilities. On the other hand, there are eight alternatives for substituting the Bigram probabilities. Eq. (6) describes how our model assigns the Bigram probabilities under different conditions in the absence of the token Bigram probabilities. (6) If ( P in ( t i \u2223 w i - 1 ) is available ) P in ( W i \u2223 w i - 1 ) = P in ( t i \u2223 w i - 1 ) else If ( P in ( w i \u2223 t i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( w i \u2223 t i - 1 ) else If ( P in ( c i \u2223 w i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( c i \u2223 w i - 1 ) else If ( P in ( w i \u2223 c i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( w i \u2223 c i - 1 ) else If ( P in ( t i \u2223 t i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( t i \u2223 t i - 1 ) else If ( P in ( c i \u2223 t i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( c i \u2223 t i - 1 ) else If ( P in ( t i \u2223 c i - 1 ) is available ) P in ( w i \u2223 w i - 1 ) = P in ( t i \u2223 c i - 1 ) else P in ( w i \u2223 w i - 1 ) = P in ( c i \u2223 c i - 1 ) After learning the necessary model parameters, a probability estimate is produced for every possible fragment in the test data. We use sliding window technique to determine the fragments and Eq. (2) to calculate the fragment probabilities. A fragment in a sliding window is a sequence of words starting from the first word of the sliding window. Because the sliding window size parameter determines the set of candidate fragments, it has also affect on system\u2019s ability to identify protein names. In our experiments, we use the maximum protein name length in the training set as the sliding window size. Fragments whose probability estimates exceeding two threshold values are considered as possible protein names. The first threshold value (T 1) is used to eliminate the weak estimates. After selecting a candidate fragment available in a sliding window, our algorithm compares its probability estimate with this threshold value which is the same value for all fragments. If it passes the first threshold, it is compared with the second threshold value. The second threshold value (T 2) is used to remove the length bias of selection. Because of the nature of Eq. (2), long fragments are more disadvantageous than short ones. We attempt to avoid this bias by adding a length (number of tokens) parameter to the equation T 2 = \u03c4 \u2113 , where T 2 is calculated for a candidate fragment. In the equation, \u2113 denotes the candidate fragment length, and \u03c4 denotes a system set parameter for the cutoff value. The value of \u03c4 is greater than T 1, and this means that the shorter candidate fragments must satisfy more strict restrictions induced by T 2. If there are candidate fragments whose probability estimates exceed both of the thresholds, the longest one is selected as a protein name from the sliding window. Conversely, if none of the fragments exceeds the threshold values, the algorithm does not extract any protein name from that sliding window. Both T 1 and \u03c4 parameters have influence on the performance of the extraction task. In order to obtain the optimum values for these parameters, a hold-out set is used. After the unigram and Bigram probabilities are learnt from the training set, the optimum values for T 1 and \u03c4 are acquired using the hold-out set. The optimum values for T 1 and \u03c4 are the values that produces maximum F-score for the hold-out set. An example case is provided in Fig. 1 in order to make the concept clearer. In the figure, an excerpt from the YAPEX corpora [13], and the possible fragment probabilities for the window starting from token \u201cAIRK2\u201d are shown. After calculating the fragment probabilities, our algorithm starts to compare each fragment probability to the threshold values. For instance, Pprot (\u201cAIRK2\u201d) is compared to T 1(=0.0005) and T 2(= \u03c4 =0.09) values. On the other hand, Pprot (\u201cAIRK2\u201d, \u201ckinase\u201d) is compared to T 1(=0.0005) and T 2(= \u03c4 2 =0.0081) values. 3.3 Learning rules for protein name extraction In our second method, we developed an automatic rule learning algorithm. The success of the rule learning depends on how well it recognizes the regularities among the target elements to be extracted. One of the main performance measures in information extraction is \u201crecall\u201d which is the percentage of correct names that are found. Because recall is directly related to the learner\u2019s generalization ability, rule generalization is one of the critical functions in the system. Our rule generalization method is based on specific generalization of strings as described in [37]. In order to generalize two strings, a unique match sequence of those strings is obtained, and the differences in the unique match sequence are replaced by variables to get a generalized form of the strings. A unique match sequence can be described as a sequence of similarities (substrings occurring in both strings) and differences (substrings differing between strings) between two strings. A unique match sequence can occur either once or not at all for any given two strings. To meet these criteria, the notion of unique match sequence has two necessary conditions: (1) a symbol cannot occur in any difference, if it occurs in a similarity, and (2) a symbol cannot occur in the second constituent of any difference if the same symbol is found in the first constituent of a difference. The examples provided below will clarify the unique match sequence concept. \u2022 UMS(\u03b5, \u03b5)\u2192SIM(\u03b5) \u2022 UMS(ab,ab)\u2192SIM(ab) \u2022 UMS(bc, ef)\u2192DIFF(bc, ef) \u2022 UMS(abcb, dbebf)\u2192DIFF (a, d) SIM(b) DIFF (c, e) SIM(b) DIFF (\u03b5, f) \u2022 UMS(abb, cdb)\u2192\u00d8 \u2022 UMS(ab, ba) \u2192 \u00d8 As evident from the examples, the unique match sequence of two empty strings is a sequence of a single similarity which is an empty string. Moreover, the unique match sequence of two identical strings is a sequence of a single similarity which is equal to that string. The unique match sequence of two totally different strings is a sequence of a single difference. In this work, we propose the use of hierarchical syntactic token types in the generalization process of protein names using the unique match sequence concept. According to our rule generalization method, the learner first generates the unique match sequence of two given protein names. The differences in a unique match sequence are generalized using the hierarchical syntactic token types described above. A difference in a unique match sequence can be replaced with one of the following four variables by our generalization method. \u2022 Type variable: A difference in a unique match sequence is generalized as a type variable, if the two constituents of the difference are the same token type. A type variable is the name of a syntactic token type. For example, the difference DIFF(1,3) is generalized as the type variable Number since the token types of 1 and 3 are numbers. A type variable represents all tokens of that type, and it can be replaced with any token of that type. \u2022 Class variable: If the constituents of a difference are not the same token type but the same token class, the difference is generalized as a class variable. A class variable is the name of a syntactic token class. For example, the difference DIFF(1,B) is generalized as the class variable Single since 1 and B are not the same type but they are in Single class. A class variable represents all the tokens in that class. \u2022 Optional variable: A difference in a unique match sequence is replaced with an optional variable, if one of the constituents in the difference is empty string. For example, the difference DIFF(B,\u03b5) is generalized as Opt(B) variable which represents B or empty string. \u2022 AnyToken variable: This operation replaces a difference in a unique match sequence with an unrestricted variable AnyToken, if any of the above conditions are not satisfied. Unrestricted variables are used to represent any token. In order to make the rule generalization concept clearer, an example rule generation is given in Fig. 2 . In the example, a generalized rule is learnt from two protein name instances: \u201cpresenilin-1\u201d and \u201cgalectin-3\u201d. First, the unique match sequence of these two protein names is found. In order to obtain the generalized rule, our learner starts to perform the generalization operations on the differences in the order of their specificity. Because the constituents of the first difference are of the same token type, the learner replaces the difference element in the unique match sequence with the token type variable of type Frequent Type-2. After generalizing the first difference, the learner substitutes the second difference element in the unique match sequence with the token type variable Number. In the example, the generalized rule states that a token with type Frequent Type-2 followed by the token \u201c\u2013\u201d and a token with type Number forms a protein name. In the example, some of the new protein names recognizable by the generalized rule are also shown: \u201ccaveolin-1\u201d, \u201cthrombospondin-1\u201d, \u201cinterleukin-3\u201d, and \u201cinterleukin-12\u201d. We validate each generalized rule on the training set in order to give their confidence factors. At the end of the confidence evaluation, each rule is assigned a confidence factor. The confidence factor of a given rule shows how well the rule classifies positive and negative instances. Our main resource for assigning a confidence factor to a rule is the result of applying that rule to the training set. Rule confidence factor is the degree, which the rule gives the correct result across individual instances (both positive and negative). In other words, the confidence factor of a rule is the percentage of correctly extracted names as a result of applying that rule to the training set. If the confidence factor of the generalized rule exceeds a certain threshold value, the learner puts the generalized rule into the final rule-set. Otherwise, the rule learnt by generalization is discarded. Furthermore, confidence factors are also used to determine the rule priorities. The generated rules are ordered according to their confidence factors and applied in that order during the test. In order to make a full use of the training data and improve the algorithm\u2019s extraction performance by further rule refinement, positive examples (i.e. protein names) that are uncovered by the learnt rule-set are added to the rule-set and each rule in the rule-set is associated with a set of exceptions. Normally, the resulting rule-set at the end of the rule generation step covers all of the positive instances in the training data. However, there may be some positive instances in the training data that are uncovered by the resulting rule-set after rule filtering step because the rules below the threshold are eliminated after this step. The problem in this particular case is that the extractor would miss a protein name in the test data, even if the missed protein name occurs in the training data, because the final rule-set generated at the end of the training does not cover that protein name. We address this issue by adding the uncovered positive examples to the final rule-set. The second problem is that of efficient utilization of the negative examples (i.e. non-protein names) in the training data, though they are used in confidence factor calculation. If it is not a 100% confident rule, a rule in the final rule-set may cover some negative instances in the training data. This leads to recognition of an incorrect protein name during the test, even if that name is marked as a non-protein name in the training data. This issue is solved by associating each rule in the final rule-set with a set of exceptions. During confidence factor calculation, every negative instance recognized by the candidate rule is put into that rule\u2019s exception set. If any of the names in a rule\u2019s exception set are recognized by that rule during the test, the recognized names are just ignored and not extracted. 4 Experimental evaluation 4.1 Corpora and methodology In order to evaluate the performance and the behavior of the proposed methods under different conditions, we conducted a set of experiments. The main objective of the experimentation is to analyze the performance and behavior of our methods on different datasets, and with different threshold values. We also compare our methods to several other studies. Moreover, we investigated the impact of using hierarchical token types and the other novelty we proposed on the extraction process. We conducted our experiments on two different datasets. The first one, the YAPEX corpora [13], contains two mutually exclusive collections of biological abstracts. The training (reference) collection contains 1745 annotated protein names, and the total number of the annotated protein names in the test set is 1966. We recorded performance scores on this dataset both with and without cross validation for comparison purposes. The second dataset used for evaluation of the proposed methods is the GENIA corpus [11] which contains 2000 labeled abstracts. We performed 10-fold cross validation on the GENIA dataset to evaluate the developed methods. We measured precision, recall, and F-score; as is commonly done in the Message Understanding Conference (MUC) evaluations. Precision is the fraction of correct outcomes divided by the number of all outcomes. For instance, precision value for the protein name extraction task is the percentage of extracted protein names that are correct. On the other hand, recall is analogous to sensitivity in binary classification. Recall can be defined as the fraction of correct outcomes divided by the total number of possible correct answers. The F-score, harmonic mean of precision and recall, provides a method for combining precision and recall scores into a single value. Franzen et al. [13] defines several notions of correctness. The most common two are \u201cstrict mode\u201d and \u201csloppy mode\u201d. In strict mode, two protein names are not considered a match unless they consist of the same character sequence in the same position in the text. In the latter approach, partial matches are also considered correct. In order not to over-estimate the performance, we used the strict mode which is the most conservative approach to determine the truth-value of the matching in our experiments. 4.2 Results and discussion 4.2.1 Comparison of methods Table 2 shows the quantitative results of the experiments performed. Our Bigram method reached F =67.7% on the YAPEX dataset and F =66.8% on the GENIA dataset without relying on hand-crafted rules, dictionaries, and deep language processing. With a lower F-score performance, the developed rule learning algorithm reached F =61.8% on the YAPEX dataset and F =61.0% on the GENIA dataset. Compared to the YAPEX system, both of the developed methods produced better results in terms of F-score. Tsuruoka and Tsujii [8,9] reported F =66.6% on GENIA corpus by using approximate string searching techniques for discovery, a probabilistic variant generator for dictionary expansion and a naive Bayes classifier for filtering. Using less resource, Bigram produced better results on the same dataset. Krauthammer et al. [6] reported F =75% (considering partial matches as correct) on a set of two papers. When only exact matches were considered as correct, their system achieved F =59.8%. Moreover, the reported performance scores are based on a very small dataset. In terms of the amount of the resources used the most similar previous work is the method developed by Seki and Mostafa [31]. They reported F =63.3% on the YAPEX dataset by employing a probabilistic model together with the surface clues specified for identifying protein names with an emphasis on finding name boundaries. Though they used a number of basic heuristic rules, their method does not rely on dictionaries, part-of-speech (POS) taggers and/or syntactic parsers. The two other methods that we can compare on the same datasets are SemiCRF and DictHMM, two learning methods for protein name recognition proposed by Kou et al. [27]. Using dictionaries with CRF-like learning methods and additionally utilizing POS tags, SemiCRF achieved F =66.1% on the YAPEX dataset and F =72.3% on the GENIA dataset. While SemiCRF produced better results on the GENIA dataset, Bigram achieved better performance on the YAPEX dataset with respect to F-score. Moreover, Bigram\u2019s high recall performance is noteworthy. On both datasets, Bigram has higher recall score. Our rule learning algorithm, on the other hand, produced lower scores on both datasets. Their novel system, DictHMM, reached F =51% on the YAPEX dataset and F =54.7% on the GENIA dataset. DictHMM combines a dictionary with an HMM to perform name matching. Compared to DictHMM, the developed Bigram and rule learning methods produced better results in terms of F-score. Though DictHMM has low F-score performance, it emphasizes recall over precision. However, Bigram has a higher recall score on both datasets. The best published result on the YAPEX dataset belongs to the NLProt system [28]. NLProt reached F =75% on the YAPEX dataset and F =71% on the GENIA dataset. NLProt combines a pre-processing dictionary and rule-based filtering step with several separately trained support vector machines. The utilization of several dictionaries (dictionary of protein names, dictionary of chemical compounds, etc.) and hand-tailored filtering rules has an impact on the performance difference between NLProt and our methods. According to their results, leaving out only the dictionary yields 3% loss on NLProt\u2019s F-score. Exploiting only hierarchical token types, our Bigram method achieved a better performance than many other techniques with respect to F-score. The primary advantage of using less resource is the decrease in the processing overhead and, therefore, faster extraction speed, which is especially important for the extraction of protein names from large number of biomedical documents. Furthermore, the method does not suffer from the drawbacks typical for dictionary-based systems and systems using hand-crafted rules. For instance, there is no need for regular dictionary updates. The methods adaptability to the other bio-entity extraction tasks is also easier for not using hand-crafted rules. The developed rule learning method is also effective for protein name extraction and achieved better performance than some of the previous work. However, its performance is behind the developed Bigram method in terms of F-score. Particularly, the difference in the recall rates is very obvious, though both methods use the same concept for generalization. This is due to the small number of variables defined for generalization. The granularity level of the learnt rules affects the performance of the rule learning systems. While over-specific rules may cause low recall, over-general rules cause low precision. This issue could be addressed by adding more comprehensive variables. The increase in the number and the flexibility of the variables used for the rule construction would improve the expressiveness of the rules, and thus result in better performance. 4.2.2 Hierarchy usage We investigated the effects of using hierarchical token types for generalization by testing the Bigram method on the YAPEX dataset with different token type sets. The performance values obtained for different token type sets are shown in Table 3 . The first set contains 11 base token types (Single Letter, Number, Roman Numeral, Greek Letter, Short Abbreviation, Long Abbreviation, Delimiter, Regular-Frequent Type, Regular-Lowercase, Regular-Proper, Other) from our original set and does not have a hierarchy. The second set contains five core classes in addition to the same token types in the first set and has a hierarchical relation between them as in our original set. The third set contains all 21 token types available in the original set but does not have a hierarchy; and the last set is the original set with a hierarchy. As seen from Table 3, hierarchy usage has significantly contributed to the recall rate. The F-score value improved from 21.2% to 62.3% on the sets that consist of base token types. Moreover, we observed even more performance improvement on the sets that consists of all token types introduced in Section 3.1. The obtained improvement is 45.2% in the F-score values. Another notable impact is the relatively small decrease in the precision rates. The decrease is inversely proportional to the broadness of the token type set. It is more considerable when the number of token types is small. For instance, the precision decrease is only 1.4% when we add a hierarchy level to original type set. On the other hand, the decrease in the precision becomes more significant for the base type set. Overall, the results show that hierarchy usage improved the performance of the extraction process. 4.2.3 Smoothing We employed two novel ideas to achieve better performance in the Bigram method: (1) modification of the standard Bigram calculation method, and (2) the use of hierarchically categorized syntactic token types to overcome the data sparseness problem. In order to evaluate the contribution of the former, we compared our modified version to the standard Bigram calculation method described in Eq. (1). Moreover, we performed comparative experiments on different smoothing schemes to see the effect of hierarchy usage to remedy the data sparseness problem. Table 4 shows the results of the comparison. Our smoothing scheme based on the hierarchical token types outperformed the others. Moreover, the modified Bigram version proposed in this paper obviously improve over the standard version. 4.2.4 Threshold factor The threshold factors also have an impact on the performance of our extraction methods. The Bigram method automatically finds threshold values (T 1 and \u03c4) by testing the obtained model on a hold-out set. However, it is also possible to manually set these values and adjust the performance tradeoff between precision and recall. Fig. 3 shows the performance of the Bigram method on the YAPEX dataset as the threshold parameter \u201c\u03c4\u201d changes and \u201cT 1\u201d stays constant. T 1 value is set to 0.0 in Fig. 3(a), while it is set to 0.0005 in Fig. 3(b). As expected, the recall value decreases with the increase in threshold in both graphs. In contrast, the precision value is directly proportional with the threshold value; the precision increases parallel to the increase in the threshold value. We observe a drastic change in the performance between \u03c4 =0.0 and \u03c4 =0.1 values in Fig. 3(a). Normally, one would expect a high recall value and a low precision value where \u03c4 value is set to 0.0. However, both precision and recall scores are very low at \u03c4 =0.0 in Fig. 3(a). This is due to the fact that the longest fragment, among the all candidate fragments whose probability estimate exceed the threshold value, is selected as a protein name. Because, T 1 is constantly set to 0.0 in Fig. 3(a), the longest fragments in the sliding windows were selected as protein names without any restriction where \u03c4 =0.0, which caused a lot of errors. In a different scheme, Fig. 3(b), where \u03c4 is set to a non-zero value, the extractor exhibits the normal behavior (high recall, low precision at \u03c4 =0.0). Fig. 4 shows the performance of the Bigram method on the YAPEX dataset as the threshold parameter \u201cT 1\u201d changes and \u201c\u03c4\u201d stays constant. \u03c4 value is set to 0.0 in Fig. 4 (a) and 0.09 in Fig. 4(b). A similar trend as in Fig. 3 is also observed in this figure: the recall value decreases with the increase in threshold and the precision increases parallel to the increase in the threshold value. Another similarity is the very low precision and recall scores obtained at T 1 =0.0 in Fig. 4(a). Moreover, we observe a drastic change in the performance between T 1 =0.0 and T 1 =0.0005 values in Fig. 4(a). The reason for such behavior is the same as discussed in Fig. 3. On the other hand, automatic rule learning method has a user-set threshold parameter. Fig. 5 shows the performance of automatic rule learning method on the YAPEX dataset as the threshold parameter changes. The optimum value for the threshold parameter is found to be 0.48, where the acquired F-score value is maximized, through experimentation. We observe the same trend in Fig. 5; the recall value decreases and the precision value increases with the increase in threshold. Another notable observation is the drop in the recall rate where the threshold parameter is 0.49. This behavior is caused by the elimination of a general rule whose true positive (TP) returns are more than its false positive (FP) returns. More importantly, the number of TPs recognized by this rule has a prominent share in the total number of TPs recognized by the rule-set. Therefore the elimination of this rule leads to a considerable decrease in the total number of TPs, which also causes the drop in the recall rate. This situation is a result of the small number of variables defined for generalization. The transition would be smoother if the learnt rules were fine-grained enough. 5 Conclusions and future work In this paper, we present two different learning approaches for identifying protein names in biomedical texts. Our first method follows a probabilistic approach and uses Bigram language model. The second method is an automatic rule learning method for protein name extraction. This method aims to learn rules automatically to recognize the protein name patterns and generalize these patterns by processing similarities and differences between them. Compared to previous work, the proposed methods exploit a two-level hierarchy of useful syntactic token types for representing the protein names instead of using a simple list of token types. In the first approach, we use the hierarchy to generalize the tokens and reduce the influence of the data sparseness problem. In the latter one, we use the hierarchy for rule generalization. The results show that using a hierarchy improved the performance of the extraction process and the impact of the hierarchy usage is directly proportional to the broadness of the token type set. We performed several experiments on different datasets to evaluate the performance of our methods. We compared the performance of the proposed approaches with previous methods. The comparison results indicate that our suggested methods can be used for protein name identification task effectively. Avoiding deep syntactic/semantic analysis, the proposed methods reduce processing overhead. They also achieve adaptability which is another major advantage. Thus, the proposed techniques can be applied to other bio-entities such as gene names. Our future work will be carried out in several directions. First, we believe that extending the token type set would provide further increase in the generalization capability, since the results show that using hierarchically categorized syntactic word types positively affects the extraction performance. Moreover, defining new generalization variables would have a positive impact on the generalization performance of our rule learning method. Currently, we generate the base rules and generalize them just one step and scan the entire rule space. Adding some more steps to the generalization process and employing a search mechanism during rule generation would improve the efficiency of rule construction. References [1] D. Zhou Y. He Extracting interactions between proteins from the literature J Biomed Inform 41 2 2008 393 407 [2] L. Tanabe W.J. Wilbur Tagging gene and protein names in biomedical text Bioinformatics 18 8 2002 1124 1132 [3] G. Zhou J. Zhang J. Su D. Shen C. Tan Recognizing names in biomedical texts: a machine learning approach Bioinformatics 20 7 2004 1178 1190 [4] R. Grishman Information extraction: techniques and challenges M.T. Pazienza International summer school on information extraction: a multidisciplinary approach to an emerging information technology, lecture notes in computer science vol. 1299 1997 Springer-Verlag London 10 27 [5] M. Krauthammer G. Nenadic Term identification in the biomedical literature J Biomed Inform 37 6 2004 512 526 [6] M. Krauthammer A. Rzhetsky P. Morozov C. Friedman Using BLAST for identifying gene and protein names in journal articles Gene 259 1\u20132 2000 245 252 [7] S. Altschul T. Madden A. Schaffer J. Zhang Z. Zhang W. Miller Gapped BLAST and PSI-BLAST: a new generation of protein database search programs Nucleic Acids Res 25 17 1997 3389 33402 [8] Y. Tsuruoka J. Tsujii Improving the performance of dictionary-based approaches in protein name recognition J Biomed Inform 37 6 2004 461 470 [9] Tsuruoka Y, Tsujii J. Boosting precision and recall of dictionary-based protein name recognition. In: Proceedings of the ACL 2003 workshop on natural language processing in biomedicine, vol. 13. Association for Computational Linguistics; 2003. p. 41\u20138. [10] M.J. Schuemie B. Mons M. Weeber J.A. Kors Evaluation of techniques for increasing recall in a dictionary approach to gene and protein name identification J Biomed Inform 40 3 2007 316 324 [11] J.-D. Kim T. Ohta Y. Tateisi J. Tsujii Genia corpus: a semantically annotated corpus for bio-textmining Bioinformatics 19 Suppl. 1 2003 180 182 [12] K. Fukuda T. Tsunoda A. Tamura T. Takagi Toward information extraction: identifying protein names from biological papers Pac Symp Biocomput 1998 707 718 [13] K. Franzen G. Eriksson F. Olsson L. Asker P. Liden J. C\u00f6ster Protein names and how to find them Int J Med Inform 67 1\u20133 2002 49 61 [14] G. Demetriou R. Gaizauskas P. Artymiuk P. Willett Protein structures and information extraction from biological texts: the PASTA system Bioinformatics 19 1 2003 135 143 [15] K. Humphreys G. Demetriou R. Gaizauskas Two applications of information extraction to biological science journal articles: enzyme interactions and protein structures Pac Symp Biocomput 2000 502 513 [16] K. Seki J. Mostafa An approach to protein name extraction using heuristics and a dictionary Am Soc Inf Sci Technol Annu Conf 2003 71 77 [17] R. Bunescu R. Ge R.J. Kate E.M. Marcotte R.J. Mooney A.K. Ramani Y.W. Wong Comparative experiments on learning information extractors for proteins and their interactions Artif Intell Med 33 2 2005 139 155 [18] Califf ME, Mooney RJ. Relational learning of pattern-match rules for information extraction. In: Proceedings of the sixteenth national conference on artificial intelligence and the eleventh innovative applications of artificial intelligence conference innovative applications of artificial intelligence. American Association for Artificial Intelligence; 1999. p. 328\u201334. [19] Freitag D, Kushmerick N. Boosted wrapper induction, In: Proceedings of the seventeenth national conference on artificial intelligence and twelfth conference on innovative applications of artificial intelligence. AAAI Press/The MIT Press; 2000. p. 577\u201383. [20] R.O. Duda P.E. Hart Pattern classification and scene analysis 1973 John Wiley and Sons New York [21] E. Brill Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging Comput Linguist 21 4 1995 543 565 [22] V.N. Vapnik The nature of statistical learning theory 1995 Springer Berlin [23] A.L. Berger S.A. Della Pietra V.J. Della Pietra A maximum entropy approach to natural language processing Comput Linguist 22 1 1996 39 71 [24] Collier N, Nobata C, Tsujii J. Extracting the names of genes and gene products with a Hidden Markov Model. In: Proceedings of the 18th international conference on computational linguistics, vol. 1. Association for Computational Linguistics; 2000. p. 201\u20137. [25] Kazama J, Makino T, Ohta Y, Tsujii J. Tuning support vector machines for biomedical named entity recognition. In: Proceedings of the Acl-02 workshop on natural language processing in the biomedical domain, vol.3. Association for Computational Linguistics; 2002. p. 1\u20138. [26] K. Takeuchi N. Collier Bio-medical entity extraction using support vector machines Artif Intell Med 33 2 2005 125 137 [27] Z. Kou W.W. Cohen R.F. Murphy High-recall protein entity recognition using a dictionary Bioinformatics 21 1 2005 266 273 [28] S. Mika B. Rost Protein names precisely peeled off free text Bioinformatics 20 1 2004 241 247 [29] W. Hou H. Chen Enhancing performance of protein and gene name recognizers with filtering and integration strategies J Biomed Inform 37 6 2004 448 460 [30] Ray S, Craven M. Representing sentence structure in hidden Markov models for information extraction. In: Proceedings of the 17th international joint conference on artificial intelligence. Morgan Kaufmann; 2001. p. 1273\u201379. [31] Seki K, Mostafa J. A Probabilistic Model for Identifying Protein Names and their Name Boundaries. In: Proceedings of the IEEE computer society conference on bioinformatics. IEEE Computer Society; 2003. p. 251\u201358. [32] H. Yu V. Hatzivassiloglou A. Rzhetsky W.J. Wilbur Automatically identifying gene/protein terms in MEDLINE abstracts J Biomed Inform 35 5\u20136 2002 322 330 [33] K. Seki J. Mostafa A hybrid approach to protein name identification in biomedical texts Inform Proc Manag 41 4 2005 723 743 [34] Yamamoto K, Kudo T, Konagaya A, Matsumoto Y. Protein name tagging for biomedical annotation in text. In: Proceedings of the ACL 2003 workshop on natural language processing in biomedicine, vol. 13. Association for Computational Linguistics; 2003. p. 65\u201372. [35] Brill E. Some advances in transformation-based part of speech tagging. In: Proceedings of the twelfth national conference on artificial intelligence, vol. 1. American Association for Artificial Intelligence; 1994. p. 722\u201327. [36] C.E. Shannon A mathematical theory of communication Bell Sys Tech J 27 1948 379 423 [and 623\u201356] [37] I. Cicekli N.K. Cicekli Generalizing predicates with string arguments Appl Intell 25 1 2006 23 36 [38] W. Gale G. Sampson Good-turing smoothing without tears J Quant Linguist 2 1995 217 237", "scopus-id": "70350569364", "pubmed-id": "19446044", "coredata": {"eid": "1-s2.0-S1532046409000768", "dc:description": "Abstract Protein name extraction, one of the basic tasks in automatic extraction of information from biological texts, remains challenging. In this paper, we explore the use of two different machine learning techniques and present the results of the conducted experiments. In the first method, Bigram language model is used to extract protein names. In the latter, we use an automatic rule learning method that can identify protein names located in the biological texts. In both cases, we generalize protein names by using hierarchically categorized syntactic token types. We conducted our experiments on two different datasets. Our first method based on Bigram language model achieved an F-score of 67.7% on the YAPEX dataset and 66.8% on the GENIA corpus. The developed rule learning method obtained 61.8% F-score value on the YAPEX dataset and 61.0% on the GENIA corpus. The results of the comparative experiments demonstrate that both techniques are applicable to the task of automatic protein name extraction, a prerequisite for the large-scale processing of biomedical literature.", "openArchiveArticle": "true", "prism:coverDate": "2009-12-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046409000768", "dc:creator": [{"@_fa": "true", "$": "Tatar, Serhan"}, {"@_fa": "true", "$": "Cicekli, Ilyas"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046409000768"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046409000768"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(09)00076-8", "prism:volume": "42", "prism:publisher": "Elsevier Inc.", "dc:title": "Two learning approaches for protein name extraction", "prism:copyright": "Copyright \u00a9 2009 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "6", "dcterms:subject": [{"@_fa": "true", "$": "Statistical learning"}, {"@_fa": "true", "$": "Bigram language model"}, {"@_fa": "true", "$": "Rule learning"}, {"@_fa": "true", "$": "Protein name extraction"}, {"@_fa": "true", "$": "Information extraction"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "6", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "1046-1055", "prism:endingPage": "1055", "prism:coverDisplayDate": "December 2009", "prism:doi": "10.1016/j.jbi.2009.05.004", "prism:startingPage": "1046", "dc:identifier": "doi:10.1016/j.jbi.2009.05.004", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "44", "@width": "336", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2342", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "45", "@width": "344", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2438", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "45", "@width": "367", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2560", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "72", "@width": "429", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2650", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "569", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "63", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "383", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "91", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "490", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "47", "@width": "365", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1495", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "188", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "53", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "310", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "199", "@width": "457", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "11066", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "126", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "551", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "112", "@width": "574", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "20331", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "43", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2587", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "118", "@width": "379", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "15345", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "68", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3527", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "167", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "29739", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "68", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3309", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "174", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33362", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "71", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3641", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "207", "@width": "333", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25530", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "136", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000768-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6470", "@ref": "gr5", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/70350569364"}}