{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014000033", "dc:identifier": "doi:10.1016/j.neunet.2014.01.002", "eid": "1-s2.0-S0893608014000033", "prism:doi": "10.1016/j.neunet.2014.01.002", "pii": "S0893-6080(14)00003-3", "dc:title": "Policy oscillation is overshooting ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "52", "prism:startingPage": "43", "prism:endingPage": "61", "prism:pageRange": "43-61", "dc:format": "application/json", "prism:coverDate": "2014-04-30", "prism:coverDisplayDate": "April 2014", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Wagner, Paul"}], "dc:description": "\n               Abstract\n               \n                  A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting, within the context of non-optimistic policy iteration, a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. Based on empirical findings, we offer a hypothesis that might explain the inferior performance levels and the associated policy degradation phenomenon, and which would partially support the suggested connection. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results by an order of magnitude.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Approximate dynamic programming"}, {"@_fa": "true", "$": "Policy gradient"}, {"@_fa": "true", "$": "Natural gradient"}, {"@_fa": "true", "$": "Policy oscillation"}, {"@_fa": "true", "$": "Policy chattering"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014000033", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014000033", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84893511689", "scopus-eid": "2-s2.0-84893511689", "pubmed-id": "24491826", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84893511689", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20140121", "$": "2014-01-21"}}}}}