{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608010002455", "dc:identifier": "doi:10.1016/j.neunet.2010.12.001", "eid": "1-s2.0-S0893608010002455", "prism:doi": "10.1016/j.neunet.2010.12.001", "pii": "S0893-6080(10)00245-5", "dc:title": "Node perturbation learning without noiseless baseline ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "24", "prism:issueIdentifier": "3", "prism:startingPage": "267", "prism:endingPage": "272", "prism:pageRange": "267-272", "prism:number": "3", "dc:format": "application/json", "prism:coverDate": "2011-04-30", "prism:coverDisplayDate": "April 2011", "prism:copyright": "Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Cho, Tatsuya"}, {"@_fa": "true", "$": "Katahira, Kentaro"}, {"@_fa": "true", "$": "Okanoya, Kazuo"}, {"@_fa": "true", "$": "Okada, Masato"}], "dc:description": "\n               Abstract\n               \n                  Node perturbation learning is a stochastic gradient descent method for neural networks. It estimates the gradient by comparing an evaluation of the perturbed output and the unperturbed output performance, which we call the baseline. Node perturbation learning has primarily been investigated without taking noise on the baseline into consideration. In real biological systems, however, neural activities are intrinsically noisy, and hence, the baseline is likely contaminated with the noise. In this paper, we propose an alternative learning method that does not require such a noiseless baseline. Our method uses a \u201csecond perturbation\u201d, which is calculated with different noise than the first perturbation. By comparing the evaluation of the outcomes with the first perturbation and with the second perturbation, the network weights are updated. We reveal that the learning speed showed only a linear decrease with the variance of the second perturbation. Moreover, using the second perturbation can lead to a decrease in residual error compared to the case of using the noiseless baseline.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Node perturbation"}, {"@_fa": "true", "$": "Noiseless baseline"}, {"@_fa": "true", "$": "Stochastic gradient method"}, {"@_fa": "true", "$": "Linear perceptron"}, {"@_fa": "true", "$": "Learning curve"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608010002455", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608010002455", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "79951515329", "scopus-eid": "2-s2.0-79951515329", "pubmed-id": "21193286", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/79951515329", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20101209", "$": "2010-12-09"}}}}}