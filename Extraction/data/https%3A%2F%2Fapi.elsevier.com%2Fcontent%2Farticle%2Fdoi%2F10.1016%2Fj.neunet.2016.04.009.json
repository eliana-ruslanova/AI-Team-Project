{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608016300399", "dc:identifier": "doi:10.1016/j.neunet.2016.04.009", "eid": "1-s2.0-S0893608016300399", "prism:doi": "10.1016/j.neunet.2016.04.009", "pii": "S0893-6080(16)30039-9", "dc:title": "Parsimonious kernel extreme learning machine in primal via Cholesky factorization ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "80", "prism:startingPage": "95", "prism:endingPage": "109", "prism:pageRange": "95-109", "dc:format": "application/json", "prism:coverDate": "2016-08-31", "prism:coverDisplayDate": "August 2016", "prism:copyright": "\u00a9 2016 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Zhao, Yong-Ping"}], "dc:description": "\n               Abstract\n               \n                  Recently, extreme learning machine (ELM) has become a popular topic in machine learning community. By replacing the so-called ELM feature mappings with the nonlinear mappings induced by kernel functions, two kernel ELMs, i.e., P-KELM and D-KELM, are obtained from primal and dual perspectives, respectively. Unfortunately, both P-KELM and D-KELM possess the dense solutions in direct proportion to the number of training data. To this end, a constructive algorithm for P-KELM (CCP-KELM) is first proposed by virtue of Cholesky factorization, in which the training data incurring the largest reductions on the objective function are recruited as significant vectors. To reduce its training cost further, PCCP-KELM is then obtained with the application of a probabilistic speedup scheme into CCP-KELM. Corresponding to CCP-KELM, a destructive P-KELM (CDP-KELM) is presented using a partial Cholesky factorization strategy, where the training data incurring the smallest reductions on the objective function after their removals are pruned from the current set of significant vectors. Finally, to verify the efficacy and feasibility of the proposed algorithms in this paper, experiments on both small and large benchmark data sets are investigated.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Extreme learning machine"}, {"@_fa": "true", "$": "Kernel method"}, {"@_fa": "true", "$": "Parsimoniousness"}, {"@_fa": "true", "$": "Constructive algorithm"}, {"@_fa": "true", "$": "Destructive algorithm"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608016300399", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608016300399", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84969240506", "scopus-eid": "2-s2.0-84969240506", "pubmed-id": "27203553", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84969240506", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20160502", "$": "2016-05-02"}}}}}