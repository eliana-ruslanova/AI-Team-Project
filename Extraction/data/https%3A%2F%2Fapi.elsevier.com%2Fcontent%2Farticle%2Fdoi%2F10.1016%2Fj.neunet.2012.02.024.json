{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608012000536", "dc:identifier": "doi:10.1016/j.neunet.2012.02.024", "eid": "1-s2.0-S0893608012000536", "prism:doi": "10.1016/j.neunet.2012.02.024", "pii": "S0893-6080(12)00053-6", "dc:title": "Hierarchical curiosity loops and active sensing ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2012 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "32", "prism:startingPage": "119", "prism:endingPage": "129", "prism:pageRange": "119-129", "dc:format": "application/json", "prism:coverDate": "2012-08-31", "prism:coverDisplayDate": "August 2012", "prism:copyright": "Copyright \u00a9 2012 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "Selected Papers from IJCNN 2011", "dc:creator": [{"@_fa": "true", "$": "Gordon, Goren"}, {"@_fa": "true", "$": "Ahissar, Ehud"}], "dc:description": "\n               Abstract\n               \n                  A curious agent acts so as to optimize its learning about itself and its environment, without external supervision. We present a model of hierarchical curiosity loops for such an autonomous active learning agent, whereby each loop selects the optimal action that maximizes the agent\u2019s learning of sensory-motor correlations. The model is based on rewarding the learner\u2019s prediction errors in an actor-critic reinforcement learning (RL) paradigm. Hierarchy is achieved by utilizing previously learned motor-sensory mapping, which enables the learning of other mappings, thus increasing the extent and diversity of knowledge and skills. We demonstrate the relevance of this architecture to active sensing using the well-studied vibrissae (whiskers) system, where rodents acquire sensory information by virtue of repeated whisker movements. We show that hierarchical curiosity loops starting from optimally learning the internal models of whisker motion and then extending to object localization result in free-air whisking and object palpation, respectively.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Intrinsic reward"}, {"@_fa": "true", "$": "Internal models"}, {"@_fa": "true", "$": "Active sensing"}, {"@_fa": "true", "$": "Whisker"}, {"@_fa": "true", "$": "Vibrissa"}, {"@_fa": "true", "$": "Touch"}, {"@_fa": "true", "$": "Object localization"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608012000536", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608012000536", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84861775137", "scopus-eid": "2-s2.0-84861775137", "pubmed-id": "22386787", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84861775137", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20120214", "$": "2012-02-14"}}}}}