{"scopus-eid": "2-s2.0-85029563607", "originalText": "serial JL 272195 291210 291735 291737 31 80 Neuron NEURON 2017-07-19 2017-07-19 2017-07-19 2017-07-19 2018-09-27T04:54:47 1-s2.0-S0896627317305093 S0896-6273(17)30509-3 S0896627317305093 10.1016/j.neuron.2017.06.011 S300 S300.2 FULL-TEXT 1-s2.0-S0896627316X00153 2018-09-27T04:03:34.741449Z 0 0 20170719 2017 2017-07-19T16:32:20.442413Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref teaserabst 0896-6273 08966273 true 95 95 2 2 Volume 95, Issue 2 6 245 258 245 258 20170719 19 July 2017 2017-07-19 2017 Reviews article rev \u00a9 2017 Elsevier Inc. NEUROSCIENCEINSPIREDARTIFICIALINTELLIGENCE HASSABIS D Main Text The Past Deep Learning Reinforcement Learning The Present Attention Episodic Memory Working Memory Continual Learning The Future Intuitive Understanding of the Physical World Efficient Learning Transfer Learning Imagination and Planning Virtual Brain Analytics From AI to Neuroscience Conclusions Acknowledgments References ADOLPH 2005 91 122 K ACTIONORGANIZERPERCEPTIONCOGNITIONDURINGLEARNINGDEVELOPMENT LEARNINGLEARNINDEVELOPMENTACTION ANDERSON 2004 1036 1060 J BADDELEY 2012 1 29 A BALAGUER 2016 893 903 J BARLOW 1959 535 539 H MECHANISATIONTHOUGHTPROCESSES SENSORYMECHANISMSREDUCTIONREDUNDANCYINTELLIGENCE BARNETT 2002 612 637 S BASTOS 2012 695 711 A BATTAGLIA 2013 18327 18332 P BI 1998 10464 10472 G BOTVINICK 2006 201 233 M BROOKS 2012 462 463 R BROWNE 2012 1 43 C CHURCHLAND 1988 741 745 P CICHON 2015 180 185 J CICHY 2014 455 462 R COLLINS 2012 e1001293 A CONSTANTINESCU 2016 1464 1468 A CRAIK 1943 K NATUREEXPLANATION CUKUR 2013 763 770 T DAW 2005 1704 1711 N DEISSEROTH 2013 568 577 K DOLAN 2013 312 325 R DOLL 2015 767 772 B DONOSO 2014 1481 1486 M DOUMAS 2008 1 43 L DURSTEWITZ 2000 1184 1191 D ELMAN 1990 179 211 J ESSER 2016 11441 11446 S FODOR 1988 3 71 J FRENCH 1999 128 135 R FUKUSHIMA 1980 193 202 K FUSI 2005 599 611 S GALLISTEL 2009 C MEMORYCOMPUTATIONALBRAINCOGNITIVESCIENCEWILLTRANSFORMNEUROSCIENCE GERSHMAN 2017 101 128 S GILMORE 2007 589 591 C GLASSER 2016 1175 1187 M GOLDMANRAKIC 1990 325 335 P GOPNIK 2004 371 377 A GRAVES 2016 471 476 A HAFNER 2011 137 169 R HARLOW 1949 51 65 H HASSABIS 2007 299 306 D HASSABIS 2009 1263 1271 D HAUGELAND 1985 J ARTIFICIALINTELLIGENCEIDEA HAYASHITAKAGI 2015 333 338 A HEBB 1949 D ORGANIZATIONBEHAVIOR HINTON 1986 77 109 G EXPLORATIONSINMICROSTRUCTURECOGNITION DISTRIBUTEDREPRESENTATIONS HINTON 2006 1527 1554 G HOCHREITER 1997 1735 1780 S HOLYOAK 1997 35 44 K HONG 2016 613 622 H HOPFIELD 1982 2554 2558 J HOPFIELD 1986 625 633 J HUBEL 1959 574 591 D HUYS 2012 e1002410 Q JOHNSON 2007 12176 12189 A JONAS 2017 e1005268 E JORDAN 1997 471 495 M KEMP 2010 1185 1243 C KHALIGHRAZAVI 2014 e1003915 S KIRKPATRICK 2017 3521 3526 J KOCH 1985 219 227 C KRAKAUER 2017 480 490 J KRIEGESKORTE 2013 401 412 N KUMARAN 2012 573 616 D KUMARAN 2016 512 534 D KURTHNELSON 2016 194 204 Z LAKE 2015 1332 1338 B LECUN 1989 541 551 Y LECUN 2015 436 444 Y LEIBO 2017 62 67 J LEGG 2007 17 24 S ADVANCESINARTIFICIALGENERALINTELLIGENCECONCEPTSARCHITECTURESALGORITHMSPROCEEDINGSAGIWORKSHOP ACOLLECTIONDEFINITIONSINTELLIGENCE LILLICRAP 2016 13276 T LLOYD 2012 87 K MARBLESTONE 2016 94 A MARCUS 1998 243 282 G MARKRAM 2006 153 160 H MARR 1976 1 22 D MCCLELLAND 2003 310 322 J MCCLELLAND 1995 419 457 J MCCULLOCH 1943 115 133 W MNIH 2015 529 533 V MOORE 2017 47 72 T MORAVCIK 2017 508 513 M NISHIYAMA 2015 63 75 J ODOHERTY 2003 329 337 J ONEILL 2010 220 229 J OREILLY 2006 283 328 R OLAFSDOTTIR 2015 e06063 H OLSHAUSEN 1993 4700 4719 B PFEIFFER 2013 74 79 B POSNER 1990 25 42 M RABY 2007 919 921 C REDISH 2016 147 159 A RIESENHUBER 1999 1019 1025 M ROSENBLATT 1958 386 408 F ROWLAND 2016 19 40 D RUMELHART 1985 318 362 D PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURECOGNITION LEARNINGINTERNALREPRESENTATIONSBYERRORPROPAGATION RUMELHART 1986 D PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURESCOGNITION SALINAS 1997 3267 3272 E SCHACTER 2012 677 694 D SCHMIDHUBER 2014 14047828 J SCHULTZ 1997 1593 1599 W SERRE 2007 411 426 T SHALLICE 1988 T NEUROPSYCHOLOGYMENTALSTRUCTURE SILVER 2016 484 489 D SINGER 2009 910 921 A SKAGGS 1996 1870 1873 W SMITH 1995 1 32 L BASICAPPLIEDPERSPECTIVESLEARNINGCOGNITIONDEVELOPMENTMINNESOTASYMPOSIACHILDPSYCHOLOGY SELFORGANIZINGPROCESSESINLEARNINGLEARNWORDSDEVELOPMENTNOTINDUCTION SOLWAY 2014 e1003779 A SPELKE 2007 89 96 E SQUIRE 2004 279 306 L STJOHN 1990 217 257 M SUMMERFIELD 2006 905 916 J SUTTON 1991 160 163 R SUTTON 1981 135 170 R SUTTON 1998 R REINFORCEMENTLEARNING TESAURO 1995 58 68 G THRUN 1995 25 46 S TOLMAN 1948 189 208 E TSUTSUI 2016 12554 K TULVING 1985 385 398 E TULVING 2002 1 25 E TURING 1936 230 265 A TURING 1950 433 460 A WERBOS 1974 P BEYONDREGRESSIONNEWTOOLSFORPREDICTIONANALYSISINBEHAVIORALSCIENCES WHITTINGTON 2017 1229 1262 J YAMINS 2016 356 365 D YANG 2009 920 924 G HASSABISX2017X245 HASSABISX2017X245X258 HASSABISX2017X245XD HASSABISX2017X245X258XD Full 2018-07-19T00:27:23Z ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window 2018-07-19T00:00:00.000Z UnderEmbargo http://creativecommons.org/licenses/by-nc-nd/4.0/ This article is made available under the Elsevier license. \u00a9 2017 Elsevier Inc. item S0896-6273(17)30509-3 S0896627317305093 1-s2.0-S0896627317305093 10.1016/j.neuron.2017.06.011 272195 2018-09-27T04:03:34.741449Z 2017-07-19 1-s2.0-S0896627317305093-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/MAIN/application/pdf/7d8fb92a4537e7241ef859c46b2bfd5c/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/MAIN/application/pdf/7d8fb92a4537e7241ef859c46b2bfd5c/main.pdf main.pdf pdf true 1472961 MAIN 14 1-s2.0-S0896627317305093-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/PREVIEW/image/png/7eb61994cdb233c44df4343bb984b366/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/PREVIEW/image/png/7eb61994cdb233c44df4343bb984b366/main_1.png main_1.png png 73365 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0896627317305093-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr1/THUMBNAIL/image/gif/0d6303cb216786997e6552911f8c73a0/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr1/THUMBNAIL/image/gif/0d6303cb216786997e6552911f8c73a0/gr1.sml gr1 gr1.sml sml 18484 155 219 IMAGE-THUMBNAIL 1-s2.0-S0896627317305093-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr2/THUMBNAIL/image/gif/3ee7a200af6a8701dc4e125c49adb1e8/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr2/THUMBNAIL/image/gif/3ee7a200af6a8701dc4e125c49adb1e8/gr2.sml gr2 gr2.sml sml 18947 154 219 IMAGE-THUMBNAIL 1-s2.0-S0896627317305093-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr1/DOWNSAMPLED/image/jpeg/9c15334b1d596b12517b2fd33e42a6a8/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr1/DOWNSAMPLED/image/jpeg/9c15334b1d596b12517b2fd33e42a6a8/gr1.jpg gr1 gr1.jpg jpg 88297 462 653 IMAGE-DOWNSAMPLED 1-s2.0-S0896627317305093-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr2/DOWNSAMPLED/image/jpeg/9c063558c93bee4f821403c44f470e8f/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr2/DOWNSAMPLED/image/jpeg/9c063558c93bee4f821403c44f470e8f/gr2.jpg gr2 gr2.jpg jpg 83667 462 656 IMAGE-DOWNSAMPLED 1-s2.0-S0896627317305093-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr1/HIGHRES/image/jpeg/75b2828388d63df930362739a6374298/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr1/HIGHRES/image/jpeg/75b2828388d63df930362739a6374298/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 478459 2043 2890 IMAGE-HIGH-RES 1-s2.0-S0896627317305093-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0896627317305093/gr2/HIGHRES/image/jpeg/199fa6afd072d64b1ef70ef82c317892/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0896627317305093/gr2/HIGHRES/image/jpeg/199fa6afd072d64b1ef70ef82c317892/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 429968 2046 2905 IMAGE-HIGH-RES NEURON 13768 S0896-6273(17)30509-3 10.1016/j.neuron.2017.06.011 Figure 1 Parallels between AI Systems and Neural Models of Behavior (A) Attention. Schematic of recurrent attention model (Mnih et al., 2014). Given an input image (xt) and foveal location (lt \u2212 1), the glimpse sensor extracts a multi-resolution \u201cretinal\u201d representation (\u03c1(xt, lt \u2212 1)). This is the input to a glimpse network, which produces a representation that is passed to the LSTM core, which defines the next location to attend to (lt) (and classification decision). (B) Schematic of complementary learning systems and episodic control. Top: non-parametric fast learning hippocampal system and parametric slow-learning neocortical system (i.e., parametric: a fixed number of parameters; non-parametric: the number of parameters can grow with the amount of data). Hippocampus/instance-based system supports rapid behavioral adjustment (i.e., episodic control; Blundell et al., 2016) and experience replay, which supports interleaved training (i.e., on random subsets of experiences) of deep neural network (Mnih et al., 2015) or neocortex. Bottom: episodic control (from Blundell et al., 2016). Game states (Atari shown) are stored within buffers (one for each possible action) together with the highest (discounted) return experienced from that state (i.e., Q-value). When experiencing a new state, the policy (\u03c0) is determined by averaging the Q-value across the k nearest neighbors in each action buffer and selecting the action with the highest expected return. (C) Illustration of parallels between macroscopic organization of models of working memory and the differentiable neural computer (Graves et al., 2016) (or Neural Turing Machine). The network controller (typically recurrent) is analogous to the central executive (typically viewed to be instantiated in the prefrontal cortex) and attends/reads/writes to an external memory matrix (phonological loo /sketchpad in working memory model). Architecture is shown performing copy task. (D) Illustration of parallel between neurobiological models of synaptic consolidation and the elastic weight consolidation (EWC) algorithm. Left: two-photon structural imaging data showing learning-related increase in size of dendrites (each corresponding approximately to a single excitatory synapse) that persists for months (from Yang et al., 2009). Middle: schematic of Cascade model of synaptic consolidation (adapted with permission from Fusi et al., 2005). Binary synapses transition between metaplastic states which are more/less plastic (least plastic states at bottom of diagram), as a function of prior potentiation/depression events. Right panel: schematic of elastic weight consolidation (EWC) algorithm. After training on the first task (A), network parameters are optimized for good performance: single weight (w1 A\u2217 illustrated). EWC implements a constraint analogous to a spring that anchors weights to the previously found solution (i.e., for task A), when training on a new task (e.g., task B), with the stiffness of the spring proportional to the importance of that parameter for task A performance (Kirkpatrick et al., 2017). Figure 2 Examples of Recent AI Systems that Have Been Inspired by Neuroscience (A) Intuitive physics knowledge. Illustration of the ability of the interaction network (Battaglia et al., 2016) to reason and make predictions about the physical interaction between objects in the bouncing ball problem (top) and spaceship problem (bottom: Hamrick et al., 2017). The network takes as input objects and their relations and accurately simulates their trajectories by modeling collisions, gravitational forces, etc., effectively acting as a learned physics engine. (B) Scene understanding through structured generative models (Eslami et al., 2016). Top: iterative inference in a variational auto-encoder architecture. The recurrent network attends to one object at a time, infers its attributes, and performs the appropriate number of inference steps for each input image (x). Scenes are described in terms of groups of latent variables (Z) that specify presence/absence (zpres), properties such as position (zwhere), and shape (zwhat). Inference network (black connections), and the generator network (red arrow), which produces reconstructed image (y). Bottom: illustration of iterative inference in multiple MNIST images (green indicates the first step and red the second step). Right: inference about the position/shape of multiple objects in realistic scene (note that inference is accurate, and hence it is difficult to distinguish inferred positions [red line] from ground truth). Latent representations in this network speed learning on downstream tasks (e.g., addition of MNIST digits) (not depicted; see Eslami et al., 2016). (C) Unsupervised learning of core object properties (Higgins et al., 2016) is shown. Left: schematic illustrating learning of disentangled factors of sensory input by deep generative model (left: variational auto-encoder [VAE]), whose representations can speed learning on downstream tasks (Eslami et al., 2016), as compared to relatively entangled representation learned by typical deep network (e.g., DQN: right). Right panel illustrates latent representation of VAE; latent units coding for factors of variation, such as object position, rotation, and scale, are shown by effect of independently changing the activity of one latent unit. Such networks can learn intuitive concepts such as \u201cobjectness,\u201d being able to support zero-shot transfer (i.e., reasoning about position or scale of an unseen object with a novel shape; Higgins et al., 2016). (D) One-shot generalization in deep sequential generative models (Rezende et al., 2016b) is shown. Deep generative models specify a causal process for generating the observed data using a hierarchy of latent variables, with attentional mechanisms supporting sequential inference. Illustrated are generated samples from the Rezende et al. model, conditioned on a single novel character from a held-out alphabet from the Omniglot dataset (Lake et al., 2015), demonstrating abilities that mirror human abilities to generalize from a single concept. (E) Imagination of realistic environments in deep networks (Chiappa et al., 2017) is shown. Generated (left) and real (right) frames from procedural mazes (i.e., new maze layout on each episode) produced by an action-conditional recurrent network model \u223c150 and 200 frames after the last observed image, respectively. Review Neuroscience-Inspired Artificial Intelligence Demis Hassabis 1 2 \u2217 dhcontact@google.com Dharshan Kumaran 1 3 Christopher Summerfield 1 4 Matthew Botvinick 1 2 1 DeepMind, 5 New Street Square, London, UK 2 Gatsby Computational Neuroscience Unit, 25 Howland Street, London, UK 3 Institute of Cognitive Neuroscience, University College London, 17 Queen Square, London, UK 4 Department of Experimental Psychology, University of Oxford, Oxford, UK \u2217 Corresponding author The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields. Hassabis et al. review how neuroscience has informed research in artificial intelligence. They argue that a better understanding of biological brains will play a vital role in building intelligent machines. Keywords artificial intelligence brain cognition neural network learning Main Text In recent years, rapid progress has been made in the related fields of neuroscience and artificial intelligence (AI). At the dawn of the computer age, work on AI was inextricably intertwined with neuroscience and psychology, and many of the early pioneers straddled both fields, with collaborations between these disciplines proving highly productive (Churchland and Sejnowski, 1988; Hebb, 1949; Hinton et al., 1986; Hopfield, 1982; McCulloch and Pitts, 1943; Turing, 1950). However, more recently, the interaction has become much less commonplace, as both subjects have grown enormously in complexity and disciplinary boundaries have solidified. In this review, we argue for the critical and ongoing importance of neuroscience in generating ideas that will accelerate and guide AI research (see Hassabis commentary in Brooks et al., 2012). We begin with the premise that building human-level general AI (or \u201cTuring-powerful\u201d intelligent systems; Turing, 1936) is a daunting task, because the search space of possible solutions is vast and likely only very sparsely populated. We argue that this therefore underscores the utility of scrutinizing the inner workings of the human brain\u2014 the only existing proof that such an intelligence is even possible. Studying animal cognition and its neural implementation also has a vital role to play, as it can provide a window into various important aspects of higher-level general intelligence. The benefits to developing AI of closely examining biological intelligence are two-fold. First, neuroscience provides a rich source of inspiration for new types of algorithms and architectures, independent of and complementary to the mathematical and logic-based methods and ideas that have largely dominated traditional approaches to AI. For example, were a new facet of biological computation found to be critical to supporting a cognitive function, then we would consider it an excellent candidate for incorporation into artificial systems. Second, neuroscience can provide validation of AI techniques that already exist. If a known algorithm is subsequently found to be implemented in the brain, then that is strong support for its plausibility as an integral component of an overall general intelligence system. Such clues can be critical to a long-term research program when determining where to allocate resources most productively. For example, if an algorithm is not quite attaining the level of performance required or expected, but we observe it is core to the functioning of the brain, then we can surmise that redoubled engineering efforts geared to making it work in artificial systems are likely to pay off. Of course from a practical standpoint of building an AI system, we need not slavishly enforce adherence to biological plausibility. From an engineering perspective, what works is ultimately all that matters. For our purposes then, biological plausibility is a guide, not a strict requirement. What we are interested in is a systems neuroscience-level understanding of the brain, namely the algorithms, architectures, functions, and representations it utilizes. This roughly corresponds to the top two levels of the three levels of analysis that Marr famously stated are required to understand any complex biological system (Marr and Poggio, 1976): the goals of the system (the computational level) and the process and computations that realize this goal (the algorithmic level). The precise mechanisms by which this is physically realized in a biological substrate are less relevant here (the implementation level). Note this is where our approach to neuroscience-inspired AI differs from other initiatives, such as the Blue Brain Project (Markram, 2006) or the field of neuromorphic computing systems (Esser et al., 2016), which attempt to closely mimic or directly reverse engineer the specifics of neural circuits (albeit with different goals in mind). By focusing on the computational and algorithmic levels, we gain transferrable insights into general mechanisms of brain function, while leaving room to accommodate the distinctive opportunities and challenges that arise when building intelligent machines in silico. The following sections unpack these points by considering the past, present, and future of the AI-neuroscience interface. Before beginning, we offer a clarification. Throughout this article, we employ the terms \u201cneuroscience\u201d and \u201cAI.\u201d We use these terms in the widest possible sense. When we say neuroscience, we mean to include all fields that are involved with the study of the brain, the behaviors that it generates, and the mechanisms by which it does so, including cognitive neuroscience, systems neuroscience and psychology. When we say AI, we mean work in machine learning, statistics, and AI research that aims to build intelligent machines (Legg and Hutter, 2007). We begin by considering the origins of two fields that are pivotal for current AI research, deep learning and reinforcement learning, both of which took root in ideas from neuroscience. We then turn to the current state of play in AI research, noting many cases where inspiration has been drawn (sometimes without explicit acknowledgment) from concepts and findings in neuroscience. In this section, we particularly emphasize instances where we have combined deep learning with other approaches from across machine learning, such as reinforcement learning (Mnih et al., 2015), Monte Carlo tree search (Silver et al., 2016), or techniques involving an external content-addressable memory (Graves et al., 2016). Next, we consider the potential for neuroscience to support future AI research, looking at both the most likely research challenges and some emerging neuroscience-inspired AI techniques. While our main focus will be on the potential for neuroscience to benefit AI, our final section will briefly consider ways in which AI may be helpful to neuroscience and the broader potential for synergistic interactions between these two fields. The Past Deep Learning As detailed in a number of recent reviews, AI has been revolutionized over the past few years by dramatic advances in neural network, or \u201cdeep learning,\u201d methods (LeCun et al., 2015; Schmidhuber, 2014). As the moniker \u201cneural network\u201d might suggest, the origins of these AI methods lie directly in neuroscience. In the 1940s, investigations of neural computation began with the construction of artificial neural networks that could compute logical functions (McCulloch and Pitts, 1943). Not long after, others proposed mechanisms by which networks of neurons might learn incrementally via supervisory feedback (Rosenblatt, 1958) or efficiently encode environmental statistics in an unsupervised fashion (Hebb, 1949). These mechanisms opened up the field of artificial neural network research, and they continue to provide the foundation for contemporary research on deep learning (Schmidhuber, 2014). Not long after this pioneering work, the development of the backpropagation algorithm allowed learning to occur in networks composed of multiple layers (Rumelhart et al., 1985; Werbos, 1974). Notably, the implications of this method for understanding intelligence, including AI, were first appreciated by a group of neuroscientists and cognitive scientists, working under the banner of parallel distributed processing (PDP) (Rumelhart et al., 1986). At the time, most AI research was focused on building logical processing systems based on serial computation, an approach inspired in part by the notion that human intelligence involves manipulation of symbolic representations (Haugeland, 1985). However, there was a growing sense in some quarters that purely symbolic approaches might be too brittle and inflexible to solve complex real-world problems of the kind that humans routinely handle. Instead, a growing foundation of knowledge about the brain seemed to point in a very different direction, highlighting the role of stochastic and highly parallelized information processing. Building on this, the PDP movement proposed that human cognition and behavior emerge from dynamic, distributed interactions within networks of simple neuron-like processing units, interactions tuned by learning procedures that adjust system parameters in order to minimize error or maximize reward. Although the PDP approach was at first applied to relatively small-scale problems, it showed striking success in accounting for a wide range of human behaviors (Hinton et al., 1986). Along the way, PDP research introduced a diverse collection of ideas that have had a sustained influence on AI research. For example, current machine translation research exploits the notion that words and sentences can be represented in a distributed fashion (i.e., as vectors) (LeCun et al., 2015), a principle that was already ingrained in early PDP-inspired models of sentence processing (St. John and McClelland, 1990). Building on the PDP movement\u2019s appeal to biological computation, current state-of-the-art convolutional neural networks (CNNs) incorporate several canonical hallmarks of neural computation, including nonlinear transduction, divisive normalization, and maximum-based pooling of inputs (Yamins and DiCarlo, 2016). These operations were directly inspired by single-cell recordings from the mammalian visual cortex that revealed how visual input is filtered and pooled in simple and complex cells in area V1 (Hubel and Wiesel, 1959). Moreover, current network architectures replicate the hierarchical organization of mammalian cortical systems, with both convergent and divergent information flow in successive, nested processing layers (Krizhevsky et al., 2012; LeCun et al., 1989; Riesenhuber and Poggio, 1999; Serre et al., 2007), following ideas first advanced in early neural network models of visual processing (Fukushima, 1980). In both biological and artificial systems, successive non-linear computations transform raw visual input into an increasingly complex set of features, permitting object recognition that is invariant to transformations of pose, illumination, or scale. As the field of deep learning evolved out of PDP research into a core area within AI, it was bolstered by new ideas, such as the development of deep belief networks (Hinton et al., 2006) and the introduction of large datasets inspired by research on human language (Deng et al., 2009). During this period, it continued to draw key ideas from neuroscience. For example, biological considerations informed the development of successful regularization schemes that support generalization beyond training data. One such scheme, in which only a subset of units participate in the processing of a given training example (\u201cdropout\u201d), was motivated by the stochasticity that is inherent in biological systems populated by neurons that fire with Poisson-like statistics (Hinton et al., 2012). Here and elsewhere, neuroscience has provided initial guidance toward architectural and algorithmic constraints that lead to successful neural network applications for AI. Reinforcement Learning Alongside its important role in the development of deep learning, neuroscience was also instrumental in erecting a second pillar of contemporary AI, stimulating the emergence of the field of reinforcement learning (RL). RL methods address the problem of how to maximize future reward by mapping states in the environment to actions and are among the most widely used tools in AI research (Sutton and Barto, 1998). Although it is not widely appreciated among AI researchers, RL methods were originally inspired by research into animal learning. In particular, the development of temporal-difference (TD) methods, a critical component of many RL models, was inextricably intertwined with research into animal behavior in conditioning experiments. TD methods are real-time models that learn from differences between temporally successive predictions, rather than having to wait until the actual reward is delivered. Of particular relevance was an effect called second-order conditioning, where affective significance is conferred on a conditioned stimulus (CS) through association with another CS rather than directly via association with the unconditioned stimulus (Sutton and Barto, 1981). TD learning provides a natural explanation for second-order conditioning and indeed has gone on to explain a much wider range of findings from neuroscience, as we discuss below. Here, as in the case of deep learning, investigations initially inspired by observations from neuroscience led to further developments that have strongly shaped the direction of AI research. From their neuroscience-informed origins, TD methods and related techniques have gone on to supply the core technology for recent advances in AI, ranging from robotic control (Hafner and Riedmiller, 2011) to expert play in backgammon (Tesauro, 1995) and Go (Silver et al., 2016). The Present Reading the contemporary AI literature, one gains the impression that the earlier engagement with neuroscience has diminished. However, if one scratches the surface, one can uncover many cases in which recent developments have been inspired and guided by neuroscientific considerations. Here, we look at four specific examples. Attention The brain does not learn by implementing a single, global optimization principle within a uniform and undifferentiated neural network (Marblestone et al., 2016). Rather, biological brains are modular, with distinct but interacting subsystems underpinning key functions such as memory, language, and cognitive control (Anderson et al., 2004; Shallice, 1988). This insight from neuroscience has been imported, often in an unspoken way, into many areas of current AI. One illustrative example is recent AI work on attention. Up until quite lately, most CNN models worked directly on entire images or video frames, with equal priority given to all image pixels at the earliest stage of processing. The primate visual system works differently. Rather than processing all input in parallel, visual attention shifts strategically among locations and objects, centering processing resources and representational coordinates on a series of regions in turn (Koch and Ullman, 1985; Moore and Zirnsak, 2017; Posner and Petersen, 1990). Detailed neurocomputational models have shown how this piecemeal approach benefits behavior, by prioritizing and isolating the information that is relevant at any given moment (Olshausen et al., 1993; Salinas and Abbott, 1997). As such, attentional mechanisms have been a source of inspiration for AI architectures that take \u201cglimpses\u201d of the input image at each step, update internal state representations, and then select the next location to sample (Larochelle and Hinton, 2010; Mnih et al., 2014) (Figure 1 A). One such network was able to use this selective attentional mechanism to ignore irrelevant objects in a scene, allowing it to perform well in challenging object classification tasks in the presence of clutter (Mnih et al., 2014). Further, the attentional mechanism allowed the computational cost (e.g., number of network parameters) to scale favorably with the size of the input image. Extensions of this approach were subsequently shown to produce impressive performance at difficult multi-object recognition tasks, outperforming conventional CNNs that process the entirety of the image, both in terms of accuracy and computational efficiency (Ba et al., 2015), as well as enhancing image-to-caption generation (Xu et al., 2015). While attention is typically thought of as an orienting mechanism for perception, its \u201cspotlight\u201d can also be focused internally, toward the contents of memory. This idea, a recent focus in neuroscience studies (Summerfield et al., 2006), has also inspired work in AI. In some architectures, attentional mechanisms have been used to select information to be read out from the internal memory of the network. This has helped provide recent successes in machine translation (Bahdanau et al., 2014) and led to important advances on memory and reasoning tasks (Graves et al., 2016). These architectures offer a novel implementation of content-addressable retrieval, which was itself a concept originally introduced to AI from neuroscience (Hopfield, 1982). One further area of AI where attention mechanisms have recently proven useful focuses on generative models, systems that learn to synthesize or \u201cimagine\u201d images (or other kinds of data) that mimic the structure of examples presented during training. Deep generative models (i.e., generative models implemented as multi-layered neural networks) have recently shown striking successes in producing synthetic outputs that capture the form and structure of real visual scenes via the incorporation of attention-like mechanisms (Hong et al., 2015; Reed et al., 2016). For example, in one state-of-the-art generative model known as DRAW, attention allows the system to build up an image incrementally, attending to one portion of a \u201cmental canvas\u201d at a time (Gregor et al., 2015). Episodic Memory A canonical theme in neuroscience is that that intelligent behavior relies on multiple memory systems (Tulving, 1985). These will include not only reinforcement-based mechanisms, which allow the value of stimuli and actions to be learned incrementally and through repeated experience, but also instance-based mechanisms, which allow experiences to be encoded rapidly (in \u201cone shot\u201d) in a content-addressable store (Gallistel and King, 2009). The latter form of memory, known as episodic memory (Tulving, 2002), is most often associated with circuits in the medial temporal lobe, prominently including the hippocampus (Squire et al., 2004). One recent breakthrough in AI has been the successful integration of RL with deep learning (Mnih et al., 2015; Silver et al., 2016). For example, the deep Q-network (DQN) exhibits expert play on Atari 2600 video games by learning to transform a vector of image pixels into a policy for selecting actions (e.g., joystick movements). One key ingredient in DQN is \u201cexperience replay,\u201d whereby the network stores a subset of the training data in an instance-based way, and then \u201creplays\u201d it offline, learning anew from successes or failures that occurred in the past. Experience replay is critical to maximizing data efficiency, avoids the destabilizing effects of learning from consecutive correlated experiences, and allows the network to learn a viable value function even in complex, highly structured sequential environments such as video games. Critically, experience replay was directly inspired by theories that seek to understand how the multiple memory systems in the mammalian brain might interact. According to a prominent view, animal learning is supported by parallel or \u201ccomplementary\u201d learning systems in the hippocampus and neocortex (Kumaran et al., 2016; McClelland et al., 1995). The hippocampus acts to encode novel information after a single exposure (one-shot learning), but this information is gradually consolidated to the neocortex in sleep or resting periods that are interleaved with periods of activity. This consolidation is accompanied by replay in the hippocampus and neocortex, which is observed as a reinstatement of the structured patterns of neural activity that accompanied the learning event (O\u2019Neill et al., 2010; Skaggs and McNaughton, 1996) (Figure 1B). This theory was originally proposed as a solution to the well-known problem that in conventional neural networks, correlated exposure to sequential task settings leads to mutual interference among policies, resulting in catastrophic forgetting of one task as a new one is learned. The replay buffer in DQN might thus be thought of as a very primitive hippocampus, permitting complementary learning in silico much as is proposed for biological brains. Later work showed that the benefits of experience replay in DQN are enhanced when replay of highly rewarding events is prioritized (Schaul et al., 2015), just as hippocampal replay seems to favor events that lead to high levels of reinforcement (Singer and Frank, 2009). Experiences stored in a memory buffer can not only be used to gradually adjust the parameters of a deep network toward an optimal policy, as in DQN, but can also support rapid behavioral change based on an individual experience. Indeed, theoretical neuroscience has argued for the potential benefits of episodic control, whereby rewarded action sequences can be internally re-enacted from a rapidly updateable memory store, implemented in the biological case in the hippocampus (Gershman and Daw, 2017). Further, normative accounts show that episodic control is particularly advantageous over other learning mechanisms when limited experience of the environment has been obtained (Lengyel and Dayan, 2007). Recent AI research has drawn on these ideas to overcome the slow learning characteristics of deep RL networks, developing architectures that implement episodic control (Blundell et al., 2016). These networks store specific experiences (e.g., actions and reward outcomes associated with particular Atari game screens) and select new actions based on the similarity between the current situation input and the previous events stored in memory, taking the reward associated with those previous events into account (Figure 1B). As predicted from the initial, neuroscience-based work (Lengyel and Dayan, 2007), artificial agents employing episodic control show striking gains in performance over deep RL networks, particularly early on during learning (Blundell et al., 2016). Further, they are able to achieve success on tasks that depend heavily on one-shot learning, where typical deep RL architectures fail. Moreover, episodic-like memory systems more generally have shown considerable promise in allowing new concepts to be learned rapidly based on only a few examples (Vinyals et al., 2016). In the future, it will be interesting to harness the benefits of rapid episodic-like memory and more traditional incremental learning in architectures that incorporate both of these components within an interacting framework that mirrors the complementary learning systems in mammalian brain. We discuss these future perspectives below in more detail later, in \u201cImagination and planning.\u201d Working Memory Human intelligence is characterized by a remarkable ability to maintain and manipulate information within an active store, known as working memory, which is thought to be instantiated within the prefrontal cortex and interconnected areas (Goldman-Rakic, 1990). Classic cognitive theories suggest that this functionality depends on interactions between a central controller (\u201cexecutive\u201d) and separate, domain-specific memory buffers (e.g., visuo-spatial sketchpad) (Baddeley, 2012). AI research has drawn inspiration from these models, by building architectures that explicitly maintain information over time. Historically, such efforts began with the introduction of recurrent neural network architectures displaying attractor dynamics and rich sequential behavior, work directly inspired by neuroscience (Elman, 1990; Hopfield and Tank, 1986; Jordan, 1997). This work enabled later, more detailed modeling of human working memory (Botvinick and Plaut, 2006; Durstewitz et al., 2000), but it also laid the foundation for further technical innovations that have proved pivotal in recent AI research. In particular, one can see close parallels between the learning dynamics in these early, neuroscience-inspired networks and those in long-short-term memory (LSTM) networks, which subsequently achieved state of the art performance across a variety of domains. LTSMs allow information to be gated into a fixed activity state and maintained until an appropriate output is required (Hochreiter and Schmidhuber, 1997). Variants of this type of network have shown some striking behaviors in challenging domains, such as learning to respond to queries about the latent state of variables after training on computer code (Zaremba and Sutskever, 2014). In ordinary LSTM networks, the functions of sequence control and memory storage are closely intertwined. This contrasts with classic models of human working memory, which, as mentioned above, separate these two. This neuroscience-based schema has recently inspired more complex AI architectures where control and storage are supported by distinct modules (Graves et al., 2014, 2016; Weston et al., 2014). For example, the differential neural computer (DNC) involves a neural network controller that attends to and reads/writes from an external memory matrix (Graves et al., 2016). This externalization allows the network controller to learn from scratch (i.e., via end-to-end optimization) to perform a wide range of complex memory and reasoning tasks that currently elude LSTMs, such as finding the shortest path through a graph-like structure, such as a subway map, or manipulating blocks in a variant of the Tower of Hanoi task (Figure 1C). These types of problems were previously argued to depend exclusively on symbol processing and variable binding and therefore beyond the purview of neural networks (Fodor and Pylyshyn, 1988; Marcus, 1998). Of note, although both LSTMs and the DNC are described here in the context of working memory, they have the potential to maintain information over many thousands of training cycles and so may thus be suited to longer-term forms of memory, such as retaining and understanding the contents of a book. Continual Learning Intelligent agents must be able to learn and remember many different tasks that are encountered over multiple timescales. Both biological and artificial agents must thus have a capacity for continual learning, that is, an ability to master new tasks without forgetting how to perform prior tasks (Thrun and Mitchell, 1995). While animals appear relatively adept at continual learning, neural networks suffer from the problem of catastrophic forgetting (French, 1999; McClelland et al., 1995). This occurs as the network parameters shift toward the optimal state for performing the second of two successive tasks, overwriting the configuration that allowed them to perform the first. Given the importance of continual learning, this liability of neural networks remains a significant challenge for the development of AI. In neuroscience, advanced neuroimaging techniques (e.g., two-photon imaging) now allow dynamic in vivo visualization of the structure and function of dendritic spines during learning, at the spatial scale of single synapses (Nishiyama and Yasuda, 2015). This approach can be used to study neocortical plasticity during continual learning (Cichon and Gan, 2015; Hayashi-Takagi et al., 2015; Yang et al., 2009). There is emerging evidence for specialized mechanisms that protect knowledge about previous tasks from interference during learning on a new task. These include decreased synaptic lability (i.e., lower rates of plasticity) in a proportion of strengthened synapses, mediated by enlargements to dendritic spines that persist despite learning of other tasks (Cichon and Gan, 2015; Yang et al., 2009) (Figure 1D). These changes are associated with retention of task performance over several months, and indeed, if they are \u201cerased\u201d with synaptic optogenetics, this leads to forgetting of the task (Hayashi-Takagi et al., 2015). These empirical insights are consistent with theoretical models that suggest that memories can be protected from interference through synapses that transition between a cascade of states with different levels of plasticity (Fusi et al., 2005) (Figure 1D). Together, these findings from neuroscience have inspired the development of AI algorithms that address the challenge of continual learning in deep networks by implementing of a form of \u201celastic\u201d weight consolidation (EWC) (Kirkpatrick et al., 2017), which acts by slowing down learning in a subset of network weights identified as important to previous tasks, thereby anchoring these parameters to previously found solutions (Figure 1D). This allows multiple tasks to be learned without an increase in network capacity, with weights shared efficiently between tasks with related structure. In this way, the EWC algorithm allows deep RL networks to support continual learning at large scale. The Future In AI, the pace of recent research has been remarkable. Artificial systems now match human performance in challenging object recognition tasks (Krizhevsky et al., 2012) and outperform expert humans in dynamic, adversarial environments such as Atari video games (Mnih et al., 2015), the ancient board game of Go (Silver et al., 2016), and imperfect information games such as heads-up poker (Morav\u010d\u00edk et al., 2017). Machines can autonomously generate synthetic natural images and simulations of human speech that are almost indistinguishable from their real-world counterparts (Lake et al., 2015; van den Oord et al., 2016), translate between multiple languages (Wu et al., 2016), and create \u201cneural art\u201d in the style of well-known painters (Gatys et al., 2015). However, much work is still needed to bridge the gap between machine and human-level intelligence. In working toward closing this gap, we believe ideas from neuroscience will become increasingly indispensable. In neuroscience, the advent of new tools for brain imaging and genetic bioengineering have begun to offer a detailed characterization of the computations occurring in neural circuits, promising a revolution in our understanding of mammalian brain function (Deisseroth and Schnitzer, 2013). The relevance of neuroscience, both as a roadmap for the AI research agenda and as a source of computational tools is particularly salient in the following key areas. Intuitive Understanding of the Physical World Recent perspectives emphasize key ingredients of human intelligence that are already well developed in human infants but lacking in most AI systems (Gilmore et al., 2007; Gopnik and Schulz, 2004; Lake et al., 2016). Among these capabilities are knowledge of core concepts relating to the physical world, such as space, number, and objectness, which allow people to construct compositional mental models that can guide inference and prediction (Battaglia et al., 2013; Spelke and Kinzler, 2007). AI research has begun to explore methods for addressing this challenge. For example, novel neural network architectures have been developed that interpret and reason about scenes in a humanlike way, by decomposing them into individual objects and their relations (Battaglia et al., 2016; Chang et al., 2016; Eslami et al., 2016) (Figures 2A and 2B). In some cases, this has resulted in human-level performance on challenging reasoning tasks (Santoro et al., 2017). In other work, deep RL has been used to capture the processes by which children gain commonsense understanding of the world through interactive experiments (Denil et al., 2016). Relatedly, deep generative models have been developed that are able to construct rich object models from raw sensory inputs (Higgins et al., 2016). These leverage constraints first identified in neuroscience, such as redundancy reduction (Barlow, 1959), which encourage the emergence of disentangled representations of independent factors such as shape and position (Figure 2C). Importantly, the latent representations learned by such generative models exhibit compositional properties, supporting flexible transfer to novel tasks (Eslami et al., 2016; Higgins et al., 2016; Rezende et al., 2016a). In the caption associated with Figure 2, we provide more detailed information about these networks. Efficient Learning Human cognition is distinguished by its ability to rapidly learn about new concepts from only a handful of examples, leveraging prior knowledge to enable flexible inductive inferences. In order to highlight this human ability as a challenge for AI, Lake and colleagues recently posed a \u201ccharacters challenge\u201d (Lake et al., 2016). Here, an observer must distinguish novel instances of an unfamiliar handwritten character from other, similar items after viewing only a single exemplar. Humans can perform this task well, but it is difficult for classical AI systems. Encouragingly, recent AI algorithms have begun to make progress on tasks like the characters challenge, through both structured probabilistic models (Lake et al., 2015) and deep generative models based on the abovementioned DRAW model (Rezende et al., 2016b). Both classes of system can make inferences about a new concept despite a poverty of data and generate new samples from a single example concept (Figure 2D). Further, recent AI research has developed networks that \u201clearn to learn,\u201d acquiring knowledge on new tasks by leveraging prior experience with related problems, to support one-shot concept learning (Santoro et al., 2016; Vinyals et al., 2016) and accelerating learning in RL tasks (Wang et al., 2016). Once again, this builds on concepts from neuroscience: learning to learn was first explored in studies of animal learning (Harlow, 1949), and has subsequently been studied in developmental psychology (Adolph, 2005; Kemp et al., 2010; Smith, 1995). Transfer Learning Humans also excel at generalizing or transferring generalized knowledge gained in one context to novel, previously unseen domains (Barnett and Ceci, 2002; Holyoak and Thagard, 1997). For example, a human who can drive a car, use a laptop computer, or chair a committee meeting is usually able act effectively when confronted with an unfamiliar vehicle, operating system, or social situation. Progress is being made in developing AI architectures capable of exhibiting strong generalization or transfer, for example by enabling zero-shot inferences about novel shapes outside the training distribution based on compositional representations (Higgins et al., 2016; Figure 2C). Others have shown that a new class of architecture, known as a progressive network, can leverage knowledge gained in one video game to learn rapidly in another, promising the sort of \u201cfar transfer\u201d that is characteristic of human skill acquisition (Rusu et al., 2016a). Progressive networks have also been successfully employed to transfer knowledge for a simulated robotic environment to a real robot arm, massively reducing the training time required on the real world (Rusu et al., 2016b). Intriguingly, the proposed architecture bears some resemblance to a successful computational model of sequential task learning in humans (Collins and Koechlin, 2012; Donoso et al., 2014). In the neuroscience literature, one hallmark of transfer learning has been the ability to reason relationally, and AI researchers have also begun to make progress in building deep networks that address problems of this nature, for example by solving visual analogies (Reed et al., 2015). More generally however, how humans or other animals achieve this sort of high-level transfer learning is unknown, and remains a relatively unexplored topic in neuroscience. New advances on this front could provide critical insights to spur AI research toward the goal of lifelong learning in agents, and we encourage neuroscientists to engage more deeply with this question. At the level of neural coding, this kind of transfer of abstract structured knowledge may rely on the formation of conceptual representations that are invariant to the objects, individuals, or scene elements that populate a sensory domain but code instead for abstract, relational information among patterns of inputs (Doumas et al., 2008). However, we currently lack direct evidence for the existence of such codes in the mammalian brain. Nevertheless, one recent report made the very interesting claim that neural codes thought to be important in the representation of allocentric (map-like) spaces might be critical for abstract reasoning in more general domains (Constantinescu et al., 2016). In the mammalian entorhinal cortex, cells encode the geometry of allocentric space with a periodic \u201cgrid\u201d code, with receptive fields that tile the local space in a hexagonal pattern (Rowland et al., 2016). Grid codes may be an excellent candidate for organizing conceptual knowledge, because they allow state spaces to be decomposed efficiently, in a way that could support discovery of subgoals and hierarchical planning (Stachenfeld et al., 2014). Using functional neuroimaging, the researchers provide evidence for the existence of such codes while humans performed an abstract categorization task, supporting the view that periodic encoding is a generalized hallmark of human knowledge organization (Constantinescu et al., 2016). However, much further work is required to substantiate this interesting claim. Imagination and Planning Despite their strong performance on goal-directed tasks, deep RL systems such as DQN operate mostly in a reactive way, learning the mapping from perceptual inputs to actions that maximize future value. This \u201cmodel-free\u201d RL is computationally inexpensive but suffers from two major drawbacks: it is relatively data inefficient, requiring large amounts of experience to derive accurate estimates, and it is inflexible, being insensitive to changes in the value of outcomes (Daw et al., 2005). By contrast, humans can more flexibly select actions based on forecasts of long-term future outcomes through simulation-based planning, which uses predictions generated from an internal model of the environment learned through experience (Daw et al., 2005; Dolan and Dayan, 2013; Tolman, 1948). Moreover, planning is not a uniquely human capacity. For example, when caching food, scrub jays consider the future conditions under which it is likely to be recovered (Raby et al., 2007), and rats use a \u201ccognitive map\u201d when navigating, allowing inductive inferences during wayfinding and facilitating one-shot learning behaviors in maze-like environments (Daw et al., 2005; Tolman, 1948). Of course, this point has not been lost on AI researchers; indeed, early planning algorithms such as Dyna (Sutton, 1991) were inspired by theories that emphasized the importance of \u201cmental models\u201d in generating hypothetical experiences useful for human learning (Craik, 1943). By now, a large volume of literature exists on AI planning techniques, including model-based RL methods, which seek to implement this forecast-based method of action selection. Furthermore, simulation-based planning, particularly Monte Carlo tree search (MCTS) methods, which use forward search to update a value function and/or policy (Browne et al., 2012), played a key role in recent work in which deep RL attained expert-level performance in the game of Go (Silver et al., 2016). AI research on planning, however, has yet to capture some of the key characteristics that give human planning abilities their power. In particular, we suggest that a general solution to this problem will require understanding how rich internal models, which in practice will have to be approximate but sufficiently accurate to support planning, can be learned through experience, without strong priors being handcrafted into the network by the experimenter. We also argue that AI research will benefit from a close reading of the related literature on how humans imagine possible scenarios, envision the future, and carry out simulation-based planning, functions that depend on a common neural substrate in the hippocampus (Doll et al., 2015; Hassabis and Maguire, 2007, 2009; Schacter et al., 2012). Although imagination has an intrinsically subjective, unobservable quality, we have reason to believe that it has a conserved role in simulation-based planning across species (Hassabis and Maguire, 2009; Schacter et al., 2012). For example, when paused at a choice point, ripples of neural activity in the rat hippocampus resemble those observed during subsequent navigation of the available trajectories (\u201cpreplay\u201d), as if the animal were \u201cimagining\u201d each possible alternative (Johnson and Redish, 2007; \u00d3lafsd\u00f3ttir et al., 2015; Pfeiffer and Foster, 2013). Further, recent work has suggested a similar process during non-spatial planning in humans (Doll et al., 2015; Kurth-Nelson et al., 2016). We have discussed above the ways in which the introduction of mechanisms that replay and learn offline from past experiences can improve the performance of deep RL agents such as DQN (as discussed above in Episodic Memory). Some encouraging initial progress toward simulation-based planning has been made using deep generative models (Eslami et al., 2016; Rezende et al., 2016a, 2016b) (Figure 2). In particular, recent work has introduced new architectures that have the capacity to generate temporally consistent sequences of generated samples that reflect the geometric layout of newly experienced realistic environments (Gemici et al., 2017; Oh et al., 2015) (Figure 2E), providing a parallel to the function of the hippocampus in binding together multiple components to create an imagined experience that is spatially and temporally coherent (Hassabis and Maguire, 2007). Deep generative models thus show the potential to capture the rich dynamics of complex realistic environments, but using these models for simulation-based planning in agents remains a challenge for future work. Insights from neuroscience may provide guidance that facilitates the integration of simulation with control. An emerging picture from neuroscience research suggests that the hippocampus supports planning by instantiating an internal model of the environment, with goal-contingent valuation of simulated outcomes occurring in areas downstream of the hippocampus such the orbitofrontal cortex or striatum (Redish, 2016). Notably, however, the mechanisms that guide the rolling forward of an internal model of the environment in the hippocampus remain uncertain and merit future scrutiny. One possibility is that this process is initiated by the prefrontal cortex through interactions with the hippocampus. Indeed, this notion has distinct parallels with proposals from AI research that a separate controller interacts with an internal model of the environment in a bidirectional fashion, querying the model based on task-relevant goals and receiving predicted simulated states as input (Schmidhuber, 2014). Further, recent efforts to develop agents have employed architectures that instantiate a separation between controller and environmental model to effect simulation-based planning in problems involving the interaction between physical objects (Hamrick et al., 2017). In enhancing agent capabilities in simulation-based planning, it will also be important to consider other salient properties of this process in humans (Hassabis and Maguire, 2007, 2009). Research into human imagination emphasizes its constructive nature, with humans able to construct fictitious mental scenarios by recombining familiar elements in novel ways, necessitating compositional/disentangled representations of the form present in certain generative models (Eslami et al., 2016; Higgins et al., 2016; Rezende et al., 2016a). This fits well with the notion that planning in humans involves efficient representations that support generalization and transfer, so that plans forged in one setting (e.g., going through a door to reach a room) can be leveraged in novel environments that share structure. Further, planning and mental simulation in humans are \u201cjumpy,\u201d bridging multiple temporal scales at a time; for example, humans seem to plan hierarchically, by considering in parallel terminal solutions, interim choice points, and piecemeal steps toward the goal (Balaguer et al., 2016; Solway et al., 2014; Huys et al., 2012). We think that ultimately these flexible, combinatorial aspects of planning will form a critical underpinning of what is perhaps the hardest challenge for AI research: to build an agent that can plan hierarchically, is truly creative, and can generate solutions to challenges that currently elude even the human mind. Virtual Brain Analytics One rather different way in which neuroscience may serve AI is by furnishing new analytic tools for understanding computation in AI systems. Due to their complexity, the products of AI research often remain \u201cblack boxes\u201d; we understand only poorly the nature of the computations that occur, or representations that are formed, during learning of complex tasks. However, by applying tools from neuroscience to AI systems, synthetic equivalents of single-cell recording, neuroimaging, and lesion techniques, we can gain insights into the key drivers of successful learning in AI research and increase the interpretability of these systems. We call this \u201cvirtual brain analytics.\u201d Recent work has made some progress along these lines. For example, visualizing brain states through dimensionality reduction is commonplace in neuroscience, and has recently been applied to neural networks (Zahavy et al., 2016). Receptive field mapping, another standard tool in neuroscience, allows AI researchers to determine the response properties of units in a neural network. One interesting application of this approach in AI is known as activity maximization, in which a network learns to generate synthetic images by maximizing the activity of certain classes of unit (Nguyen et al., 2016; Simonyan et al., 2013). Elsewhere, neuroscience-inspired analyses of linearized networks have uncovered important principles that may be of general benefit in optimizing learning these networks, and understanding the benefits of network depth and representational structure (McClelland and Rogers, 2003; Saxe et al., 2013). While this initial progress is encouraging, more work is needed. It remains difficult to characterize the functioning of complex architectures such as networks with external memory (Graves et al., 2016). Nevertheless, AI researchers are in the unique position of having ground truth knowledge of all components of the system, together with the potential to causally manipulate individual elements, an enviable scenario from the perspective of experimental neuroscientists. As such, we encourage AI researchers to use approaches from neuroscience to explore properties of network architectures and agents through analysis, visualization, causal manipulation, not forgetting the need for carefully designed hypothesis-driven experiments (Jonas and Kording, 2017; Krakauer et al., 2017). We think that virtual brain analytics is likely to be an increasingly integral part of the pipeline of algorithmic development as the complexity of architectures increases. From AI to Neuroscience Thus far, our review has focused primarily on the role of neuroscience in accelerating AI research rather than vice versa. Historically, however, the flow of information between neuroscience and AI has been reciprocal. Machine learning techniques have transformed the analysis of neuroimaging datasets\u2014for example, in the multivariate analysis of fMRI and magnetoencephalographic (MEG) data (Cichy et al., 2014; \u00c7ukur et al., 2013; Kriegeskorte and Kievit, 2013)\u2014with promise for expediting connectomic analysis (Glasser et al., 2016), among other techniques. Going further, we believe that building intelligent algorithms has the potential to offer new ideas about the underpinnings of intelligence in the brains of humans and other animals. In particular, psychologists and neuroscientists often have only quite vague notions of the mechanisms that underlie the concepts they study. AI research can help, by formalizing these concepts in a quantitative language and offering insights into their necessity and sufficiency (or otherwise) for intelligent behavior. A key illustration of this potential is provided by RL. After ideas from animal psychology helped to give birth to reinforcement learning research, key concepts from the latter fed back to inform neuroscience. In particular, the profile of neural signals observed in midbrain dopaminergic neurons in conditioning paradigms was found to bear a striking resemblance to TD-generated prediction errors, providing neural evidence that the brain implements a form of TD learning (O\u2019Doherty et al., 2003; Schultz et al., 1997). This overall narrative arc provides an excellent illustration of how the exchange of ideas between AI and neuroscience can create a \u201cvirtuous circle\u201d advancing the objectives of both fields. In another domain, work focused on enhancing the performance of CNNs has also yielded new insights into the nature of neural representations in high-level visual areas (Khaligh-Razavi and Kriegeskorte, 2014; Yamins and DiCarlo, 2016). For example, one group systematically compared the ability of more than 30 network architectures from AI to explain the structure of neural representations observed in the ventral visual stream of humans and monkeys, finding favorable evidence for deep supervised networks (Khaligh-Razavi and Kriegeskorte, 2014). Further, these deep convolutional network architectures offer a computational account of recent neurophysiological data demonstrating that the coding of category-orthogonal properties of objects (e.g., position, size) actually increases as one progresses higher up the ventral visual stream (Hong et al., 2016). While these findings are far from definitive as yet, it shows how state-of-the-art neural networks from AI can be used as plausible simulacra of biological brains, potentially providing detailed explanations of the computations occurring therein (Khaligh-Razavi and Kriegeskorte, 2014; Yamins and DiCarlo, 2016). Relatedly, properties of the LSTM architecture have provided key insights that motivated the development of working memory models that afford gating-based maintenance of task-relevant information in the prefrontal cortex (Lloyd et al., 2012; O\u2019Reilly and Frank, 2006). We also highlight two recent strands of AI research that may motivate new research in neuroscience. First, neural networks with external memory typically allow the controller to iteratively query or \u201chop through\u201d the contents of memory. This mechanism is critical for reasoning over multiple supporting input statements that relate to a particular query (Sukhbaatar et al., 2015). Previous proposals in neuroscience have argued for a similar mechanism in human cognition, but any potential neural substrates, potentially in the hippocampus, remain to be described (Kumaran and McClelland, 2012). Second, recent work highlights the potential benefits of \u201cmeta-reinforcement learning,\u201d where RL is used to optimize the weights of a recurrent network such that the latter is able to implement a second, emergent RL algorithm that is able to learn faster than the original (Duan et al., 2016; Wang et al., 2016). Intriguingly, these ideas connect with a growing neuroscience literature indicating a role for the prefrontal cortex in RL, alongside more established dopamine-based mechanisms (Schultz et al., 1997). Specifically, they indicate how a relatively slow-learning dopaminergic RL algorithm may support the emergence of a freestanding RL algorithm instantiated with the recurrent activity dynamics of the prefrontal cortex (Tsutsui et al., 2016). Insights from AI research are also providing novel perspectives on how the brain might implement an algorithmic parallel to backpropagation, the key mechanism that allows weights within multiple layers of a hierarchical network to be optimized toward an objective function (Hinton et al., 1986; Werbos, 1974). Backpropagation offers a powerful solution to the problem of credit assignment within deep networks, allowing efficient representations to be learned from high dimensional data (LeCun et al., 2015). However, until recently, several aspects of the backpropagation algorithm were viewed to be biologically implausible (e.g., see Bengio et al., 2015). One important factor is that backpropagation has typically been thought to require perfectly symmetric feedback and feedforward connectivity, a profile that is not observed in mammalian brains. Recent work, however, has demonstrated that this constraint can in fact be relaxed (Liao et al., 2015; Lillicrap et al., 2016). Random backward connections, even when held fixed throughout network training, are sufficient to allow the backpropagation algorithm to function effectively through a process whereby adjustment of the forward weights allows backward projections to transmit useful teaching signals (Lillicrap et al., 2016). A second core objection to the biological plausibility of backpropagation is that weight updates in multi-layered networks require access to information that is non-local (i.e., error signals generated by units many layers downstream) (for review, see Bengio et al., 2015). In contrast, plasticity in biological synapses depends primarily on local information (i.e., pre- and post-synaptic neuronal activity) (Bi and Poo, 1998). AI research has begun to address this fundamental issue. In particular, recent work has shown that hierarchical auto-encoder networks and energy-based networks (e.g., continuous Hopfield networks) (Scellier and Bengio, 2016; Whittington and Bogacz, 2017)\u2014models that have strong connections to theoretical neuroscience ideas about predictive coding (Bastos et al., 2012)\u2014are capable of approximating the backpropagation algorithm, based on weight updates that involve purely local information. Indeed, concrete connections have been drawn between learning in such networks and spike-timing dependent plasticity (Scellier and Bengio, 2016), a Hebbian mechanism instantiated widely across the brain (Bi and Poo, 1998). A different class of local learning rule has been shown to allow hierarchical supervised networks to generate high-level invariances characteristic of biological systems, including mirror-symmetric tuning to physically symmetric stimuli, such as faces (Leibo et al., 2017). Taken together, recent AI research offers the promise of discovering mechanisms by which the brain may implement algorithms with the functionality of backpropagation. Moreover, these developments illustrate the potential for synergistic interactions between AI and neuroscience: research aimed to develop biologically plausible forms of backpropagation have also been motivated by the search for alternative learning algorithms. Given the increasingly deep networks (e.g., >20 layer) used in AI research, factors such as the compounding of successive non-linearities pose challenges for optimization using backpropagation (Bengio et al., 2015). Conclusions In this perspective, we have reviewed some of the many ways in which neuroscience has made fundamental contributions to advancing AI research, and argued for its increasingly important relevance. In strategizing for the future exchange between the two fields, it is important to appreciate that the past contributions of neuroscience to AI have rarely involved a simple transfer of full-fledged solutions that could be directly re-implemented in machines. Rather, neuroscience has typically been useful in a subtler way, stimulating algorithmic-level questions about facets of animal learning and intelligence of interest to AI researchers and providing initial leads toward relevant mechanisms. As such, our view is that leveraging insights gained from neuroscience research will expedite progress in AI research, and this will be most effective if AI researchers actively initiate collaborations with neuroscientists to highlight key questions that could be addressed by empirical work. The successful transfer of insights gained from neuroscience to the development of AI algorithms is critically dependent on the interaction between researchers working in both these fields, with insights often developing through a continual handing back and forth of ideas between fields. In the future, we hope that greater collaboration between researchers in neuroscience and AI, and the identification of a common language between the two fields (Marblestone et al., 2016), will permit a virtuous circle whereby research is accelerated through shared theoretical insights and common empirical advances. We believe that the quest to develop AI will ultimately also lead to a better understanding of our own minds and thought processes. Distilling intelligence into an algorithmic construct and comparing it to the human brain might yield insights into some of the deepest and the most enduring mysteries of the mind, such as the nature of creativity, dreams, and perhaps one day, even consciousness. Acknowledgments We thank Peter Battaglia, Koray Kavukcuoglu, Neil Rabinowitz, Adam Santoro, Greg Wayne, Daan Wierstra, Jane Wang, Martin Chadwick, Joel Leibo, and David Barrett for useful discussions and comments. References Adolph, 2005 K. Adolph Learning to learn in the development of action J. Lockman J. Reiser C.A. Nelson Action as an organizer of perception and cognition during learning and development 2005 Erlbaum Press 91 122 Anderson et al., 2004 J.R. Anderson D. Bothell M.D. Byrne S. Douglass C. Lebiere Y. Qin An integrated theory of the mind Psychol. Rev. 111 2004 1036 1060 Ba et al., 2015 Ba, J.L., Mnih, V., and Kavukcuoglu, K. (2015). Multiple object recognition with visual attention. arXiv, arXiv:14127755. Baddeley, 2012 A. Baddeley Working memory: theories, models, and controversies Annu. Rev. Psychol. 63 2012 1 29 Bahdanau et al., 2014 Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv, arXiv:14090473. Balaguer et al., 2016 J. Balaguer H. Spiers D. Hassabis C. Summerfield Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network Neuron 90 2016 893 903 Barlow, 1959 H. Barlow Sensory mechanisms, the reduction of redundancy, and intelligence The Mechanisation of Thought Processes 1959 National Physical Laboratory, UK: H.M. Stationery Office 535 539 Barnett and Ceci, 2002 S.M. Barnett S.J. Ceci When and where do we apply what we learn? A taxonomy for far transfer Psychol. Bull. 128 2002 612 637 Bastos et al., 2012 A.M. Bastos W.M. Usrey R.A. Adams G.R. Mangun P. Fries K.J. Friston Canonical microcircuits for predictive coding Neuron 76 2012 695 711 Battaglia et al., 2013 P.W. Battaglia J.B. Hamrick J.B. Tenenbaum Simulation as an engine of physical scene understanding Proc. Natl. Acad. Sci. USA 110 2013 18327 18332 Battaglia et al., 2016 Battaglia, P., Pascanu, R., Lai, M., Rezende, D., and Kavukcuoglu, K. (2016). Interaction networks for learning about objects, relations and physics. arXiv, arXiv:161200222. Bengio et al., 2015 Bengio, Y., Lee, D.H., Bornschein, J., Mesnard, T., and Lin, Z. (2015). Towards biologically plausible deep learning. arXiv, arXiv:150204156. Bi and Poo, 1998 G.Q. Bi M.M. Poo Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type J. Neurosci. 18 1998 10464 10472 Blundell et al., 2016 Blundell, C., Uria, B., Pritzel, A., Yazhe, L., Ruderman, A., Leibo, J.Z., Rae, J., Wierstra, D., and Hassabis, D. (2016). Model-free episodic control. arXiv, arXiv:160604460. Botvinick and Plaut, 2006 M.M. Botvinick D.C. Plaut Short-term memory for serial order: a recurrent neural network model Psychol. Rev. 113 2006 201 233 Brooks et al., 2012 R. Brooks D. Hassabis D. Bray A. Shashua Turing centenary: is the brain a good model for machine intelligence? Nature 482 2012 462 463 Browne et al., 2012 C. Browne E. Powley D. Whitehouse S.M. Lucas P.I. Cowling P. Rohlfshagen S. Tavener D. Perez S. Samothrakis S. Colton A survey of Monte-Carlo tree search methods IEEE Trans. Comput. Intell. AI Games 4 2012 1 43 Chang et al., 2016 Chang, M.B., Ullman, T., Torralba, A., and Tenenbaum, J.B. (2016). A compositional object-based approach to learning physical dynamics. arXiv, arXiv:161200341. Chiappa et al., 2017 Chiappa, S., Racaniere, S., Wierstra, D., and Mohamed, S. (2017). Recurrent environment simulators. Proceedings of the 32nd International Conference on Machine Learning, pp. 1\u201361. Churchland and Sejnowski, 1988 P.S. Churchland T.J. Sejnowski Perspectives on cognitive neuroscience Science 242 1988 741 745 Cichon and Gan, 2015 J. Cichon W.B. Gan Branch-specific dendritic Ca(2+) spikes cause persistent synaptic plasticity Nature 520 2015 180 185 Cichy et al., 2014 R.M. Cichy D. Pantazis A. Oliva Resolving human object recognition in space and time Nat. Neurosci. 17 2014 455 462 Collins and Koechlin, 2012 A. Collins E. Koechlin Reasoning, learning, and creativity: frontal lobe function and human decision-making PLoS Biol. 10 2012 e1001293 Constantinescu et al., 2016 A.O. Constantinescu J.X. O\u2019Reilly T.E. Behrens Organizing conceptual knowledge in humans with a gridlike code Science 352 2016 1464 1468 Craik, 1943 K. Craik The Nature of Explanation 1943 Cambridge University Press \u00c7ukur et al., 2013 T. \u00c7ukur S. Nishimoto A.G. Huth J.L. Gallant Attention during natural vision warps semantic representation across the human brain Nat. Neurosci. 16 2013 763 770 Daw et al., 2005 N.D. Daw Y. Niv P. Dayan Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control Nat. Neurosci. 8 2005 1704 1711 Deisseroth and Schnitzer, 2013 K. Deisseroth M.J. Schnitzer Engineering approaches to illuminating brain structure and dynamics Neuron 80 2013 568 577 Deng et al., 2009 Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., and Fei-Fei, L. (2009). Imagenet: a large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pp. 1\u20138. Denil et al., 2016 Denil, M., Agrawal, P., Kulkarni, T.D., Erez, T., Battaglia, P., and de Freitas, N. (2016). Learning to perform physics experiments via deep reinforcement learning. arXiv, arXiv:161101843. Dolan and Dayan, 2013 R.J. Dolan P. Dayan Goals and habits in the brain Neuron 80 2013 312 325 Doll et al., 2015 B.B. Doll K.D. Duncan D.A. Simon D. Shohamy N.D. Daw Model-based choices involve prospective neural activity Nat. Neurosci. 18 2015 767 772 Donoso et al., 2014 M. Donoso A.G. Collins E. Koechlin Human cognition. Foundations of human reasoning in the prefrontal cortex Science 344 2014 1481 1486 Doumas et al., 2008 L.A. Doumas J.E. Hummel C.M. Sandhofer A theory of the discovery and predication of relational concepts Psychol. Rev. 115 2008 1 43 Duan et al., 2016 Duan, Y., Schulman, J., Chen, X., Bartlett, P.L., Sutskever, I., and Abbeel, P. (2016). RL\u02c62: fast reinforcement learning via slow reinforcement learning. arXiv, arXiv:1611.02779. Durstewitz et al., 2000 D. Durstewitz J.K. Seamans T.J. Sejnowski Neurocomputational models of working memory Nat. Neurosci. 3 Suppl 2000 1184 1191 Elman, 1990 J.L. Elman Finding structure in time Cogn. Sci. 14 1990 179 211 Eslami et al., 2016 Eslami, A., Heess, N., Weber, T.Y.T., Szepesvari, D., Kavukcuoglu, K., and Hinton, G. (2016). Attend, infer, repeat: fast scene understanding with generative models. arXiv, arXiv:160308575. Esser et al., 2016 S.K. Esser P.A. Merolla J.V. Arthur A.S. Cassidy R. Appuswamy A. Andreopoulos D.J. Berg J.L. McKinstry T. Melano D.R. Barch Convolutional networks for fast, energy-efficient neuromorphic computing Proc. Natl. Acad. Sci. USA 113 2016 11441 11446 Fodor and Pylyshyn, 1988 J.A. Fodor Z.W. Pylyshyn Connectionism and cognitive architecture: a critical analysis Cognition 28 1988 3 71 French, 1999 R.M. French Catastrophic forgetting in connectionist networks Trends Cogn. Sci. 3 1999 128 135 Fukushima, 1980 K. Fukushima Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position Biol. Cybern. 36 1980 193 202 Fusi et al., 2005 S. Fusi P.J. Drew L.F. Abbott Cascade models of synaptically stored memories Neuron 45 2005 599 611 Gallistel and King, 2009 C. Gallistel A.P. King Memory and the Computational Brain: Why Cognitive Science will Transform Neuroscience 2009 Wiley-Blackwell Gatys et al., 2015 Gatys, L.A., Ecker, A.S., and Bethge, M. (2015). A neural algorithm of artistic style. arXiv, arXiv:1508.06576. Gemici et al., 2017 Gemici, M., Hung, C., Santoro, A., Wayne, G., Mohamed, S., Rezende, D., Amos, D., and Lillicrap, T. (2017). Generative temporal models with memory. arXiv, arXiv:170204649. Gershman and Daw, 2017 S.J. Gershman N.D. Daw Reinforcement learning and episodic memory in humans and animals: an integrative framework Annu. Rev. Psychol. 68 2017 101 128 Gilmore et al., 2007 C.K. Gilmore S.E. McCarthy E.S. Spelke Symbolic arithmetic knowledge without instruction Nature 447 2007 589 591 Glasser et al., 2016 M.F. Glasser S.M. Smith D.S. Marcus J.L. Andersson E.J. Auerbach T.E. Behrens T.S. Coalson M.P. Harms M. Jenkinson S. Moeller The Human Connectome Project\u2019s neuroimaging approach Nat. Neurosci. 19 2016 1175 1187 Goldman-Rakic, 1990 P.S. Goldman-Rakic Cellular and circuit basis of working memory in prefrontal cortex of nonhuman primates Prog. Brain Res. 85 1990 325 335 Gopnik and Schulz, 2004 A. Gopnik L. Schulz Mechanisms of theory formation in young children Trends Cogn. Sci. 8 2004 371 377 Graves et al., 2014 Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. arXiv, arXiv:1410.5401. Graves et al., 2016 A. Graves G. Wayne M. Reynolds T. Harley I. Danihelka A. Grabska-Barwi\u0144ska S.G. Colmenarejo E. Grefenstette T. Ramalho J. Agapiou Hybrid computing using a neural network with dynamic external memory Nature 538 2016 471 476 Gregor et al., 2015 Gregor, K., Danihelka, I., Graves, A., Renzende, D., and Wierstra, D. (2015). DRAW: a recurrent neural network for image generation. arXiv, arXiv:150204623. Hafner and Riedmiller, 2011 R. Hafner M. Riedmiller Reinforcement learning in feedback control Mach. Learn. 84 2011 137 169 Hamrick et al., 2017 Hamrick, J.B., Ballard, A.J., Pascanu, R., Vinyals, O., Heess, N., and Battaglia, P.W. (2017). Metacontrol for adaptive imagination-based optimization. Proceedings of the 5th International Conference on Learning Representations (ICLR 2017), pp. 1\u201321. Harlow, 1949 H.F. Harlow The formation of learning sets Psychol. Rev. 56 1949 51 65 Hassabis and Maguire, 2007 D. Hassabis E.A. Maguire Deconstructing episodic memory with construction Trends Cogn. Sci. 11 2007 299 306 Hassabis and Maguire, 2009 D. Hassabis E.A. Maguire The construction system of the brain Philos. Trans. R. Soc. Lond. B Biol. Sci. 364 2009 1263 1271 Haugeland, 1985 J. Haugeland Artificial Intelligence: The Very Idea 1985 MIT Press Hayashi-Takagi et al., 2015 A. Hayashi-Takagi S. Yagishita M. Nakamura F. Shirai Y.I. Wu A.L. Loshbaugh B. Kuhlman K.M. Hahn H. Kasai Labelling and optical erasure of synaptic memory traces in the motor cortex Nature 525 2015 333 338 Hebb, 1949 D.O. Hebb The Organization of Behavior 1949 John Wiley & Sons Higgins et al., 2016 Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. (2016). Early visual concept learning with unsupervised deep learning. arXiv, arXiv:160605579. Hinton et al., 1986 G.E. Hinton J.L. McClelland D.E. Rumelhart Distributed Representations Explorations in the Microstructure of Cognition 1986 MIT Press 77 109 Hinton et al., 2006 G.E. Hinton S. Osindero Y.W. Teh A fast learning algorithm for deep belief nets Neural Comput. 18 2006 1527 1554 Hinton et al., 2012 Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv, arXiv:12070580. Hochreiter and Schmidhuber, 1997 S. Hochreiter J. Schmidhuber Long short-term memory Neural Comput. 9 1997 1735 1780 Holyoak and Thagard, 1997 K.J. Holyoak P. Thagard The analogical mind Am. Psychol. 52 1997 35 44 Hong et al., 2015 Hong, S., Oh, J., Bohyung, H., and Lee, H. (2015). Learning transferrable knowledge for semantic segmentation with deep convolutional neural network. arXiv, arXiv:151207928. Hong et al., 2016 H. Hong D.L. Yamins N.J. Majaj J.J. DiCarlo Explicit information for category-orthogonal object properties increases along the ventral stream Nat. Neurosci. 19 2016 613 622 Hopfield, 1982 J.J. Hopfield Neural networks and physical systems with emergent collective computational abilities Proc. Natl. Acad. Sci. USA 79 1982 2554 2558 Hopfield and Tank, 1986 J.J. Hopfield D.W. Tank Computing with neural circuits: a model Science 233 1986 625 633 Hubel and Wiesel, 1959 D.H. Hubel T.N. Wiesel Receptive fields of single neurones in the cat\u2019s striate cortex J. Physiol. 148 1959 574 591 Huys et al., 2012 Q.J. Huys N. Eshel E. O\u2019Nions L. Sheridan P. Dayan J.P. Roiser Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by pruning decision trees PLoS Comput. Biol. 8 2012 e1002410 Johnson and Redish, 2007 A. Johnson A.D. Redish Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point J. Neurosci. 27 2007 12176 12189 Jonas and Kording, 2017 E. Jonas K.P. Kording Could a neuroscientist understand a microprocessor? PLoS Comput. Biol. 13 2017 e1005268 Jordan, 1997 M.I. Jordan Serial order: a parallel distributed processing approach Adv. Psychol. 121 1997 471 495 Kemp et al., 2010 C. Kemp N.D. Goodman J.B. Tenenbaum Learning to learn causal models Cogn. Sci. 34 2010 1185 1243 Khaligh-Razavi and Kriegeskorte, 2014 S.M. Khaligh-Razavi N. Kriegeskorte Deep supervised, but not unsupervised, models may explain IT cortical representation PLoS Comput. Biol. 10 2014 e1003915 Kirkpatrick et al., 2017 J. Kirkpatrick R. Pascanu N. Rabinowitz J. Veness G. Desjardins A.A. Rusu K. Milan J. Quan T. Ramalho A. Grabska-Barwinska Overcoming catastrophic forgetting in neural networks Proc. Natl. Acad. Sci. USA 114 2017 3521 3526 Koch and Ullman, 1985 C. Koch S. Ullman Shifts in selective visual attention: towards the underlying neural circuitry Hum. Neurobiol. 4 1985 219 227 Krakauer et al., 2017 J.W. Krakauer A.A. Ghazanfar A. Gomez-Marin M.A. MacIver D. Poeppel Neuroscience needs behavior: correcting a reductionist bias Neuron 93 2017 480 490 Kriegeskorte and Kievit, 2013 N. Kriegeskorte R.A. Kievit Representational geometry: integrating cognition, computation, and the brain Trends Cogn. Sci. 17 2013 401 412 Krizhevsky et al., 2012 Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Kumaran and McClelland, 2012 D. Kumaran J.L. McClelland Generalization through the recurrent interaction of episodic memories: a model of the hippocampal system Psychol. Rev. 119 2012 573 616 Kumaran et al., 2016 D. Kumaran D. Hassabis J.L. McClelland What learning systems do intelligent agents need? Complementary learning systems theory updated Trends Cogn. Sci. 20 2016 512 534 Kurth-Nelson et al., 2016 Z. Kurth-Nelson M. Economides R.J. Dolan P. Dayan Fast sequences of non-spatial state representations in humans Neuron 91 2016 194 204 Lake et al., 2015 B.M. Lake R. Salakhutdinov J.B. Tenenbaum Human-level concept learning through probabilistic program induction Science 350 2015 1332 1338 Lake et al., 2016 Lake, B.M., Ullman, T.D., Tenenbaum, J.B., and Gershman, S.J. (2016). Building machines that learn and think like people. arXiv, arXiv:1604.00289. Larochelle and Hinton, 2010 Larochelle, H., and Hinton, G. (2010). Learning to combine foveal glimpses with a third-order Boltzmann machine. NIPS\u201910 Proceedings of the International Conference on Neural Information Processing Systems, pp. 1243\u20131251. LeCun et al., 1989 Y. LeCun B. Boser J.S. Denker D. Henderson R.E. Howard W. Hubbard L.D. Jackel Backpropagation applied to handwritten zip code recognition Neural Comput. 1 1989 541 551 LeCun et al., 2015 Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 2015 436 444 Leibo et al., 2017 J.Z. Leibo Q. Liao F. Anselmi W.A. Freiwald T. Poggio View-tolerant face recognition and Hebbian Learning imply mirror-symmetric neural tuning to head orientation Curr. Biol. 27 2017 62 67 Legg and Hutter, 2007 S. Legg M. Hutter A collection of definitions of intelligence B. Goertzel P. Wang Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms\u2014Proceedings of the AGI Workshop 2007 Amsterdam IOS 17 24 Lengyel and Dayan, 2007 Lengyel, M., and Dayan, P. (2007). Hippocampal contributions to control: the third way. In Advances in Neural Information Processing Systems 20, pp. 889\u2013896. Liao et al., 2015 Liao, Q., Leibo, J.Z., and Poggio, T. (2015). How important is weight symmetry in backpropagation? arXiv, arXiv:151005067. Lillicrap et al., 2016 T.P. Lillicrap D. Cownden D.B. Tweed C.J. Akerman Random synaptic feedback weights support error backpropagation for deep learning Nat. Commun. 7 2016 13276 Lloyd et al., 2012 K. Lloyd N. Becker M.W. Jones R. Bogacz Learning to use working memory: a reinforcement learning gating model of rule acquisition in rats Front. Comput. Neurosci. 6 2012 87 Marblestone et al., 2016 A.H. Marblestone G. Wayne K.P. Kording Toward an integration of deep learning and neuroscience Front. Comput. Neurosci. 10 2016 94 Marcus, 1998 G.F. Marcus Rethinking eliminative connectionism Cognit. Psychol. 37 1998 243 282 Markram, 2006 H. Markram The blue brain project Nat. Rev. Neurosci. 7 2006 153 160 Marr and Poggio, 1976 D. Marr T. Poggio From understanding computation to understanding neural circuitry A.I. Memo 357 1976 1 22 McClelland and Rogers, 2003 J.L. McClelland T.T. Rogers The parallel distributed processing approach to semantic cognition Nat. Rev. Neurosci. 4 2003 310 322 McClelland et al., 1995 J.L. McClelland B.L. McNaughton R.C. O\u2019Reilly Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory Psychol. Rev. 102 1995 419 457 McCulloch and Pitts, 1943 W. McCulloch W. Pitts A logical calculus of ideas immanent in nervous activity Bull. Math. Biophys. 5 1943 115 133 Mnih et al., 2014 Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual attention. arXiv, arXiv:14066247. Mnih et al., 2015 V. Mnih K. Kavukcuoglu D. Silver A.A. Rusu J. Veness M.G. Bellemare A. Graves M. Riedmiller A.K. Fidjeland G. Ostrovski Human-level control through deep reinforcement learning Nature 518 2015 529 533 Moore and Zirnsak, 2017 T. Moore M. Zirnsak Neural mechanisms of selective visual attention Annu. Rev. Psychol. 68 2017 47 72 Morav\u010d\u00edk et al., 2017 M. Morav\u010d\u00edk M. Schmid N. Burch V. Lis\u00fd D. Morrill N. Bard T. Davis K. Waugh M. Johanson M. Bowling DeepStack: expert-level artificial intelligence in heads-up no-limit poker Science 356 2017 508 513 Nguyen et al., 2016 Nguyen, A., Dosovitskiy, A., Yosinski, J., Borx, T., and Clune, J. (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. arXiv, arXiv:160509304. Nishiyama and Yasuda, 2015 J. Nishiyama R. Yasuda Biochemical computation for spine structural plasticity Neuron 87 2015 63 75 O\u2019Doherty et al., 2003 J.P. O\u2019Doherty P. Dayan K. Friston H. Critchley R.J. Dolan Temporal difference models and reward-related learning in the human brain Neuron 38 2003 329 337 O\u2019Neill et al., 2010 J. O\u2019Neill B. Pleydell-Bouverie D. Dupret J. Csicsvari Play it again: reactivation of waking experience and memory Trends Neurosci. 33 2010 220 229 O\u2019Reilly and Frank, 2006 R.C. O\u2019Reilly M.J. Frank Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia Neural Comput. 18 2006 283 328 Oh et al., 2015 Oh, J., Guo, X., Lee, H., Lewis, R., and Singh, S. (2015). Action-conditional video prediction using deep networks in Atari games. arXiv, arXiv:150708750. \u00d3lafsd\u00f3ttir et al., 2015 H.F. \u00d3lafsd\u00f3ttir C. Barry A.B. Saleem D. Hassabis H.J. Spiers Hippocampal place cells construct reward related sequences through unexplored space eLife 4 2015 e06063 Olshausen et al., 1993 B.A. Olshausen C.H. Anderson D.C. Van Essen A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information J. Neurosci. 13 1993 4700 4719 Pfeiffer and Foster, 2013 B.E. Pfeiffer D.J. Foster Hippocampal place-cell sequences depict future paths to remembered goals Nature 497 2013 74 79 Posner and Petersen, 1990 M.I. Posner S.E. Petersen The attention system of the human brain Annu. Rev. Neurosci. 13 1990 25 42 Raby et al., 2007 C.R. Raby D.M. Alexis A. Dickinson N.S. Clayton Planning for the future by western scrub-jays Nature 445 2007 919 921 Redish, 2016 A.D. Redish Vicarious trial and error Nat. Rev. Neurosci. 17 2016 147 159 Reed et al., 2015 Reed, S., Zhang, Y., Zhang, Y., and Lee, S. (2015). Deep visual analogy-making. In NIPS\u201915 Proceedings of the 28th International Conference on Neural Information Processing Systems, pp. 1252\u20131260. Reed et al., 2016 Reed, S., Akata, Z., Mohan, S., Tenka, S., Schiele, B., and Lee, H. (2016). Learning what and where to draw. arXiv, arXiv:161002454. Rezende et al., 2016a Rezende, D., Eslami, A., Mohamed, S., Battaglia, P., Jaderberg, M., and Heess, N. (2016a). Unsupervised learning of 3D structure from images. arXiv, arXiv:160700662. Rezende et al., 2016b Rezende, D., Mohamed, S., Danihelka, I., Gregor, K., and Wierstra, D. (2016b). One-shot generalization in deep generative models. arXiv, arXiv:160305106. Riesenhuber and Poggio, 1999 M. Riesenhuber T. Poggio Hierarchical models of object recognition in cortex Nat. Neurosci. 2 1999 1019 1025 Rosenblatt, 1958 F. Rosenblatt The perceptron: a probabilistic model for information storage and organization in the brain Psychol. Rev. 65 1958 386 408 Rowland et al., 2016 D.C. Rowland Y. Roudi M.B. Moser E.I. Moser Ten years of grid cells Annu. Rev. Neurosci. 39 2016 19 40 Rumelhart et al., 1985 D.E. Rumelhart G. Hinton R.J. Williams Learning internal representations by error propagation D.E. Rumelhart J.L. McClelland P.R. Group Parallel Distributed Processing: Explorations in the Microstructure of Cognition Volume 1 1985 MIT Press 318 362 Rumelhart et al., 1986 D.E. Rumelhart J.L. McClelland P.R. Group Parallel Distributed Processing: Explorations in the Microstructures of Cognition Volume 1 1986 MIT Press Rusu et al., 2016a Rusu, A.A., Rabinowitz, N., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016a). Progressive neural networks. arXiv, arXiv:160604671. Rusu et al., 2016b Rusu, A.A., Vecerik, M., Rothorl, T., Heess, N., Pascanu, R., and Hadsell, R. (2016b). Sim-to-real robot learning from pixels with progressive nets. arXiv, arXiv:161004286. Salinas and Abbott, 1997 E. Salinas L.F. Abbott Invariant visual responses from attentional gain fields J. Neurophysiol. 77 1997 3267 3272 Santoro et al., 2016 Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. (2016). One-shot Learning with Memory-Augmented Neural Networks. arXiv, arXiv:160506065. Santoro et al., 2017 Santoro, A., Raposo, D., Barrett, D.G.T., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap. T. (2017). A simple neural network module for relational reasoning. arXiv, arXiv:1706.01427, https://arxiv.org/abs/1706.01427. Saxe et al., 2013 Saxe, A.M., Ganguli, S., and McClelland, J.L. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv, arXiv:13126120v3. Scellier and Bengio, 2016 Scellier, B., and Bengio, Y. (2016). Equilibrium propagation: bridging the gap between energy-based models and backpropagation. arXiv, arXiv:160205179. Schacter et al., 2012 D.L. Schacter D.R. Addis D. Hassabis V.C. Martin R.N. Spreng K.K. Szpunar The future of memory: remembering, imagining, and the brain Neuron 76 2012 677 694 Schaul et al., 2015 Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized experience replay. bioRxiv, arXiv:1511.05952. Schmidhuber, 2014 J. Schmidhuber Deep learning in neural networks: an overview arXiv 2014 14047828 Schultz et al., 1997 W. Schultz P. Dayan P.R. Montague A neural substrate of prediction and reward Science 275 1997 1593 1599 Serre et al., 2007 T. Serre L. Wolf S. Bileschi M. Riesenhuber T. Poggio Robust object recognition with cortex-like mechanisms IEEE Trans. Pattern Anal. Mach. Intell. 29 2007 411 426 Shallice, 1988 T. Shallice From Neuropsychology to Mental Structure 1988 Cambridge University Press Silver et al., 2016 D. Silver A. Huang C.J. Maddison A. Guez L. Sifre G. van den Driessche J. Schrittwieser I. Antonoglou V. Panneershelvam M. Lanctot Mastering the game of Go with deep neural networks and tree search Nature 529 2016 484 489 Simonyan et al., 2013 Simonyan, K., Vedaldi, A., and Zisserman, A. (2013). Deep inside convolutional networks: visualising image classification models and saliency maps. arXiv, arXiv:13126034. Singer and Frank, 2009 A.C. Singer L.M. Frank Rewarded outcomes enhance reactivation of experience in the hippocampus Neuron 64 2009 910 921 Skaggs and McNaughton, 1996 W.E. Skaggs B.L. McNaughton Replay of neuronal firing sequences in rat hippocampus during sleep following spatial experience Science 271 1996 1870 1873 Smith, 1995 L.B. Smith Self-organizing processes in learning to learn words: development is not induction Basic and Applied Perspectives on Learning, Cognition, and Development. The Minnesota Symposia on Child Psychology 1995 Lawrence Erlbaum Associates 1 32 Solway et al., 2014 A. Solway C. Diuk N. C\u00f3rdova D. Yee A.G. Barto Y. Niv M.M. Botvinick Optimal behavioral hierarchy PLoS Comput. Biol. 10 2014 e1003779 Spelke and Kinzler, 2007 E.S. Spelke K.D. Kinzler Core knowledge Dev. Sci. 10 2007 89 96 Squire et al., 2004 L.R. Squire C.E. Stark R.E. Clark The medial temporal lobe Annu. Rev. Neurosci. 27 2004 279 306 St. John and McClelland, 1990 M.F. St. John J.L. McClelland Learning and applying contextual constraints in sentence comprehension Artif. Intell. 46 1990 217 257 Stachenfeld et al., 2014 Stachenfeld, K., Botvinick, M.M., and Gershman, S.J. (2014). Design principles of hippocampal cognitive maps. In Advances in Neural Information Processing Systems 27, pp. 2528\u20132536. Sukhbaatar et al., 2015 Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015). End-to-end memory networks. arXiv, arXiv:150308895. Summerfield et al., 2006 J.J. Summerfield J. Lepsien D.R. Gitelman M.M. Mesulam A.C. Nobre Orienting attention based on long-term memory experience Neuron 49 2006 905 916 Sutton, 1991 R.S. Sutton Dyna, an integrated architecture for learning, planning, and reacting ACM SIGART Bull. 2 1991 160 163 Sutton and Barto, 1981 R.S. Sutton A.G. Barto Toward a modern theory of adaptive networks: expectation and prediction Psychol. Rev. 88 1981 135 170 Sutton and Barto, 1998 R. Sutton A. Barto Reinforcement Learning 1998 MIT Press Tesauro, 1995 G. Tesauro Temporal difference learning and TD-Gammon Commun. ACM 38 1995 58 68 Thrun and Mitchell, 1995 S. Thrun T.M. Mitchell Lifelong robot learning Robot. Auton. Syst. 15 1995 25 46 Tolman, 1948 E.C. Tolman Cognitive maps in rats and men Psychol. Rev. 55 1948 189 208 Tsutsui et al., 2016 K. Tsutsui F. Grabenhorst S. Kobayashi W. Schultz A dynamic code for economic object valuation in prefrontal cortex neurons Nat. Commun. 7 2016 12554 Tulving, 1985 E. Tulving How many memory systems are there? American Psychologist 40 1985 385 398 Tulving, 2002 E. Tulving Episodic memory: from mind to brain Annu. Rev. Psychol. 53 2002 1 25 Turing, 1936 A.M. Turing On computable numbers, with an application to the Entscheidungs problem Proc. Lond. Math. Soc. 2 1936 230 265 Turing, 1950 A. Turing Computing machinery and intelligence Mind 236 1950 433 460 van den Oord et al., 2016 van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). WaveNet: a generative model for raw audio. arXiv, arXiv:160903499. Vinyals et al., 2016 Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016). Matching networks for one shot learning. arXiv, arXiv:160604080. Wang et al., 2016 Wang, J., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M.M. (2016). Learning to reinforcement learn. arXiv, arXiv:161105763. Werbos, 1974 P.J. Werbos Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences 1974 Harvard University Weston et al., 2014 Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv, arXiv:14103916. Whittington and Bogacz, 2017 J.C.R. Whittington R. Bogacz An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity Neural Comput. 29 2017 1229 1262 Wu et al., 2016 Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016). Google\u2019s neural machine translation system: bridging the gap between human and machine translation. arXiv, arXiv:160908144. Xu et al., 2015 Xu, K., Kiros, J., Courville, A., Salakhutdinov, R., and Bengio, Y. (2015). Show, attend and tell: neural image caption generation with visual attention. arXiv, arXiv:150203044. Yamins and DiCarlo, 2016 D.L. Yamins J.J. DiCarlo Using goal-driven deep learning models to understand sensory cortex Nat. Neurosci. 19 2016 356 365 Yang et al., 2009 G. Yang F. Pan W.B. Gan Stably maintained dendritic spines are associated with lifelong memories Nature 462 2009 920 924 Zahavy et al., 2016 Zahavy, T., Zrihem, N.B., and Mannor, S. (2016). Graying the black box: understanding DQNs. arXiv, arXiv:160202658. Zaremba and Sutskever, 2014 Zaremba, W., and Sutskever, I. (2014). Learning to execute. arXiv, arXiv:1410.4615.", "scopus-id": "85029563607", "pubmed-id": "28728020", "coredata": {"eid": "1-s2.0-S0896627317305093", "dc:description": "The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.", "openArchiveArticle": "true", "prism:coverDate": "2017-07-19", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0896627317305093", "dc:creator": [{"@_fa": "true", "$": "Hassabis, Demis"}, {"@_fa": "true", "$": "Kumaran, Dharshan"}, {"@_fa": "true", "$": "Summerfield, Christopher"}, {"@_fa": "true", "$": "Botvinick, Matthew"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0896627317305093"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0896627317305093"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0896-6273(17)30509-3", "prism:volume": "95", "prism:publisher": "Elsevier Inc.", "dc:title": "Neuroscience-Inspired Artificial Intelligence", "prism:copyright": "\u00a9 2017 Elsevier Inc.", "openaccess": "1", "prism:issn": "08966273", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "artificial intelligence"}, {"@_fa": "true", "$": "brain"}, {"@_fa": "true", "$": "cognition"}, {"@_fa": "true", "$": "neural network"}, {"@_fa": "true", "$": "learning"}], "openaccessArticle": "true", "prism:publicationName": "Neuron", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "245-258", "prism:endingPage": "258", "pubType": "Review", "prism:coverDisplayDate": "19 July 2017", "prism:doi": "10.1016/j.neuron.2017.06.011", "prism:startingPage": "245", "dc:identifier": "doi:10.1016/j.neuron.2017.06.011", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "155", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "18484", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "154", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "18947", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "462", "@width": "653", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "88297", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "462", "@width": "656", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "83667", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2043", "@width": "2890", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "478459", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2046", "@width": "2905", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0896627317305093-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "429968", "@ref": "gr2", "@mimetype": "image/jpeg"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85029563607"}}