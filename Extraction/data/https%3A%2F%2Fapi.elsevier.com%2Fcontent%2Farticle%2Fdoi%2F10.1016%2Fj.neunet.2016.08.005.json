{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608016301095", "dc:identifier": "doi:10.1016/j.neunet.2016.08.005", "eid": "1-s2.0-S0893608016301095", "prism:doi": "10.1016/j.neunet.2016.08.005", "pii": "S0893-6080(16)30109-5", "dc:title": "Model-based reinforcement learning with dimension reduction ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "84", "prism:startingPage": "1", "prism:endingPage": "16", "prism:pageRange": "1-16", "dc:format": "application/json", "prism:coverDate": "2016-12-31", "prism:coverDisplayDate": "December 2016", "prism:copyright": "\u00a9 2016 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Tangkaratt, Voot"}, {"@_fa": "true", "$": "Morimoto, Jun"}, {"@_fa": "true", "$": "Sugiyama, Masashi"}], "dc:description": "\n               Abstract\n               \n                  The goal of reinforcement learning is to learn an optimal policy which controls an agent to acquire the maximum cumulative reward. The model-based reinforcement learning approach learns a transition model of the environment from data, and then derives the optimal policy using the transition model. However, learning an accurate transition model in high-dimensional environments requires a large amount of data which is difficult to obtain. To overcome this difficulty, in this paper, we propose to combine model-based reinforcement learning with the recently developed least-squares conditional entropy (LSCE) method, which simultaneously performs transition model estimation and dimension reduction. We also further extend the proposed method to imitation learning scenarios. The experimental results show that policy search combined with LSCE performs well for high-dimensional control tasks including real humanoid robot control.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Model-based reinforcement learning"}, {"@_fa": "true", "$": "Transition model estimation"}, {"@_fa": "true", "$": "Sufficient dimension reduction"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608016301095", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608016301095", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84988028250", "scopus-eid": "2-s2.0-84988028250", "pubmed-id": "27639719", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84988028250", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20160824", "$": "2016-08-24"}}}}}