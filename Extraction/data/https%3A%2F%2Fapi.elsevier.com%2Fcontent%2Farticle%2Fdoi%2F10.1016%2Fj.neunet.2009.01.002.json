{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608009000045", "dc:identifier": "doi:10.1016/j.neunet.2009.01.002", "eid": "1-s2.0-S0893608009000045", "prism:doi": "10.1016/j.neunet.2009.01.002", "pii": "S0893-6080(09)00004-5", "dc:title": "Adaptive importance sampling for value function approximation in off-policy reinforcement learning ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "22", "prism:issueIdentifier": "10", "prism:startingPage": "1399", "prism:endingPage": "1410", "prism:pageRange": "1399-1410", "prism:number": "10", "dc:format": "application/json", "prism:coverDate": "2009-12-31", "prism:coverDisplayDate": "December 2009", "prism:copyright": "Copyright \u00a9 2009 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Hachiya, Hirotaka"}, {"@_fa": "true", "$": "Akiyama, Takayuki"}, {"@_fa": "true", "$": "Sugiayma, Masashi"}, {"@_fa": "true", "$": "Peters, Jan"}], "dc:description": "\n               Abstract\n               \n                  Off-policy reinforcement learning is aimed at efficiently using data samples gathered from a policy that is different from the currently optimized policy. A common approach is to use importance sampling techniques for compensating for the bias of value function estimators caused by the difference between the data-sampling policy and the target policy. However, existing off-policy methods often do not take the variance of the value function estimators explicitly into account and therefore their performance tends to be unstable. To cope with this problem, we propose using an adaptive importance sampling technique which allows us to actively control the trade-off between bias and variance. We further provide a method for optimally determining the trade-off parameter based on a variant of cross-validation. We demonstrate the usefulness of the proposed approach through simulations.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Off-policy reinforcement learning"}, {"@_fa": "true", "$": "Value function approximation"}, {"@_fa": "true", "$": "Policy iteration"}, {"@_fa": "true", "$": "Adaptive importance sampling"}, {"@_fa": "true", "$": "Importance-weighted cross-validation"}, {"@_fa": "true", "$": "Efficient sample reuse"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608009000045", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608009000045", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "70549113878", "scopus-eid": "2-s2.0-70549113878", "pubmed-id": "19216050", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/70549113878", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20090123", "$": "2009-01-23"}}}}}