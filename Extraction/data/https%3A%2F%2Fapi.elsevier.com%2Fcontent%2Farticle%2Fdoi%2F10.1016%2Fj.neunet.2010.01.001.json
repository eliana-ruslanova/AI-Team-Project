{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S089360801000002X", "dc:identifier": "doi:10.1016/j.neunet.2010.01.001", "eid": "1-s2.0-S089360801000002X", "prism:doi": "10.1016/j.neunet.2010.01.001", "pii": "S0893-6080(10)00002-X", "dc:title": "Online learning of shaping rewards in reinforcement learning ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2010 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "23", "prism:issueIdentifier": "4", "prism:startingPage": "541", "prism:endingPage": "550", "prism:pageRange": "541-550", "prism:number": "4", "dc:format": "application/json", "prism:coverDate": "2010-05-31", "prism:coverDisplayDate": "May 2010", "prism:copyright": "Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "The 18th International Conference on Artificial Neural Networks, ICANN 2008", "dc:creator": [{"@_fa": "true", "$": "Grze\u015b, Marek"}, {"@_fa": "true", "$": "Kudenko, Daniel"}], "dc:description": "\n               Abstract\n               \n                  Potential-based reward shaping has been shown to be a powerful method to improve the convergence rate of reinforcement learning agents. It is a flexible technique to incorporate background knowledge into temporal-difference learning in a principled way. However, the question remains of how to compute the potential function which is used to shape the reward that is given to the learning agent. In this paper, we show how, in the absence of knowledge to define the potential function manually, this function can be learned online in parallel with the actual reinforcement learning process. Two cases are considered. The first solution which is based on the multi-grid discretisation is designed for model-free reinforcement learning. In the second case, the approach for the prototypical model-based R-max algorithm is proposed. It learns the potential function using the free space assumption about the transitions in the environment. Two novel algorithms are presented and evaluated.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Potential-based reward shaping"}, {"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Learning heuristic"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S089360801000002X", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S089360801000002X", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "77950298151", "scopus-eid": "2-s2.0-77950298151", "pubmed-id": "20116208", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/77950298151", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20100111", "$": "2010-01-11"}}}}}