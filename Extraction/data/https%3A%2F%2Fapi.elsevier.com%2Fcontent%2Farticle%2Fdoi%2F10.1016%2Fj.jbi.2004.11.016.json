{"scopus-eid": "2-s2.0-15944378618", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2004-12-24 2004-12-24 2010-10-05T16:27:29 1-s2.0-S1532046404001674 S1532-0464(04)00167-4 S1532046404001674 10.1016/j.jbi.2004.11.016 S300 S300.2 FULL-TEXT 1-s2.0-S1532046400X0024X 2015-05-15T06:30:58.184067-04:00 0 0 20050401 20050430 2005 2004-12-24T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 38 38 2 2 Volume 38, Issue 2 7 145 153 145 153 200504 April 2005 2005-04-01 2005-04-30 2005 article fla Copyright \u00a9 2004 Elsevier Inc. All rights reserved. PROSPECTIVERECRUITMENTPATIENTSCONGESTIVEHEARTFAILUREUSINGADHOCBINARYCLASSIFIER PAKHOMOV S 1 Introduction 2 Previous work 3 Na\u00efve Bayes vs. Perceptron 4 CHF pilot study 5 Human expert agreement 6 Feature extraction 7 Experimental setup 7.1 Data 7.2 Training 7.3 Results 8 Follow-up experiment with active CHF data 9 Conclusion and future directions Acknowledgments References HORN 1989 L ANATURALHISTORYNEGATION JOHNSON 2002 428 438 D ARONOW 1999 393 411 D MANNING 1999 C FOUNDATIONSSTATISTICALNATURALLANGUAGEPROCESSING ANDERSON 1995 J INTRODUCTIONNEURALNETWORKS PAKHOMOVX2005X145 PAKHOMOVX2005X145X153 PAKHOMOVX2005X145XS PAKHOMOVX2005X145X153XS 2013-07-17T11:42:39Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(04)00167-4 S1532046404001674 1-s2.0-S1532046404001674 10.1016/j.jbi.2004.11.016 272371 2010-11-08T01:47:00.890693-05:00 2005-04-01 2005-04-30 1-s2.0-S1532046404001674-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046404001674/MAIN/application/pdf/924eabe280949ac0c9a9f123f65a5c60/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046404001674/MAIN/application/pdf/924eabe280949ac0c9a9f123f65a5c60/main.pdf main.pdf pdf true 166563 MAIN 9 1-s2.0-S1532046404001674-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046404001674/PREVIEW/image/png/8cfb68b04276412822f7a5194dd0cc03/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046404001674/PREVIEW/image/png/8cfb68b04276412822f7a5194dd0cc03/main_1.png main_1.png png 61612 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046404001674-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046404001674/STRIPIN/image/gif/9c5993e498bf8694f4c089d0a8f2a0c1/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046404001674/STRIPIN/image/gif/9c5993e498bf8694f4c089d0a8f2a0c1/si3.gif si3 si3.gif gif 469 34 72 ALTIMG 1-s2.0-S1532046404001674-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046404001674/STRIPIN/image/gif/4841ce5d11f7f514483a397e8da3ac3f/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046404001674/STRIPIN/image/gif/4841ce5d11f7f514483a397e8da3ac3f/si2.gif si2 si2.gif gif 1023 36 204 ALTIMG 1-s2.0-S1532046404001674-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046404001674/STRIPIN/image/gif/aa2463b697134359a3390c9cd508fdf3/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046404001674/STRIPIN/image/gif/aa2463b697134359a3390c9cd508fdf3/si1.gif si1 si1.gif gif 1173 44 211 ALTIMG YJBIN 1188 S1532-0464(04)00167-4 10.1016/j.jbi.2004.11.016 Elsevier Inc. Table 1 Breakdown of training features by type Feature type N features Proportion (%) MeSH headings 6631 60 HICDA categories 2721 24 Single words 1635 15 Demographic features 131 01 Total 11,118 100 Table 2 Classification test results illustrating the differences between Perceptron and Na\u00efve Bayes Fold Na\u00efve Bayes Perceptron Delta PR (%) Acc (%) PR (%) Acc (%) PR (%) Acc (%) 1 89.21 84.06 78.42 88.39 \u221210.79 4.33 2 88.16 82.41 74.88 85.30 \u221213.28 2.89 3 89.34 82.74 75.74 86.09 \u221213.61 3.35 4 90.77 82.02 79.62 87.07 \u221211.15 5.05 5 90.54 82.07 76.51 86.54 \u221214.03 4.47 6 89.55 82.74 80.27 87.40 \u22129.29 4.66 7 88.16 82.41 74.88 85.30 \u221213.28 2.89 8 88.10 81.16 78.62 86.28 \u22129.48 5.12 9 89.26 81.69 79.36 86.68 \u22129.90 4.99 10 88.12 80.45 76.59 85.89 \u221211.53 5.44 Mean 89.12 82.18 77.49 86.49 \u221211.63 4.32 Table 3 Test results for Type II-CA data (annotated by retired physicians with complete agreement) Classifier PR (%) Acc (%) Na\u00efve Bayes 100 69.2 Perceptron 85 76.92 TermSpotter 85 56 Table 4 Test results for Type II-PA data (annotated by retired physicians with partial agreement) Classifier PR (%) Acc (%) Na\u00efve Bayes 95 57 Perceptron 86 65 TermSpotter 71 54 Table 5 Classification test results illustrating the differences between Perceptron and Na\u00efve Bayes trained on active CHF Fold Na\u00efve Bayes Perceptron Delta PR (%) Acc (%) PR (%) Acc (%) PR (%) Acc (%) 1 65 61.43 15 64.29 \u221250 2.86 2 45 52.86 35 72.86 \u221210 20 3 45 44.29 35 70 \u221210 25.71 4 45 50 25 67.14 \u221220 17.14 5 60 51.43 5 70 \u221255 18.57 6 40 55.71 30 61.43 \u221210 5.72 7 30 51.43 55 77.14 25 25.71 8 30 45.71 30 72.86 0 27.15 9 55 48.57 25 65.71 \u221230 17.14 10 25 55.71 30 67.14 5 11.43 Mean 44 51.714 28.5 68.857 \u221215.5 17.143 Stdev 13.2916 5.074967 13.13393 4.65578 Prospective recruitment of patients with congestive heart failure using an ad-hoc binary classifier Serguei V. Pakhomov \u204e pakhomov@mayo.edu James Buntrock buntrock@mayo.edu Christopher G. Chute chute@mayo.edu Division of Biomedical Informatics, Mayo Clinic College of Medicine, SW, Rochester, MN 55905, USA \u204e Corresponding author. Fax: +1 507 284 0360 Abstract This paper addresses a very specific problem of identifying patients diagnosed with a specific condition for potential recruitment in a clinical trial or an epidemiological study. We present a simple machine learning method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes dictated at Mayo Clinic. This method relies on an automatic classifier trained on comparable amounts of positive and negative samples of clinical notes previously categorized by human experts. The documents are represented as feature vectors, where features are a mix of demographic information as well as single words and concept mappings to MeSH and HICDA classification systems. We compare two simple and efficient classification algorithms (Na\u00efve Bayes and Perceptron) and a baseline term spotting method with respect to their accuracy and recall on positive samples. Depending on the test set, we find that Na\u00efve Bayes yields better recall on positive samples (95 vs. 86%) but worse accuracy than Perceptron (57 vs. 65%). Both algorithms perform better than the baseline with recall on positive samples of 71% and accuracy of 54%. Keywords Automatic classification Na\u00efve Bayes Perceptron Machine learning Congestive heart failure Natural language processing Medical informatics 1 Introduction Epidemiological research frequently requires recruiting a set of human subjects that are deemed relevant for a particular study. Clinical trials constitute another area where human subject recruitment is necessary. The recruitment is a tedious and difficult process and still remains a bottleneck for clinical research [1]. In this paper, we focus on an epidemiological study where patients with acute congestive heart failure need to be identified, preferably as soon as they are diagnosed in the clinic, so that they may be recruited to participate in the study. One of the requirements for an epidemiological study of this kind Do we know what kind? (Maybe we can determine the kind and combine this sentence and the next) is the completeness of the subject pool. Incidence or prevalence studies rely on complete population cohort identification. The identification of the candidates relies on a large number of sources, some of which do not exist in an electronic format, but it may start with the clinical notes dictated by the treating physician. Another aspect of candidate identification is prospective patient recruitment. Prospective recruitment is based on inclusion or exclusion criteria and is of great interest to physicians for enabling just-in-time treatment, clinical trial enrollment, or research study options for patients. At Mayo Clinic, most clinical documents are transcribed within 24h of patient consultation, which creates an ideal resource for enabling prospective recruitment based on criteria present in clinical documents. Probably the most basic approach to identification of candidates for recruitment is to develop a set of terms the presence of which in the note may be indicative of the diagnoses of interest. This term set may be used as a filtering mechanism, either by searching an indexed collection of clinical notes or simply by doing term spotting if the size of the collection allows it. For example, in case of congestive heart failure, one could define the following set of search terms: \u201cCHF,\u201d \u201cheart failure\u201d \u201ccardiomyopathy\u201d \u201cvolume overload\u201d \u201cfluid overload,\u201d and \u201cpulmonary edema.\u201d The number of possible variants is virtually unlimited, which is the inherent problem with this approach. It would be hard to guarantee the completeness of this set to begin with, and the problem is further complicated by morphological and spelling variants. This problem is serious affecting recall and therefore completeness of the candidate pool (already established point). Another problem is that such term spotting or indexing approach would have to be intelligent enough to identify the search terms in negated and other contexts that would render documents containing these terms irrelevant. A note containing \u201cno evidence of heart failure\u201d should not be retrieved, for example. Identifying negation and its scope reliably is far from trivial and is in fact a notoriously difficult problem in linguistics [2]. This problem is slightly less serious than the completeness problem since it only affects precision, which is less important in the given context than recall, but high precision is still very desirable because it would minimize the amount of manual review of false positives needed. To be able to identify automatically whether a given patient note contains evidence that the patient is relevant to a congestive heart failure study, a computer system has to \u201cunderstand\u201d the note. Currently, there are no systems capable of human-like \u201cunderstanding\u201d of natural language; however, there are methods that allow at least partial solutions to the language understanding problem once the problem is constrained in very specific ways. One such constraint is to treat language understanding as a classification problem and to use available machine learning approaches to automatic classification to solve the problem. Clearly, this is a limited view of language understanding, but we hypothesize that it is sufficient for the purposes referred to in this paper. 2 Previous work The classification problems that have been investigated in the past are just as varied as the machine learning algorithms that have been used to solve these problems. Linear least squares fit [3], support vector machines, decision trees, Bayesian learning [4,5], symbolic rule induction [6], maximum entropy [7], and expert networks [8] are just a few of the algorithms that have been applied to classifying e-mail, Web pages, news articles, and medical reports, among other documents. Aronsky and Haug [5] have developed and tested a system based on Bayesian learning for identification of patients with pneumonia for a suggested clinical guideline. The evaluation of the system showed 68.5% specificity at 95% sensitivity which, according to the authors, was acceptable for a real-time diagnostic system that has more stringent requirements for recall than for precision. The requirements for our application are similar. A difference, however, is that we are not attempting to diagnose patients for congestive heart failure automatically; rather, we are using the diagnoses and observations already issued by physicians to identify candidates that meet specific selection criteria. Our application also relies on natural language processing for analyzing the text of clinical notes to extract salient features predictive of patients with ongoing CHF. A number of successful approaches to similar problems have been taken in the past using natural language processing (NLP). Wilcox [9] have experimented with a number of classification algorithms for identifying clinical conditions, such as congestive heart failure and chronic obstructive pulmonary disease, in radiology reports. They found that using an NLP system such as Medical Language Extraction and Encoding System (MedLEE) and domain knowledge sources such as UMLS [10] for feature extraction can significantly improve classification accuracy over the baseline where single words are used to represent training samples. Jain and Friedman [11] have also demonstrated the feasibility of using MedLEE for classifying mammogram reports. Unlike Wilcox [9], this work does not use an automatic classifier. Instead, it uses the MedLEE NLP system to identify findings that are considered suspicious for breast cancer directly to profile potential candidates. In both cases, automatic classification and profiling, NLP plays an important role, whether it is used for feature extraction or for term spotting. Aronow et al. [12] have also investigated a problem with one particular aspect that is similar to the one described in the present work. This aspect is the acuity of the condition being identified. The authors developed an ad-hoc classifier based on a variation of relevance feedback technique for mammogram reports, where the reports were classified into three \u201cbins\u201d: relevant, irrelevant, and unsure. One of the features of the text processing system they used had to do with the ability to detect and take into account negated elements of the reports. Another system developed by Aronow et al. [13] is designed for classifying electronic patient encounter notes for the purpose of identifying patients with acute cases of pediatric asthma exacerbation. The system uses an inference network information retrieval system based on relevance feedback (INQURY) in comparison to an inductive text classification system based on ID3 decision tree classifier (FIGLEAF) to filter a large collection of medical records for asthma in acute exacerbation condition. The INQUERY system is designed to help define sets of terms that are seen in relevant and irrelevant documents, where their combination is used to filter the whole collection of documents for acute asthma exacerbation. FIGLEAF, on the other hand, requires a relatively large amount of hand-labeled training data. The authors report a precision better than 80% on both systems. INQUERY is reported to perform better than FIGLEAF and to requires less manual labor. The particular problem of identifying patients with congestive heart failure has several important aspects that are somewhat similar to identifying patients with acute exacerbation of asthma. It is important to distinguish between congestive heart failure and active congestive heart failure. The notion of a condition being active is not always explicitly stated in medical records at Mayo Clinic; rather, it can be inferred from the context of the note in which the diagnosis appears. If the diagnosis of \u201ccongestive heart failure\u201d or even \u201cacute congestive heart failure\u201d appears in the History of Present Illness part of the note, chances are that the patient referred to by the note is not a good candidate for the specific study on CHF described in this article. In this article, we report on the findings from a set of experiments with two machine learning techniques, Na\u00efve Bayes and Perceptron neural network, used to classify the clinical notes of Mayo Clinic patients. First, we tested automatic classifiers for the purpose of identifying patients with any cases of congestive heart failure, active or non-active, and started the process of manual review of candidates for recruitment. In the process of the manual review, a nurse abstractor selected active CHF candidates from a wide stream of potential candidates with acute and non-active CHF. The review process resulted in a set of patients manually categorized for evidence of active congestive heart failure. Based on the new data, we re-trained the classifiers and compared the results. 3 Na\u00efve Bayes vs. Perceptron We experimented with two widely used machine learning algorithms, Perceptron and Na\u00efve Bayes, to train models capable of distinguishing clinical notes that contain sufficient evidence of the patient having the diagnosis of congestive heart failure (positive examples) from those that do not contain such evidence (negative examples). The choice of the problem was dictated by a specific grant aimed at studying patients with congestive heart failure. The choice of the algorithms was largely dictated by efficiency considerations. Both Perceptron and Na\u00efve Bayes belong to a family of linear classifiers, which tend to be computationally more manageable than other algorithms on large feature sets such as the one we are addressing. Damerau et al. [14] show on the Reuters corpus that sparse feature implementations of linear algorithms are capable of handling large feature sets. We used a sparse feature implementation of these two algorithms available in SNoW (Sparse Networks of Winnows) Version 2.1.2 [15]. Perceptron is a simple iterative learning algorithm that represents in its simplest form a two-layer (input/output) neural network, where each node in the input layer is connected to each node in the output layer. A detailed description can be found in [16,17]. There are several well-known limitations of this algorithm. The most significant is that the simple Perceptron is unable to learn nonlinearly separable problems. In order for this algorithm to work, one should be able to draw a hyperplane in the training data feature space that will linearly separate positive examples from negative. With large multidimensional feature spaces, it is hard to know a priori whether the space is linearly separable; however, a good indication of that can be gleaned from the classification accuracy testing on several folds of training/testing data. If the accuracy results show large fluctuations between folds, then that would be a good indication that the space is not linearly separable. If the standard deviation on such a cross-validation task is relatively small, then one could be reasonably certain that Perceptron is a usable technique for the problem. The other less serious limitation is that there is a chance that the algorithm will falsely conclude convergence in a local minimum on the error function curve without reaching the global minimum, which could also account for low or inconsistent accuracy results. This limitation is less serious because it can be controlled to some extent with the learning rate parameter, which sets the amount by which the weights are adjusted each time Perceptron makes a classification error during training [17]. Na\u00efve Bayes does not have the limitations of Perceptron, but does have limitations of its own. The Bayes decision rule chooses the class that maximizes the conditional probability of the class given the context in which it occurs (1) C \u2032 = argmax P ( C ) \u220f j = 1 n P ( V j | C ) . Here, C\u2032 is the chosen category, C is the set of all categories, and V j is the context. The Na\u00efve Bayes decision algorithm makes a simplifying assumption that the words in V j are independent of each other. A particular implementation of the Na\u00efve Bayes decision rule based on the independence assumption to text categorization and word sense disambiguation problems is also known as the \u201cbag of words\u201d approach [16]. This approach does not attempt to take into account any sort of possible dependency between the individual words in any given context; in fact, it assumes that the word \u201cheart\u201d and the word \u201cfailure,\u201d for example, occur completely independently of each other. Theoretically, such an assumption makes Na\u00efve Bayes classifiers unappealing for text categorization problems, but in practice it has been shown to perform well on a much wider range of domains than the theory would support. The common feature between the two techniques is that both are linear classifiers and are relatively efficient, which makes them attractive for learning from large feature sets with many training samples. 4 CHF pilot study As part of our preliminary grant work to investigate and evaluate incidence, outcome, and etiology trends of heart failure, we conducted a pilot study for prospective recruitment using term spotting techniques. Prospective recruitment was needed for rapid case identification within 24h of newly diagnosed heart failure patients. Within Mayo Clinic, approximately 75% of clinical dictations are electronically transcribed on the date of the diagnosis, which makes it possible to process them with natural language techniques. Using the terms \u201ccardiomyopathy,\u201d \u201cheart failure,\u201d \u201ccongestive heart failure,\u201d \u201cpulmonary edema,\u201d \u201cdecompensated heart failure,\u201d \u201cvolume overload,\u201d and \u201cfluid overload\u201d all electronic outpatient, emergency department, and hospital dismissal notes were processed. Trained nurse abstractors reviewed these results to determine whether this technique could provide identification of patients with clinically active heart failure. The term spotting technique found all cases identified with the standard human coding of the final diagnosis methods. This pilot provided a valid basis for using term spotting for prospective recruitment; however, the nurse abstractors reported filtering out a large number of documents that were irrelevant to the query, thus indicating that there was room for improvement, especially in precision. These results were not quantified at the time; however, the results derived from the test sets used for the study described in this paper display similar tendencies. 5 Human expert agreement When testing a classifier, it is important to have a test bed that contains positive as well as negative examples that have been annotated by human experts. It is also important to establish some sort of an agreement between annotators. For this study we used a test bed created with a specific focus on the diagnosis of the patient described in the clinical note. This test bed was created for a separate pilot study of agreement between physicians in identifying diagnoses recorded in clinical notes (de Groen et al., personal communication). One of the topics selected for this test bed creation study included congestive heart failure. For each topic, 90 documents were selected for evaluation. Seventy of the 90 documents were chosen from documents with a high likelihood of containing diagnostic information regarding the topic of inquiry. Specifically, 35 documents were randomly selected from a pool of documents based on a coded final diagnosis; 35 documents were randomly selected from a pool of documents based on a textual retrieval of lexical surface forms (term spotting). The final 20 documents were randomly selected from the remaining documents, not originally included in the coded or text-identified collections. A group of Emeritus physicians acted as the human experts for this annotation task. The experts were instructed to determine whether the information contained in the clinical note could support inclusion of the patient in a clinical/research investigation, if such an investigation were centered on patients having the disorder of interest. Each document was judged by three physicians on the following scale: confirmed-probable-indeterminate-probably not-definitely not. For the purposes of our study we collapsed \u201cconfirmed\u201d and \u201cprobable\u201d categories into one \u201cpositive\u201d category. We also collapsed \u201cprobably not\u201d and \u201cdefinitely not\u201d into a \u201cnegative\u201d category. The \u201cindeterminate\u201d category happened to include such artifacts as differential diagnosis, as well as uncertain judgments, and therefore was ignored for our purposes. The agreement on this particular topic happened to be low. Only 31% of the instances were agreed upon by all three experts; therefore, we decided to use only the agreed-upon subset of the notes for testing our approach. The low level of agreement was partly attributable to the breadth of the topic and partly to how the experts interpreted the instructions. Despite the low level of agreement, we were able to select a subset of 26 documents where all three annotators agreed. These were the documents where all three annotators assigned either the \u201cpositive\u201d or the \u201cnegative\u201d category. Seven documents were judged as \u201cpositive\u201d and 19 were judged as \u201cnegative\u201d by all three experts. We believe that the low agreement among the experts is due partly to the complexity of the annotation instructions and partly to the differences of opinion among the experts. For this study we used the cases where all of the experts agreed; however, in the future we would use a different methodology\u2014namely, we would \u201cbootstrap\u201d a data set by using a term spotter that will identify all patients whose records show even a remote possibility of having a particular diagnosis of interest, and then ask the nurse abstractors assigned to a particular clinical trial or study to identify positive and negative samples. From a theoretical standpoint, this methodology does not really address the issues with the annotator agreement; however, from a pragmatic standpoint, it does establish a \u201cceiling\u201d that can be used for testing automatic classifiers for practical purposes. 6 Feature extraction Arguably, the most important part of training any text document classifier is extracting relevant features from the training data. The resulting data set looks like a set of feature vectors, where each vector should represent all the relevant information encoded in the document and as little as possible of the irrelevant information. To capture the relevant information and give it more weight we used two classification schemes: Medical Subjects Headings (MeSH) [18] and Hospital International Classification of Diseases Adaptation (HICDA) [19]. The MeSH classification is available as part of the Unified Medical Language System (UMLS) compiled and distributed by the National Library of Medicine (NLM) [10]. HICDA is a hierarchical classification with 19 root nodes and 4334 leaf nodes. Since 1975 it has been loosely expanded to comprise 35,676 rubrics or leaf nodes. HICDA is an adaptation of the 8th edition of the International Classification of Diseases and contains primarily diagnostic statements, whereas MeSH is not limited to diagnostic statements. Therefore, the two complement each other. For mapping the text of clinical notes to these two classifications, some lexical and syntactic variants found empirically in medical texts were also added in addition to the text phrases present in HICDA and MeSH. For MeSH, UMLS developers derived these variants from MEDLINE articles. For HICDA, the variants came from coded diagnoses. Having these lexical and syntactic variants in conjunction with text lemmatization made the job of mapping relatively easy. Text lemmatization was done using the Lexical Variant Generator\u2019s (lvg) 1 umlslex.nlm.nih.gov. 1 \u2018norm\u2019 function, also developed at the NLM. For the purposes of this experiment, we represented each document as a mixed set of features of the following types: MeSH code mappings, HICDA code mapping, single word tokens, demographic data. First, we identified MeSH and HICDA mappings by stemming and lowercasing all words in the notes and finding their matches in the two classifications. Next, we deleted stop words from the text that remained unmapped. We treated the remaining words as single word token features. In addition to these lexical features, we used a set of demographic features, such as age, gender, service code [the type of specialty provider where the patient was seen (e.g., \u2018cardiology\u2019)], and the death indicator (whether the patient was alive at the time the note was created). Since age is a continuous feature, we had to discretize it by introducing ranges A\u2013N arbitrarily distributed across five-year intervals from 0 to more than 70 years old. For this experiment, features that occurred less than two times were ignored. The extracted feature \u201cvocabulary\u201d consists of 11,118 unique features. Table 1 shows the breakdown of the feature vocabulary by type. 7 Experimental setup Both Na\u00efve Bayes and Perceptron were trained on the same data and tested using a 10-fold cross-validation technique as well as a held-out test set of 26 notes mentioned in Section 4. 7.1 Data Two types of annotated testing/training data were used in this study. The first type (Type I) is the data generated by medical coders for conceptual indexing of the clinical notes. The second type (Type II) is the data annotated by the Emeritus physicians. For Type I data, we collected a set of clinical notes for six months in 2001, resulting in a corpus of 1,117,284 notes. Most of these notes contain a set of final diagnoses established by the physician and coded by specially trained staff using the HICDA classification. The coding makes it easy to extract a set of notes whose final diagnoses suggest that the patient has congestive heart failure or a closely related condition,or a symptom, such as pulmonary edema. Once this positive set was extracted (2945 notes), the remainder was randomized and a similar set of negative samples was extracted (4675 notes). The total size of the corpus is 7620 notes. Each note was then run through feature extraction. The entire collection of feature-extracted samples was then split 10 times into train/test folds by randomly selecting 20% of the 7620 notes to set aside for testing for each fold. Type II data set was split into two subsets: a complete agreement (Type II-CA) set and a partial agreement set (Type II-PA). The complete agreement set was created by taking 26 notes that were reliably categorized by the experts specifically with respect to congestive heart failure. These 26 notes represent a set where all three annotators agreed, at least to a large extent, on the categorization. \u201cA large extent\u201d here means that all three annotators labeled the positive samples as either \u201cconfirmed\u201d or \u201cprobable\u201d and the negative samples as either \u201cprobably not\u201d or \u201cdefinitely not.\u201d The set contains seven positive and 19 negative samples. The partial agreement set was created by labeling as \u201cpositive\u201d all samples for which at least one expert made a positive judgment and no experts made a \u201cnegative\u201d judgment, and then labeling as \u201cnegative\u201d all samples for which at least one expert made a negative judgment and no experts made a positive judgment. This procedure resulted in reducing the initial set of 90 samples to 74, of which 21 were positive and 53 were negative for congestive heart failure. This partial agreement set is obviously weaker in its reliability, but it does provide substantially more data to test on and would enable us to judge, at the very least, the consistency of the automatic classifiers being tested. 7.2 Training The following parameters were used for training the classifiers. Na\u00efve Bayes was used with the default smoothing parameter of 15. For Perceptron, the most optimal combination of parameters was to have the learning rate set at 0.0001 (very small increments in weights), where the error threshold was set at 15. The algorithm with these settings was run for 1000 iterations. 7.3 Results We used the following standard classifier accuracy computation [16] for binary classifiers (2) Acc = TP + TN TP + TN + FP + FN , where TP represents the number of times the classifier guessed a correct positive value (true positives); TN is the number of times the classifier correctly guessed a negative value (true negatives); FP is the number of times the classifier predicted a positive value, but the correct value was negative (false positives); and FN (false negatives) is the inverse of FP. In addition to standard accuracy, we also used recall on positive samples (PR): (3) PR = TP P , where TP is the number of correctly classified positive samples and P is the number of actual positive samples in the test data set. We are interested in the PR value because of the strong preference toward perfect recall in document retrieval for epidemiological studies, even if it comes at the expense of precision. We would like to examine the performance of the classifiers on the positive samples regardless of their performance on the negative samples. The rule is that it is better to identify irrelevant data that can be discarded upon review than to miss any of the relevant patients. We established a baseline by running a simple term spotter that looked for the CHF-related terms mentioned in Section 2 (and their normalized variants) in the collection of normalized 2 Normalization was done with the lvg stemmer (umlslex.nlm.nih.gov). 2 documents from the Type II data set. The accuracy of the term spotter is 56% on the Type II-CA set and 54% on the Type II-PA set. The recall on positive samples on the Type II-CA set is 85% and on the Type II-PA set, 71%. The recall on positive samples on the Type II-CA set reflects the spotter missing only one document out of seven identified as positive by the experts. Tables 3 and 4 summarize the results on Type II sets.. Table 2 shows the results of testing the two classifiers on Type I data. The Na\u00efve Bayes algorithm achieves 82.2% accuracy and Perceptron, 86.5%. The standard deviation on the Perceptron classifier results appears to be relatively small, which leads us to believe that this particular classification problem is linearly separable. The difference of 4.3% happens to be statistically significant, as evidenced by a t test at 0.01 confidence level. The difference in the recall on positive samples is also significant; however, it is inversely related to the difference in accuracy. Perceptron models perform on average 11 absolute percentage points worse than Na\u00efve Bayes models. Table 2 shows results that represent the accuracy of the classifiers in classifying the Type I test data that have been generated by the medical coders. Clearly, Type I data are not generated in exactly the same way as Type II data. Although Type I data are captured reliably and are highly accurate, Type II data are classified specifically only with respect to congestive heart failure by expert physicians. We believe Type II data reflect the nature of the task at hand a little better. To test the classifiers on Type II data, we re-trained them on the full set of 7620 notes of Type I data using the same parameters as were used for the 10-fold cross-validation test. Table 3 shows the results of testing the classifiers on Type II-Complete Agreement data (Type II-CA). These results are consistent with the ones displayed in Table 2 in that Perceptron tends to be more accurate overall but less accurate in predicting positive samples. Table 4 summarizes the same results for the Type II-PA test set. The results appear to be oriented in the same general direction as the ones reported in Tables 2 and 3. From a practical standpoint, the results presented here are interesting in that they suggest that the most accurate classifier may not be the most useful one for a given task. In our case, if we were to use these classifiers for routing a stream of electronic clinical notes, the gains in precision that would be attained with the more accurate classifier would most likely be wiped out by the losses in recall since recall is more important than precision for our particular task. However, for a different task that may be more focused on precision, Perceptron may be a better choice. Finally, both Perceptron and Na\u00efve Bayes performance appears to be superior to the baseline performance of the term spotter. Clearly, such comparison is only an indicator because our implementation of the term spotter is simple. A more sophisticated term spotting algorithm may be able to infer semantic relations between various terms, to compensate for misspellings, and to carry out other functions, resulting in better performance. However, even the most sophisticated term spotter will only be as good as the initial list of terms supplied to it. The advantage of automatic classification is that classifiers encode terminological information implicitly, obviating the need to rely on managing lists of terms and the risk of such lists being incomplete. The disadvantage of automatic classification is that the classifier\u2019s performance is heavily data dependent, which raises the need for sufficient amounts of annotated training data and limits this methodology to environments where such data are available. The error analysis of the misclassified notes shows that a more intelligent feature selection process that takes into account discourse characteristics and semantics of negation in the clinical notes is required. For example, one of the misclassified notes contained \u201cno evidence of CHF\u201d as part of the History of Present Illness (HPI) section. Clearly, the presence of a particular concept in a clinical note is not always relevant. For example, various terms and concepts may appear in the Review of Systems (ROS) section of the note; however, the ROS section is often used as a preset template and may have little to do with the present condition. The same is true for other sections, such as Family History, Surgical History, etc. It is not clear at this point which sections are to be included in the feature selection process. The choice will most likely be task specific. The current study did not use any negation identification, which we think accounted for some of the errors. In future we plan to implement a negation detector, such as the NegExpander used by Aronow et al. [12]. The work on negation identification in clinical reports by Chapman et al. [20] indicates that about half (51%) of all negated clinical observations are introduced by using the determiner \u201cno\u201d immediately preceding the term, followed by the verb \u201cdenies\u201d (15%) and the preposition \u201cwithout\u201d (8%). This is not to say that negation identification is a simple matter; determining the scope of negation is extremely complicated. However, even a modest negation detection algorithm will be extremely beneficial to automatic classification of clinical reports. Another finding reported in Chapman et al. [20] that supports this assumption is that between 39 and 83% of all clinical observations are negated. 8 Follow-up experiment with active CHF data Over a period of approximately six months, a trained nurse abstractor collected and manually classified for evidence of active CHF a small but sufficient number of clinical records (clinical notes for 732 patients). Apart from size, the main difference between this collection of data and the one used in the previous experiment is that this data are organized at the patient level rather than the document level. In this collection, the entire record of a patient is classified as belonging to a negative or positive category with respect to active CHF, as opposed to each clinical note being classified, whether or not it contains evidence of CHF. The 732 patient records collected for this experiment contain 11,754 clinical notes. Of the 732 records, 203 have been classified as positive for active CHF and 529 as negative. We ran the same feature extraction process for each of these patient records as we did in the previous experiments, where each patient was represented as a sample vector of demographic and lexical features. We were unable to use the \u201cservice code\u201d feature (e.g., Internal Medicine, Cardiology, etc.) because each record contained clinical notes from multiple service providers. Eventually, it may be beneficial to use the prevalent service provider, either by itself or in combination with other available information, as a predictive feature. For now, we simply mapped the service code feature to the \u201cUNK\u201d value for all patients, thus effectively excluding it from computation. We then created 10-fold of training and testing data from the 732 feature vectors by randomly selecting 20 positive and 50 negative samples, which represent 10% of the overall data available for positive and negative cases, respectively. Because of the relatively small amount of data available, we opted for a 10% testing 90% training data distribution for this experiment. For each fold, we then trained a Na\u00efve Bayes and a Perceptron model with the same parameters as in the previous experiments and tested the models on the test data. Table 5 contains the results of this experiment. As in the previous experiments, Perceptron neural network outperforms the simple Na\u00efve Bayes classifier on this task with respect to accuracy but not recall on positive samples. However, even so, Perceptron\u2019s overall accuracy and PR are relatively low\u201469 and 28%, respectively. The low PR is of special concern due to stringent requirements on completeness. Clearly, neither one of these classifiers can be used for the practical task of prospective patient recruitment for active CHF. Such low accuracy on this particular task as compared to the previous experiments with general CHF is not unexpected. Classifying patients with respect to active CHF is a considerably more difficult task than classifying them with respect to CHF, regardless whether or not it is active. As we mentioned earlier, the notion of active condition is often not explicit in the patient\u2019s medical record. For example, a statement such as \u201cpatient arrived in congestive heart failure\u201d indicates the active nature of the condition. However, it is not entirely clear at this point how one would capture such temporal characteristics of a medical record, which happen to be critical to the notion of an active condition. To do so for this particular example, one would have to have a robust description of the semantics of the preposition \u201cin\u201d and the fact that this preposition is used metaphorically in a temporal rather than spatial sense. 9 Conclusion and future directions In this paper, we have presented a methodology for generating on-demand binary classifiers for filtering clinical patient notes with respect to a particular condition of interest to a clinical investigator. Implementation of this approach is feasible in environments where some quantity of coded clinical notes can be used as training data. We have experimented with HICDA codes; however, other coding schemes may also be usable or even better suited to the task. We do not claim that either Na\u00efve Bayes or Perceptron are the best possible classifiers that could be used for the task of identifying patients with certain conditions. All we show is that either one of these two classifiers is reasonably suitable for the task, and offers the benefits of computational efficiency and simplicity. The results of the experiments with the classifiers suggest that although Perceptron has higher accuracy than the Na\u00efve Bayes classifier overall, its recall on positive samples is significantly lower. The latter result makes it less usable for a practical binary classification task focused on identifying patient records that have evidence of congestive heart failure. It may be worthwhile to pursue an approach that would use the two classifiers in tandem. The classifier with the highest PR would be used to make the first cut to maximize recall and the more accurate classifier would be used to rank the output for subsequent review. Finally, our follow-up study on classifying active cases of CHF shows that the present methodology, based almost entirely on the features extracted from the text of clinical notes, is not suitable for capturing the notion of an active condition. It is likely that to do so successfully, one would have to capture temporal characteristics of a patient\u2019s clinical record and represent them as training features. At this point it is unclear how to represent such temporal characteristics and how to extract them from the text of clinical notes or other sources of data. This limitation constitutes a future direction for our research. It is clear at this point that, at the very least, we are able to filter candidates relevant to a specific condition where active vs. non-active determination can be made in the process of manual review. The methodology described in this paper is applicable not only to the particular study for which it was initially designed. It can be generalized to all cases where it is necessary to perform prospective (or retrospective) recruitment of candidates for conditions for which enough patients have been treated and whose records have been coded (classified) to create training data for machine learning algorithms. A fully automated system that would accept a request from an investigator, assign a set of suitable classification codes to the request, collect all records that correspond to the codes, extract lexical and demographic training features from the records, and train and provisionally test a filtering mechanism can be constructed. To create such an automated system, we will need to make sure that our feature extraction mechanism will generalize to conditions other than congestive heart failure. This, too, constitutes a direction for future research. Acknowledgments We are thankful to Dr. Veronique Roger who is the PI of the \u201cHeart Failure in the Community\u201d Grant (RO1-HL-72435), as well as Kay Traverse and Susan Stoltz who also worked on that project and who have provided valuable input, data, and recommendations for this research. References [1] Afrin B, Oates J, Boyd C, Daniels M. Leveraging open EMR architecture for clinical trial accrual. In: Proceedings of American Medical Informatics Association Fall Symposium, 2003. pp. 16\u201320 [2] L. Horn A natural history of negation 1989 University Of Chicago Press Chicago [3] Yang Y, Chute C. A linear least squares fit mapping method for information retrieval from natural language texts. In: Proceeding of 14th International Conference on Computational Linguistics (COLING) 1992. 1992. p. 447\u201353 [4] Lewis D. Naive (Bayes) at forty: the independence assumption in information retrieval. In: Proceedings of ECML-98, 10th European Conference on Machine Learning. 1998. p. 4\u201315 [5] Aronsky D, Haug P. Automatic identification of patients eligible for a pneumonia guideline. In: Proceedings of American Medical Informatics Association Fall Symposium, 2000. p. 12\u20136 [6] D. Johnson F. Oles T. Zhang T. Goetz A decision-tree-based symbolic rule induction system for text categorization IBM Syst. J. 41 3 2002 428 438 [7] Nigam K, Lafferty J, McCullum A. Using maximum entropy for text classification. In: Proceeding of IJCAI-99 Workshop on Machine Learning for Information Filtering. 1999. p. 61\u20137 [8] Yang Y. Expert network: effective and efficient learning from human decisions in text categorization and retrieval. In: Proceeding of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR\u201994). 1994. p. 11\u201321 [9] Wilcox A. Automated classification of text reports. Ph.D. Thesis, Columbia University, NY; 2000 [10] NLM, UMLS. National Library of Medicine. 2001 [11] Jain N, Friedman C. Identification of finding suspicious for breast cancer based on natural language processing of mammogram reports. In: Proceedings of American Medical Informatics Association Fall Symposium 1997. 1997. p. 829\u201333 [12] D. Aronow F. Fangfang B. Croft Ad hoc classification of radiology reports J Am Med Inform Assoc 6 5 1999 393 411 [13] Aronow D, Soderland S, Ponte J, Feng F, Croft W, Lehnert W. Automated classification of encounter notes in a computer based medical record. In: Proceeding of Medinfo Symposium 199. 1995. p. 8\u201312 [14] Damerau F, Zhang T, Weiss S, Indurkhya N. Experiments in high dimensional text categorization. In: Proceeding of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 2002. p. 357\u201358 [15] Carlson AJ, et al., SNoW User\u2019s Guide. Cognitive Computations Group\u2014University of Illinois at Urbana/Champaign. Available from: (http://l2r.cs.uiuc.edu/~danr/snow.html) [16] C. Manning H. Schutze Foundations of statistical natural language processing 1999 MIT Press Cambridge [17] J. Anderson Introduction to neural networks 1995 MIT Press Boston [18] NLM, Fact sheet medical subject headings (MeSH\u00ae). 2000 [19] Commission on Professional and Hospital Activities, Hospital Adaptation of ICDA. 2nd ed. vol. 1. Ann Arbor, MI: Commission on Professional and Hospital Activities; 1973 [20] Chapman W, Bridewell W, Hanbury P, Cooper G, Buchanan B. Evaluation of negation phrases in narrative clinical reports. In: Proceedings of American Medical Informatics Association Fall Symposium 2001. 2001. p. 105\u20139", "scopus-id": "15944378618", "pubmed-id": "15797003", "coredata": {"eid": "1-s2.0-S1532046404001674", "dc:description": "Abstract This paper addresses a very specific problem of identifying patients diagnosed with a specific condition for potential recruitment in a clinical trial or an epidemiological study. We present a simple machine learning method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes dictated at Mayo Clinic. This method relies on an automatic classifier trained on comparable amounts of positive and negative samples of clinical notes previously categorized by human experts. The documents are represented as feature vectors, where features are a mix of demographic information as well as single words and concept mappings to MeSH and HICDA classification systems. We compare two simple and efficient classification algorithms (Na\u00efve Bayes and Perceptron) and a baseline term spotting method with respect to their accuracy and recall on positive samples. Depending on the test set, we find that Na\u00efve Bayes yields better recall on positive samples (95 vs. 86%) but worse accuracy than Perceptron (57 vs. 65%). Both algorithms perform better than the baseline with recall on positive samples of 71% and accuracy of 54%.", "openArchiveArticle": "true", "prism:coverDate": "2005-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046404001674", "dc:creator": [{"@_fa": "true", "$": "Pakhomov, Serguei V."}, {"@_fa": "true", "$": "Buntrock, James"}, {"@_fa": "true", "$": "Chute, Christopher G."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046404001674"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046404001674"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(04)00167-4", "prism:volume": "38", "prism:publisher": "Elsevier Inc.", "dc:title": "Prospective recruitment of patients with congestive heart failure using an ad-hoc binary classifier", "prism:copyright": "Copyright \u00a9 2004 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Automatic classification"}, {"@_fa": "true", "$": "Na\u00efve Bayes"}, {"@_fa": "true", "$": "Perceptron"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Congestive heart failure"}, {"@_fa": "true", "$": "Natural language processing"}, {"@_fa": "true", "$": "Medical informatics"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "145-153", "prism:endingPage": "153", "prism:coverDisplayDate": "April 2005", "prism:doi": "10.1016/j.jbi.2004.11.016", "prism:startingPage": "145", "dc:identifier": "doi:10.1016/j.jbi.2004.11.016", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "34", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046404001674-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "469", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "36", "@width": "204", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046404001674-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1023", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "44", "@width": "211", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046404001674-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1173", "@ref": "si1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/15944378618"}}