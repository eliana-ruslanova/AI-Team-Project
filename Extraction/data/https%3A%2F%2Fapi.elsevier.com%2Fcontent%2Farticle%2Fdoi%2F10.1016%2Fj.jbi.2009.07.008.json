{"scopus-eid": "2-s2.0-74649087307", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2009-07-30 2009-07-30 2010-10-31T09:02:21 1-s2.0-S1532046409001014 S1532-0464(09)00101-4 S1532046409001014 10.1016/j.jbi.2009.07.008 S300 S300.1 FULL-TEXT 1-s2.0-S1532046410X00028 2015-05-15T06:30:58.184067-04:00 0 0 20100201 20100228 2010 2009-07-30T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 1532-0464 15320464 43 43 1 1 Volume 43, Issue 1 4 15 23 15 23 201002 February 2010 2010-02-01 2010-02-28 2010 article fla Copyright \u00a9 2009 Elsevier Inc. All rights reserved. ANOVELFEATURESELECTIONAPPROACHFORBIOMEDICALDATACLASSIFICATION PENG Y 1 Introduction 2 Sequential feature selection, SVM classification and ROC 2.1 Sequential feature selection 2.2 The SVM classification 2.3 ROC for the evaluation of biomedical classification performance 3 The proposed approach 3.1 Characterization of the discrimination of features 3.2 Characterization of the complementarity of features 3.3 Filter method for feature pre-selection 3.3.1 Criterion for feature pre-selection 3.3.2 Method for feature pre-selection 3.4 Wrapper of SVM and SFFS for feature search 3.5 The characteristics of the proposed algorithm 4 Experimental results 4.1 The datasets and the experimental setup 4.2 Evaluation of the classification performance 4.3 Effectiveness of the proposed pre-selection approach 5 Remarks, conclusion and discussion Acknowledgments References KONONENKO 1993 317 337 I WOLBERG 1994 163 171 W WOLBERG 1995 77 87 W KURGAN 2001 149 169 L ANTONIADIS 2003 563 570 A GUYON 2002 389 422 I YU 2005 2200 2209 J OH 2004 1424 1437 I SAEYS 2007 2507 2517 Y LIU 1998 H FEATUREEXTRACTIONCONSTRUCTIONSELECTIONADATAMININGPERSPECTIVE CONILIONE 2005 54 66 P DEGROEVE 2002 75 83 S GUYON 2003 1157 1182 I LIU 1998 H FEATURESELECTIONFORKNOWLEDGEDISCOVERYDATAMINING LIU 2005 491 502 H INZA 2004 91 103 I BLANCO 2004 1373 1390 R PECHENIZKIY 2006 533 539 M ZENG 2008 S8 X DASH 1997 131 156 M FERRI 1994 403 413 F PATTERNRECOGNITIONINPRACTICEIV COMPARATIVESTUDYTECHNIQUESFORLARGESCALEFEATURESELECTION JAIN 1997 153 158 A KUDO 2000 25 41 M PUDIL 1994 1119 1125 P SOMOL 1999 1157 1163 P WHITNEY 1971 1100 1103 A MARILL 1963 11 17 T CRISTANINI 2000 N INTRODUCTIONSUPPORTVECTORMACHINESOTHERKERNELBASEDLEARNINGMETHODS PENG 2006 553 573 Y ZHANG 2006 197 X BROWN 2000 262 267 M LASKO 2005 404 415 T SWETS 1988 1285 1293 J ZWEIG 1993 561 577 M OBUCHOWSKI 2003 3 8 N HANLEY 1982 29 36 J FARAGGI 2002 3093 3106 D PENGX2010X15 PENGX2010X15X23 PENGX2010X15XY PENGX2010X15X23XY 2013-07-17T11:42:38Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S1532-0464(09)00101-4 S1532046409001014 1-s2.0-S1532046409001014 10.1016/j.jbi.2009.07.008 272371 2010-12-11T03:03:43.497057-05:00 2010-02-01 2010-02-28 1-s2.0-S1532046409001014-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/MAIN/application/pdf/74cef0c44f4feeb97c13cad0dc5165da/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/MAIN/application/pdf/74cef0c44f4feeb97c13cad0dc5165da/main.pdf main.pdf pdf true 720286 MAIN 9 1-s2.0-S1532046409001014-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/PREVIEW/image/png/654d6f810c797670a8bd1fdf615e7e6d/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/PREVIEW/image/png/654d6f810c797670a8bd1fdf615e7e6d/main_1.png main_1.png png 58869 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046409001014-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/d97e59492acaede36ab4698a140b2e24/si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/d97e59492acaede36ab4698a140b2e24/si9.gif si9 si9.gif gif 2239 40 365 ALTIMG 1-s2.0-S1532046409001014-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/9a90e26bdb7fb053d2c87c8327ee43b8/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/9a90e26bdb7fb053d2c87c8327ee43b8/si8.gif si8 si8.gif gif 2021 38 344 ALTIMG 1-s2.0-S1532046409001014-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/bad26f228659310d62cc55de14c5716e/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/bad26f228659310d62cc55de14c5716e/si7.gif si7 si7.gif gif 658 17 158 ALTIMG 1-s2.0-S1532046409001014-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/5fa0422768d98a5b7ada284c0b40ec2b/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/5fa0422768d98a5b7ada284c0b40ec2b/si6.gif si6 si6.gif gif 665 18 145 ALTIMG 1-s2.0-S1532046409001014-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/bde61dbbb04daa97a883543b8450b155/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/bde61dbbb04daa97a883543b8450b155/si5.gif si5 si5.gif gif 948 22 192 ALTIMG 1-s2.0-S1532046409001014-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/9e6c173d3d9abda66522d552d6efd7bc/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/9e6c173d3d9abda66522d552d6efd7bc/si4.gif si4 si4.gif gif 1555 44 266 ALTIMG 1-s2.0-S1532046409001014-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/350448aeb724fbc42c2d63eca1b81867/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/350448aeb724fbc42c2d63eca1b81867/si3.gif si3 si3.gif gif 1092 20 215 ALTIMG 1-s2.0-S1532046409001014-si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/23757175d2da34444efa748fe2400dfb/si20.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/23757175d2da34444efa748fe2400dfb/si20.gif si20 si20.gif gif 858 41 139 ALTIMG 1-s2.0-S1532046409001014-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/58f8595c67e749033820b41953683b25/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/58f8595c67e749033820b41953683b25/si2.gif si2 si2.gif gif 546 18 103 ALTIMG 1-s2.0-S1532046409001014-si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/58f5449c42690a442131473a9343cff7/si19.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/58f5449c42690a442131473a9343cff7/si19.gif si19 si19.gif gif 419 18 78 ALTIMG 1-s2.0-S1532046409001014-si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/14ac19597e5b70dac02bb8a7df7fbcf8/si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/14ac19597e5b70dac02bb8a7df7fbcf8/si18.gif si18 si18.gif gif 599 17 144 ALTIMG 1-s2.0-S1532046409001014-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/60ea925b16bca0539efbc6192e6de960/si17.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/60ea925b16bca0539efbc6192e6de960/si17.gif si17 si17.gif gif 2869 68 394 ALTIMG 1-s2.0-S1532046409001014-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/65e7be4d4ce45827f32636b9369b6362/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/65e7be4d4ce45827f32636b9369b6362/si16.gif si16 si16.gif gif 775 41 135 ALTIMG 1-s2.0-S1532046409001014-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/66b61cad9bdc6efd59fcdd826d898187/si15.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/66b61cad9bdc6efd59fcdd826d898187/si15.gif si15 si15.gif gif 827 41 118 ALTIMG 1-s2.0-S1532046409001014-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/f0859567852bf746b1d7d66ded815fec/si14.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/f0859567852bf746b1d7d66ded815fec/si14.gif si14 si14.gif gif 1301 43 183 ALTIMG 1-s2.0-S1532046409001014-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/65448a4ed34035ec660979300e000c85/si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/65448a4ed34035ec660979300e000c85/si13.gif si13 si13.gif gif 1299 43 183 ALTIMG 1-s2.0-S1532046409001014-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/ca98d59eb96976dc37fa589f56253592/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/ca98d59eb96976dc37fa589f56253592/si12.gif si12 si12.gif gif 1012 18 221 ALTIMG 1-s2.0-S1532046409001014-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/e3874e4a334b787e21ddcd9b6decb7d6/si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/e3874e4a334b787e21ddcd9b6decb7d6/si11.gif si11 si11.gif gif 994 18 200 ALTIMG 1-s2.0-S1532046409001014-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/55e217a3a93e652c1a9c3e02c8295c15/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/55e217a3a93e652c1a9c3e02c8295c15/si10.gif si10 si10.gif gif 1428 30 312 ALTIMG 1-s2.0-S1532046409001014-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/STRIPIN/image/gif/a5ac78987a4671069e78287e8e2ae9bc/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/STRIPIN/image/gif/a5ac78987a4671069e78287e8e2ae9bc/si1.gif si1 si1.gif gif 438 15 88 ALTIMG 1-s2.0-S1532046409001014-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr8/DOWNSAMPLED/image/jpeg/f1dd1dfd997365b53e0b510a925ca3ac/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr8/DOWNSAMPLED/image/jpeg/f1dd1dfd997365b53e0b510a925ca3ac/gr8.jpg gr8 gr8.jpg jpg 49036 407 533 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr8/THUMBNAIL/image/gif/05e4e6e3036082e0180e7997740a9f4d/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr8/THUMBNAIL/image/gif/05e4e6e3036082e0180e7997740a9f4d/gr8.sml gr8 gr8.sml sml 4580 164 214 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr10/DOWNSAMPLED/image/jpeg/8615a5480fe60d397c0647d2c85ff61c/gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr10/DOWNSAMPLED/image/jpeg/8615a5480fe60d397c0647d2c85ff61c/gr10.jpg gr10 gr10.jpg jpg 21090 212 366 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr10/THUMBNAIL/image/gif/59270f8ae953b0f430ce502efdd0c49d/gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr10/THUMBNAIL/image/gif/59270f8ae953b0f430ce502efdd0c49d/gr10.sml gr10 gr10.sml sml 6792 127 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr2/DOWNSAMPLED/image/jpeg/91626f94df6e7542b5e68bcdfb18d140/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr2/DOWNSAMPLED/image/jpeg/91626f94df6e7542b5e68bcdfb18d140/gr2.jpg gr2 gr2.jpg jpg 28482 219 545 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr2/THUMBNAIL/image/gif/c93a64e453732e962ddf0197cecec02e/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr2/THUMBNAIL/image/gif/c93a64e453732e962ddf0197cecec02e/gr2.sml gr2 gr2.sml sml 3618 88 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr3/DOWNSAMPLED/image/jpeg/52a8c6c1bf9e6f148552ab5cd5c43a98/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr3/DOWNSAMPLED/image/jpeg/52a8c6c1bf9e6f148552ab5cd5c43a98/gr3.jpg gr3 gr3.jpg jpg 12826 110 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr3/THUMBNAIL/image/gif/55bd2b3c45f678539555e3fb3032cb0b/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr3/THUMBNAIL/image/gif/55bd2b3c45f678539555e3fb3032cb0b/gr3.sml gr3 gr3.sml sml 3298 65 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr4/DOWNSAMPLED/image/jpeg/04a1c877ced9f19264c53a3c6c22d0e1/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr4/DOWNSAMPLED/image/jpeg/04a1c877ced9f19264c53a3c6c22d0e1/gr4.jpg gr4 gr4.jpg jpg 7582 174 273 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr4/THUMBNAIL/image/gif/675638f1e778621e8a8f3ed7f0656dcf/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr4/THUMBNAIL/image/gif/675638f1e778621e8a8f3ed7f0656dcf/gr4.sml gr4 gr4.sml sml 3233 140 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr5/DOWNSAMPLED/image/jpeg/92dd2a2497c99187f4404a5d0ddb0162/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr5/DOWNSAMPLED/image/jpeg/92dd2a2497c99187f4404a5d0ddb0162/gr5.jpg gr5 gr5.jpg jpg 19168 229 334 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr5/THUMBNAIL/image/gif/4ff656c4a25c59c9a36ed80f4591490f/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr5/THUMBNAIL/image/gif/4ff656c4a25c59c9a36ed80f4591490f/gr5.sml gr5 gr5.sml sml 5983 150 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr6/DOWNSAMPLED/image/jpeg/c377908560bb202d74f33357f97f195c/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr6/DOWNSAMPLED/image/jpeg/c377908560bb202d74f33357f97f195c/gr6.jpg gr6 gr6.jpg jpg 42351 627 321 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr6/THUMBNAIL/image/gif/b3c8c59f3696fa2329afb2807d8c737e/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr6/THUMBNAIL/image/gif/b3c8c59f3696fa2329afb2807d8c737e/gr6.sml gr6 gr6.sml sml 3217 164 84 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr7/DOWNSAMPLED/image/jpeg/62e531cb9b563585f3649826b4e3199f/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr7/DOWNSAMPLED/image/jpeg/62e531cb9b563585f3649826b4e3199f/gr7.jpg gr7 gr7.jpg jpg 19395 211 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr7/THUMBNAIL/image/gif/adb7327ee4f9efe32854e07a0ae01000/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr7/THUMBNAIL/image/gif/adb7327ee4f9efe32854e07a0ae01000/gr7.sml gr7 gr7.sml sml 5602 124 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr9/DOWNSAMPLED/image/jpeg/66e8476e866cdb83208e5c8819506f13/gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr9/DOWNSAMPLED/image/jpeg/66e8476e866cdb83208e5c8819506f13/gr9.jpg gr9 gr9.jpg jpg 20786 219 373 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr9/THUMBNAIL/image/gif/e891aaab269fdbf20514e55b29b54378/gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr9/THUMBNAIL/image/gif/e891aaab269fdbf20514e55b29b54378/gr9.sml gr9 gr9.sml sml 5968 129 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409001014-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr1/DOWNSAMPLED/image/jpeg/ef96ede9d13b6ffbb00b35d80e2a8034/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr1/DOWNSAMPLED/image/jpeg/ef96ede9d13b6ffbb00b35d80e2a8034/gr1.jpg gr1 gr1.jpg jpg 56580 528 579 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409001014-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409001014/gr1/THUMBNAIL/image/gif/1ca80b9f44d6a27494cbb962cae702e6/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409001014/gr1/THUMBNAIL/image/gif/1ca80b9f44d6a27494cbb962cae702e6/gr1.sml gr1 gr1.sml sml 4871 164 180 IMAGE-THUMBNAIL YJBIN 1571 S1532-0464(09)00101-4 10.1016/j.jbi.2009.07.008 Elsevier Inc. Fig. 1 The three feature selection approaches. Fig. 2 The classic SFFS algorithm. Fig. 3 Confusion matrix. Fig. 4 ROC curve. Fig. 5 Framework of the proposed feature selection. Fig. 6 The proposed algorithm. Fig. 7 Classification performance of SFFS and the proposed algorithm. Fig. 8 The ROC of classifiers based on the selected feature subsets. Fig. 9 Classification performance of SFFS and the proposed algorithm with/without \u03bc. Fig. 10 Classification performance of SFFS and the proposed algorithm with/without renormalization. Table 1 The experimental datasets. Dataset name Number of features Number of samples Number of training samples Number of testing samples Wisconsin Breast Cancer \u2013 Diagnosis (WBCD) 32 569 285 284 Breast Cancer Wisconsin \u2013 Prognosis (WBCP) 34 198 a 97 97 SPECTF Heart 44 267 133 134 Microcalcification Detection (MD) 39 1132 566 566 a Four samples with missing values were removed from the original SPECTE heart data. Table 2 The AUCs and the numbers of features selected by SFFS and the proposed algorithm. Dataset name AUC of ROC Number of features selected SFFS Proposed algorithm SFFS Proposed algorithm WBCD 0.995 0.997 12 18 WBCP 0.735 0.774 21 9 SPECTF 0.816 0.832 16 14 MD 0.870 0.916 20 7 A novel feature selection approach for biomedical data classification Yonghong Peng \u204e y.h.peng@bradford.ac.uk Zhiqing Wu Jianmin Jiang School of Informatics, University of Bradford, Bradford BD7 1DP, UK \u204e Corresponding author. Fax: +44 (00)1274233920. Abstract This paper presents a novel feature selection approach to deal with issues of high dimensionality in biomedical data classification. Extensive research has been performed in the field of pattern recognition and machine learning. Dozens of feature selection methods have been developed in the literature, which can be classified into three main categories: filter, wrapper and hybrid approaches. Filter methods apply an independent test without involving any learning algorithm, while wrapper methods require a predetermined learning algorithm for feature subset evaluation. Filter and wrapper methods have their, respectively, drawbacks and are complementary to each other in that filter approaches have low computational cost with insufficient reliability in classification while wrapper methods tend to have superior classification accuracy but require great computational power. The approach proposed in this paper integrates filter and wrapper methods into a sequential search procedure with the aim to improve the classification performance of the features selected. The proposed approach is featured by (1) adding a pre-selection step to improve the effectiveness in searching the feature subsets with improved classification performances and (2) using Receiver Operating Characteristics (ROC) curves to characterize the performance of individual features and feature subsets in the classification. Compared with the conventional Sequential Forward Floating Search (SFFS), which has been considered as one of the best feature selection methods in the literature, experimental results demonstrate that (i) the proposed approach is able to select feature subsets with better classification performance than the SFFS method and (ii) the integrated feature pre-selection mechanism, by means of a new selection criterion and filter method, helps to solve the over-fitting problems and reduces the chances of getting a local optimal solution. Keywords Feature selection Biomedical data classification SFFS ROC 1 Introduction Machine learning and data mining techniques have been successfully applied in various biomedical domains, for example the detection of tumors, the diagnosis and prognosis of cancers and other complex diseases [1\u20134]. One of the core issues in biomedical data analysis and mining is the so-called \u2018curse of dimensionality\u2019 [5\u20137], particularly the biomedical data are characterized by relatively few instances and presented in a high-dimensional feature space. Irrelevant features not only lead to insufficient classification accuracy, but also add extra difficulties in finding potentially useful knowledge [8,9]. Excluding irrelevant features facilitates data visualization and improves the understanding of the computational models, the feature selection has thus become one of the main sub-fields in biomedical data mining [10\u201312]. In addition, appropriate feature selection is able to reduce the requirements of measurement and storage and thus minimize the cost in database storage and management [10,13]. In the context of classification, the main goal of feature selection is to search for an optimal feature subset from the initial feature set that lead to improved classification performance and efficiency in generating classification model. During the past decades, extensive research has been conducted by researchers from multidisciplinary fields including statistics, pattern recognition, machine learning and data mining [14,15]. Dozens of feature selection methods have been developed during the past years and these can be divided into three categories: (i) filter methods, (ii) wrapper methods, and (iii) hybrid methods, in terms of the interaction between feature selection and classification model [16\u201319]. Fig. 1 shows, respectively, the procedures of these three feature selection approaches, where D is the training dataset with the initial feature set, X best is the optimal feature subset to be selected, and J(Xk ) denotes an evaluation function to measure the performance of a feature subset Xk , based on the independent test (M) or the machine learning algorithm (A), respectively, in filter or wrapper methods. Filter methods search for significant features by looking at the characteristics of each individual feature using an independent test such as the information entropy and statistical dependence test. As shown in Fig. 1(a), the filter algorithm starts the search from a feature subset X 0 (an empty set or any randomly selected subset) and searches through the feature space by the following steps: (i) evaluating current feature subset X\u2217 using a independent test method (M) and (ii) comparing it with the best feature subset obtained in previous step, Xk \u22121. If it is found to be better, it is regarded as the current best subset. Here k =|Xk | is the number of features in Xk representing the cardinality of the feature subset. The search process is carried out until a pre-defined criterion \u03b4 is fulfilled. The criterion \u03b4 could be one or more of following: (1) subsequently addition or deletion of any feature does not produce a better feature subset; (2) the performance requirement is satisfied; and (3) some given bound is reached, such as the maximum number of search iterations or the minimum number of features. In the end, the algorithm outputs the last current best subset (X best). The wrapper approach, as shown in Fig. 1(b), instead of using an independent test as in filter approaches, applies a specific machine learning algorithm such as the decision tree or support vector machine (denoted as A) and utilizes the corresponding classification performance to guide the feature selection. For each iteration of the feature searching, the performance of feature subset X\u2217 is evaluated by the quality of the classification model corresponding to the features of X\u2217. The classification performance of X\u2217 is compared with the best feature subset obtained during the previous steps. If the classification performance is better than the previous one, then Xk = X\u2217. This searching process is carried until a pre-defined criterion \u03b4 is fulfilled, as discussed above in filter approaches. Advantages of the filter-based techniques are that they can easily scale up to high-dimensional datasets and that they are computationally fast and independent of the learning algorithm. A common disadvantage, however, is that the interaction with the classifier and the dependence among features are ignored, which leads to varied classification performance when the selected features are applied to different classification algorithms. On the other hand, the advantage of wrapper approaches is that they have a high probability of producing classifiers with better classification performances than the filter approaches as they take into account the feature dependencies and their collectively contribution to model generation. A common drawback, however, is that the wrapper approaches have a higher risk of over-fitting and can be very computationally intensive when processing a large number of features. To exploit the advantages of filter and wrapper approaches, the hybrid technique has been recently emerged [15,8]. A typical hybrid approach employs both an independent test and a performance evaluation function of the feature subset. As shown in Fig. 1(c), the hybrid search, starting from a given subset X 0, uses the filter approach together with wrapper approach to find the best subsets at increasing cardinality. The filter approach, based on an independent test method (M) and the associated criterion \u03b4 1, is used for the selection of candidate features, and the wrapper approach is employed to evaluate the candidate features using a specific learning algorithm (A) and the associated criterion \u03b4 2. Once the best subset at cardinality k has been found, the overall performance of classification is evaluated against a specific criterion \u03b4 3. If the performance has reached the specific criterion \u03b4 3, the feature selection procedure is completed and outputs the current best subset of features as the optimal feature subset, otherwise it carries the searching at cardinality of k +1 by adding one feature from the remaining features and repeating above steps. The hybrid approach attempts to overcome the respective weakness of filter and wrapper approaches by the following principles: (1) to improve the classification performance of filter approaches by including a specific learning algorithm in the selection procedure; (2) to improve the efficiency of wrapper approaches by narrowing the searching space. Overall, the results of feature selection are determined by (a) the search method, (b) the evaluation method, and (c) the learning algorithm. By varying these elements, different hybrid feature selection approaches can be designed. Much research effort has recently been made to the development of effective feature evaluation criteria, and the development of efficient search methods. Dash and Liu grouped the evaluation criteria into five categories: distance, information, dependence, and consistency and classifier accuracy [20]. Many search algorithms have been recently developed, which can be divided into two main categories: sequential search and the Genetic Algorithm (GA) method [8]. Although there was much debate about which one is better between sequential search and GA methods for feature selection, a few recent comparative studies have confirmed that the sequential search methods are always the preferred choice as they are able to produce a classifier with better or at least comparable classification performance [21\u201323]. For example, Ferry et al. demonstrated that (1) the \u2018Sequential Floating Forward Search (SFFS)\u2019 originally developed by Pudil et al. [24] is the best among the sequential search algorithms; (2) although the performances of the SFFS and the GA method are comparable, the SFFS is preferred when the dimensionality increases [21,22]. To investigate further the difference between sequential search and GA approaches, Kudo and Sklansky [23] conducted a comparison on three different sizes of datasets, small size with D \u2a7d19, medium size with 20\u2a7d D \u2a7d49, and large with D \u2a7e50. Their investigation showed that the SFFS is the best for small and medium-sized problems, while the GA is better for large-sized problems. This argument obviously contradicts the findings of Ferry et al. It is believed that one important factor affecting the GA results is due to the varying implementation of the GA method. As a result the SFFS approach has become the most preferred choice in many applications [8,24\u201326], particularly when the classification reliability is the main priority. To fulfil the requirements and priorities in biomedical data mining applications, the proposed approach is designed to search for the significant features that improve the overall classification performance. In this study, the SFFS is employed as the searching mechanism and a new searching criterion is designed to combine the classification performance and the complementarity of features as shown in Section 3 in detail. This paper is organized into five sections. Section 1 is the introduction. Section 2 introduces the conventional SFFS search method, the support vector machine (SVM) used to generate the classification model, and the Receiver Operating Characteristic (ROC) curve to define the SFFS search criteria, while Section 3 presents the proposed algorithm including the search criteria and search procedures. Section 4 presents experimental evaluation of the performance of the proposed approach with four biomedical applications. Finally, Section 5 provides concluding remarks. 2 Sequential feature selection, SVM classification and ROC 2.1 Sequential feature selection The sequential search looks for the optimal feature subset by either adding (or removing) a single feature or a small number of features at a time until the specified criteria are fulfilled [24\u201326]. The search strategies can be classified into three classes: (i) sequential forward selection (SFS), (ii) sequential backward search (SBS), and (iii) bidirectional selection. The SFS begins with an empty set (X 0) and successively built up until the desired feature subset is achieved, while the SBS starts from the complete set of the initial features (Y) and successively eliminates relevant features until the desired feature subset is achieved. Since the first SFS approach was developed by Whitney [27] and the first SBS was originally introduced by Marill and Green [28], there have been many variations to sequential searching developed during past years [25,26]. The most successful approach is the comprehensive floating search proposed by Pudil et al. The floating search methodology includes the \u2018Sequential Forward Floating Search (SFFS)\u2019 and the \u2018Sequential Backward Floating Search (SBFS)\u2019. The SFFS and SBFS methods were originally developed to overcome the so-called \u2018nesting effect\u2019 problem of conventional SFS and SBS, i.e. the SFS approach does not re-select the discarded features while the SBS approach is not able to discard the features once they have been selected [24,25]. Fig. 2 illustrates the procedure of SFFS, which starting from X 0 = \u0424 continuously performs the loop of feature inclusion, conditional exclusion and continuation of conditional exclusion, based on the evaluation of the feature subset. The symbol U denotes the initial complete set of features, Xk denotes the feature subset that contains k features i.e. |Xk |= k, and Yk denotes the set of remaining features, i.e. U = X k \u222a Y k . The upper case X and Y denote the feature subsets and the lower case x and y denote the individual features. In Fig. 2, J(Xk ) denotes the evaluation function that measures the performance or characteristics of the corresponding feature subset Xk . The pre-defined evaluation function determines whether the SFFS or SBFS work as wrapper or filter approaches, respectively. For a wrapper-based SFFS or SBFS approach the function J(Xk ) evaluates the performance of a classifier trained by a special machine learning algorithm while in a filter-based SFFS or SBFS approach the function J(Xk ) is defined by an independent test which measures the characteristics of features in Xk . 2.2 The SVM classification The support vector machine (SVM) is a constructive learning algorithm which originated in statistics and has shown great promise in high dimensional data classification [29]. It has been successfully applied to biomedical data mining [30\u201332]. Unlike most of the modeling methods attempting to minimize an objective function (such as the mean square error) for the whole training instances, SVM attempts to find the hyperplanes that produce the largest separation between the decision function values for the instances located at the borderline between two classes. Given labeled training data M = { ( x i , j , y i ) } , where i =1,2,,\u2026, m (m is the number of data samples), j =1,2,\u2026, n (n is the number of features), the SVM classifier first maps the input vectors into a decision value through a nonlinear transform function \u03d5(x), and then performs a classification using an appropriate threshold value. The procedure can be described as: (1) Mapping: f ( x ) = w T \u03d5 ( x ) + b where w is a weight vector, b is a bias, and f(x): Rn \u2192 R is a decision function which yields a final classification by means of a linear classification for each xi : (2) Classification: y i = + 1 f ( x ) \u2a7e \u03b4 - 1 f ( x ) < \u03b4 where \u03b4 is a user-specified threshold value. These parameters (w and b) are determined by the training data through minimizing a cost function J ( w , \u03be ) = 1 2 \u2016 W \u2016 2 + C \u2211 i = 1 m \u03be i under the constraint of y i ( w T x i + b ) \u2a7e 1 - \u03be , where C >0 and \u03be i \u2a7e 0 ( i = 1 , 2 , \u2026 , m ) are two user-defined parameters. 2.3 ROC for the evaluation of biomedical classification performance Receiver Operating Characteristics (ROC) curves provides an effective approach to characterize the performance of classifiers on sensitivity vs specificity, and have been frequently used in biomedical informatics [33\u201336]. In this paper, we focus on the binary classification problems, in which a classifier yields two discrete results: positive and negative. As shown in Fig. 3 , in a binary classification, given a classifier and an instance, there are four possible outcomes. When a positive instance is classified correctly as positive, it is counted as a true positive (TP); however if it is classified wrongly as negative, it is counted as a false negative (FN). If the instance is negative and has been classified correctly, it is counted as a true negative (TN), otherwise it is counted as a false positive (FP). The TP rate (TPr) and FP rate (FPr) are calculated by: (3) TPr = Positives correctly classified Total Positives = TP TP + FN (4) FPr = Negatives incorrectly classified Total Negatives = FP FP + TN (5) Classification accuracy is acc = TP + TN TP + FP + TN + FN (6) Sensitivity(SN): SN = TPr (7) Specificity(SP): SP = 1 - FPr An ROC curve plots the TPr on the Y axis vs the FPr on the X axis, as shown in Fig. 4 . An ROC curve located completely above and to the left of another curve, i.e. closer to the upper left hand corner, indicates that the associated classifier produces better global performance than another. For example the classifier \u2018a\u2019 always outperform the classifier \u2018b\u2019 as shown in Fig. 4. To measure how well a classifier performs, the area under the curve (AUC) is used to measure how close an ROC curve is to the upper left hand corner [37,38]. The ROC curve for a perfect classifier runs vertically from the point (0,0) to (0,1) and then horizontally to point (1,1) at the top right of the graph, and the corresponding AUC is equal to 1.0. For an ROC curve following a diagonal path from (0,0) to (1,1), the AUC is 0.5. In practice a curve typically lies in the upper left of the plot, and the associated AUC usually ranges between 0.5 and 1.0 and the larger the AUC area this is the better the classifier is. 3 The proposed approach The proposed approach is classified as a hybrid method that combines the filter and wrapper methods. Fig. 5 shows the framework of the proposed approach which consists of four main steps: (1) feature characterization; (2) feature pre-selection using the filter approach; (3) wrapper feature selection using the SVM and AUC of the ROC based on cross-validation; and (4) the SFFS for feature searching. Fig. 6 presents the details of the proposed algorithm. The main steps and algorithms are discussed in the following: 3.1 Characterization of the discrimination of features A desired feature subset contains features that have great discriminative ability and are complementary to each other. The classification accuracy of a single feature classifier (a classifier involving only one feature) has been used as a traditional method to measure the discrimination ability of a feature. As discussed in Section 2.3, the AUC presents the global performance of the associated features as well as the trade-off between their sensitivity and specificity. In this paper, the AUC is used to estimate the discriminative capability of each feature, for which a classifier needed to be generated. Various methods can be used to generate a single feature classifier, the simplest binary classifier is developed with a threshold: if a feature value is greater (or less) than the threshold, it is predicted as positive, else negative, i.e. (8) c = positive if x \u2a7e \u03c2 negative if x < \u03c2 or (9) c = negative if x \u2a7e \u03c2 positive if x < \u03c2 Given a threshold value and a set of testing instances, the corresponding TP rate and FP rate can be calculated for each feature with Eqs. (3) and (4). As a result, each threshold value produces a specific point in ROC space, and varying the threshold values from the minimal value to maximal value of each associated feature result in a curve through ROC space. The classifier (8) or (9) is selected so that the corresponding AUC range is between 0.5 and 1.0. For example, when the AUC for a classifier defined by (8) is less than 0.5, the classifier (9) should be applied. The estimated AUC is then normalized cross all the features to indicate its relative significance, by: (10) A i = AUC i \u2211 i = 1 n AUC i where n is the number of features under consideration. The higher the value of A is the greater the discriminative ability the feature has. 3.2 Characterization of the complementarity of features A feature to be considered as a good candidate for selection should have good discrimination capability and it should also be complementary to the features that have already been selected in the feature subset. In this study, the complementarity between a feature and a group of features is estimated by: (11) \u03bc = \u2211 i = 1 k ( 1 - | p i | ) k where pi is the correlation between the target feature and a feature in the feature subset (Xk ), which is calculated by the Pearson Correlation in this study. For a training dataset with m samples, let us denote the value of a feature as a vector x =(x 1,\u2026, xm ) and a feature in Xk is denoted as yi =(y i1,\u2026, yim ), the Pearson Correlation between x and yi is calculated by: (12) p i = m \u2211 j = 1 m x i y ij - \u2211 k = 1 m x k \u2211 j = 1 m y ij m \u2211 j = 1 m x j 2 - \u2211 j = 1 m x j 2 m \u2211 j = 1 m y ij 2 - \u2211 j = 1 m y ij 2 The value pi \u2208[\u22121,1] indicates the correlation or dependency between features x and yi , while (1\u2212|pi |)\u2208[0,1] indicates the independence between x and yi . For two features that are completely dependent on each other the associated correlation value is equal to 1. The value of \u03bc in Eq. (11) measures the (average) complementarity of a features to all the features in Xk . A greater value of \u03bc indicates that the corresponding feature is more complementary to the features in Xk and the value of \u03bc varies when the features in Xk change. 3.3 Filter method for feature pre-selection The feature pre-selection plays an important role in the proposed approach and is designed to exclude the irrelevant features so that the wrapper-based SFFS mechanism can derive the suitable features more efficiently, without searching through the whole feature space, which is usually needed in the conventional SFFS approaches. In the following the proposed filter criterion and selection method for feature pre-selection is discussed. 3.3.1 Criterion for feature pre-selection The proposed filter searching criterion is presented as the following: (13) T = w 1 \u2217 A + w 2 \u2217 \u03bc where w 1 and w 2 are two so-called balance factors where w 1 + w 2 =1, A is the normalized AUC of the single feature classifier of the candidate feature, and \u03bc is the complementarity between the candidate feature and the selected feature subset, as calculated by Eq. (11). As shown in Eq. (13), the proposed criterion attempts to balance (a) the discrimination capability of a feature by itself; and (b) its interaction with the features that have been already selected in the current feature subset (Xk ). This ensures that the candidate features are good enough and complementary, as much as possible, to the features already in Xk . 3.3.2 Method for feature pre-selection Traditional filter approaches usually select the top ranked features or use a threshold to exclude the irrelevant features. These methods have a significant disadvantage as only the top ranked features are considered, but the top ranked features may not be the optimal candidates. Differently, the proposed feature pre-selection method is designed, based on the idea of the random sampling method that retrieves a number of individuals from the original population without replacement, to select a number of features (Zk ) from Yk without replacement in terms of their significance. It consists of the following steps: (1) Given a set of n features, denoted by x 1, x 2,\u2026, xn . (2) Assigning a T value as defined by Eq. (13) to each feature to define its significance level, and normalize the significant level such that \u2211 i = 1 n T i = 1 . (3) Ranking the features in terms of the associated significance in ascending order. As a result a distribution of significance is formed for these n features, denoted as W =[T 1, T 2,\u2026,Tn ]. (4) Automatically generating a random number \u03b3 \u2208[0,1]. Comparing \u03b3 with the sorted significance level of n features, if T j\u22121 < \u03b3 \u2a7d T j , then the feature x j , which is associated with Tj , is then selected as one of the candidate features to be processed by the SFFS. (5) Repeating (4) until k features have been selected. Clearly, a feature with a large T value would have more chance to be selected. However, there is no guarantee that it will be selected. One the other hand, a feature with a small T value would have less chance to be selected. However, different from the conventional method that excluded completely a feature with a T value smaller than the given threshold, the proposed feature pre-selection method does not completely exclude it. In other words, the proposed method adds more flexibility so that not only top ranked features will be considered but also the \u2018less significant features\u2019. 3.4 Wrapper of SVM and SFFS for feature search A classical SFFS method, starting from X 0, continuously performs the loop of feature inclusion, conditional exclusion and continuation of conditional exclusion on the features in Xk , based on a specifically defined evaluation function J(Xk ). In conventional wrapper approaches, the classification accuracy is normally used to define the J(Xk ). In this paper, the J(Xk ) is defined by the average AUC of the cross-validation of classification using the associated feature subset Xk and the SVM. For k-fold cross-validation, a training dataset D is divided into k data subsets, denoted as Di , i =1\u223c k. Each of these data subsets (e.g. Di ) is used as the validation data, and the rest (D \u2212 Di ) is then used to train an SVM classifier. The evaluation function J(X) for the associated feature subset X is defined by the average of the AUCs: (14) J ( X ) = \u2211 i = 1 k AUC i k where AUC i is the area under the testing ROC curve of the classifier trained by the data (D \u2212 Di ) and tested by Di . 3.5 The characteristics of the proposed algorithm The advantage of the proposed method can be summarized by the following: (1) A filter approach for feature pre-selection is designed to improve the effectiveness in feature searching. Unlike the classic wrapper SFFS methods in which all the features in Yk = U \u2212 Xk need to be checked, the proposed algorithm only tests the features retrieved by the pre-selection method. (2) Differently from the conventional filter approaches that use a fixed threshold value to exclude features or select the top ranked features, the proposed pre-selection method adds a certain level of randomness into the selection. On one hand this method increases the flexibility in feature selection and on the other hand it avoids unnecessary exclusion of important features. (3) A new filter criterion is proposed for the feature pre-selection, which takes into account both the discrimination capability and complementarity of the corresponding features. (4) In the SFFS and wrapper procedures, the approach presented uses the AUC of ROC under cross-validation, unlike most of existing approaches that use the classification accuracy to evaluate the performance of feature subset. The use of cross-validation ensures the selected features are more reliable and the use of ROC fulfils the requirements of the biomedical informatics. 4 Experimental results 4.1 The datasets and the experimental setup The presented approach has been evaluated by experiments on various biomedical data including breast cancer datasets and the SPECTF heart data. The four biomedical datasets used to test the proposed approach are summarized in Table 1 . The first three datasets were selected from the UCI machine learning repository [39], with the condition that number of features is greater than 30, and all the features are numeric, including: (1) Breast Cancer Wisconsin (Diagnostic) dataset; (2) Breast Cancer Wisconsin (Prognostic) dataset; and (3) SPECTF Heart dataset. The fourth dataset concerns the detection of microcalcifications for breast cancer diagnosis based on screening mammography. This dataset is prepared by the authors based on the DDSM database (LUMISYS scanner with a resolution of 50\u03bcm) [40], in which a total of 1132 suspicious clusters have been collected from 460 full-field mammograms, and a total of 39 features were extracted presenting the suspicious clusters [41]. The experimental parameters are specified as following: (1) The number of features in the pre-selection subset Zk : |Zk |=10, i.e. 10 candidate features are selected from Yk for the consideration of wrapper and the SFFS procedures; different values of |Zk | would lead varied speed of feature selection. If |Zk | is equally to the number of remaining features, it becomes the conventional SFFS approach. If |Zk | is too small, then the pre-selection would limit the flexibility of the SFFS. |Zk |=10 is selected to cover about 30% of remaining features as the numbers of features of the datasets used in this study range between 30 and 50. (2) In wrapper selection, cross-validation is used to evaluate the performance of the features selected. The number of folds for cross-validation is set to be k =5. A large number of folds results in much greater computational cost. Our empirical study reveals that the choice k >5 for this parameter does not make much difference in terms of classification performance, and k =5 is a good choice to balance the reliability of the result with the demands for computational power. (3) The parameters that define the filter searching criterion are set to be w 1 =0.7 and w 2 =0.3. Our empirical study showed that a range of w 1 \u2208[0.6,0.8] and w 2 \u2208[0.2,0.4] produced consistent results. (4) The evaluation of the feature subset was conducted in terms of the AUC of the ROC in independent testing. Each dataset was randomly split into two halves: one half was used for feature selection and training the classifier, the other unseen half was used as an independent testing dataset to test the performance of the classifiers, i.e. the testing dataset has not been used in the feature selection phases and the classification model generation. The particular numbers of the training data and testing data are shown in Table 1. 4.2 Evaluation of the classification performance The first set of experiments is designed to evaluate the performance of the proposed approach by comparing the results with that of wrapper SFFS as it is one of the best feature selection methods in the literature. The performance of the selected feature subset is evaluated by the corresponding AUC of the independent testing data. Table 2 and Fig. 7 show the AUC of ROC for the associated four datasets produced, respectively, by the proposed approach and the classic SFFS approach. The result indicates that the feature subset selected by the proposed algorithm is able to produce classifiers with larger AUC than that of SFFS. This means the feature subsets selected by the proposed approach have a better classification performance than that produced by the classic SFFS algorithm. In addition, as shown in Table 2, except for the Wisconsin Breast Cancer (Diagnosis) dataset, for the other three datasets the proposed algorithm is able to retrieve a smaller number of features that achieve a better performance than the SFFS. This means that the proposed approach tends to retrieve the significant feature subset with a smaller number of features. This is particularly useful for biomedical data mining and knowledge discovery so that effective visualization can be possible. The corresponding ROCs are shown in Fig. 8 , which show that the major part of the ROCs produced by the proposed algorithm is above and to the left of the ROCs produced by the SFFS, indicating that they achieve a better performance than the SFFS. 4.3 Effectiveness of the proposed pre-selection approach As shown in Section 3, two key elements ensuring the success of the proposed approach are (1) the inclusion of a complementarity factor (\u03bc) in defining the pre-selection criteria and (2) the randomness imposed in the pre-selection. The experiments presented in this section are to evaluate the effectiveness of the proposed pre-selection method in the above two aspects. To evaluate the effectiveness of the inclusion of the complementarity factor (\u03bc), a set of experiments have been performed by setting w 1 =1 and w 2 =0. As shown in Eq. (13), w 2 =0 means that the influence of the complementarity factor is eliminated, and the feature pre-selection is performed based only on the discrimination capability of the features. The comparison of the classification performance is shown in Fig. 9 , which indicates (1) by means of pre-selection much better performances have been achieved than that of conventional SFFS; (2) involving \u03bc produces better performance than not (indicated by \u2018AUC only\u2019 in Fig. 9). Furthermore, to test the effectiveness of the involvement of randomization in the feature pre-selection, the conventional filter approach which selects the top features has been used for the feature pre-selection. In the experiments the top 10 features in terms of the normalized T values were pre-selected in Zk at each iteration. The classification performances are shown in Fig. 10 . The results show that the proposed algorithm using random selection produces better testing performance than that which does not involve any randomness in the pre-selection. 5 Remarks, conclusion and discussion Feature selection is one of the main issues in biomedical data classification and appropriate feature selection has demonstrated great promise for enhancing the knowledge discovery and model interpretation. In this paper, a new approach is proposed for feature selection in biomedical data. The proposed method is featured by (i) a so-called feature pre-selection approach embedded in the SFFS and wrapper procedures, and (ii) the use of ROC and cross-validation to define the criteria of SFFS and wrapper feature searching. The feature pre-selection approach employs a filter approach with the objective of reducing the computational cost of wrapper and SFFS procedures and most importantly improving the classification performance. There are two new characteristics of the proposed pre-selection approach, which are different from conventional filter approaches. The first one is the new proposed filter criterion. The conventional filter approaches only consider the discrimination capability of features. The proposed approach takes into account not only the discrimination capability of a candidate feature but also the complementary relationship between the candidate feature and the features that have already been selected. The second difference is the filter selection method. The conventional filter approaches usually use a fixed threshold value to exclude features or select the top ranked features. The proposed method randomly selects candidate features in the light of the discrimination capability and the complementary. This method increases the flexibility of feature pre-selection and avoids unnecessary exclusion of the important features. In the wrapper SFFS feature search, the traditional approaches use the classification accuracy to evaluate the performance of a selected feature subset. To fulfil the requirements of biomedical data classification and enhance the reliability of the features selected, the proposed feature selection approach uses the AUC of ROC with cross-validation. Experiments on various biomedical databases indicate that the proposed system achieves a much improved performance, measured by the AUC of ROC, over the conventional SFFS approach. These results clearly demonstrate the great potential of the proposed approach in the classification of biomedical data. Acknowledgments The work presented in this paper is supported by a research grant awarded by the Breast Cancer Research Trust in the United Kingdom. References [1] I. Kononenko Inductive and Bayesian learning in medical diagnosis Appl Artif Intell 7 4 1993 317 337 [2] W.-H. Wolberg W.-N. Street O.L. Mangasarian Machine learning techniques to diagnose breast cancer from fine-needle aspirates Cancer Lett 77 1994 163 171 [3] W.-H. Wolberg W.-N. Street O.L. Mangasarian Image analysis and machine learning applied to breast cancer diagnosis and prognosis Anal Quant Cytol Histol 17 2 1995 77 87 [4] L.-A. Kurgan K.-J. Cios R. Tadeusiewicz M. Ogiela L.-S. Goodenday Knowledge discovery approach to automated cardiac SPECT diagnosis Artif Intell Med 23 2 2001 149 169 [5] A. Antoniadis S. Lambert-Lacroix F. Leblanc Effective dimension reduction methods for tumor classification using gene expression data Bioinformatics 19 5 2003 563 570 [6] I. Guyon J. Weston S. Barnhill V. Vapnik Gene selection for cancer classification using support vector machines Mach Learn 46 2002 389 422 [7] J.-S. Yu S. Ongarello R. Fiedler X.-W. Chen G. Toffolo C. Cobelli Ovarian cancer identification based on dimensionality reduction for high-throughput mass spectrometry data Bioinformatics 21 2005 2200 2209 [8] I.I.-S. Oh J.-S. Lee B.-R. Moon Hybrid genetic algorithms for feature selection IEEE Trans Pattern Anal Mach Intell 26 11 2004 1424 1437 [9] Y. Saeys I. Inza P. Larranaga A review of feature selection techniques in bioinformatics Bioinformatics 23 19 2007 2507 2517 [10] H. Liu H. Motoda Feature extraction, construction and selection: a data mining perspective 1998 Kluwer Norwell, MA [11] P. Conilione D. Wang A comparative study on feature selection for E. coli promoter recognition Int J Inf Technol 11 2005 54 66 [12] S. Degroeve B. De Baets Y. Van de Peer P. Rouz\u00e9 Feature subset selection for splice site prediction Bioinformatics 18 Suppl. 2 2002 75 83 [13] I. Guyon A. Elisseeff An introduction to variable and feature selection J Mach Learn Res 3 2003 1157 1182 [14] H. Liu H. Motoda Feature selection for knowledge discovery and data mining 1998 Kluwer Academic [15] H. Liu L. Yu Toward integrated feature selection algorithms for classification and clustering IEEE Trans Knowl Data Eng 17 4 2005 491 502 [16] I. Inza P. Larranaga R. Blanco A.-J. Cerrolaza Filter versus wrapper gene selection approaches in DNA microarray domains Artif Intell Med 31 2004 91 103 [17] R. Blanco P. Larranaga I. Inza B. Sierra Gene selection for cancer classification using wrapper approaches Int J Pattern Recogn Artif Intell 18 2004 1373 1390 [18] M. Pechenizkiy A. Tsymbal S. Puuronen Local dimensionality reduction and supervised learning within natural clusters for biomedical data analysis IEEE Trans Inf Tech Biomed 10 3 2006 533 539 [19] X.-Q. Zeng G.-Z. Li J.-Y. Yang M.-Q. Yang G.-F. Wu Dimension reduction with redundant gene elimination for tumor classification BMC Bioinform. 9 Suppl 6 2008 S8 [20] M. Dash H. Liu Feature selection for classification Intell Data Anal 1 3 1997 131 156 [21] F.-J. Ferri P. Pudil M. Hatef J. Kittler Comparative study of techniques for large-scale feature selection E.S. Gelsema L.N. Kanal Pattern recognition in practice IV 1994 Elsevier 403 413 [22] A. Jain D. Zongker Feature selection: evaluation, application, and small sample performance IEEE Trans Pattern Anal Mach Intell 19 2 1997 153 158 [23] M. Kudo J. Sklansky Comparison of algorithms that select features for pattern recognition Pattern Recognit 33 1 2000 25 41 [24] P. Pudil J. Novovi\u010dov\u00e1 J. Kittler Floating search methods in feature selection Pattern Recognit Lett 15 11 1994 1119 1125 [25] P. Somol P. Pudil J. Novovi\u010dov\u00e1 P. Pacl\u00edk Adaptive floating search methods in feature selection Pattern Recognit Lett 20 1999 1157 1163 [26] Somol P, Novovi\u010dov\u00e1 J, Pudil P. Flexible-hybrid sequential floating search in statistical feature selection. In: Lecture notes in computer science (LNCS 4109). Springer-Verlag; 2006. p. 632\u20139. [27] A.-W. Whitney A direct method of nonparametric measurement selection IEEE Trans Comput 20 9 1971 1100 1103 [28] T. Marill D. Green On the effectiveness of receptors in recognition systems IEEE Trans Inf Theory 9 1 1963 11 17 [29] N. Cristanini J. Shawe-Taylor An introduction to support vector machines and other kernel-based learning methods 2000 Cambridge University Press [30] Y.-H. Peng A novel ensemble machine learning for robust microarray data classification Comput Biol Med 36 6 2006 553 573 [31] X. Zhang X. Lu Q. Shi X.-Q. Xu H.-C. Leung L.-N. Harris Recursive SVM feature selection and sample classification for mass-spectrometry and microarray data BMC Bioinform 7 2006 197 [32] M.-P. Brown W.-N. Grundy D. Lin N. Cristianini C.-W. Sugnet T.-S. Furey Knowledge-based analysis of microarray gene expression data by using support vector machines Proc Natl Acad Sci USA 97 1 2000 262 267 [33] T.-A. Lasko J.-G. Bhagwat K.-H. Zou L. Ohno-Machado The use of receiver operating characteristic curves in biomedical informatics J Biomed Inform 38 2005 404 415 [34] J.-A. Swets Measuring the accuracy of diagnostic systems Science 240 4857 1988 1285 1293 [35] M.-H. Zweig G. Campbell Receiver-operating characteristic (ROC) plots: a fundamental evaluation tool in clinical medicine Clin Chem 39 4 1993 561 577 [36] N.-A. Obuchowski Receiver operating characteristic curves and their use in radiology Radiology 229 1 2003 3 8 [37] J.-A. Hanley B.-J. McNeil The meaning and use of the area under a receiver operating characteristic (ROC) curve Radiology 143 1 1982 29 36 [38] D. Faraggi B. Reiser Estimation of the area under the ROC curve Stat Med 21 20 2002 3093 3106 [39] Asuncion A, Newman DJ. UCI machine learning repository. Irvine, CA: University of California, School of Information and Computer Science; 2007. Available from: http://www.ics.uci.edu/~mlearn/MLRepository.html. [40] DDSM. Available from: http://marathon.csee.usf.edu/Mammography/Database.html [accessed 20.06.2008]. [41] Wu Z, Jiang J, Peng Y-H. Effective features based on normal linear structures for detecting microcalcifications in mammograms. In: The 19th international conference on pattern recognition, Florida, USA; 2008.", "scopus-id": "74649087307", "pubmed-id": "19647098", "coredata": {"eid": "1-s2.0-S1532046409001014", "dc:description": "Abstract This paper presents a novel feature selection approach to deal with issues of high dimensionality in biomedical data classification. Extensive research has been performed in the field of pattern recognition and machine learning. Dozens of feature selection methods have been developed in the literature, which can be classified into three main categories: filter, wrapper and hybrid approaches. Filter methods apply an independent test without involving any learning algorithm, while wrapper methods require a predetermined learning algorithm for feature subset evaluation. Filter and wrapper methods have their, respectively, drawbacks and are complementary to each other in that filter approaches have low computational cost with insufficient reliability in classification while wrapper methods tend to have superior classification accuracy but require great computational power. The approach proposed in this paper integrates filter and wrapper methods into a sequential search procedure with the aim to improve the classification performance of the features selected. The proposed approach is featured by (1) adding a pre-selection step to improve the effectiveness in searching the feature subsets with improved classification performances and (2) using Receiver Operating Characteristics (ROC) curves to characterize the performance of individual features and feature subsets in the classification. Compared with the conventional Sequential Forward Floating Search (SFFS), which has been considered as one of the best feature selection methods in the literature, experimental results demonstrate that (i) the proposed approach is able to select feature subsets with better classification performance than the SFFS method and (ii) the integrated feature pre-selection mechanism, by means of a new selection criterion and filter method, helps to solve the over-fitting problems and reduces the chances of getting a local optimal solution.", "openArchiveArticle": "true", "prism:coverDate": "2010-02-28", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046409001014", "dc:creator": [{"@_fa": "true", "$": "Peng, Yonghong"}, {"@_fa": "true", "$": "Wu, Zhiqing"}, {"@_fa": "true", "$": "Jiang, Jianmin"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046409001014"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046409001014"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(09)00101-4", "prism:volume": "43", "prism:publisher": "Elsevier Inc.", "dc:title": "A novel feature selection approach for biomedical data classification", "prism:copyright": "Copyright \u00a9 2009 Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "1", "dcterms:subject": [{"@_fa": "true", "$": "Feature selection"}, {"@_fa": "true", "$": "Biomedical data classification"}, {"@_fa": "true", "$": "SFFS"}, {"@_fa": "true", "$": "ROC"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "1", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "15-23", "prism:endingPage": "23", "prism:coverDisplayDate": "February 2010", "prism:doi": "10.1016/j.jbi.2009.07.008", "prism:startingPage": "15", "dc:identifier": "doi:10.1016/j.jbi.2009.07.008", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "40", "@width": "365", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2239", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "38", "@width": "344", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2021", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "158", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "658", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "145", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "665", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "22", "@width": "192", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "948", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "44", "@width": "266", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1555", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "215", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1092", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "41", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "858", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "103", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "546", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "78", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "419", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "144", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "599", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "68", "@width": "394", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2869", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "41", "@width": "135", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "775", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "41", "@width": "118", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "827", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "183", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1301", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "183", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1299", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "221", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1012", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "200", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "994", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "312", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1428", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "88", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "438", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "407", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "49036", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "214", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4580", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "212", "@width": "366", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "21090", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "127", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6792", "@ref": "gr10", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "219", "@width": "545", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28482", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "88", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3618", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "110", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "12826", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "65", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3298", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "174", "@width": "273", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "7582", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "140", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3233", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "229", "@width": "334", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "19168", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "150", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5983", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "627", "@width": "321", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "42351", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "84", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3217", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "211", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "19395", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "124", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5602", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "219", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "20786", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "129", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5968", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "528", "@width": "579", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "56580", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "180", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409001014-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4871", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/74649087307"}}