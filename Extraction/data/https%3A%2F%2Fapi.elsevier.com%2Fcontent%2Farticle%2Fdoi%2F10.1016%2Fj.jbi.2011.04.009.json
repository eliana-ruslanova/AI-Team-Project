{"scopus-eid": "2-s2.0-83755202239", "originalText": "serial JL 272371 291210 291682 291870 291901 31 90 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2011-05-06 2011-05-06 2014-09-30T23:54:04 1-s2.0-S1532046411000736 S1532-0464(11)00073-6 S1532046411000736 10.1016/j.jbi.2011.04.009 S300 S300.2 FULL-TEXT 1-s2.0-S1532046411X00084 2015-05-15T06:30:58.184067-04:00 0 0 20111201 20111231 2011 2011-05-06T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1532-0464 15320464 UNLIMITED ELS false 44 44 S1 Volume 44, Supplement 1 5 S17 S23 S17 S23 201112 December 2011 2011-12-01 2011-12-31 2011 AMIA Joint Summits on Translational Science \u2013 2011 Dr. Philip R.O. Payne Dr. Neil Sarkar Dr. Peter J. Embi Dr. Michael Kahn Summit on Translational Bioinformatics (TBI) article fla Copyright \u00a9 2011 Elsevier Inc. TRANSFERLEARNINGCLASSIFICATIONRULESFORBIOMARKERDISCOVERYVERIFICATIONMOLECULARPROFILINGSTUDIES GANCHEV P 1 Introduction 2 Materials and methods 2.1 Rule learning with RL 2.2 Transfer of rules 2.3 Whole-rule transfer 2.4 Rule structure transfer 2.5 Effect of transfer on RL\u2019s search process 2.6 Experiments 2.7 Varying the relative sizes of the source and target data 2.8 Comparison of the transfer methods 3 Discussion 4 Conflict of interest Acknowledgments References SEMMES 2005 102 O PELIKAN 2007 3065 3072 R CARUANA 1997 41 75 R PAN 2010 1345 1359 S WU 2004 871 878 P PROCEEDINGS21STINTERNATIONALCONFERENCEMACHINELEARNING IMPROVINGSVMACCURACYBYTRAININGAUXILIARYDATASOURCES HENNESSY 1994 179 187 D PROCEEDINGSINTERNATIONALCONFERENCEINTELLIGENTSYSTEMSFORMOLECULARBIOLOGY INDUCTIONRULESFORBIOLOGICALMACROMOLECULECRUSTALLIZATION LEE 1995 127 149 Y LEE 1998 217 240 Y GOPALAKRISHNAN 2004 1705 1716 V RYBERG 2010 104 111 H RANGANATHAN 2005 1461 1471 S GOPALAKRISHNAN 2006 93 105 V GOPALAKRISHNAN 2010 668 V QUINLAN 1993 J C45PROGRAMSFORMACHINELEARNING BREIMAN 1984 L CLASSIFICATIONREGRESSIONTREES YILDIZ 2007 893 P LUSTGARTEN 2008 1418 1419 J GOLUB 1999 531 T GANCHEVX2011XS17 GANCHEVX2011XS17XS23 GANCHEVX2011XS17XP GANCHEVX2011XS17XS23XP Full 2013-07-16T21:17:53Z ElsevierBranded http://creativecommons.org/licenses/by-nc-nd/3.0/ item S1532-0464(11)00073-6 S1532046411000736 1-s2.0-S1532046411000736 10.1016/j.jbi.2011.04.009 272371 2014-10-01T02:31:26.310049-04:00 2011-12-01 2011-12-31 UNLIMITED ELS 1-s2.0-S1532046411000736-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/MAIN/application/pdf/4140f7d1da165dd3e3ec68748b0e7d1c/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/MAIN/application/pdf/4140f7d1da165dd3e3ec68748b0e7d1c/main.pdf main.pdf pdf true 845251 MAIN 7 1-s2.0-S1532046411000736-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/PREVIEW/image/png/c4dac12088a68c74e7f016f6ab145530/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/PREVIEW/image/png/c4dac12088a68c74e7f016f6ab145530/main_1.png main_1.png png 61207 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046411000736-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/STRIPIN/image/gif/d9e9e5c204ea90596b4884eb01a0b4a1/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/STRIPIN/image/gif/d9e9e5c204ea90596b4884eb01a0b4a1/si4.gif si4 si4.gif gif 1139 18 293 ALTIMG 1-s2.0-S1532046411000736-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/STRIPIN/image/gif/151c3606617d35e375e0cbab09477a59/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/STRIPIN/image/gif/151c3606617d35e375e0cbab09477a59/si3.gif si3 si3.gif gif 1459 18 379 ALTIMG 1-s2.0-S1532046411000736-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/STRIPIN/image/gif/303dc150ed83ee89cda57bba4529c1ac/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/STRIPIN/image/gif/303dc150ed83ee89cda57bba4529c1ac/si2.gif si2 si2.gif gif 2178 43 487 ALTIMG 1-s2.0-S1532046411000736-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/STRIPIN/image/gif/866f494c24699295734a2ff6fb2e340f/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/STRIPIN/image/gif/866f494c24699295734a2ff6fb2e340f/si1.gif si1 si1.gif gif 1263 14 349 ALTIMG 1-s2.0-S1532046411000736-gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr7/HIGHRES/image/jpeg/a9174625e4ccea5a2b5263ce1a6e599b/gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr7/HIGHRES/image/jpeg/a9174625e4ccea5a2b5263ce1a6e599b/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 223690 757 2147 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr6/HIGHRES/image/jpeg/85d4100eeb78809e7647107e74f4d5a8/gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr6/HIGHRES/image/jpeg/85d4100eeb78809e7647107e74f4d5a8/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 224808 722 1457 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr5/HIGHRES/image/jpeg/61b3c6e8b23b1ffa66a910fd9c23d758/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr5/HIGHRES/image/jpeg/61b3c6e8b23b1ffa66a910fd9c23d758/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 201903 969 2093 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr4/HIGHRES/image/jpeg/e905449ff66358af6c080bcb1535fac9/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr4/HIGHRES/image/jpeg/e905449ff66358af6c080bcb1535fac9/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 185975 1013 2086 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr3/HIGHRES/image/jpeg/d75587509cc0c704df19333b94db84a7/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr3/HIGHRES/image/jpeg/d75587509cc0c704df19333b94db84a7/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 226530 776 1333 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr2/HIGHRES/image/jpeg/f2b708c38ed1947c04d52c888ba990c4/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr2/HIGHRES/image/jpeg/f2b708c38ed1947c04d52c888ba990c4/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 354574 1390 1576 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr1/HIGHRES/image/jpeg/fcce9c9cd2f1ebd73d29bbdf7b198bfd/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr1/HIGHRES/image/jpeg/fcce9c9cd2f1ebd73d29bbdf7b198bfd/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 111180 737 1380 IMAGE-HIGH-RES 1-s2.0-S1532046411000736-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr7/DOWNSAMPLED/image/jpeg/4e8b060cba5022c6775b4a9c1ab40bdc/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr7/DOWNSAMPLED/image/jpeg/4e8b060cba5022c6775b4a9c1ab40bdc/gr7.jpg gr7 gr7.jpg jpg 26170 171 485 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr6/DOWNSAMPLED/image/jpeg/b5fe8ba672a270a56d786c2de0ea437d/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr6/DOWNSAMPLED/image/jpeg/b5fe8ba672a270a56d786c2de0ea437d/gr6.jpg gr6 gr6.jpg jpg 26304 163 329 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr5/DOWNSAMPLED/image/jpeg/c8454efd5cff8fe1844e1a45b652de64/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr5/DOWNSAMPLED/image/jpeg/c8454efd5cff8fe1844e1a45b652de64/gr5.jpg gr5 gr5.jpg jpg 30349 219 473 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr4/DOWNSAMPLED/image/jpeg/bd7bebac1d084fd84c5cd3a4d013879a/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr4/DOWNSAMPLED/image/jpeg/bd7bebac1d084fd84c5cd3a4d013879a/gr4.jpg gr4 gr4.jpg jpg 27189 229 471 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr3/DOWNSAMPLED/image/jpeg/fe69b2a551612d6d075983d0af3c3f09/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr3/DOWNSAMPLED/image/jpeg/fe69b2a551612d6d075983d0af3c3f09/gr3.jpg gr3 gr3.jpg jpg 25419 175 301 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr2/DOWNSAMPLED/image/jpeg/f61bfd1cf2d034e4f33b1d2ea61230ff/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr2/DOWNSAMPLED/image/jpeg/f61bfd1cf2d034e4f33b1d2ea61230ff/gr2.jpg gr2 gr2.jpg jpg 39259 314 356 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr1/DOWNSAMPLED/image/jpeg/532d104b04f5e4ddae8b79b7158b16fc/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr1/DOWNSAMPLED/image/jpeg/532d104b04f5e4ddae8b79b7158b16fc/gr1.jpg gr1 gr1.jpg jpg 13642 166 311 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000736-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr7/THUMBNAIL/image/gif/c7a0ba17853adbaa2f96de0c831aa14b/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr7/THUMBNAIL/image/gif/c7a0ba17853adbaa2f96de0c831aa14b/gr7.sml gr7 gr7.sml sml 5030 77 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr6/THUMBNAIL/image/gif/8ab298d0f45d9a387753a520e59510e9/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr6/THUMBNAIL/image/gif/8ab298d0f45d9a387753a520e59510e9/gr6.sml gr6 gr6.sml sml 8254 109 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr5/THUMBNAIL/image/gif/4ba4fa9465958b3aefae1341478a8f9d/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr5/THUMBNAIL/image/gif/4ba4fa9465958b3aefae1341478a8f9d/gr5.sml gr5 gr5.sml sml 6383 101 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr4/THUMBNAIL/image/gif/6715b272d7e4ce5db8e8385e2088122e/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr4/THUMBNAIL/image/gif/6715b272d7e4ce5db8e8385e2088122e/gr4.sml gr4 gr4.sml sml 5599 106 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr3/THUMBNAIL/image/gif/495ad51f31c41da5b2c4d6d5bd69a1e7/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr3/THUMBNAIL/image/gif/495ad51f31c41da5b2c4d6d5bd69a1e7/gr3.sml gr3 gr3.sml sml 9167 127 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr2/THUMBNAIL/image/gif/7baa50155d841cb7672504015d9c1124/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr2/THUMBNAIL/image/gif/7baa50155d841cb7672504015d9c1124/gr2.sml gr2 gr2.sml sml 7475 164 186 IMAGE-THUMBNAIL 1-s2.0-S1532046411000736-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000736/gr1/THUMBNAIL/image/gif/e3468f05cbbd8827a5ccad30550fb4e9/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000736/gr1/THUMBNAIL/image/gif/e3468f05cbbd8827a5ccad30550fb4e9/gr1.sml gr1 gr1.sml sml 5085 117 219 IMAGE-THUMBNAIL YJBIN 1775 S1532-0464(11)00073-6 10.1016/j.jbi.2011.04.009 Elsevier Inc. Fig. 1 The TRL framework. Fig. 2 Illustration of the TRL algorithm. Fig. 3 Illustration of discretization issues. Variable A has the same number of discrete values, in the source data and the target data; although the intervals have different endpoints, there is a clear mapping between these sets values. Variables B and C have different numbers of values, and so pose a challenge for transfer. Fig. 4 Intra-set whole-rule transfer on the gene expression data [26]. Horizontal axes: proportion of data set used as target. Vertical axis: change in predictive performance compared to training only on the target data. The results are averages of 10\u00d75-fold cross-validation. Fig. 5 Intra-set whole-rule transfer on the gene expression data [26]. Horizontal axes: proportion of data set used as target. Vertical axis: amount of information retained during transfer learning; rr/rp: number of rules retained as a proportion of prior rules; rr/rl: number of rules retained as a proportion of rules in the new model; ar/al: number of variables in the retained rules, as a proportion of all the variables in the new model. The results are averages of 10\u00d75-fold cross-validation. Fig. 6 Number of improvements, ties and decreases in performance compared to training on the target data set alone. The results are averages over 10-fold cross-validation over the four protein profiling data sets. Fig. 7 Inter-set transfer with the four variations of the algorithm (horizontal axes). (a) Mean change in predictive performance compared to training on the target data alone. (b) Mean amount of information retained during transfer learning; rr/rp, rr/rl and ar/al have the same meaning as in Fig. 5. The results are averages of the two pairs of protein profiling lung cancer data sets, using each data set once as target data in turn and the other one as source, and 10-fold cross-validation over the target data. Table 1 Data sets used in the experiments. Horizontal lines delimit pairs of data sets where each data set was used once as the target when the other one was the source. \u201cSize\u201d is number of clinical samples from each class. Type Disease Name Size Ref. Genomic Leukemia Train 27+11 [26] Genomic Leukemia Test 20+14 [26] Proteomic Lung cancer WCX UPCI 95+90 [2] Proteomic Lung cancer WCX Vand 114+88 [2] Proteomic Lung cancer IMAC UPCI 95+90 [2] Proteomic Lung cancer IMAC Vand 114+89 [2] Transfer learning of classification rules for biomarker discovery and verification from molecular profiling studies Philip Ganchev a \u204e phil.ganchev@gmail.com David Malehorn b William L. Bigbee b Vanathi Gopalakrishnan a c vanathi@pitt.edu a Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA, United States b Department of Pathology, University of Pittsburgh, Pittsburgh, PA, United States c Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA, United States \u204e Corresponding author. Abstract We present a novel framework for integrative biomarker discovery from related but separate data sets created in biomarker profiling studies. The framework takes prior knowledge in the form of interpretable, modular rules, and uses them during the learning of rules on a new data set. The framework consists of two methods of transfer of knowledge from source to target data: transfer of whole rules and transfer of rule structures. We evaluated the methods on three pairs of data sets: one genomic and two proteomic. We used standard measures of classification performance and three novel measures of amount of transfer. Preliminary evaluation shows that whole-rule transfer improves classification performance over using the target data alone, especially when there is more source data than target data. It also improves performance over using the union of the data sets. Keywords Biomarker discovery Molecular profiling Machine learning Rule learning Transfer learning 1 Introduction Molecular profiling data is used extensively to learn classifiers, such as rule models, and discover biomarkers for early detection, diagnosis and prognosis of diseases. Biomarkers are also critical for furthering understanding of disease mechanisms and creating treatments. The aim in biomarker discovery is to find a small set of measured variables that can be used to accurately predict a disease state. This is particularly challenging because typically we must choose among tens or hundreds of thousands of variables, representing molecules in complex mixtures, often with high measurement error. Also, data sets are typically very small, usually tens or hundreds of patients in a study. All these factors make statistical analyses more error-prone. Fortunately, there are often multiple similar studies, each producing a data set. In order to draw on all the available data, researchers typically analyze each data set separately, then compare the biomarkers discovered [1,2]. However, this is sub-optimal because the analysis is still done on the separate small data sets. A simple way to combine the data is to use the union of the data sets. But such attempts are typically confounded by variability in sample processing and by systematic measurement error specific to each data set. For example, the same numerical measurement might mean a high abundance of some protein in one data set but low abundance in another data set. We propose a novel framework for transfer learning, called Transfer Rule Learner (TRL), that is particularly well-suited to biomarker discovery. Transfer learning is the use of data from one learning task to help in learning another task [3]. Pan and Yang [4] offer a survey of transfer learning. Various methods for transfer learning have been applied to various domains, such as part-of-speech tagging [5] and leaf classification [6]. Transfer learning for biomarker discovery is promising because previous studies have found reproducibility of information collected in different experimental sessions when using the same protocols [1,2]. Unfortunately, many transfer learning frameworks typically produce classifiers that are difficult for human users to understand or that use many variables [3,5], which makes them less useful for biomarker discovery. Unlike other transfer learning methods, TRL transfers knowledge in the form of modular, interpretable classification rules, and uses them to seed learning of a new classifier on a new data set. Rule learning has the advantage that variable selection is embedded in the learning algorithm, and the new model uses only a few of the many measured variables to explain the data. TRL is an extension of the classification rule learning algorithm RL [7], which been used successfully to solve biomedical problems for more than three decades [8\u201311] and in the past decade has been adapted and used for biomarker profiling [12\u201317]. We demonstrate our method on five clinical data sets, and find that more often than not, transfer learning improves performance over using one data set alone, and even more often over learning on the union of the data sets. We evaluate the methods using standard performance measures and three novel measures of transfer. To our knowledge, this is the first effort to apply transfer of rules or rule structure between related biomedical data sets. 2 Materials and methods Our transfer learning framework is based on the classification rule learner RL [7]. Models learned by RL are simple to understand and can represent non-linear relationships. RL covers data with replacement (see Section 2.1), which is beneficial when training data are scarce. Fig. 1 shows an overview of our transfer learning approach. A source data set is used to train a set of prior rules that are then used as seeds for learning on the target data. Section 2.1 provides a brief overview of RL, which is useful in understanding transfer learning with TRL described in Sections 2.2, 2.3, 2.4. 2.1 Rule learning with RL RL is a classification learning algorithm that outputs a rule-based classifier. RL\u2019s input is a set of training data instances, where each instance is a vector of values for the input variables, and a class value. The learned classifier comprises a set of rules of the form: IF < antecedent > THEN < consequent > where the antecedent consists of a conjunction of one or more variable-value pairs (conjuncts), and the consequent is a prediction of the class variable. For example, a rule learned from proteomic mass spectra might be: IF ( ( mz _ 2 . 05 = High ) AND ( mz _ 9 . 65 = Low ) ) THEN Class = Control which is interpreted as \u201cif the variable for m/z 2.05 kilo Daltons (kDa) has a value in the High range and the m/z 9.65 has a value in the Low range, then predict the class value Control.\u201d Values such as Low and High represent intervals of real numbers that result from discretizing the variables before training with RL. (See Section 2.2.) A rule is said to cover or match a data instance if each variable value of the instance is in the range specified in the rule antecedent. RL covers data with replacement, which means that multiple rules are allowed to cover the same training instance. This is unlike most other classification rule and tree learning algorithms, such as C4.5 [18] and CART [19], which cover data without replacement, so that each data instance is covered by only one rule. In small sample size data sets, covering with replacement allows RL to utilize more of the available evidence for each rule when computing the generalizability of the rule. The classifier also includes an evidence gathering method for breaking ties when the antecedents of several rules are met but their consequents are different. We use the default evidence gathering method: voting weighted by the rules\u2019 certainty factor values. RL is shown in Algorithm 1. The input is a set of data instance vectors and a set of values for the learning parameters specified by the user. The parameters define constraints on the acceptable rules in terms of a number of quantities defined with respect to a rule and a data set. The constraints are minimum coverage, minimum certainty factor value, maximum false positive rate, and inductive strengthening. Coverage is the fraction of training examples for which the rule antecedent is satisfied. An additional parameter is the certainty factor function. The Certainty factor function (CF) is a measure of the rule\u2019s accuracy; several alternative certainty factor functions are defined and implemented in RL, and the specific function to use can be specified as a parameter to the algorithm. As a CF function, we used the true positive rate: the number of examples the rule predicts correctly divided by the number of examples it matches. False positive rate is the number of examples the rule predicts incorrectly divided by the number of examples it matches. Inductive strengthening is a bias toward training new rules that cover uncovered training instances. Specifically, the parameter specifies the minimum number of previously uncovered examples that a proposed rule must cover. The smaller this number, the larger the overlap of instances covered by different rules. Because RL covers data with replacement, using some non-zero inductive strengthening helps to learn a more generalizable model. Maximum conjuncts is the maximum number of variable-value pairs allowed in the antecedent of any rule. The algorithm proceeds as a heuristic beam search through the space of rules from general to specific [20]. Starting with all rules containing no variable-value pairs, it iteratively specializes the rules by adding conjuncts to the antecedent. It evaluates the rules and inserts promising rules onto the beam, sorted by decreasing certainty factor value. Beam search is used to limit the running time and space of the algorithm. 2.2 Transfer of rules Algorithm 1 TRL. Differences from RL are underlined. Function import() takes a list of prior rules and removes from them any variables that do not appear in the data set. satisfies() checks if the rule satisfies the user-specified constraints. The second call to satisfies() checks only the minimum-coverage constraint because coverage of any specialized rule will be equal or smaller. specialize() creates all non-redundant specialized rules by adding to the original rule antecedent single variable-value pairs from the data set. Input : data, a set of training instance vectors Input : priorRules, a list of prior rules Parameters: constraints, constraints on acceptable rules Parameters: minCoverage, minimum-coverage constraint beam \u2190 import( priorRules )+[\u2205 \u21d2 class 1, \u2205\u21d2 class 2\u2026]; beam \u2190 sort(beam); model \u2190[]; while beam is not empty do beam new \u2190[]; foreach rule \u2208 beam do if satisfies(rule, constraints, data) then model \u2190 model+[rule]; beam new \u2190 beam new + specialize(rule); else if satisfies(rule, minCoverage, data) then beam new \u2190 beam new + specialize(rule); end beam \u2190 sort(beam new ); end end Return model The transfer scheme we propose is to load the prior rules into the beam along with the initial rules, before the first evaluate-specialize iteration. Fig. 2 illustrates the algorithm. RL\u2019s input data must be discrete, that is, each variable has a small number of values in the data. Many molecular profiling methods create real-valued data, so these data must be discretized before use with RL; that is, for each continuous (real-valued) variable, we must partition the set of real numbers into some small set of intervals, and substitute each observed value of the variable by its corresponding interval. What intervals are chosen in the partition affects the usefulness of the variable in predicting the class variable. Because the variable might have a different distribution in the source data than in the target data, the optimal discretization in each case might also differ. However, it is important for rule transfer that a variable in a prior rule has the same number of intervals in the target data as in the source data. This is because in this case there is a clear mapping between the two sets of intervals: the interval \u201cLow\u201d in the source data corresponds to \u201cLow\u201d in the target data, and so on. The end-points of corresponding intervals might differ, due to the different distribution of values in the two data sets; but this does not matter because transfer learning aims to make use of the commonalities between the two distributions. A difficulty arises if the number of intervals differs between source and target, because in that case a value in a prior rule does not have a clear meaning in the target data. For example, suppose a variable A has intervals High, Middle and Low in the source data but only High and Low in the target data (see Fig. 3 ). How should we transfer a rule that has A=Middle in its antecedent? To address this issue, we can either apply the same discretization to both source and target data and transfer whole rules; or we can discretize the data sets separately, and transfer only the \u201cstructure\u201d of the prior rules, without the variable values (rule structure transfer); this is done by instantiating the new discrete values for each variable. These approaches are discussed in Sections 2.3 and 2.4 respectively. For discretizing, we used EBD [21,17]. EBD is a univariate supervised discretizer that uses a Bayesian score to evaluate discretizations for each continuous-valued variable, and runs efficiently on high-dimensional data such as molecular profiling data. EBD performed favorably compared to Fayyad and Irani\u2019s MDLPC [22], which is efficient and commonly-used state-of-the-art method [23]. EBD tended to perform better with rule learners such as RL that learn rules having many attributes in the antecedent. It utilizes a Poisson prior, and by default we set the Poisson parameter \u03bb =1, so that the mean and variance of the number of cut points has an expected value of 1. Having a single cut point is equivalent to having two intervals, High and Low. A further consideration is how prior rules will affect RL\u2019s search process. This is discussed in Section 2.5. 2.3 Whole-rule transfer The simplest way to avoid possible differences in the discretization between source and target is to ensure that they are discretized identically. Specifically, we discretized the target data and imposed the same discretization on the source before running RL to compute prior rules. This guarantees that there is a clear mapping between values in the two data sets, even if it means that the prior rules will be less accurate on the target data. 2.4 Rule structure transfer As mentioned in Section 2.2, it might not be optimal to apply the same discretization to the source and target data sets, due to systematic differences between them. For example, two sets of proteomic mass spectra often have different baseline signals caused by the state of the measurement equipment. Even after post-processing such as baseline subtraction, numerical values in one data set might not correspond to numerical values in the other. Ideally, the data sets can be discretized separately to find an optimal discretization for each variable in each data set. However, this might result in a different number of discrete values for a variable in the two data sets, and therefore no clear mapping between values for transfer learning. To address this problem, we instantiate each prior rule with all possible target values for its variables. In a sense, this means transferring the just the \u201cstructure\u201d of the rule without the variable values. For example, a prior rule: IF ( mz _ 7 . 23 = High ) THEN ( Group = Cancer ) is converted to a prior rule structure: IF ( mz _ 7 . 23 = ? ) THEN ( Group = ? ) The rule is then instantiated from the target data discretization as: IF (mz_7.23 = High) THEN (Group = Cancer) IF (mz_7.23 = Low) THEN (Group = Cancer) IF (mz_7.23 = High) THEN (Group = Healthy) IF (mz_7.23 = Low) THEN (Group = Healthy) We consider all class values in addition to all discrete variable values because that more completely represents the possible relationships in the target data. Transferring rule structure has the additional advantage that the source data set is not needed during the transfer learning. Thus, the prior rules can be used when the source data set is not available, for example by extracting them from literature. For example, serum amyloid A (SAA) has been reported as a serum biomarker for lung cancer [24]. Prior rules could be formulated from known m/z values for this marker, for example by using the EPO-KB data base of biomarker-to-protein links [25]. 2.5 Effect of transfer on RL\u2019s search process As explained in Section 2.1, to avoid overfitting, RL checks that any new rules added to the model cover at least \u03c3 previously uncovered training instances, where \u03c3 is the inductive strengthening parameter. When prior rules are added to the beam, they may cover some target data instances that would be covered by better performing rules learned in the absence of the prior rules. Thus, the prior rules might displace these better rules from the model and so reduce its classification performance. To reduce this effect, we created a variation of the algorithm that we called \u201cnc\u201d (for \u201cno-covering\u201d). In this variation, the target data instances covered by prior rules are not considered as \u201ccovered\u201d for purposes of inductive strengthening. Note that prior rules retained in the classifier can still influence the predictions, and therefore the performance, of the classifier. 2.6 Experiments We compared the four transfer methods to the baseline condition of learning on the target data set alone, based on classification performance. We also compared the four methods with each other in terms of the three measures of amount of information transferred. Using the data sets, we created two experimental setups, namely inter-set transfer and intra-set transfer. Using cross-validation, we evaluated the methods on three pairs of data sets (see Table 1 ): one of gene expression and two of protein profiling using SELDI TOF MS. Within each pair, each data set was produced from the same overall clinical population and the same type of clinical samples, and using the same protocols and measurement platform (e.g. \u201cProteinChip\u201d type). For the proteomic data, separate samples were accrued at University of Pittsburgh Cancer Institute (UPCI) and Vanderbilt University, as described in [2] and [24] respectively. SELDI analysis was done concurrently and with the same conditions as described in [2], using two types of ProteinChip: WCX and IMAC. Most of the spectra were generated with replicates, which were averaged before learning. Discretization was done separately on each step of cross-validation, using EBD [23] with default parameters, similar to those used in [17]. RL parameter settings were: CF function: true positive rate; minimum CF value: 85%; minimum coverage: 4 instances; maximum false positive rate: 10%; inductive strengthening: 1 instance; maximum conjuncts per rule: 5. The purpose of our transfer learning methods is to evaluate the agreement of new data with the prior information, and simultaneously learn new information that incorporates as much of the prior knowledge as useful. High agreement would mean much prior information is retained and is accurate on the new data set. To evaluate our methods, we measured several variables: (1) the performance of the learned classifier, namely accuracy, sensitivity, specificity, and (2) the amount of information transferred, defined by three measures of the amount of transfer: (2a) rr/rp: number of rules retained as a proportion of prior rules; (2b) rr/rl: number of rules retained as a proportion of rules in the new model; (2c) ar/al: number of variables in the retained rules, as a proportion of all the variables in the new model. We recorded these measures for the cross-validation folds and for the final model learned on all the target data. We now describe the two experimental setups, intra-set transfer and inter-set transfer. 2.7 Varying the relative sizes of the source and target data An important aim of the study is to clarify what factors affect the performance of classifiers learned by TRL and the amount of information transferred. This understanding is important for evaluating the properties of the transfer learning methods developed here, as well as for designing new methods. In a particular real-world setting, this understanding can suggest what results can be expected from transfer learning. To further this understanding, we investigated how the methods perform on data drawn from the same distribution, and the effect of the relative sizes of the source and target data sets. We sampled target sets of different sizes at random, and each time used the remainder of the data as the corresponding source set. We call this experimental setup \u201cintra-set transfer\u201d. We used the genomic data from Golub et al. [26], which is a well-known data set with a good classification performance. The learned models are evaluated using 10 times 5-fold cross-validation on the target data. This provides sufficient statistical support for the results, while leaving enough training data when the target set contains only 10% of all the available data. Fig. 4 shows the change in performance of whole-rule transfer compared to using only the target data. As expected, this works well because the source and target data are drawn from the same data set. Transfer never decreases performance, and significantly increases performance when the source data set is large compared to the target data, that is when the source classifier generalizes better than the classifier learned on the target data. The improvement decreases as the size of the target data increases. An exception to that trend occurs with 10% of the target data; in this case, there is no benefit from transfer because the target data is too small to accurately evaluate the goodness of a classifier, and all the transferred rules are discarded. In fact, no rules are learned at all on the target data set of seven examples, regardless of whether or not transfer is used. Rule structure transfer and transfer with no coverage for prior rules show similar trends. The amount of transfer (Fig. 5 ) shows similar trends; the measures rr/rl and ar/al are high when the target data set is small, and low when it is large compared to the source data. This is because the source data allows the learning of many good rules, and most of the rules learned on the target data are derived from prior rules. By contrast, the proportion of rules retained out of all prior rules, rr/rp, had a peak around 50%. This means that to maximize retention of prior rules the sizes of the source and target should be roughly equal. This observation agrees with expectation because at one extreme, when the target data is large, the source data is very small and the learned rules are not accurate; few rules are learned and even fewer are retained during transfer, leading to a small fraction rr/rp. At the other extreme, when the target data set is very small, it is insufficient for evaluating the learned prior rules accurately; thus again few rules are retained. We also examined the sizes of the antecedents of the learned rules (not shown). In this experimental setup, all rules had one conjunct in their antecedents. 2.8 Comparison of the transfer methods Using the target data alone performed better than the union of the data sets, so we use it as the baseline. Figs. 6 and 7 a compare the performance of all four variants of transfer rule learning. The results in each figure are averaged over 10-fold cross-validation for each experimental setting. The final learned (non cross-validation) rules had at most four conjuncts in their antecedents (not shown in the diagrams). Each column of Fig. 6 shows the proportion of datasets that each method improved and worsened compared to using the target data alone. We see that the whole-rule transfer methods (\u201ctr\u201d and \u201ctr_nc\u201d) improved accuracy in 75% of the cases, while structure transfer methods (\u201cts\u201d and \u201cts_nc\u201d) only in 0\u201326% of the cases. Similarly, whole-rule transfer improved sensitivity in 75\u2013100% of the cases, while structure transfer only in 25\u201350%. Specificity decreased more often than it increased, with all methods. These trends are visible also in Fig. 7a, which shows the mean changes in performance across all our experiments. Overall, transfer of whole rules performed better than transfer of structures. Moreover, whole-rule transfer with \u201cno coverage\u201d (tr_nc) was better on average for sensitivity and accuracy, while the basic rule transfer (tr) was slightly better for specificity. The decrease for structure transfer was due to a dramatic decrease when transferring lung cancer IMAC UPCI to IMAC Vand. Without that experiment, the mean performances are markedly greater, and are positive for most measures, though still lower for structure transfer than whole-rule transfer. That whole-rule transfer performed better than structure transfer is surprising because structure transfer was intended as a way to overcome the difference in discretization between the source data and target data, and thus make prior rules more generalizable on the target data. Recall that structure transfer instantiates the prior rule variables with all possible target data values, and adds all those instantiated rules to the beam for evaluation and further specialization. Thus, this result suggests that the benefit gained from more accurate discretization of the prior rules comes at the cost of adding less accurate rules to the final classifier, which more often than not mis-classify target examples. This could be because the large number of instantiated rules are displacing from the beam other rules that would be more accurate if they survived to be specialized; therefore if the beam was big enough, structure transfer should outperform whole-rule transfer. Another hypothesis is that structure transfer performed poorly because of the way RL learns rules, which is not geared towards learning rule structure (see Section 3). Another trend in the results is that the no-coverage (\u201cnc\u201d) transfer conditions show a less pronounced change than transfer where prior rules affect the coverage of examples. This is interesting because with the no-coverage condition, prior rules do not interact with other rules via covering during learning. However, rules do interact in this way during inference. Thus, the result suggests that it is beneficial to take account of these coverage interactions between all rules while training, rather than ignore them. 3 Discussion In this paper, we introduced a simple novel paradigm for transfer learning with interpretable rules. We demonstrated that this method is effective for mass spectrometry and gene expression data, and is particularly helpful when there is a relative abundance of source data compared to target data. In addition to the basic framework, we also demonstrated a novel method to transfer only the rule structure, and this can also be used to transfer knowledge from the literature to a new data set. An interesting line of future research is to investigate why structure transfer performed poorly. One hypothesis is that structure transfer adds too many instantiated prior rules to the beam and those rules are likely to displace other promising rules whose specializations would have proved more accurate. If the beam is large enough, such rules should not be displaced. To test this hypothesis, we could experiment with a much larger beam. But because a large beam makes the search intractable for practical use, we could also investigate two new transfer methods that avoid unnecessary combinatorial instantiation of variables, yet allow for different distributions between data sets. In particular, both methods would discretize a variable on the source data if this yields the same number of discrete values as discretizing on the target data; only if the number of values differs would we either impose the target data discretization or instantiate the target data values when that variable appears in a prior rule. Another hypothesis to explain the poor performance of structure transfer is that the way RL learns rules is not geared toward learning structure. We would like to experiment with structure transfer using BRL [17], a rule learner based on RL that uses Bayesian scoring and learns a single rule structure with all possible variable-value instantiations. It would also be interesting to explore transfer between different types of data, such as between proteomic and genomic data, by creating a mapping between them. Another extension of the algorithm is to allow it to correct errors in variable naming (such as m/z shift in mass spectrometry), by constructing compound variables during the search. 4 Conflict of interest The authors declare that there are no conflicts of interest. Acknowledgments This work was partly supported by Grant W81XWH-05-2-0066 from TATRC Department of Defense, and 5P50CA09044010 from National Institutes of Health. References [1] O. Semmes Z. Feng B. Adam L. Banez W. Bigbee D. Campos Evaluation of serum protein profiling by surface-enhanced laser desorption/ionization time-of-flight mass spectrometry for the detection of prostate cancer: I. Assessment of platform reproducibility Clinical Chemistry 51 1 2005 102 [2] R. Pelikan W. Bigbee D. Malehorn J. Lyons-Weiler M. Hauskrecht Intersession reproducibility of mass spectrometry profiles and its effect on accuracy of multivariate classification models Bioinformatics 23 22 2007 3065 3072 [3] R. Caruana Multitask learning Machine Learning 28 1 1997 41 75 [4] S.J. Pan Q. Yang Survey on transfer learning IEEE Transactions on Knowledge Engineering 22 10 2010 1345 1359 [5] Blitzer, J., McDonald, R., Pereira, F., 2006. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 Conference on Empirical Methods on Natural Language Processing, pp. 120\u2013128. [6] P. Wu T.G. Dietterich Improving SVM accuracy by training on auxiliary data sources Proceedings of the 21st International Conference on Machine Learning 2004 Morgan Kaufmann 871 878 [7] Clearwater, S., Provost, F., 1990. RL4: a tool for knowledge-based induction. In: Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence, pp. 24\u201330. [8] D. Hennessy V. Gopalakrishnan B.G. Buchanan J.M. Rosenberg D. Subramanian Induction of rules for biological macromolecule crustallization Proceedings of the International Conference on Intelligent Systems for Molecular Biology 1994 AAAI Press 179 187 [9] Y.W. Lee B.G. Buchanan D.M. Mattison G. Klopman H.S. Rosenkranz Learning rules to predict rodent carcinogenicity of nongenotoxic chemicals Mutation Research \u2013 Fundamental and Molecular Mechanisms of Mutagenesis 328 2 1995 127 149 [10] Y.W. Lee B.G. Buchanan J.M. Aronis Knowledge-based learning in exploratory science: learning rules to predict rodent carcinogenicity Machine Learning 30 2-3 1998 217 240 [11] V. Gopalakrishnan G. Livingston D. Hennessy B.G. Buchanan J.M. Rosenberg Machine-learning techniques for macromolecular crystallization data Acta Crustallographica, Section D: Biological Crystallography 60 Pt 10 2004 1705 1716 [12] H. Ryberg J. An S. Darko J.L. Lustgarten M. Jaffa V. Gopalakrishnan Discovery and verification of amyotrophic lateral sclerosis biomarkers by proteomics Muscle and Nerve 42 1 2010 104 111 [13] S. Ranganathan E. Williams P. Ganchev Proteomic profiling of cerebrospinal fluid identifies biomarkers for amyotrophic lateral sclerosis Journal of Neurochemistry 95 5 2005 1461 1471 [14] V. Gopalakrishnan P. Ganchev S. Ranganathan R. Bowser Rule learning for disease-specific biomarker discovery from clinical proteomic mass spectra Data Mining for Biomedical Applications 2006 93 105 [15] Lustgarten, J.L., Visweswaran, H., Grover, S., Gopalakrishnan, V., 2008. An evaluation of discretization methods for learning rules from biomedical data sets. In: Proceedings of the International Conference on Bioinformatics and Computational Biology (BIOCOMP\u201908), pp. 527\u2013632. [16] Kolli, V.S.K., Seth, B., Weaver, L., Lustgarten, J.L., Grover, H., Gopalakrishnan, V., Malehorn, D., 2009. Maldi-tof profiling of breast-cancer sera for pattern analysis. In: Human Proteome Organization (HUPO) Proceedings. [17] V. Gopalakrishnan J.L. Lustgarten S. Visweswaran G.F. Cooper Bayesian rule learning for biomedical data mining Bioinformatics 26 5 2010 668 [18] J.R. Quinlan C4.5: Programs for Machine Learning 1993 Morgan Kaufman San Francisco [19] L. Breiman J. Friedman R. Olshen C. Stone Classification and Regression Trees 1984 Chapman & Hall [20] Provost, F., Aronis, J., Buchanan, B., 1999. Rule-space search for knowledge-based discovery. Tech. Rep. IS 99-012, Stern School of Business, New York University. [21] Lustgarten, J.L., 2009. A Bayesian Rule Generation Framework for \u2019Omic\u2019 Biomedical Data Analysis. Ph.D. thesis, University of Pittsburgh. [22] Fayyad, U.M., Irani, K.B., 1993. Multi-interval discretization of continuous-valued attributes for classification learning. In: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCKAI-993), pp. 1022\u20131027. [23] Lustgarten, J., Gopalakrishnan, V., Grover, H., Visweswaran, S., 2008. Improving classification performance with discretization on biomedical datasets. In: AMIA Annual Symposium Proceedings, vol. 2008, AMIA, pp. 445\u2013449. [24] P. Yildiz Y. Shyr J. Rahman Diagnostic accuracy of MALDI mass spectrometric analysis of unfractionated serum in lung cancer Journal of Thoracic Oncology 2 10 2007 893 [25] J.L. Lustgarten C. Kimmel H. Ryberg W. Hogan EPO-KB: a searchable knowledge base of biomarker to protein links Bioinformatics 24 11 2008 1418 1419 [26] T. Golub D. Slonim P. Tamayo Molecular classification of cancer: class discovery and class prediction by gene expression monitoring Science 286 5439 1999 531", "scopus-id": "83755202239", "pubmed-id": "21571094", "coredata": {"eid": "1-s2.0-S1532046411000736", "dc:description": "Abstract We present a novel framework for integrative biomarker discovery from related but separate data sets created in biomarker profiling studies. The framework takes prior knowledge in the form of interpretable, modular rules, and uses them during the learning of rules on a new data set. The framework consists of two methods of transfer of knowledge from source to target data: transfer of whole rules and transfer of rule structures. We evaluated the methods on three pairs of data sets: one genomic and two proteomic. We used standard measures of classification performance and three novel measures of amount of transfer. Preliminary evaluation shows that whole-rule transfer improves classification performance over using the target data alone, especially when there is more source data than target data. It also improves performance over using the union of the data sets.", "openArchiveArticle": "false", "prism:coverDate": "2011-12-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/3.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046411000736", "dc:creator": [{"@_fa": "true", "$": "Ganchev, Philip"}, {"@_fa": "true", "$": "Malehorn, David"}, {"@_fa": "true", "$": "Bigbee, William L."}, {"@_fa": "true", "$": "Gopalakrishnan, Vanathi"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046411000736"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046411000736"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(11)00073-6", "prism:volume": "44", "prism:publisher": "Elsevier Inc.", "dc:title": "Transfer learning of classification rules for biomarker discovery and verification from molecular profiling studies", "prism:copyright": "Copyright \u00a9 2011 Elsevier Inc.", "prism:issueName": "AMIA Joint Summits on Translational Science \u2013 2011", "openaccess": "1", "prism:issn": "15320464", "dcterms:subject": [{"@_fa": "true", "$": "Biomarker discovery"}, {"@_fa": "true", "$": "Molecular profiling"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Rule learning"}, {"@_fa": "true", "$": "Transfer learning"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "S17-S23", "prism:endingPage": "S23", "prism:coverDisplayDate": "December 2011", "prism:doi": "10.1016/j.jbi.2011.04.009", "prism:startingPage": "S17", "dc:identifier": "doi:10.1016/j.jbi.2011.04.009", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "18", "@width": "293", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1139", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "379", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1459", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "487", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2178", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "349", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1263", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "757", "@width": "2147", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "223690", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "722", "@width": "1457", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "224808", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "969", "@width": "2093", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "201903", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1013", "@width": "2086", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "185975", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "776", "@width": "1333", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "226530", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1390", "@width": "1576", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "354574", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "737", "@width": "1380", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "111180", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "171", "@width": "485", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "26170", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "163", "@width": "329", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "26304", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "219", "@width": "473", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "30349", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "229", "@width": "471", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27189", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "175", "@width": "301", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25419", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "314", "@width": "356", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "39259", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "166", "@width": "311", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "13642", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "77", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5030", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "109", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8254", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "101", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6383", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "106", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5599", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "127", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9167", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "186", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7475", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "117", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000736-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5085", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/83755202239"}}