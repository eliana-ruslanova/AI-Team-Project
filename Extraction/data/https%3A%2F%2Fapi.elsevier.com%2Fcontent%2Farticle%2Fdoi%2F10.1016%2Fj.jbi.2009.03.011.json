{"scopus-eid": "2-s2.0-70349456966", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2009-04-02 2009-04-02 2010-10-09T20:28:24 1-s2.0-S1532046409000501 S1532-0464(09)00050-1 S1532046409000501 10.1016/j.jbi.2009.03.011 S300 S300.1 FULL-TEXT 1-s2.0-S1532046409X00067 2015-05-15T06:30:58.184067-04:00 0 0 20091001 20091031 2009 2009-04-02T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content oa subj ssids 1532-0464 15320464 42 42 5 5 Volume 42, Issue 5 17 895 904 895 904 200910 October 2009 2009-10-01 2009-10-31 2009 Biomedical Natural Language Processing Wendy W. Chapman a K. Bretonnel Cohen b a Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15260, USA b Center for Computational Pharmacology, Biomedical Text Mining Group, University of Colorado School of Medicine, Denver, CO 80202 Research Papers article fla Published by Elsevier Inc. VALUEPARSINGFEATUREGENERATIONFORGENEMENTIONRECOGNITION SMITH L 1 Introduction 2 Methods 2.1 The GENETAG corpus 2.2 Base noun phrases 2.3 Gene names 2.4 Contextual clues and manual validation 2.5 Machine learning 2.6 Lexical features 2.7 Grammatical features 2.8 Evaluation 3 Results 3.1 Nominal position 3.2 Lexical features 3.3 Grammatical features 3.4 Machine learning algorithms 3.5 Manual evaluation 4 Discussion 5 Conclusions Acknowledgments Appendix A Union of BNPs Appendix B Training of machine learning parameters References TANABE 2002 1124 1132 L YEH 2005 S2 A SMITH 2004 2 L SCHAPIRE 1999 297 336 R BERGER 1996 39 71 A DIETTERICH 1998 29 T HAND 2001 16 D ZHANG 2001 5 31 T JOACHIMS 1999 T ADVANCESINKERNELMETHODSSUPPORTVECTORLEARNING MAKINGLARGESCALESVMLEARNINGPRACTICAL SMITHX2009X895 SMITHX2009X895X904 SMITHX2009X895XL SMITHX2009X895X904XL 2013-08-22T00:00:26Z Full http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window ElsevierBranded item S1532-0464(09)00050-1 S1532046409000501 1-s2.0-S1532046409000501 10.1016/j.jbi.2009.03.011 272371 2010-11-01T14:20:30.573958-04:00 2009-10-01 2009-10-31 1-s2.0-S1532046409000501-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/MAIN/application/pdf/ebe5dc524723b128ef64dc3a8412273d/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/MAIN/application/pdf/ebe5dc524723b128ef64dc3a8412273d/main.pdf main.pdf pdf true 452183 MAIN 10 1-s2.0-S1532046409000501-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/PREVIEW/image/png/b3da6bb435d647b4fecbba4cf6c44301/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/PREVIEW/image/png/b3da6bb435d647b4fecbba4cf6c44301/main_1.png main_1.png png 88177 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046409000501-si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c79cce6bce91c3282039a78830482630/si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c79cce6bce91c3282039a78830482630/si9.gif si9 si9.gif gif 433 17 97 ALTIMG 1-s2.0-S1532046409000501-si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/e198931f796a92486501b7a4c9262cf8/si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/e198931f796a92486501b7a4c9262cf8/si8.gif si8 si8.gif gif 389 18 56 ALTIMG 1-s2.0-S1532046409000501-si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/08321e1ab907d65dc430827e61df0953/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/08321e1ab907d65dc430827e61df0953/si7.gif si7 si7.gif gif 494 18 76 ALTIMG 1-s2.0-S1532046409000501-si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si6 si6.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/be12ba928e51cbbde7db7a03c5245ddf/si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/be12ba928e51cbbde7db7a03c5245ddf/si5.gif si5 si5.gif gif 332 19 37 ALTIMG 1-s2.0-S1532046409000501-si42.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/0cc2e208da5459355ebdd745c3f09159/si42.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/0cc2e208da5459355ebdd745c3f09159/si42.gif si42 si42.gif gif 196 12 13 ALTIMG 1-s2.0-S1532046409000501-si41.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/05965ab7a2358d1cef31ad8547e83d8d/si41.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/05965ab7a2358d1cef31ad8547e83d8d/si41.gif si41 si41.gif gif 181 12 13 ALTIMG 1-s2.0-S1532046409000501-si40.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/7899bf06c05a58d253db0c81e5af0193/si40.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/7899bf06c05a58d253db0c81e5af0193/si40.gif si40 si40.gif gif 228 12 18 ALTIMG 1-s2.0-S1532046409000501-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/151f0a619c090620f7e3a73ccc229d8d/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/151f0a619c090620f7e3a73ccc229d8d/si4.gif si4 si4.gif gif 921 43 165 ALTIMG 1-s2.0-S1532046409000501-si39.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/569ebb1210ac87da78eecca5b8f3295d/si39.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/569ebb1210ac87da78eecca5b8f3295d/si39.gif si39 si39.gif gif 408 13 75 ALTIMG 1-s2.0-S1532046409000501-si38.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/476bdd5fdcc3543063c49d60dd641da7/si38.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/476bdd5fdcc3543063c49d60dd641da7/si38.gif si38 si38.gif gif 411 17 72 ALTIMG 1-s2.0-S1532046409000501-si37.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/d7c4cfad3ff7710f247b25fdb8dd8d1a/si37.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/d7c4cfad3ff7710f247b25fdb8dd8d1a/si37.gif si37 si37.gif gif 408 17 72 ALTIMG 1-s2.0-S1532046409000501-si36.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/d0842ecfd571e1fa36cd3ceedd30aadb/si36.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/d0842ecfd571e1fa36cd3ceedd30aadb/si36.gif si36 si36.gif gif 413 17 72 ALTIMG 1-s2.0-S1532046409000501-si35.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/da378d78cbbd05ef3b595d09b91aad76/si35.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/da378d78cbbd05ef3b595d09b91aad76/si35.gif si35 si35.gif gif 403 17 70 ALTIMG 1-s2.0-S1532046409000501-si34.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/989110f5601c0328ed7d5a3ae296b23d/si34.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/989110f5601c0328ed7d5a3ae296b23d/si34.gif si34 si34.gif gif 406 17 70 ALTIMG 1-s2.0-S1532046409000501-si33.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/cb967a3d9edd6a4f0d78142090f9e0bb/si33.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/cb967a3d9edd6a4f0d78142090f9e0bb/si33.gif si33 si33.gif gif 410 17 70 ALTIMG 1-s2.0-S1532046409000501-si32.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c302ea812d7916805ccc1544dde076e7/si32.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c302ea812d7916805ccc1544dde076e7/si32.gif si32 si32.gif gif 385 13 75 ALTIMG 1-s2.0-S1532046409000501-si31.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/4e5dffb63164ada269e14044e408d338/si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/4e5dffb63164ada269e14044e408d338/si31.gif si31 si31.gif gif 365 21 54 ALTIMG 1-s2.0-S1532046409000501-si30.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif si30 si30.gif gif 208 15 15 ALTIMG 1-s2.0-S1532046409000501-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/743034d04699a7450789a2aef2f466ed/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/743034d04699a7450789a2aef2f466ed/si3.gif si3 si3.gif gif 332 16 61 ALTIMG 1-s2.0-S1532046409000501-si29.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/ae82dd3fc0d6622f22e1b15a4ceaba2b/si29.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/ae82dd3fc0d6622f22e1b15a4ceaba2b/si29.gif si29 si29.gif gif 259 15 43 ALTIMG 1-s2.0-S1532046409000501-si28.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif si28 si28.gif gif 208 15 15 ALTIMG 1-s2.0-S1532046409000501-si27.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif si27 si27.gif gif 208 15 15 ALTIMG 1-s2.0-S1532046409000501-si26.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/126ea37d1d5a48afa9018879b86bfa58/si26.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/126ea37d1d5a48afa9018879b86bfa58/si26.gif si26 si26.gif gif 547 18 86 ALTIMG 1-s2.0-S1532046409000501-si25.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/7e70ec718111b26a5b5dae955d219a33/si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/7e70ec718111b26a5b5dae955d219a33/si25.gif si25 si25.gif gif 275 20 23 ALTIMG 1-s2.0-S1532046409000501-si24.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/9b1fb60802121af8fbb35aee6eb91588/si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/9b1fb60802121af8fbb35aee6eb91588/si24.gif si24 si24.gif gif 195 10 11 ALTIMG 1-s2.0-S1532046409000501-si23.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/08321e1ab907d65dc430827e61df0953/si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/08321e1ab907d65dc430827e61df0953/si7.gif si23 si23.gif gif 494 18 76 ALTIMG 1-s2.0-S1532046409000501-si22.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif si22 si22.gif gif 210 13 13 ALTIMG 1-s2.0-S1532046409000501-si21.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si21 si21.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/c49a56c78c94a29d2f8c023bd1a324e3/si28.gif si20 si20.gif gif 208 15 15 ALTIMG 1-s2.0-S1532046409000501-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/8a1f32b08dad0b47f3c4a84adaa9ccfb/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/8a1f32b08dad0b47f3c4a84adaa9ccfb/si2.gif si2 si2.gif gif 497 17 90 ALTIMG 1-s2.0-S1532046409000501-si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/d5c5f51afc24cb56d4ab5c225b4dc13d/si19.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/d5c5f51afc24cb56d4ab5c225b4dc13d/si19.gif si19 si19.gif gif 525 22 97 ALTIMG 1-s2.0-S1532046409000501-si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/b2e49e3003a636f4cbada73162ac9cf3/si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/b2e49e3003a636f4cbada73162ac9cf3/si18.gif si18 si18.gif gif 745 43 116 ALTIMG 1-s2.0-S1532046409000501-si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si17 si17.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif si16 si16.gif gif 210 13 13 ALTIMG 1-s2.0-S1532046409000501-si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si15 si15.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si14 si14.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/86b93f9eb89e8f8c4c7fdb4e60b977db/si6.gif si13 si13.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046409000501-si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/4a1637cd5245170b3c4f95910c012f12/si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/4a1637cd5245170b3c4f95910c012f12/si12.gif si12 si12.gif gif 1141 43 177 ALTIMG 1-s2.0-S1532046409000501-si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/e56aa90b494731f6557b8703fc7e6342/si16.gif si11 si11.gif gif 210 13 13 ALTIMG 1-s2.0-S1532046409000501-si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/a640570b39d70565fa62d55d638be9fc/si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/a640570b39d70565fa62d55d638be9fc/si10.gif si10 si10.gif gif 947 42 189 ALTIMG 1-s2.0-S1532046409000501-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/STRIPIN/image/gif/476bdd5fdcc3543063c49d60dd641da7/si38.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/STRIPIN/image/gif/476bdd5fdcc3543063c49d60dd641da7/si38.gif si1 si1.gif gif 411 17 72 ALTIMG 1-s2.0-S1532046409000501-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr1/DOWNSAMPLED/image/jpeg/4a54985f611f015782d309120d4539a3/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr1/DOWNSAMPLED/image/jpeg/4a54985f611f015782d309120d4539a3/gr1.jpg gr1 gr1.jpg jpg 36613 263 533 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000501-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr1/THUMBNAIL/image/gif/8043939c93040a9d0a4a777cdd352459/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr1/THUMBNAIL/image/gif/8043939c93040a9d0a4a777cdd352459/gr1.sml gr1 gr1.sml sml 4780 108 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000501-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr2/DOWNSAMPLED/image/jpeg/0302da941abb70342724f3acd370518d/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr2/DOWNSAMPLED/image/jpeg/0302da941abb70342724f3acd370518d/gr2.jpg gr2 gr2.jpg jpg 50149 338 534 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000501-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr2/THUMBNAIL/image/gif/0f3e91cc4ee9eb147cd8f62eba86caa7/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr2/THUMBNAIL/image/gif/0f3e91cc4ee9eb147cd8f62eba86caa7/gr2.sml gr2 gr2.sml sml 7561 139 219 IMAGE-THUMBNAIL 1-s2.0-S1532046409000501-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr3/DOWNSAMPLED/image/jpeg/c2cfd12c6b5051d0ac37a0e348e4ac3a/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr3/DOWNSAMPLED/image/jpeg/c2cfd12c6b5051d0ac37a0e348e4ac3a/gr3.jpg gr3 gr3.jpg jpg 43677 345 534 IMAGE-DOWNSAMPLED 1-s2.0-S1532046409000501-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046409000501/gr3/THUMBNAIL/image/gif/3270eb150a7c62a688719dd24ca6fb1c/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046409000501/gr3/THUMBNAIL/image/gif/3270eb150a7c62a688719dd24ca6fb1c/gr3.sml gr3 gr3.sml sml 6665 141 219 IMAGE-THUMBNAIL YJBIN 1540 S1532-0464(09)00050-1 10.1016/j.jbi.2009.03.011 Fig. 1 Break even of experiments in the lexical study. The A series of experiments measure the effect of lexical feature components (Table 3), and the B series measure the effect of adding window and substring features. The legend for experiments is given in Table 5. Fig. 2 Break even of grammatical study. The axis labels correspond to the parser application described in Table 4. Each parser application is tested with grammatical features only (G), lexical features only (L), and with both grammatical and lexical features (LG). The corpus consisted of 108,227 distinct BNPs from the intersection of those obtained from MedPost, Charniak-Lease, and Enju, of which 4919 contained a gene name. Fig. 3 Break even of grammatical study for the union corpus. The axis labels correspond to the parser application described in Table 4. Each parser application is tested with grammatical features only (G), lexical features only (L), and with both grammatical and lexical features (LG). The corpus consisted of 144,837 distinct BNPs from the union of those obtained from MedPost, Charniak-Lease, and Enju, of 6345 contained a gene name. Table 1 The manual review of base noun phrase candidates. The seven sources listed correspond to seven meaningful intersections of candidates predicted by MedPost (M), Charniak-Lease (L), and Enju (E). The Total column is the total number of base noun phrases found in GENETAG for each set. Precision is based on the number of BNPs that were manually found to be correct out of each 100 random selections, and this number is used to estimate the true number of BNPs in the set. The estimated number of true BNPs is used to estimate the recall, assuming that combining all sources yields 100% recall. Finally, the estimated positives were used to estimate the precision and recall for individual taggers. The BNPs in M \u2229 L \u2229 E (shown in bold) were used in this paper. Source Total Precision (%) Est. true Est. recall (%) M \u29f9 L \u29f9 E 8148 42 3422 3.2 L \u29f9 M \u29f9 E 2548 18 952 0.9 E \u29f9 L \u29f9 M 3043 18 548 0.5 M \u2229 L \u29f9 E 5506 74 4,074 3.8 M \u2229 E \u29f9 L 6751 78 5,266 4.9 L \u2229 E \u29f9 M 770 52 4,025 3.7 M \u2229 L \u2229 E 108,361 83 89,940 83.1 Total 144,837 74.7 108,227 100 Individual taggers (est.) M 128,766 79.8 102,702 98.9 L 126,895 78.0 98,991 95.3 E 125,895 79.3 99,779 96.1 Table 2 The GENETAG corpus. The number of sentences, BNPs, actual GMs, and positive BNPs in the training and test sets. The actual number of GMs includes all GMs of the GENETAG gold standard whether they appear in our accepted set of BNPs or not. A row headed \u201cGM within\u201d represents the subset of sentences that contain at least one BNP having a GM within it. In such a row, the number of BNPs is the total number of BNPs in those sentences. A row headed \u201cGM head\u201d represents the subset of sentences that contain at least one BNP having a GM in head position. Subset # Sentences # BNPs # Actual GMs # Positive BNPs Training set All 15,000 81,101 18,265 14,243 GM within 6934 42,425 \u2014 14,243 GM head 6078 37,792 \u2014 11,804 Test set All 5000 27,260 6331 4919 GM within 2324 14,307 \u2014 4919 GM head 2027 12,732 \u2014 4045 Table 3 Components of Lexical Features. A lexical feature can reference any subset of components described in this table. 1 Word Parsers in their processing tokenize sentences, and the tokenizations vary to some extent with the parser. By word, we mean one of the tokens produced in this way. Word features may use the word itself, or any of its substrings, in original or lower case. When substrings were used to generate features, a special character was affixed before and after each word and included in the substrings, so as to uniquely identify prefix and suffix characters of a word 2 Part of speech The part of speech for the word as assigned by MedPost (in the lexical study) or by the parser (in the grammatical study) 3 Location The location of the word relative to the BNP, that is, words to the left are numbered negatively and words to the right are numbered positively. The location may also be reduced to direction as plus or minus one Table 4 Parsers used in grammatical study, and their particular configurations. Each parser produces or utilizes a part of speech tag set, and some parsers accept pre-tagged text. Parsers output phrase structure trees, grammatical relations, predicate-argument structures, dependency relations, or annotated phrase structure trees. Configuration Parser Tag set Input Output Charniak Charniak parser dated August 16, 2005 [37] Penn Treebank Untagged sentences Phrase structure trees Bikel+ Bikel parser copyright 2004 for academic users [38] Penn Treebank MedPost tagged (translated) sentences Phrase structure trees Rasp+ Rasp parser, third distribution [39] CLAWS2 MedPost tagged (translated) sentences Phrase structure trees with grammatical relations Minipar Minipar parser, 1998 [40] Minipar Untagged sentences Dependency relations Enju The Enju parser version 2.3. [19] Penn Treebank Untagged sentences Phrase structure trees, annotated with predicate-argument structures Stanford+ Stanford lexicalized parser, version 1.5.1. [41,42] Penn Treebank MedPost tagged (translated) sentences Phrase structure trees with dependency relations Rasp Rasp parser, third distribution CLAWS2 Untagged sentences Phrase structure trees with grammatical relations Stanford Stanford lexicalized parser, version 1.5.1. Penn Treebank Untagged sentences Phrase structure trees with dependency relations Lease Charniak-Lease parser, dated July 9, 2005 Penn Treebak Untagged sentences Phrase structure trees Table 5 Machine learning experiments in the lexical study. The features of the lexical study were formed from words that could be mapped to lower case (L), left in their original case (O), or both were used (B). The location of the word relative to the BNP could be given as relative (R) or as a direction (D) encoded as \u00b11. Part of speech tags from MedPost were encoded in the feature (M) or not. The feature types could be unary, when only one component appeared in a feature, binary (B) when two or more components appeared, or ternary (T) when all three components were used. Features could be limited to words within a window 2 or 5 around the BNP. Pairs of predicate features could be conjoined to form additional compound features (C). Substrings up to length 4 (S) of the word could be used to form new features, replacing the word with its substring. BNPs were defined as positive if they contained a gene name within them (W), but experiments were also conducted that defined a BNP to be positive only if a gene name appeared in head position (H). The A series culminates in the selection of A 12, and the B series continues with this as a basis to explore windowed features, compound features, and substrings. Alg Case Loc Tag Type Win Comp Substr Posit A 1 L U W A 2 O U W A 3 L M B W A 4 O M B W A 5 L D M T W A 6 O D M T W A 7 L R B W A 8 O R B W A 9 L R M T W A 10 O R M T W A 11 B R M T W A 12 L R M B W A 13 L R M B H B 1 L R M B 2 W B 2 L R M B 2 C W B 3 L R M B 5 W B 4 L R M B 5 C W B 5 L R M B S W B 6 L R M B 5 S W B 7 L R M B 5 S H Table 6 Pairwise comparisons of five machine learning algorithms: support vector machines (SVM), maximum entropy (MXE), boosted decision trees (BDT), modified Huber cost (HUB), and naive Bayes (BAY). Each method was used to train with all nine parser configurations using lexical and grammatical features, and the results were compared. The average rank over the nine configurations is shown twice, in the second row and second column. The table entries give the number of times out of the nine experiments that the McNemar test showed significance at the 5% level. For example, MXE training was ranked first in all nine configurations, with statistical significance over the next best, HUB, in two out of nine. Algorithm MXE HUB SVM BDT BAY Avg rank 1 2.33 2.67 4 5 MXE 1 \u2014 2/9 4/9 9/9 9/9 HUB 2.33 \u2014 \u2014 0/9 9/9 9/9 SVM 2.67 \u2014 \u2014 \u2014 9/9 9/9 BDT 4 \u2014 \u2014 \u2014 \u2014 8/9 BAY 5 \u2014 \u2014 \u2014 \u2014 \u2014 The value of parsing as feature generation for gene mention recognition Larry H. Smith a \u204e lsmith@ncbi.nlm.nih.gov W. John Wilbur b wilbur@ncbi.nlm.nih.gov a Computational Biology Branch, National Center for Biotechnology Information, Room 6S614-N, Building 38A, 8600 Rockville Pike, Bethesda, MD 20894, USA b Computational Biology Branch, National Center for Biotechnology Information, Room 6S606, Building 38A, 8600 Rockville Pike, Bethesda, MD 20894, USA \u204e Corresponding author. Fax: +1 301 480 2288. Abstract We measured the extent to which information surrounding a base noun phrase reflects the presence of a gene name, and evaluated seven different parsers in their ability to provide information for that purpose. Using the GENETAG corpus as a gold standard, we performed machine learning to recognize from its context when a base noun phrase contained a gene name. Starting with the best lexical features, we assessed the gain of adding dependency or dependency-like relations from a full sentence parse. Features derived from parsers improved performance in this partial gene mention recognition task by a small but statistically significant amount. There were virtually no differences between parsers in these experiments. Keywords Natural language processing Named entity recognition Biological text Gene mention recognition Parsers Machine learning Support vector machines 1 Introduction Gene Mention Recognition (GMR) is a special case of Named Entity Recognition (NER) with a history going back to 1998 [1,2]. With important direct applications in itself, GMR is frequently also a critical step in applications of natural language processing to biomedical text. BioCreative workshops I and II each included a track devoted exclusively to GMR, where the highest attained F-scores were 83 [3] and 87.21 [4], respectively. It was also demonstrated that an F-score exceeding 90 is theoretically achievable. The present paper is aimed at understanding an aspect of GMR, namely, the potential contribution of full syntactic parsing. Parsers have been applied to NER [5\u20137] as well as GMR [8,9] and other biomedical NLP tasks [10\u201313]. Parser technology has improved in recent years, especially in the number of high quality, publically available parsers. The intrinsic accuracy of parsers has been a subject of much interest, both generally [11,14\u201316], and in the domain of biological text [17]. The extrinsic evaluation of parsers examines the usefulness and variability of parsers in performing other tasks, an example being extraction of protein\u2013protein interactions [10,12,13,18]. In this study, we examine the usefulness of parsing to the GMR problem. Others have examined this problem with equivocal results [8,9], but our approach is different. We devised a more direct test of usefulness: to predict whether a candidate noun phrase contains a gene name or not given all the information from the sentence except that for the candidate noun phrase itself. We take this approach for two reasons. First, because it is possible to view the GMR problem as a two step procedure that first isolates those base noun phrases that contain gene mentions as candidate GMR phrases and then localizes the gene names within candidate GMR phrases. We are not proposing that this approach is necessarily the best way to approach GMR. However, state of the art GMR is not as good as general NER and we think it is appropriate to investigate new strategies to solve GMR. Second, in the current investigation there are advantages to approaching GMR with the suggested two step procedure. The base noun phrase is a natural grammatical unit which fits well into just about any parsing scheme. The contents of the base noun phrase can be hidden and the external relationships of the base noun phrase, as detected by the parser, can be examined in regard to their potential usefulness in detecting the gene name occurrence. We think this second point is especially important as the contents of the noun phrase are likely to provide a strong signal which is little affected by the results of a parse of the base noun phrase itself, but will tend to overshadow the information that might otherwise be a detectable contribution coming from the parse of the remainder of the sentence, i.e., the external relations of the base noun phrase. For this purpose, parsers were allowed to provide grammatical information, in the form of grammatical relations [15], predicate-argument structures [19], or dependency relations [20]. But since any type of linguistic information could potentially appear in the parser output, we first sought the best prediction that could be made using lexical, non-parse data, and only then did we ask if parsers were able to improve on the result. Grammatical information has not been fully explored in NER, especially in GMR. And advancing parser technology will open new possibilities for applying grammatical analysis to these problems. Our research explores this new opportunity, and offers a new application specific method for evaluating parser technology [13]. 2 Methods 2.1 The GENETAG corpus The GENETAG corpus consists of 20,000 sentences, selected randomly from MEDLINE in such a way that approximately half of the sentences were likely to contain gene names. Each sentence has been manually reviewed and annotated to indicate gene names by their starting and ending points, and some are also given acceptable alternative starting and ending points. This corpus has been used in two workshops in which participants developed systems to recognize gene names. In the most recent workshop, the BioCreative II challenge evaluation, the sentences of the corpus were divided into 15,000 for use in training and 5000 (previously unseen) used for final evaluation. In our research, we respect this corpus division: training of machine learning algorithms was performed on the 15,000 designated training sentences and scores are reported by evaluating the 5000 additional sentences. 2.2 Base noun phrases Gene names are noun phrases, and theoretically they may contain arbitrarily complex grammatical constructions. However, since genes are common nouns and often subjected to repeated use, one would expect most gene names to have the simplest grammatical structure. Although some gene names contain complex phrases (prepositional phrases or conjunctions), the majority of them consist of a sequence of one or more nouns and adjectives. Such simple noun phrases have been called by different names in the literature. Here we use the term base noun phrase (BNP) to denote a maximal noun phrase, including nouns, adjectives and determiners, but excluding non-nominal words, such as prepositions, verbs, and conjunctions. Using the MedPost part of speech tagger [21] to obtain base noun phrases, we found that those BNPs contained 92.84% of all gene mentions from the GENETAG test set. However, we did not want to rely on MedPost as the sole determinant of BNPs as this might bias our subsequent analysis. Therefore, in addition to the BNPs implied by MedPost (denoted by M), we considered BNPs implied by the Charniak-Lease parser [22] (denoted by L), and the Enju parser [19] (denoted by E). These were chosen because of their previous training on biological text and their radically different approaches to parsing. There are 144,837 distinct BNPs from these three sources, which we hypothesize is a comprehensive list. We estimated the precision of these BNP candidates by randomly and blindly selecting one hundred from each of the seven meaningful intersections. For example, the set L \u2229 E \u29f9 M consisted of base noun phrases implied by Charniak-Lease and Enju but not by MedPost. These seven hundred sentences were then randomized and reviewed by the authors, first independently and then together, to arrive at a precision for each set. These results are presented in Table 1 . As a result of this inquiry, we estimate that there are 108,227 distinct, true BNPs in the corpus, and thus 23.3% of the 144,837 candidates are erroneous. The comprehensive list is not only erroneous, it is ambiguous: there are many overlapping BNP candidates and many gene names in the GENETAG corpus are contained in more than one of these candidates. Finally, evaluating parser performance on the comprehensive list of BNP candidates would convey an unequal advantage, depending on the number of BNPs found in the corpus that are compatible with each parser\u2019s analysis. We were able to overcome all of these objections by studying only those BNP candidates in the three-way intersection ( M \u2229 L \u2229 E ) , for 83% precision and an estimated 83.1% recall. Throughout the study, these 108,361 candidates are presumed to be base noun phrases. For those interested in results based on the union of BNPs coming from M, L, and E, see Appendix A. 2.3 Gene names Returning to gene names, from Table 2 we see there were 4045 BNPs in the test set whose right-most, or head word position, was a gene mention (or alternate) defined by the gold standard of the GENETAG corpus. These BNPs are explicit references to genes, such as this one: the phosphorylated Fc gamma RIIB1 immunoreceptor tyrosine-based inhibitory motif. There were an additional 874 BNPs that contained gene names in modifier position, and with no alternate in head position. These are references to semantic types other than gene names, like this one: the phosphoenolpyruvate-dependent sugar-transporting phosphotransferase system. We considered both concepts, and present evidence below that it is more advantageous to classify BNPs containing gene names in any position rather than classifying BNPs with gene names in head position. Accepting those results, 4919 BNPs were marked as positive instances in the test set because they fully contained a gene mention. All other BNPs were marked as being negative, even those that overlapped with part of a gene mention from the gold standard without containing it entirely. The same strategy was applied to mark the positives and negatives in the training set. The 4919 BNPs represent 77.7% of all gene mentions of the GENETAG test set. Most gene names that were not contained within a BNP contained some excluded part of speech, such as a preposition or conjunction, as in: cyclin A- and E-associated kinase. This is a reference to a single gene that itself contains two references to genes (cyclin A and cyclin E). This also illustrates a \u201cdiscontinuous\u201d noun phrase (cyclin E) that is often the result of coordination. There were also a small number of gene names which themselves were simple nouns but nevertheless did not occur in a base noun phrase. The example Soluble extracts from FGF-treated as compared with quiescent fibroblasts. shows a gene name FGF, itself a noun, but part of the adjective FGF-treated. The adjective is not followed by the remainder of the base noun phrase (fibroblasts), but by a comparative adjective as compared with quiescent. Using the data in Table 2 we observe that if an annotation algorithm ranked the 27,260 BNPs of the test set randomly, then the observed break even (BE) value (the precision at which precision equals recall [23]) would be 18.04% on average, and the 99% upper confidence limit on this value is 18.59%. If an annotation algorithm marked BNPs from the 14,307 sentences containing gene names ahead of all others, but was otherwise random, then the observed BE would be 34.38% on average, with a 99% upper confidence limit for this value of 35.31%. Thus, if a BE on this test set is greater than 35.31%, then we may conclude that the algorithm is recognizing something specific and significant 1 The qualifier significant will be used throughout to mean statistically significant (p <0.05). 1 about the BNPs that contain gene names. 2.4 Contextual clues and manual validation One goal of our study was to determine the extent to which information surrounding a gene name reflected the presence of the gene name. This can be reduced to an explicit question: is there information in a sentence surrounding a particular BNP that suggests whether or not the BNP contains a gene name. To test this hypothesis as directly as possible, we conducted a small pilot study with human annotators. Using a sentence classifier (trained earlier on the GENETAG corpus), we scored sentences from recent abstracts from MEDLINE according to whether a gene mention was expected to appear within them. We then selected sentences at random from those that scored highly (but without requiring that a gene name was present), and within each sentence a BNP was highlighted at random. We, the authors, then looked at each selected sentence, and categorized the selected BNP as to whether it had a gene name in it or not. This was continued until approximately 100 gene names had been identified. The authors subsequently conferred to reach a consensus on 374 sentences containing 113 (30.21%) gene mentions in the highlighted BNPs. The BNPs were then hidden, and each replaced with identically sized black rectangles, and presented to two knowledgeable reviewers (not the authors) who were asked to score each one from 0 to 4 corresponding to their subjective confidence (0%, 25%, 50%, 75% or 100%) that the BNP contained a gene name. Because all the sentences were equally likely to contain gene names, the annotators had to look closely at each BNP and could not make a reliable prediction based solely on the subject matter apparent in a sentence, or without paying attention to the context of the BNP. The annotator\u2019s scores were used to rank the BNPs, and the BE was computed, with averaging over equal ranks, yielding 56.42% and 65.06% from the two annotators. For comparison, 99% of the time, a random ranking of the 374 BNPs would result in a BE less than 32.66%. From this we conclude that context does indeed provide information reflecting whether or not a BNP contains a gene name, and is at least evident to a human reader. 2.5 Machine learning To further test our hypothesis, we used supervised machine learning with the support vector machine (SVM) algorithm [24\u201326], trained on the 81,101 BNPs in the training set, where 14,243 were marked positive, and tested on the 27,260 BNPs in the test set, where 4919 were marked positive (Table 2). Each BNP in the corpus was associated with its containing sentence in which the BNP itself was replaced with a nonsensical string (arbitrarily chosen as \u201cQQQQQQ\u201d), and features for the BNP instance were derived from that sentence, i.e., from its words, parts of speech obtained from tagging and parsing, and dependency-like relations obtained from parsing. The research was done in two parts. The first part was aimed at optimizing the lexical features for recognizing BNPs containing gene names. By lexical feature, we mean features that pertain to a single word in the sentence, such as the lexeme, one of its substrings, its location relative to the BNP, and its part of speech. In the second part, we applied seven different publically available parsers and observed the improvement in performance when features derived from the parser output were added to the optimally constructed lexical features. The parsers were chosen because of their ready availability and ease-of-use generally, but Charniak-Lease and Enju were specifically included because they were available and trained on biological text. We also compared five different machine learning algorithms: (SVM) support vector machines (using the algorithm of [26] with parameter optimization as described in Appendix B), which was the basis for the study; (BDT) boosted decision trees with a maximum branching depth of four and 10,000 iterations [27]; (MXE) maximum entropy (as in [28], with regularization parameter \u03bb = 10 - 3 ); (HUB) minimization of the modified Huber cost function (as in [29], with regularization parameter optimized in the same manner as described in Appendix B); and (BAY) na\u00efve Bayes [30\u201332]. 2.6 Lexical features All features were conveniently represented by strings (without whitespace) encoding a logical predicate of the form: func(arg1,arg2,\u2026). The string func defines the type of feature, and the arguments, arg1,arg2,\u2026 give additional specifics about the feature, such as the word, location, and part of speech. We will illustrate several features for the underlined BNP in the following sentence: When extracorporeal CO2 removal approximated CO2 production (VCO2), alveolar ventilation almost ceased. Lexical features were defined for each word outside of a BNP, and were constructed from three components: the word, its part of speech and its location, as shown in Table 3 . The functor for the lexical features is a string corresponding to the system responsible for the tokenization and part of speech tagging. For instance, in the lexical study, the parts of speech were computed by the MedPost part of speech tagger [21], and the functor for those lexical features was \u201cmedpost\u201d (or a qualified variant). The arguments to lexical features are derived in various ways from the three components of Table 3. For the above example, the word ventilation occurs four words to the right of the BNP (the right parenthesis and comma are counted separately) and is tagged NN (noun). These components are combined in a single ternary feature as: medpost(ventilation,NN,4). The components can also be combined two at a time to form three separate binary features, as in: medpost-12(extracorporeal,JJ). This indicates that the word extracorporeal with tag JJ (adjective) occurs somewhere in the sentence outside of the BNP. The qualifier \u201c-12\u201d means that the feature arguments are formed from components 1 (word) and 2 (part of speech). 2.7 Grammatical features An important objective was to evaluate the contextual information available from a grammatical analysis of a sentence, and the effectiveness of several different publically available parsers. The parsers, and their different configurations, are listed in Table 4 . Three of the parsers (Stanford, Bikel, and Rasp) optionally accepted input sentences pre-tagged with parts of speech. In this application we used the MedPost part of speech tagger to pre-tag sentences by first tagging and then translating to the tag set of the parser. When those parsers are employed with MedPost pre-tagging, they appear with a \u201c+\u201d after the parser name. The Bikel parser was prohibitively slow when called without pre-tagging and so that configuration is not included in this study. The Charniak and Minipar parsers did not permit pre-tagging. And the Charniak-Lease and Enju parsers were already trained on a biomedical corpus, and no pre-tagging was attempted. Thus, grammatical features were defined in nine different ways using seven different parsers. The lexical features defined for a parser were derived from the tokens and part of speech tags that the parser produced. The particular combination of lexical components was selected from the best result in the lexical study (see Section 3) with the parser name for the functor, for example: stanford-sub-13(lar$,3). This represents that a word ending in lar ($ is the end-of-word marker) occurs three words to the right of the BNP. The grammatical features for each parser were defined by the set of grammatical relations, predicate-argument structures, or dependency relations, derived from the parse output. The Stanford parser outputs dependency relations. The Rasp parser outputs grammatical relations. The Bikel, Charniak and Charniak-Lease parsers output phrase structure trees, from which dependency relations were extracted using the program \u201cTypedDependenciesDemo,\u201d a java tool provided with the Stanford parser [20], which was invoked in the \u201ccollapsed\u201d mode. The Minipar parser produces dependency relations between words. And the Enju parser output consists of a phrase structure tree with annotated predicate-argument structure, which was interpreted as an n-ary relation holding between the head words of the arguments. All of these various dependency-like specifications were coded as grammatical features in the form of logical predicates, with the implied functor, and with arguments corresponding to words in the sentence and offset relative to the BNP. For example, the Stanford parser produced the following feature: nn(production,-2,co2,-3). This encodes the dependency \u201cnn\u201d (a noun compound modifier) between the words \u201cproduction\u201d at two words to the left of the BNP and \u201cco2\u201d at three words to the left. In addition to the coding of parser output, additional grammatical features were defined for dependency paths. First, each n-ary dependency, with n >2, was treated as n \u2212 1 binary relations holding between the first argument and the remaining arguments. Then the set of induced binary relations were treated as edges in a directed dependency graph, labeled by the functor. The reverse edges were also included by pre-pending a minus sign to the functor. All non-intersecting paths in this graph were generated (limited to length 5 to reduce computation), and only those paths that included the BNP were retained. Each dependency path was then coded as a functor by retaining the first and last word, and the BNP, with and without the concatenated labels between the words. For example, the Charniak parser produced the following dependency path features: dpath-ALQ(removal,-nsubj1+dobj1+abbrev1,qqqqqq), dpath-AQ(removal,qqqqqq). The first example has an argument for the concatenated edge labels in the path: nsubj(approximated,-4,removal,-5), dobj(approximated,-4,production,-2), abbrev(production,-2,qqqqqq,0). The second example codes the first without the path labels. The suffixes \u2013ALQ and \u2212AQ code the fact that the path starts at a word and ends at the BNP, with and without edge labels. 2.8 Evaluation The result of each machine learning experiment was applied to the test set to score each BNP, and the resulting rank order was used to calculate the BE. In order to compare two machine learning experiments, the McNemar test was used [33] to obtain a test of significance. For that test, a BNP is predicted to be positive if it ranked in the top 4919 (the number of positives in the test set), as this is the precise point at which precision equals recall. The test relies on four numbers n 00 =the number of instances classified incorrectly in both experiments, n 01 =the number of instances classified correctly in the second, but not in the first experiment, n 10 =the number of instances classified correctly in the first, but not in the second experiment, n 11 =the number of instances classified correctly in both experiments. The McNemar statistic is (1) M = ( | n 01 - n 10 | - 1 ) 2 n 01 + n 10 . This statistic is approximately distributed \u03c7 2 ( 1 ) , so that an observed value of M >3.841459 is significant, with p <0.05. 3 Results 3.1 Nominal position While the ultimate goal is to discover all gene names, the problem defined in this study was to identify those BNPs that contained gene names, which is 77.7% of all gene names in the test set. A BNP with a gene mention in head position is itself a reference to a gene, and we expected that this specificity would make the class of such BNPs easier for machine learning to classify than for the class of BNPs containing gene names anywhere within them. In order to examine this, we used SVM learning with features made from binary combinations of word, location, and part of speech (cases A 12 and A 13 in Table 5 and Fig. 1 ). In experiment A 12, a BNP was marked as positive if a gene name occurred anywhere within it; and in experiment A 13, a BNP was marked positive only if a gene name occurred in the head position of the BNP. The BE values were 51.98% within versus 50.33% in head position. However, these test sets have different numbers of positives (4919 within and 4045 in head position), and so we also looked at the area under the ROC curve (AUC) [34], which is considered to be distribution independent. The AUC values were 82.23% within versus 81.71% in head position. We cannot say whether these differences are statistically significant, since the two test sets have a different number of positives. We then asked whether it was possible to learn the difference between BNPs with genes in head position and BNPs with gene names in modifier position. We used the same features as A 12 with a corpus limited to BNPs containing a gene name and with those BNPs marked positive which contain a gene name in head position. Our experiment observed a BE of 87.43%. For a random ranking of the 4919 instances, the 99% upper confidence limit on this value is 83.12%, thus the observed BE is statistically significant, and we can conclude that the concept is meaningful. However, the result is not much above random, suggesting that the distinction is difficult. This provides an explanation for the improved results when both types of BNPs are treated as positives: with both as positives, one no longer has to distinguish between them. Based on this conclusion, and the fact that BNPs containing gene names in any position represent 13.8% more gene names of the gold standard than if we restricted to head position, we chose to use BNPs containing gene names in any position as our definition of positive. All subsequent experiments in the lexical and grammatical study have BNPs marked positive which contain a gene name in any position. 3.2 Lexical features The lexical experiments were conducted in stages, progressively selecting feature combinations that achieve the best possible performance, measured by precision-recall break even (BE). A description of all lexical experiments is given in Table 5, labeled A 1\u2013A 13 and B 1\u2013B 7. Results are shown in Fig. 1. In the first series, we began with words alone as features, and progressed by considering the effect of lower case versus original case words, the effect of including part of speech tags, the effect of including the location of the word relative to the BNP, or merely whether the word occurred before or after, and finally the effect of all three components used in a single feature versus multiple features formed from binary combinations of the components. The first experiment (A 1 =37.53%) uses words alone, without regard to position and without parts of speech. This is slightly above the significance level of 35.31% that is theoretically achievable by ranking BNPs from sentences likely to contain gene names ahead of all others. As the experiments progress, we see an immediate advantage to retaining relative location (A 7 =49.08%) compared with words alone (A 1 =37.53%). We do not see a significant difference between retaining parts of speech (A 3 =37.91% versus A 1 =37.53%, or A 9 =49.58% versus A 7 =49.08%) until we reach the binary combination features (A 12 =51.98% versus A 7 =49.08%). We do not see a significant advantage to retaining the original word case, or to combining original case with lower case. The BE obtained in A 12 =51.98%, was significantly higher than all other BE in the A series. In the second series, we start with features used in A 12, being the best of the first series, and go on to consider limiting features to a window around the BNP, forming compound features, and adding substrings of the word. The window of size 5 (B 3 =53.12%) shows an advantage over a window of size 2 (B 1 =49.81%), and even improves on the non-windowed features (A 12 =51.98%). Forming compound features, i.e. combining pairs of features to make new features, further increases the BE (B 4 =54.46%). Although the McNemar test is significant (p =0.047), we did not feel that the improvement in BE over B 3 justified the added burden of training with the large number of additional features of B 4. Substrings also did not produce a significant improvement (B 5 =52.73%) over the baseline (A 12 =51.98%). But when the features were limited to a window of 5 (B 6 =54.62%) the difference was significant. 3.3 Grammatical features Each of the nine different parser configurations described in Table 4 were used to parse each sentence corresponding to each BNP in the corpus (substituting for the BNP as described above). For each parser, we formed lexical features using the parts of speech produced and the method of combination identical to B 6. Grammatical features were also defined, as described in the Methods section. Machine learning was conducted for each parser using grammatical features (G), lexical features (L), and both (LG), and the results are shown in Fig. 2 . Although the lexical features were limited to a window of 5, as in B 6, the grammatical features were not limited. To assess whether grammatical features give information that is not implied by lexical features, we compare the precision-recall break even (BE) value using lexical features derived from each parser only (L) with the lexical plus grammatical features derived from the same parser (LG). The LG-L differences are in a narrow range from 1.18 to 1.71, and all of them are significant using the McNemar test (p <0.05). The BE values are also in a narrow range between 55.58% for the Charniak-Lease parser to 56.33% for the Charniak parser, and these two values were the only pair that was statistically significant (marginally significant at p =0.03, which should not be considered significant in view of the multiple comparisons). 3.4 Machine learning algorithms In addition to training with support vector machines (SVM), we also tested all nine parser configurations using maximum entropy (MXE), boosted decision trees (BDT), modified Huber (HUB), and na\u00efve Bayes (BAY), and the results are shown in Table 6 . MXE learning produced a higher BE value than all other machine learning algorithms. After this, HUB had an average rank of 2.33 and SVM 2.67, while BDT consistently ranked fourth and BAY was consistently lowest. The comparisons were statistically significant (by the McNemar test) in 2/9 for MXE over HUB and in 4/9 for MXE over SVM while HUB and SVM had no significant differences. The rank of BDT (less than MXE, HUB and SVM) was statistically significant in all cases. The rank of BAY (less than all others) was statistically significant in all cases except for one comparison with BDT. 3.5 Manual evaluation Using the SVM classifier with lexical and grammatical features of the Charniak parser, trained on the GENETAG training set, and evaluated on the 374 BNPs of the manual evaluation, we obtain a BE of 53.10%. This result cannot be directly compared with results on the GENETAG test set (there, the same training produced a BE of 56.33%). However, this suggests that the tasks are roughly equivalent, and that the trained algorithm produces a result reasonably close to the human annotators. 4 Discussion If an algorithm could perfectly classify sentences as to whether they contained a gene name or not, and assigned a score of 1 to the BNPs of those sentences and a score of 0 to all BNPs of sentences not containing a gene name, then for a ranking consistent with these scores but otherwise random, the BE would be less than or equal to 35.31% with 99% confidence. All lexical experiments reported in Fig. 1 (Table 5) and grammatical experiments reported in Fig. 2 (Table 4) produced BEs above 35.31%, and therefore learned something specific about the context of a BNP. Interestingly, a break even of 37.53% was obtained with words alone, with no positional information (experiment B 1). Apparently, the words outside of a BNP can provide significant clues that the BNP contains a gene name, since the only distinction made between two BNPs in the same sentences are the words excluded by the BNP itself. All other experiments gave even more significant results. Although grammatical features yield a statistically significant improvement, the magnitude of the improvement over lexical features is rather small. One possible explanation of this phenomenon could be that lexical features are a strong baseline [13]. Alternatively, the human results suggest significant room for improvement in the task. The Charniak parser achieved a break even of 53.10% on the manual set of 374 sentences, compared with the performance of human annotators at 56.42% and 65.06%. One would expect a share of this possible improvement to be realized from future improvements in parser technology. Our results show, over the nine different parser configurations, an improvement in BE of from 1.18 to 1.71. This improvement may be compared with the improvement seen by Vlachos [8] of 0.08 (F-score) in adding syntactic information from the Rasp parser to a system using word-based and part of speech features. Finkel, et al. report results of biological NER using the Stanford parser to obtain the heads or governors of noun phrases to be used as features [9]. They found such features useful in a setting with many different entity types to be distinguished. But, in experiments on the BioCreative I gene mention task, they did not find these features helpful. Numerical results are not reported, so we are unable to ascertain whether such features were minimally helpful or completely unhelpful. These results found by others justify our approach, in that we are able to see larger effects from the use of grammatical features. By masking out the information coming from the BNP, one can much more effectively study the contributions attributable to different aspects of the context. One of the challenges of the gene mention task is how to use the information coming from a BNP and combine it with the external or contextual information we have used in our study. In many cases, the information contained within a BNP is clearly decisive, and we suspect this causes training to yield high weights for features coming from the BNP. Consequently, in cases where the BNP contains an unknown string, there is a strong bias that is difficult for contextual features to override. This is a subject for future research. Overall we found that the nine different parser configurations gave very comparable improvements over their respective lexical baselines, and were essentially equivalent in their performance on the task. Based on the McNemar test, the only significant difference we found was between the Charniak and Charniak-Lease parsers (p =0.03, which is borderline and in view of the multiple comparisons, also questionable). This agrees with previous findings of Miyao, et al. in which they found that dependency parsing, phrase structure parsing, and deep parsing performed equally well in providing features for a protein\u2013protein interaction task [13]. Compared with their baseline F-score of 51.1%, they saw improvements to 55.1% using the Stanford parser, 55.2% using the Charniak parser, and 55.3% using the Enju parser. None of these results made use of training on biological text, but when retrained on GENIA, the improvements were 55.8%, 56.8%, and 57.2% for the same parsers. Our improvements are smaller than these. However, their task is quite different in that they provide protein name annotations as information given and only ask the classifier to recognize which proteins interact. Parsing based features may be more effective for this task. Another aspect is that we did not see better performance from configurations in which the parser was trained on biological text (Charniak-Lease and Enju). It is unclear what significance to assign to this observation. Even in the protein\u2013protein interaction task reported by Miyao, et al. retraining on biological text did not result in a large improvement. We were somewhat surprised that boosted decision trees (BDTs) did not achieve higher break even scores, since BDTs are able to make complex logical combinations of features to describe much more subtle concepts. A possible explanation is that BDTs iteratively learn concepts from the training set that are highly specific. The concepts learned in most iterations may be so specific that they do not appear in the test set. So it is yet possible that with more training data, BDTs may achieve better performance. Finally, we expected machine learning to be more effective on the problem of recognizing BNPs with gene name in head position than it was in recognizing BNPs containing gene names in any position. Our intuition was that the context of a BNP would tend to imply its semantic type, and that the semantic type of the BNP would be the crucial determinant of whether a gene name was present. Clearly, if the semantic type of a BNP is inferred as \u201cgene\u201d, it should contain a gene name, and in head position. But it is not clear that machine learning makes this distinction effectively. The evidence suggests that the context of a BNP is more strongly indicative of whether the noun mentions a gene, than whether the noun itself is a gene reference. 5 Conclusions Our results show, over the nine different parser configurations we tested, an improvement in BE of from 1.18 to 1.71 due to the addition of grammatical features. All of the parser configurations produced comparable grammatical features, in that there was virtually no difference in the machine learning performance. This is in contrast with the findings of intrinsic evaluations, where significant differences are seen on biological text [11,17]. Acknowledgments We thank Dr. D. Comeau and Dr. L. Tanabe for their contributions in annotation. This research was supported by the Intramural Research Program of the NIH, NLM, and NCBI. Appendix A Union of BNPs We also formed a corpus of BNPs by taking the union of BNPs derived from MedPost, the Charniak-Lease parser, and the Enju parser. In all, this resulted in 144,837 BNPs (compared to 108,361 in the three-way intersection). These BNPs were marked as positive, as above, if they wholly contained a gene name (or acceptable variant) anywhere within, resulting in 6345 positive instances (compared to 4919). As noted above, some of the BNPs in this corpus were overlapping each other, and some genes were contained within more than one BNP. A consequence of using the union of BNPs to define the corpus, which illustrates the inequality of comparison, is that there were several sentences added to the corpus that could not be parsed by one or more of the parsers. Lexical (L) and grammatical (G) features were defined as in the main study, and the three feature sets, G, L, and LG, for each of the nine parser configurations, were used with SVM machine learning (with parameter optimization as described in Appendix B) to predict BNPs containing a gene name. These results appear in Fig. 3 . The break even values are all less than the corresponding results in Fig. 2 by approximately 2.0. This decrease in precision might be expected given that the recall has increased, at least in terms of BNPs. These LG\u2212L differences for each of the nine parsers range from 0.91 to 1.88 and all of them are significant using the McNemar test (p <0.05). The BE values are also in a narrow range between 53.08% for the Bikel+ parser to 54.48% for the Stanford+ parser. In many ways the picture is very similar to that presented by Fig. 2. However, there are some differences in comparing the different parsers. Whereas in Fig. 2, all the parsers are essentially equivalent, here, the Stanford and Stanford+ parsers were significantly different, and strikingly, the Bikel+ parser was significantly worse than all other parsers. These differences appear to be due to sentences that fail to parse. For example, the Bikel+ parser failed to parse 1066 sentences out of 144,837 of the corpus formed from the union of BNPs (but all of the 108,361 sentences of the corpus formed from the intersection of BNPs parse). In comparison, the Stanford+ parser parses all 144,837 sentences of the corpus formed from the union of BNPs. Appendix B Training of machine learning parameters Here we describe our approach to optimization of the parameter C for the SVM algorithm [26] on which this study is based. Suppose we are given training data { ( x \u2192 i , y i ) } i = 1 n where x \u2192 i \u2208 R N and y i \u2208 { - 1 , + 1 } . Following [35], let us define the function (2) g ( z ) = 1 - z if z \u2a7d 1 0 if z > 1 . Then the primal problem solved by the SVM algorithm is to find that weight vector w \u2192 that minimizes the cost function (3) 1 2 w \u2192 T w \u2192 + C n \u2211 i = 1 n g ( y i w \u2192 T x \u2192 i ) . The proper choice of C here is important for optimal learning. If we choose C too small the weight vector will be vary restricted in size and unable to optimally fit the truly informative features of the training data. If, on the other hand, we take C too large there will be too much freedom in choosing w \u2192 and it will over train and fit features of the training data that are the result of noise in the data and this will degrade the quality of the result. Fortunately, as we and others have observed [35], the quality of the learning is not very sensitive to the choice of C . We also make use of an approach that confers additional robustness to the results coming from a particular parameter choice. Define (4) x ave = 1 n \u2211 i = 1 n \u2016 x \u2192 i \u2016 and set (5) C = C \u2032 ( x ave ) 2 . We actually set the parameter C \u2032 and let the algorithm determine C for the particular training data based on (5). It is easy to see from these relations and expression (3) that if w \u2192 is the optimal solution for training data { ( x \u2192 i , y i ) } i = 1 n then for any nonzero constant \u03b1 , 1 \u03b1 w \u2192 is the optimal solution for training data { ( \u03b1 x \u2192 i , y i ) } i = 1 n provided C \u2032 is kept fixed for the two problems. It is this invariance under arbitrary magnifications that confers extra robustness. We borrowed this approach from SVM-Light [36], but are unable to find a discussion in the literature. For our purposes we optimized the choice of C \u2032 using cross validation for the problem A 12 and found the optimal value of 1. We have used this value for all the calculations reported in Figs. 1\u20133. In order to see how close to optimal C \u2032 = 1 is for the best value in Fig. 2 (case LG and the Charniak parser), we examined the results of varying the value of C \u2032 through the values { 2 i } i = - 3 3 and found that 1 is indeed the optimal choice. References [1] Fukuda K, Tsunoda T, Tamura A, Takagi T. Toward information extraction: identifying protein names from biological papers. Proceedings of the Pacific symposium on biocomputing (PSB\u201998); 1998. p. 705\u201316. [2] L. Tanabe W.J. Wilbur Tagging gene and protein names in biomedical text Bioinformatics 18 8 2002 1124 1132 [3] A. Yeh A. Morgan M. Colosimo L. Hirschman BioCreAtIvE Task 1A: gene mention finding evaluation BMC Bioinform 6 Suppl. 1 2005 S2 [4] Wilbur WJ, Smith L, Tanabe L. BioCreative 2. Gene mention task. The second BioCreative challenge evaluation workshop. Madrid, Spain; 2007. p. 7\u201316. [5] Collins M, Singer Y. Unsupervised models for named entity classification. 1999 Joint SIGDAT conference on EMNLP and VLC; 1999. [6] Miller S, Fox H, Ramshaw L, Weischedel R. A novel use of statistical parsing to extract information from text. Sixth applied natural language processing conference; 2000. [7] Ray S, Craven M. Representing sentence structure in hidden markov models for information extraction. 17th International joint conference on artificial intelligence (IJCAI-2001); 2001. [8] Vlachos A. Tackling the BioCreative 2 Gene mention task with conditional random fields and syntactic parsing. Second BioCreative challenge evaluation workshop. Madrid, Spain; 2006. p. 85\u20138. [9] Finkel J, Dingare S, Nguyen H, Nissim M, Manning C, Sinclair G. Exploiting context for biomedical entity recognition: from syntax to the web. JNLPBA; 2004. [10] Erkan G, Ozgur A, Radev D. Semi-supervised classification for extracting protein interaction sentences using dependency parsing. EMNLP; 2007. [11] Sagae K, Miyao T, Matsuzaki T, Tsujii J. Challenges in mapping of syntactic representations for framework-independent parser evaluation. Workshop on automated syntactic annotations for interoperable language resources; 2008. [12] Saetre R, Sagae K, Tsujii J. Syntactic features for protein\u2013protein interaction extraction. LBM; 2007. [13] Miyao Y, Saetre R, Sagae K, Matsuzaki T, Tsujii J. Task-oriented evaluation of syntactic parsers and their representations. ACL-08: HLT; Columbus, OH: Association for computational linguistics; 2008. [14] Carroll J, Minnen G, Briscoe E. Corpus annotation for parser evaluation. EACL-99 Post-conference workshop on linguistically interpreted corpora. Bergen, Norway; 1999. [15] Carroll J, Briscoe T, Sanfilippo A. Parser evaluation: a survey and new proposal. The first international conference on language resources and evaluation (LREC). Granada, Spain; 1998. [16] Miyao Y, Sagae K, Tsujii J. Towards framework-independent evaluation of deep linguistic parsers. Grammar engineering across frameworks. Stanford University; 2007. [17] Clegg AB, Shepherd AJ. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC Bioinform 2007:17. [18] Katrenko S, Adriaans P. Learning relations from biomedical corpora using dependency trees. KDECB; 2006. [19] Miyao Y, Tsujii J. Probabilistic disambiguation models for wide-coverage HPSG parsing. 43rd Annual meeting on association for computational linguistics. Ann Arbor, Michigan: Association for Computational Linguistics; 2005. [20] de Marneffe M-C, MacCartney B, Manning CD. Generating typed dependency parses from phrase structure parses. Proceedings of fifth international conference on language resources and evaluation (LREC2006). Genoa, Italy; 2006. [21] L. Smith T. Rindflesch W. Wilbur MedPost: a part of speech tagger for biomedical text Bioinformatics 20 14 2004 2 [22] Lease M, Charniak E. Parsing biomedical literature. Second international joint conference on natural language processing (IJCNLP\u201905); Korea: Springer; 2005. [23] Evans DA, Lefferts RG, Grefenstette G, Handerson SK, Hersh WR, Archbold AA. CLARIT TREC design, experiments, and results. The first text retrieval conference (TREC-1) 1992. National institute of standards and technology. NIST Special Publication; 1992. p.500\u2013207:251\u201386. [24] Burges CJC. A tutorial on support vector machines for pattern recognition. Available electronically from the author: Bell Laboratories, Lucent Technologies;1999. [25] Joachims T. Text categorization with support vector machines: learning with many relevant features. Dortmund, Germany: Universitat Dortmund1997 November 27, 1997. Report No.: LS-8 Report 23. [26] Joachims T. Training linear SVMs in linear time. Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining. Philadelphia, PA, USA: ACM; 2006. [27] R.E. Schapire Y. Singer Improved boosting algorithms using confidence-rated predictions Machine Learn 37 3 1999 297 336 [28] A.L. Berger S.A.D. Pietra V.J.D. Pietra A maximum entropy approach to natural language processing Comput Linguistics 22 1 1996 39 71 [29] Zhang T. Solving large scale linear prediction problems using stochastic gradient descent algorithms. Twenty-first international conference on machine learning. Banff, Alberta, Canada; 2004. [30] Langley P, Sage S. Induction of selective Bayesian classifiers. Tenth conference on uncertainty in artificial intelligence. Seattle, WA: Morgan Kaufmann; 1994. p. 399\u2013406. [31] Langley P, Iba W, Thompson K. An analysis of Bayesian classifiers. Tenth national conference on artificial intelligence. San Jose: AAAI Press; 1992. p. 223\u20138. [32] Wilbur WJ. Boosting naive bayesian learning on a large subset of MEDLINE. American medical informatics 2000 annual symposium. Los Angeles, CA: American Medical Informatics Association: 2000. p. 918\u201322. [33] T. Dietterich Approximate statistical tests for comparing supervised classification learning algorithms Neural Comput 10 7 1998 29 [34] D.G. Hand R.J. Till A simple generalisation of the area under the roc curve for multiple class classification problems Machine Learn 45 2 2001 16 [35] T. Zhang F.J. Oles Text categorization based on regularized linear classification methods Info Retrieval 4 1 2001 5 31 [36] T. Joachims Making large-scale SVM learning practical B. Scholkopf C. Burges A. Smola Advances in Kernel methods \u2013 support vector learning 1999 MIT-Press [37] Charniak E. A maximum-entropy-inspired parser. The first conference on North American chapter of the association for computational linguistics. San Francisco, CA: Morgan Kaufmann Publishers Inc.; 2000. p. 132\u20139. [38] Bikel DM. Design of a multi-lingual, parallel-processing statistical parsing engine. human language technology conference 2002 (HLT2002). San Diego, CA; 2002. [39] Briscoe E, Carroll J, Watson R. The second release of the rasp system. COLING/ACL 2006 interactive presentation sessions; 2006. [40] Lin D. Principle Based parsing without overgeneration. 31st annual meeting of the association for computational linguistics. Columbus, Ohio; 1993. [41] Klein D, Manning C. Fast exact inference with a factored model for natural language processing. Advances in neural information processing systems; 2002. p. 15. [42] Klein D, Manning C. Accurate unlexicalized parsing. 41st meeting of the association for computational linguistics (ACL\u201903). Sapporo, Japan; 2003.", "scopus-id": "70349456966", "pubmed-id": "19345281", "coredata": {"eid": "1-s2.0-S1532046409000501", "dc:description": "Abstract We measured the extent to which information surrounding a base noun phrase reflects the presence of a gene name, and evaluated seven different parsers in their ability to provide information for that purpose. Using the GENETAG corpus as a gold standard, we performed machine learning to recognize from its context when a base noun phrase contained a gene name. Starting with the best lexical features, we assessed the gain of adding dependency or dependency-like relations from a full sentence parse. Features derived from parsers improved performance in this partial gene mention recognition task by a small but statistically significant amount. There were virtually no differences between parsers in these experiments.", "openArchiveArticle": "true", "prism:coverDate": "2009-10-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046409000501", "dc:creator": [{"@_fa": "true", "$": "Smith, Larry H."}, {"@_fa": "true", "$": "Wilbur, W. John"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046409000501"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046409000501"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(09)00050-1", "prism:volume": "42", "dc:title": "The value of parsing as feature generation for gene mention recognition", "prism:copyright": "Published by Elsevier Inc.", "prism:issueName": "Biomedical Natural Language Processing", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "5", "dcterms:subject": [{"@_fa": "true", "$": "Natural language processing"}, {"@_fa": "true", "$": "Named entity recognition"}, {"@_fa": "true", "$": "Biological text"}, {"@_fa": "true", "$": "Gene mention recognition"}, {"@_fa": "true", "$": "Parsers"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Support vector machines"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "5", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "895-904", "prism:endingPage": "904", "prism:coverDisplayDate": "October 2009", "prism:doi": "10.1016/j.jbi.2009.03.011", "prism:startingPage": "895", "dc:identifier": "doi:10.1016/j.jbi.2009.03.011", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "17", "@width": "97", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "433", "@ref": "si9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "56", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "389", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "76", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "494", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "37", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "332", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si42.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "196", "@ref": "si42", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si41.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "181", "@ref": "si41", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "18", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si40.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "228", "@ref": "si40", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "165", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "921", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "75", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si39.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "408", "@ref": "si39", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si38.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "411", "@ref": "si38", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si37.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "408", "@ref": "si37", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si36.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "413", "@ref": "si36", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "70", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si35.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "403", "@ref": "si35", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "70", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si34.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "406", "@ref": "si34", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "70", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si33.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "410", "@ref": "si33", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "75", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si32.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "385", "@ref": "si32", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "54", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si31.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "365", "@ref": "si31", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si30.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "208", "@ref": "si30", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "61", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "332", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "43", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si29.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "259", "@ref": "si29", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si28.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "208", "@ref": "si28", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si27.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "208", "@ref": "si27", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "86", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si26.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "547", "@ref": "si26", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si25.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "275", "@ref": "si25", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "10", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si24.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si24", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "76", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si23.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "494", "@ref": "si23", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si22.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "210", "@ref": "si22", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si21.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si21", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "208", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "90", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "497", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "22", "@width": "97", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "525", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "116", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "745", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "210", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "177", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1141", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "210", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "189", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "947", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "411", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "263", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "36613", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "108", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4780", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "338", "@width": "534", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "50149", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "139", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7561", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "345", "@width": "534", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "43677", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "141", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046409000501-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6665", "@ref": "gr3", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/70349456966"}}