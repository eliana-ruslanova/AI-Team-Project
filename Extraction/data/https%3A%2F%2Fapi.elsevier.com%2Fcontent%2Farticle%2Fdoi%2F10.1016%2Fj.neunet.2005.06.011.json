{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S089360800500119X", "dc:identifier": "doi:10.1016/j.neunet.2005.06.011", "eid": "1-s2.0-S089360800500119X", "prism:doi": "10.1016/j.neunet.2005.06.011", "pii": "S0893-6080(05)00119-X", "dc:title": "Training neural networks with heterogeneous data ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2005 Special issue\n            ", "prism:issn": "08936080", "prism:volume": "18", "prism:issueIdentifier": "5-6", "prism:startingPage": "595", "prism:endingPage": "601", "prism:pageRange": "595-601", "prism:number": "5-6", "dc:format": "application/json", "prism:coverDate": "2005-08-31", "prism:coverDisplayDate": "July\u2013August 2005", "prism:copyright": "Copyright \u00a9 2005 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "IJCNN 2005", "dc:creator": [{"@_fa": "true", "$": "Drakopoulos, John A."}, {"@_fa": "true", "$": "Abdulkader, Ahmad"}], "dc:description": "\n               Abstract\n               \n                  Data pruning and ordered training are two methods and the results of a small theory that attempts to formalize neural network training with heterogeneous data. Data pruning is a simple process that attempts to remove noisy data. Ordered training is a more complex method that partitions the data into a number of categories and assigns training times to those assuming that data size and training time have a polynomial relation. Both methods derive from a set of premises that form the \u2018axiomatic\u2019 basis of our theory. Both methods have been applied to a time-delay neural network\u2014which is one of the main learners in Microsoft's Tablet PC handwriting recognition system. Their effect is presented in this paper along with a rough estimate of their effect on the overall multi-learner system. The handwriting data and the chosen language are Italian.\n                        1\n                     \n                     \n                        1\n                        An abbreviated version of some portions of this article appeared in Drakopoulos and Abdulkader, 2005, published under the IEEE copyright.\n                     \n                  \n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Heterogeneous data"}, {"@_fa": "true", "$": "Neural networks"}, {"@_fa": "true", "$": "Training schedule"}, {"@_fa": "true", "$": "Data emphasizing"}, {"@_fa": "true", "$": "Boosting"}, {"@_fa": "true", "$": "Growing cell structure"}, {"@_fa": "true", "$": "Neural gas"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S089360800500119X", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S089360800500119X", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "27744462407", "scopus-eid": "2-s2.0-27744462407", "pubmed-id": "16095874", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/27744462407", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20050810", "$": "2005-08-10"}}}}}