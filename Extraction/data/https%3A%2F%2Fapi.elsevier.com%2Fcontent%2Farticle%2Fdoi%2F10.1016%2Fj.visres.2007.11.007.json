{"scopus-eid": "2-s2.0-38549125396", "originalText": "serial JL 271122 291210 291684 291736 31 80 Vision Research VISIONRESEARCH 2008-02-01 2008-02-01 2010-11-17T12:21:58 1-s2.0-S0042698907005032 S0042-6989(07)00503-2 S0042698907005032 10.1016/j.visres.2007.11.007 S300 S300.1 FULL-TEXT 1-s2.0-S0042698908X07515 2015-05-14T02:56:28.155321-04:00 0 0 20080101 20080131 2008 2008-02-01T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table e-component body acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 0042-6989 00426989 48 48 2 2 Volume 48, Issue 2 8 235 243 235 243 200801 January 2008 2008-01-01 2008-01-31 2008 Regular Articles article fla Copyright \u00a9 2007 Elsevier Ltd. All rights reserved. AMACHINELEARNINGPREDICTORFACIALATTRACTIVENESSREVEALINGHUMANLIKEPSYCHOPHYSICALBIASES KAGIAN A 1 Introduction 2 Materials and methods 2.1 Rating the facial database 2.2 Data preprocessing and representation 2.3 Predictor construction and validation 2.3.1 Experiment 1 2.3.2 Experiment 2 2.3.3 Experiment 3 2.4 Facial features and attractiveness 3 Results 3.1 Prediction accuracy of facial attractiveness 3.2 Similarity of machine and human judgments 3.3 Human-like biases in the machine\u2019s performance 3.3.1 Experiment 1: The averageness hypothesis: A preference for averaged face composites 3.3.2 Experiment 2: Perfectly symmetric averaged faces 3.3.3 Experiment 3: Asymmetry of facial attractiveness perception 3.4 Facial features and attractiveness 4 Discussion Acknowledgments Appendix A Supplementary data References ALLEY 1991 123 125 T ANDERSSON 1994 M SEXUALSELECTION BECKER 1999 347 374 S CUNNINGHAM 1986 925 935 M CUNNINGHAM 2002 M ADVANCESINVISUALCOGNITIONVOL1FACIALATTRACTIVENESS DIMENSIONSFACIALPHYSICALATTRACTIVENESSINTERSECTIONBIOLOGYCULTURE CUNNINGHAM 1995 261 279 M DAILEY 2002 1158 1173 M EISENTHAL 2006 119 142 Y FINK 2001 92 99 B GALTON 1878 132 142 F GRAF 2006 143 165 A GRAMMER 1994 233 242 K GRAMMER 2002 K ADVANCESINVISUALCOGNITIONVOL1FACIALATTRACTIVENESS FEMALEFACESBODIESNDIMENSIONALFEATURESPACEATTRACTIVENESS HALBERSTADT 2003 149 156 J HJELMAS 2001 236 274 E JOHNSTON 1993 183 199 V LANGLOIS 1990 115 121 J LANGLOIS 1987 363 369 J LITTLE 2002 A ADVANCESINVISUALCOGNITIONVOL1FACIALATTRACTIVENESS EVOLUTIONINDIVIDUALDIFFERENCESINPERCEPTIONATTRACTIVENESSHOWCYCLICHORMONALCHANGESSELFPERCEIVEDATTRACTIVENESSINFLUENCEFEMALEPREFERENCESFORMALEFACES MOLLER 1997 A ASYMMETRYDEVELOPMENTALSTABILITYEVOLUTION OTOOLE 1999 9 19 A PERRETT 1994 239 242 D REBER 2004 364 382 R RHODES 1999 52 58 G RUBENSTEIN 2002 A ADVANCESINVISUALCOGNITIONVOL1FACIALATTRACTIVENESS MAKESAFACEATTRACTIVEROLEAVERAGENESSINDEFININGFACIALBEAUTY SCHAEFFER 2002 1 7 J SLEZAK 1991 175 201 P THORNHILL 1999 452 460 R ZAIDEL 1995 649 655 D ZEBROWITZ 2002 L ADVANCESINVISUALCOGNITIONVOL1FACIALATTRACTIVENESS NATURELETAHUNDREDFLOWERSBLOOMMULTIPLEWAYSWHEREFORESATTRACTIVENESS KAGIANX2008X235 KAGIANX2008X235X243 KAGIANX2008X235XA KAGIANX2008X235X243XA 2013-07-18T13:41:53Z OA-Window Full ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S0042-6989(07)00503-2 S0042698907005032 1-s2.0-S0042698907005032 10.1016/j.visres.2007.11.007 271122 2010-12-21T18:10:59.251772-05:00 2008-01-01 2008-01-31 1-s2.0-S0042698907005032-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/MAIN/application/pdf/72ef177573502e8a7c7be9081401aad1/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/MAIN/application/pdf/72ef177573502e8a7c7be9081401aad1/main.pdf main.pdf pdf true 491921 MAIN 9 1-s2.0-S0042698907005032-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/PREVIEW/image/png/b4c031a08471632999cbb70084078c93/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/PREVIEW/image/png/b4c031a08471632999cbb70084078c93/main_1.png main_1.png png 65371 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0042698907005032-mmc1.doc https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/mmc1/MAIN/application/msword/694e32992fa103a804ae1bd0397b926b/mmc1.doc https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/mmc1/MAIN/application/msword/694e32992fa103a804ae1bd0397b926b/mmc1.doc mmc1 mmc1.doc doc 311808 APPLICATION 1-s2.0-S0042698907005032-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr1/DOWNSAMPLED/image/jpeg/9fb6fd9ca40aebeca8565b0cdfcbf4fb/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr1/DOWNSAMPLED/image/jpeg/9fb6fd9ca40aebeca8565b0cdfcbf4fb/gr1.jpg gr1 gr1.jpg jpg 22137 228 203 IMAGE-DOWNSAMPLED 1-s2.0-S0042698907005032-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr1/THUMBNAIL/image/gif/a84afbd0bb025949d90b3d110c13303a/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr1/THUMBNAIL/image/gif/a84afbd0bb025949d90b3d110c13303a/gr1.sml gr1 gr1.sml sml 4771 93 83 IMAGE-THUMBNAIL 1-s2.0-S0042698907005032-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr1/HIGHRES/image/jpeg/b2d13505dd990a4660e3698f4e4ac035/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr1/HIGHRES/image/jpeg/b2d13505dd990a4660e3698f4e4ac035/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 84623 707 630 IMAGE-HIGH-RES 1-s2.0-S0042698907005032-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr2/DOWNSAMPLED/image/jpeg/c92370f9ba913c9ce802357b9e2c0a9a/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr2/DOWNSAMPLED/image/jpeg/c92370f9ba913c9ce802357b9e2c0a9a/gr2.jpg gr2 gr2.jpg jpg 20339 320 380 IMAGE-DOWNSAMPLED 1-s2.0-S0042698907005032-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr2/THUMBNAIL/image/gif/ed11ed41a5f65094311f175f74671ec4/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr2/THUMBNAIL/image/gif/ed11ed41a5f65094311f175f74671ec4/gr2.sml gr2 gr2.sml sml 2123 94 111 IMAGE-THUMBNAIL 1-s2.0-S0042698907005032-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr2/HIGHRES/image/jpeg/137955dc466fc6d3373ce59af4d85a0c/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr2/HIGHRES/image/jpeg/137955dc466fc6d3373ce59af4d85a0c/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 74376 1090 1294 IMAGE-HIGH-RES 1-s2.0-S0042698907005032-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr3/DOWNSAMPLED/image/jpeg/43a47f3b7c6fc412de9bcd8d48982131/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr3/DOWNSAMPLED/image/jpeg/43a47f3b7c6fc412de9bcd8d48982131/gr3.jpg gr3 gr3.jpg jpg 35517 262 539 IMAGE-DOWNSAMPLED 1-s2.0-S0042698907005032-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr3/THUMBNAIL/image/gif/81ed3d5fcb4f529c020f66fe294b549a/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr3/THUMBNAIL/image/gif/81ed3d5fcb4f529c020f66fe294b549a/gr3.sml gr3 gr3.sml sml 2407 61 125 IMAGE-THUMBNAIL 1-s2.0-S0042698907005032-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr3/HIGHRES/image/jpeg/efc89ebe0b7d2bf0afd37634470c10e5/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr3/HIGHRES/image/jpeg/efc89ebe0b7d2bf0afd37634470c10e5/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 161154 891 1834 IMAGE-HIGH-RES 1-s2.0-S0042698907005032-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr4/DOWNSAMPLED/image/jpeg/15b7ddc73b5627738c7d7c7695bd4195/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr4/DOWNSAMPLED/image/jpeg/15b7ddc73b5627738c7d7c7695bd4195/gr4.jpg gr4 gr4.jpg jpg 54707 381 572 IMAGE-DOWNSAMPLED 1-s2.0-S0042698907005032-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr4/THUMBNAIL/image/gif/10901aa34340d64e949ee98bf80618d6/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr4/THUMBNAIL/image/gif/10901aa34340d64e949ee98bf80618d6/gr4.sml gr4 gr4.sml sml 2983 83 125 IMAGE-THUMBNAIL 1-s2.0-S0042698907005032-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr4/HIGHRES/image/jpeg/ed35e012f867208aac5dfa2b748da1bd/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr4/HIGHRES/image/jpeg/ed35e012f867208aac5dfa2b748da1bd/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 273701 1298 1947 IMAGE-HIGH-RES 1-s2.0-S0042698907005032-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr5/DOWNSAMPLED/image/jpeg/5ae397710e88482ce3a5e71ea5d30ed5/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr5/DOWNSAMPLED/image/jpeg/5ae397710e88482ce3a5e71ea5d30ed5/gr5.jpg gr5 gr5.jpg jpg 53383 378 570 IMAGE-DOWNSAMPLED 1-s2.0-S0042698907005032-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr5/THUMBNAIL/image/gif/b0b6e85f102cbe9f9669a57b2d8f1688/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr5/THUMBNAIL/image/gif/b0b6e85f102cbe9f9669a57b2d8f1688/gr5.sml gr5 gr5.sml sml 2958 83 125 IMAGE-THUMBNAIL 1-s2.0-S0042698907005032-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698907005032/gr5/HIGHRES/image/jpeg/40f4aea9149a65e50e67f88e467dbd40/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698907005032/gr5/HIGHRES/image/jpeg/40f4aea9149a65e50e67f88e467dbd40/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 281830 1285 1938 IMAGE-HIGH-RES VR 5220 S0042-6989(07)00503-2 10.1016/j.visres.2007.11.007 Elsevier Ltd Fig. 1 Facial coordinates with hair and skin sample regions as represented by the facial feature extractor. Coordinates are used for calculating geometric features and asymmetry. Sample regions are used for extracting color values and smoothness. (The sample image, used for illustration only, is of T.G. and is presented with her full consent.) Fig. 2 Distribution of mean Euclidean distance from each human rater to all other raters in the ratings space. The machine\u2019s average distance from all other raters (left bar) is smaller than the average distance of each of the human raters to all others. Fig. 3 Location of machine ratings among the 28 human ratings. Ratings were projected into 2 dimensions (a) and 3 dimensions (b) by performing PCA on all ratings and projecting them on the first principal components. The projected data explain 29.8% of the variance in (a) and 36.6% in (b). Fig. 4 Experiment 1: The averageness hypothesis: (a) percent of components that were rated as less attractive than their corresponding composite, accompanied with mean scores of composites and the mean scores of their components (scores are normalized to the range [0,1], actual attractiveness scores are reported in Table 1). (b) Mean values of smoothness and asymmetry of 1000 composites for each number of components, n c. Fig. 5 Mean results over 1000 perfectly symmetric composites made of varying numbers of perfectly symmetric image components: (a) percent of components which were rated as less attractive than their corresponding composite, accompanied with mean scores of composites and the mean scores of their components (scores are normalized to the range [0,1]). (b) Mean values of smoothness and asymmetry of 1000 composites for each number of components, n c. Table 1 Mean results over 1000 composites made of varying numbers of component images Number of components in composite Mean composite score Mean components score Components rated lower than composite (%) 2 3.46 3.34 55 4 3.66 3.33 64 12 3.74 3.32 70 25 3.82 3.32 75 50 3.94 3.33 81 A machine learning predictor of facial attractiveness revealing human-like psychophysical biases Amit Kagian a amit.kagian@gmail.com Gideon Dror b Tommer Leyvand a Isaac Meilijson c Daniel Cohen-Or a Eytan Ruppin a d \u204e ruppin@post.tau.ac.il a School of Computer Sciences, Tel-Aviv University, Tel-Aviv 69978, Israel b School of Computer Sciences, The Academic College of Tel-Aviv-Yaffo, Tel-Aviv 64044, Israel c School of Mathematical Sciences, Tel-Aviv University, Tel-Aviv 69978, Israel d School of Medicine, Tel-Aviv University, Tel-Aviv 69978, Israel \u204e Corresponding author. Address: School of Computer Sciences and School of Medicine, Tel-Aviv University, Tel-Aviv 69978, Israel. Fax: +972 3 640 9357. Abstract Recent psychological studies have strongly suggested that humans share common visual preferences for facial attractiveness. Here, we present a learning model that automatically extracts measurements of facial features from raw images and obtains human-level performance in predicting facial attractiveness ratings. The machine\u2019s ratings are highly correlated with mean human ratings, markedly improving on recent machine learning studies of this task. Simulated psychophysical experiments with virtually manipulated images reveal preferences in the machine\u2019s judgments that are remarkably similar to those of humans. Thus, a model trained explicitly to capture a specific operational performance criteria, implicitly captures basic human psychophysical characteristics. Keywords Face perception Facial attractiveness Machine learning Aesthetics Computational neuroscience 1 Introduction Philosophers, artists and scientists have been trying to capture the nature of beauty since the early days of philosophy. Although in modern days a common layman\u2019s notion is that judgments of beauty are a matter of subjective opinion alone, recent findings suggest that people share a common taste for facial attractiveness and that their preferences may be an innate part of our primary constitution. Several experiments have shown that 2\u20138months old infants prefer looking at faces rated by adults as more attractive (Langlois et al., 1987). In addition, attractiveness ratings show very high agreement between groups of raters belonging to the same culture and even across cultures (Cunningham, Roberts, Wu, Barbee, & Druen, 1995). Such findings give rise to the quest for common factors which determine human facial attractiveness. Accordingly, various hypotheses, from cognitive, evolutional and social perspectives, have been put forward to describe and interpret the common preferences for facial beauty. Inspired by Sir Francis Galton\u2019s photographic method of composing faces (Galton, 1878), Langlois and Rogmann have created averaged faces by morphing multiple images together. Human judges found these averaged faces to be attractive and rated them with attractiveness ratings higher than the mean rating of the component faces composing them, proposing that averageness is the answer for facial attractiveness (Langlois & Roggman, 1990; Rubenstein, Langlois, & Roggman, 2002). Investigating symmetry and averageness of faces, Grammer and Thornhill concluded that symmetry was more important than averageness in facial attractiveness (Grammer & Thornhill, 1994). Other studies have agreed that average faces are attractive but claim that faces with certain extreme features, such as extreme sexually dimorphic traits, may be more attractive than average faces (Little, Penton-Voak, Burt, & Perrett, 2002). Yet other researchers have suggested various conditions which may contribute to facial attractiveness such as neonate features, pleasant expressions and familiarity (Zebrowitz & Rhodes, 2002). Finally, Cunningham et al. have suggested a multiple fitness model in which there is no single constructing line that determines attractiveness (e.g. perception of fitness as implying an ideal romantic partner). Instead, different categories of features signal different desirable qualities of the perceived target (Cunningham, Barbee, & Philhower, 2002). Even so, the multiple fitness model agrees that some facial qualities are universally physically attractive to people. Apart from eliciting the facial characteristics which account for attractiveness, modern researchers have aimed to describe the mechanisms underlying these preferences. Many contributors refer to the evolutionary origins of attractiveness preferences (Andersson, 1994; M\u00f8ller & Swaddle, 1997; Thornhill & Gangsted, 1999). According to this view, facial traits signal mate quality and imply chances for reproductive success and parasite resistance. Some evolutionary theorists suggest that preferred features might not signal mate quality but that the \u201cgood taste\u201d by itself is an evolutionary adaptation (individuals with a preference for attractiveness will have attractive offspring that will be favored as mates) (Thornhill & Gangsted, 1999). Another mechanism explains attractiveness\u2019 preferences through a cognitive theory\u2014a preference for attractive faces might be induced as a by-product of general perception or recognition mechanisms (Rubenstein et al., 2002; Zebrowitz & Rhodes, 2002): attractive faces might be pleasant to look at since they are closer to the cognitive representation of the face category in the mind. Halberstadt and Rhodes have further demonstrated that not just average faces are attractive but also birds, fish, and automobiles become more attractive after being averaged with computer manipulation (Halberstadt & Rhodes, 2003). Such findings led researchers to propose that as perceivers can process an object more fluently, aesthetic response becomes more positive (Reber, Schwarz, & Winkielman, 2004). A third view suggests that facial attractiveness originates in a social mechanism, where preferences may be dependent on the learning history of the individual and even on his social goals (Zebrowitz & Rhodes, 2002). Other studies have used computational methods to analyze facial attractiveness. In several cases faces were averaged using morphing tools (e.g. Perrett, May, & Yoshikawa, 1994; Rubenstein et al., 2002). Laser scans of faces were put into complete correspondence with the average face in order to examine the relationship between facial attractiveness, age and averageness (O\u00b4Toole, Price, Vetter, Bartlett, & Blanz, 1999). A genetic algorithm, guided by interactive user selections was programmed to evolve a \u201cmost beautiful\u201d female face (Johnston & Franklin, 1993). Machine learning methods have been used recently to investigate whether a machine can predict attractiveness ratings by learning a mapping from facial images to their attractiveness scores (Eisenthal, Dror, & Ruppin, 2006). The latter predictor achieved a correlation of 0.6 with average human ratings, demonstrating that facial beauty can be learned by a machine, at least to some moderate extent. However, as human raters significantly outperform the predictor of Eisenthal et al., the challenge of constructing a facial attractiveness machine predictor with human-level accuracy has remained open. A primary goal of this study is to surpass these results by developing a machine which obtains human-level performance in predicting facial attractiveness and, thus, passes what Kurzweil calls a subject matter expert turing test (SME TT) (Kurzweil, 2005). Having accomplished this, our second main goal is to conduct a series of simulated psychophysical experiments and study the resemblance between human and machine judgments. This latter task carries two potential rewards: first, to determine whether the machine can aid in understanding the psychophysics of human facial attractiveness, capitalizing on the ready accessibility of manipulating and studying its performance, and second, to study whether learning an explicit operational ratings prediction task also entails learning implicit human-like biases, at least for the case of facial attractiveness. In the past decades machines have achieved human-level performance in rule-based systems such as playing games (Schaeffer & Herik, 2002) and in various expert systems (Slezak, 1991). Impressive progress has been displayed in simulating various tasks which involve face perception, such as face detection (Hjelmas & Low, 2001), face recognition (Becker, 1999; Zhao, Chellappa, Rosenfeld, & Phillips, 2000) and tasks of facial category learning such as emotion (Dailey, Cottrell, Padgett, & Adolphs, 2002) and gender (Graf, Wichmann, B\u00fclthoff, & Sch\u00f6lkopf, 2006) recognition. The task of evaluating human attractiveness ratings adds the notion of judgment of taste to the previous achievements in machine perception of faces. Learning the concept of facial attractiveness could form an important demonstration of a computer\u2019s ability to learn to master a quantitative, basic, human judgment task. To this end we have collected human scores of facial attractiveness for a given dataset of female facial images. We developed an algorithm for automatic extraction of a very large set of geometric facial features, which, combined with a set of global features, yields a principled representation of each facial image via a set of image-features in an appropriate dimension-reduced space. Using this data of facial representations and their associated rating scores, we have employed standard supervised learning algorithms to construct a facial attractiveness prediction machine. Given a new, unseen face, this machine predicts its human attractiveness score in an accurate manner. We then turned to performing a series of simulated psychophysical experiments, modeled after known experiments in the psychological literature, to study the resemblance between human and machine preferences. These experiments are particularly interesting since the machine is trained on an explicit operational ratings prediction task with no defined instructions specifying the human-like biases in question. 2 Materials and methods 2.1 Rating the facial database The chosen database was composed of 91 facial images of American females, taken by the Japanese photographer Akira Gomi. All 91 samples were frontal color photographs of young Caucasian females with a neutral expression. All samples were of similar age, skin color and gender. The subjects\u2019 portraits had no accessories or other distracting items such as jewelry. We focused on female faces since experimental results shows that there is a greater agreement on human ratings of female faces while male face preferences are more largely influenced by the menstrual cycle and self-perceived attractiveness of the raters (Little et al., 2002). All 91 facial images in the dataset were rated for attractiveness by 28 human raters (15 males, 13 females) on a 7-point Likert scale (1=very unattractive, 7=very attractive). Ratings were collected with a specifically designed html interface. Each rater was asked to view the entire set before rating in order to acquire a notion of attractiveness scale. There was no time limit for judging the attractiveness of each sample and raters could go back and adjust the ratings of previously rated samples. The images were presented to each rater in a random order and each image was presented on a separate page. The final attractiveness rating of each sample was its mean rating across all raters. To validate that the number of ratings collected adequately represented the \u201ccollective attractiveness rating\u201d we randomly divided the raters into two disjoint groups of equal size. For each facial image, we calculated the mean rating on each group, and calculated the Pearson correlation between the mean ratings of the two groups. This process was repeated 1000 times. The mean correlation between two groups was 0.92 (\u03c3 =0.01). It should be noted that the split-half correlations reported were high in all 1000 trials (as evident from the low standard deviation) and not only over the average. This correlation corresponds well to the known level of consistency among groups of raters reported in the literature (e.g. Cunningham et al., 1995). Hence, the mean ratings collected are stable indicators of attractiveness that can be used for the learning task. The facial set contained faces in all ranges of attractiveness. Final attractiveness ratings range from 1.42 to 5.75 and the mean rating was 3.33 (\u03c3 =0.94). 2.2 Data preprocessing and representation Preliminary experimentation with various ways of representing a facial image (e.g. Eisenthal et al., 2006) have systematically shown that features based on measured proportions, distances and angles of faces are most effective in capturing the notion of facial attractiveness. To extract facial features we developed an automatic engine that is capable of identifying eyes, nose, lips, eyebrows and head contour. In total, we measured 84 coordinates describing the locations of those facial features (Fig. 1 ). Several regions are automatically suggested for sampling mean hair color, mean skin color and skin texture. The feature extraction process was basically automatic but some coordinates needed to be manually adjusted in some of the images. The facial coordinates are used to create a distances-vector of all 3486 distances between all pairs of coordinates in the complete graph created by all coordinates. For each image, all distances are normalized by face length (as measured from the coordinate at the top of the forehead to the coordinate at the bottom of the chin). In a similar manner, a slopes-vector of all the 3486 slopes of the lines connecting the facial coordinates is computed. Central fluctuating asymmetry (CFA) is calculated from the coordinates as well. CFA corresponds to the sum of the absolute values of the differences of the midpoints of adjacent horizontal lines which connect matching bilateral facial coordinates (see Grammer & Thornhill, 1994). The application also provides, for each face, hue, saturation and value (HSV) values of hair color and skin color, and a measurement of skin smoothness. Smoothness of skin was calculated with an edge-detection algorithm in which many detected edges suggest a low level of skin smoothness. Combining the distances-vector and the slopes-vector yields a vector representation of 6972 geometric features for each image. Since strong correlations are expected among the features in such representation, principal component analysis (PCA) was applied to the 6972 geometric features, producing 90 principal components that span the sub-space defined by the 91 image vector representations. The geometric features are projected on those 90 principal components to produce 90 decorrelated eigenfeatures representing the geometric features of the images. Eight non-geometric measured features were not included in the PCA analysis, including CFA, smoothness, hair color coordinates (HSV) and skin color coordinates. These features are assumed to be directly connected to human perception of facial attractiveness and are hence kept at their original values. These 8 features were added to the 90 geometric eigenfeatures, resulting in a total of 98 image-features representing each facial image in the dataset. 2.3 Predictor construction and validation We experimented with several induction algorithms including simple Linear Regression, Least Squares Support Vector Machine (LS-SVM) both linear as well as non-linear (Suykens, Van Gestel, De Brabanter, De Moor, & Vandewalle, 2002) and Gaussian Processes (GP) (Rasmussen & Williams, 2006). However, as the LS-SVM and GP showed no substantial advantage over Linear Regression, the latter was used and is presented in the sequel. A key ingredient in our method is to use a proper image-features selection strategy. To this end we used subset feature selection, implemented by ranking the image-features by their Pearson correlation with the target. Other ranking functions produced no substantial gain. To measure the performance of our method we removed one sample from the whole dataset. This sample served as a test set. We found, for each left out test sample, the optimal number of image-features by performing leave-one-out-cross-validation (LOOCV) on the remaining samples and selecting the number of features that minimized the absolute difference between the algorithm\u2019s output and the targets of the training set. Ranking of features was conducted independently for each held out image and performance was measured by aggregating together the scores of all images. In other words, while setting aside a test sample, we used LOOCV on the remaining training samples in order to optimize the number of features to select, mi , and afterwards used this number of features to predict a single, fixed attractiveness score for the left out test sample, that is, the score for a test example was predicted using a single model based on the training set only. This process was repeated n =91 times, once for each image sample, resulting with a vector of attractiveness predictions for all images. In order to avoid overfitting, the entire learning procedure (including feature selection) is repeated from scratch for each data partition, so that a different number of features are selected for each data partition. The number of selected features, mi , ranges between 50 and 77. mi =67 features were most frequently selected. To examine the influence of resetting the number of selected features at each fold, we tested the predictor in a leave-one-out-cross-validation, on the entire dataset, while keeping the number of selected features constant in all iterations (and not reselecting it each time). The fixed number of selected features ranged between m =1 (a single feature) and m =98 (all features). The best Pearson correlation of 0.87 was achieved with m =64. These 64 features include 7 of the 8 non-geometric features (all but Hair-hue). The remaining 57 geometric eigenfeatures explain 96% of the variance of the geometric features. For all 54\u2a7d m \u2a7d74, Pearson and Spearman correlations were above 0.8. As is evident, a fixed number of selected features can yield better performance than the one our method of reselection produced. Still, we did not use a fixed value for the number of selected features, m, since we did not want to rely on test performance when choosing the constant value of m in order to avoid overfitting. It should be noted that we tried to use the same feature selection and training procedure with the original geometric features (without PCA) instead of using the eigenfeatures. This, however, has failed to produce good predictors due to strong correlations between the original geometric features (the maximal Pearson correlation obtained was 0.26). Once the predictor was constructed and validated, we turn to simulate a number of psychophysical experiments that were previously conducted with human subjects. 2.3.1 Experiment 1 We created virtual face composites for the machine to rate by simulating a morphing technique similar to the one used by Rubenstein et al. (2002). Coordinate values of the original component faces were averaged to create a new set of coordinates for the virtual face composite. These coordinates were used to calculate the geometrical features and CFA of the averaged face. Smoothness and HSV values for the composite face were calculated by averaging the corresponding values of the component faces (HSV values are converted to RGB before averaging). To study the effect of the number of component faces, n c, on the attractiveness score of face composites we produced 1000 virtual morph images for each value of n c between 2 and 50, and used our attractiveness predictor to compute the attractiveness scores of the resulting composites. 2.3.2 Experiment 2 In order to further examine the importance of symmetry on the machine\u2019s attractiveness judgments of averaged composites, we repeated the virtual composites experiment (Experiment 1) using perfectly symmetric faces as image components. Perfectly symmetric virtual versions of the original images were created by a similar technique to the one used by Rhodes, Sumich, and Byatt (1999), that is, each original face was virtually morphed with its mirror image in order to create a perfectly symmetric version of it. Averaging together perfectly symmetric component faces produces perfectly symmetric face composites. In the same manner as in Experiment 1, 1000 virtual composites were created for each number of components, n c, between 2 and 50, and the machine rated them for attractiveness. 2.3.3 Experiment 3 Analogously to Zaidel, Chen, and German (1995) who have created chimeric facial composites by attaching one-half of the face to its mirror image, Right\u2013right and left\u2013left virtual chimeric composites were produced from the extracted coordinates of all original images and the machine was used to predict their attractiveness ratings. Learning was repeated for each chimeric composite with the original image used for each chimeric composition being excluded from the training set, to avoid a misleading positive bias as a consequence of the fact that the original image contains many features which are identical to those of the matching composite. 2.4 Facial features and attractiveness The original measured facial features were ranked according to their correlation to human and machine ratings and the Spearman rank correlation between the two rankings was calculated. This analysis was repeated three times: (a) with 6980 predictor-features, (b) with 28 features investigated in previous studies and (c) with 13 features previously found to be significantly related to facial attractiveness. To determine the P-value of the rank correlation we repeated the features ranking 100 times with shuffled machine and human ratings. In none of the shuffled trials was the rank correlation as high as with the actual ratings. The 28 features we focused on are (features marked with an \u2217 were previously found to be significantly correlated with facial attractiveness): (1) forehead height, (2) eye height\u2217, (3) eye width\u2217, (4) separation of eyes\u2217, (5) nose tip width, (6) nostril width\u2217, (7) nose length (to eye top), (8) nose area\u2217, (9) upper lip thickness, (10) lower lip thickness, (11) chin length\u2217, (12) cheekbone width\u2217, (13) jaw (cheek) width\u2217, (14) mid-face length, (15) eyebrow height\u2217, (16) mouth width\u2217 (features 1\u201316 are taken from Cunningham, 1986), (17) outer eye corner width\u2217, (18) inner eye corner width, (19) cheek width, (20) cheekbone prominence\u2217, (21) lower face proportions (features 17\u201312 are taken from Grammer & Thornhill, 1994), (22) forehead height (to eyes), (23) brow height, (24) brow curvature, (25) lower face length, (26) nose length, (27) mouth height\u2217, (28) cheekbone height (features 22\u201328 are taken from Grammer, Fink, Juette, Ronzal, & Thornhill, 2002). (See Supporting Information for a detailed description of the calculation of these 28 feature measurements according to the raw coordinate representation.) 3 Results 3.1 Prediction accuracy of facial attractiveness Machine attractiveness ratings of all sample images obtained a high Pearson correlation of 0.82 (P-value<10\u221223) with the mean ratings of human raters (the learning targets), corresponding to a normalized mean squared error of 0.39. This accuracy is a marked improvement over the recently published performance results of a Pearson correlation of 0.6 on a similar dataset (Eisenthal et al., 2006). The average correlation of an individual human rater to the mean ratings of all other human raters in our dataset is 0.67 and the average correlation between the mean ratings of groups of human raters is 0.92 (see Section 2). The Spearman rank correlation between machine and mean human ratings is 0.83. To further validate the correlation measures we removed the most attractive 12% and the least attractive 12% of the samples from the dataset and recalculated the correlations. Correlation values remained high with 0.80 (Pearson) and 0.81 (Spearman). To get a notion of the contribution of the 8 global, non-geometric features to attractiveness prediction, we have trained the predictor while removing them one at a time. This resulted in correlations of 0.68 when excluding asymmetry, 0.80 when excluding smoothness, 0.77 when excluding hair color (3 attributes) and 0.77 when excluding skin color (3 attributes). Excluding all non-geometric features and using geometric features alone yielded a correlation of 0.74. 3.2 Similarity of machine and human judgments The ratings of each rater (28 human raters and the machine predictor) form a 91 dimensional rating vector describing its attractiveness ratings of all 91 images. These vectors can be embedded in a 91 dimensional ratings space. The Euclidean distance between all raters (human and machine) in this space was computed. Compared with each of the human raters, the ratings of the machine were the closest, on average, to the ratings of all other human raters (Fig. 2 ). Although, by construction, the machine\u2019s rating vector lies near the mean of human ratings, it may still be very different from any individual human rating vector. This may happen, e.g. when the distribution of human ratings forms several clusters or is non-convex. To assure this is not the case, we counted the number of human ratings vectors within small multidimensional spheres around each human rater as well as the rating of the machine. The machine had more human neighbors than the mean number of neighbors that a human rater had, even when the radiuses of the spheres were very small, testifying that it does not fall between clusters. Finally, to visualize the machine ratings among human ratings we applied PCA to machine and human ratings in the rating space and projected all ratings onto the resulting first 2 and 3 principal components. Indeed, the machine is well placed in a mid-zone of human raters (Fig. 3 ). 3.3 Human-like biases in the machine\u2019s performance 3.3.1 Experiment 1: The averageness hypothesis: A preference for averaged face composites Rubenstein et al. (2002) discuss a morphing technique to create mathematically averaged faces from multiple face images. They report that averaged faces made of 16 and 32 original component images were rated by humans higher in attractiveness than the mean attractiveness ratings of their component faces and higher than composites consisting of fewer faces. In their experiment, 32-component composites were found to be the most attractive. In accordance with these experimental results, the predictor manifests a human-like bias for higher scores for averaged composites over their components\u2019 mean score. Fig. 4 a shows the percent of components which were rated as less attractive than their corresponding composite, for each number of components n c. As evident, the attractiveness rating of a composite surpasses a larger percent of its components\u2019 ratings as n c increases. Fig. 4a also shows the mean scores of 1000 composites and the mean scores of their components, for each n c (scores are normalized to the range [0,1]). Their actual attractiveness scores are reported in Table 1 . As expected, the mean scores of the component images are independent of n c, while composites\u2019 scores increase with n c (see Supporting Information for a more detailed analysis of the difference between composites and components scores). Recent studies have provided evidence that skin texture influences judgments of facial attractiveness (Fink, Grammer, & Thornhill, 2001). Since blurring and smoothing of faces occur when faces are averaged together (Rubenstein et al., 2002), the smooth complexion of composites may underlie the attractiveness of averaged composites. In our experiment, a preference for averageness is found even though our method of virtual-morphing does not produce the smoothening effect and the mean smoothness value of composites corresponds to the mean smoothness value in the original dataset, for all n c (see Fig. 4b). Researchers have also suggested that averaged faces are attractive since they are exceptionally symmetric (Alley & Cunningham, 1991). Fig. 3a and b shows that the mean level of asymmetry (CFA, see Section 2) is indeed highly correlated with the mean scores of the composites (Pearson correlation of \u22120.91, P-value<10\u221219). However, examining the correlation between the rest of the image-features and the composites\u2019 scores reveals that this high correlation is not at all unique to asymmetry. In fact, as the images are being morphed, the changes in 45 of the 98 image-features are strongly correlated with the changes in attractiveness scores (|Pearson correlation|>0.9). The high correlation between these numerous features and attractiveness scores of averaged faces indicates that symmetry level is not an exceptional factor in the machine\u2019s preference for averaged faces. Instead, it suggests that averaging causes many features to change in a direction which causes an increase in attractiveness. It has been argued that although averaged faces are found to be attractive, very attractive faces are not average (Alley & Cunningham, 1991). A virtual composite made of the 12 most attractive faces in the set (as rated by humans) was rated by the machine with a high score of 5.6 while 1000 composites made of 50 faces from random levels of attractiveness got a maximum score of only 5.3. (Their mean score was only 3.94 as reported in Table 1.) This type of preference resembles the findings of Perrett et al. (1994) in which a highly attractive composite, morphed from only attractive faces, was preferred by humans over a composite made of 60 images of all levels of attractiveness. 3.3.2 Experiment 2: Perfectly symmetric averaged faces Rhodes et al. (1999) inquired whether changes in attractiveness produced by manipulating the averageness of individual faces should disappear when all the images are made perfectly symmetric. They created perfectly symmetric composites by morphing original images with their matching mirror images. In their experiment human subjects showed a preference for averaged face composites even when the effect of symmetry is controlled for. Similarly, in our experiment, the effect of symmetry was neutralized by using only perfectly symmetric component faces which yielded perfectly symmetric composites (see Fig. 5 b). It can be seen that the results presented in Fig. 5a are similar to those of Experiment 1 (Fig. 4a). That is, even though the effect of symmetry is controlled for, attractiveness scores of averaged face composites increases with the number of components, n c. Mean values of smoothness and asymmetry of the composites are presented in Fig. 5b. These results show that the machine\u2019s preference for averaged composites is not dependent on symmetry alone, in accordance with the experimental results of Rhodes et al. (1999) and with our conclusions from Experiment 1 (see Supporting Information for a more detailed analysis of the difference between perfectly symmetric composites and components scores). 3.3.3 Experiment 3: Asymmetry of facial attractiveness perception A recent study examining the asymmetry of attractiveness perception has offered an intriguing relationship between facial attractiveness and hemispheric specialization (Zaidel et al., 1995). In this research, right\u2013right and left\u2013left chimeric composites (where \u2018left\u2019 refers to the subject\u2019s side of the face) were created by attaching each half of the face to its mirror image. Human subjects were asked to look at left\u2013left and right\u2013right composites of the same image and judge which one is more attractive. For women\u2019s faces, right\u2013right composites, composed of the right half of the subject\u2019s face, got twice as many \u2018more attractive\u2019 responses than left\u2013left composites. Interestingly, similar results to those were found in Experiment 3 in which we simulated this phenomenon by comparing the machine\u2019s rating of facial attractiveness for left\u2013left and right\u2013right composites. The machine gave 63 out of 91 right\u2013right composites a higher rating than their matching left\u2013left composite, while only 28 left\u2013left composites were judged as more attractive. A paired t-test shows these results to be statistically significant with P-value<10\u22127 (scores of chimeric composites are approximately normally distributed). When rating composites created from a certain image the machine was trained without the original image in its training set. Since the machine representation of the images is completely symmetric, any asymmetric bias revealed is likely to be an implicit manifestation of a psychophysical bias of the human raters. It is interesting to see that the machine manifests the same kind of asymmetry bias reported by Zaidel et al. (1995), though it has never been explicitly trained for that. 3.4 Facial features and attractiveness After establishing that our machine exhibits human-like biases, we turn to compare its processing with those reported in the pertaining human psychophysics literature. A number of studies have singled out facial features that are especially relevant to facial attractiveness, by identifying significant correlations between facial features measurements and human attractiveness ratings (Cunningham et al., 2002; Grammer & Thornhill, 1994; Little et al., 2002). In analogy, facial features that are significantly correlated with the machine\u2019s ratings may be considered as important in determining the machine\u2019s perception of attractiveness. In order to examine whether the important features according to the machine are similar to those of humans, we calculated the correlation between each of the 6980 features used by our predictor (6972 geometric features and 8 non-geometric measurements) and the machine and human ratings (separately). The features were ranked according to their absolute correlation to attractiveness ratings which resulted with two feature rankings: human and machine. The Spearman rank correlation between human and machine ranking was 0.57 and significant (P-value<0.01, see Section 2). To further compare between feature rankings of humans and machine, we repeated the above computation focusing on 28 facial features which were previously studied in the literature of human facial attractiveness (see Section 2). Those 28 features were now ranked according to machine and human ratings and the Spearman rank correlation between the two rankings was 0.68 (P-value<0.01). Out of those 28 features, only 13 were found to be significantly related to facial attractiveness in the original studies (Cunningham et al., 2002; Grammer & Thornhill, 1994; Little et al., 2002). Ranking these 13 facial features according to machine and human ratings, yields a Spearman rank correlation of 0.75 (P-value<0.01) between the rankings. These results provide further evidence of the human-like nature of the machine\u2019s perception of attractiveness, as they show that features that were previously related to facial attractiveness are ranked similarly according to the machine and according to human raters. 4 Discussion In this work, we constructed a high quality training set for learning facial attractiveness of human faces. Using a combination of extensive automatic facial feature extraction, dimension reduction and feature selection, and supervised learning methodologies; we created the first accurate facial attractiveness predictor. Our results add the task of facial attractiveness prediction to the collection of abstract tasks that have been successfully accomplished with current machine learning techniques. While previous machines that successfully passed a subject matter expert turing test (SME TT) have dealt with rule-based cognitive systems, such as playing games, or perceptual tasks of category learning, such as emotion recognition, our machine predicts continuous facial attractiveness ratings and passes a perceptual SME TT that concerns simulating judgment of taste. Whether to compare the machine\u2019s performance in the task to the performance of an individual human rater or to a group of raters is an interesting issue: the machine is an \u2018individual rater\u2019 which learns \u2018group average ratings\u2019 and thus is essentially a hybrid between the two. For that reason we report on both benchmarks, that is, the human individual-to-group mean correlation of 0.67 and the human group-to-group mean correlation of 0.92, and indeed, we find the machine\u2019s performance (correlation of 0.82) between the two. One of the main improvements over previous similar works, such as the work of Eisenthal et al. (2006), is the much richer representation of 84 facial coordinates and 6972 distance and angle features (induced from the full graph on the facial coordinates). This suggests that improving the facial representation might be valuable for future research. One promising suggestion is to employ a non-metric facial representation which may expedite the learning of human facial attractiveness. Examining the machine and human raters\u2019 representations in the ratings space identifies the ratings of the machine near the center of the distribution of human ratings, and closest, on average, to other human raters. The ranking of facial features according to their correlations with machine ratings is correlated significantly with the ranking of those features according to human ratings. The similarity between human and machine preferences has prompted us to further study the machine\u2019s operation. To this end, we have found that the machine favors averaged faces made of several component faces. While this preference is known to be common to humans as well, researchers have previously offered different reasons for favoring averageness. Our analysis has revealed that symmetry is strongly related to the attractiveness of averaged faces, but is definitely not the only factor in the equation, since about half of the image-features relate to the ratings of averaged composites in a similar manner as symmetry and since a preference for averaged faces was found even when the effect of symmetry was neutralized. This suggests that a general movement of features toward attractiveness, rather than a simple increase in symmetry, is responsible for the attractiveness of averaged faces. This movement suggests a convergence towards a prototypical facial representation that matches the cognitive explanations of the averageness hypothesis (Rubenstein et al., 2002). Obviously, this is true only for the machine, but given the human-like biases displayed by our predictor, this may extend also to human perception of facial attractiveness. Overall, it is quite surprising and pleasing to find that a model trained explicitly to capture a specific operational performance criteria such as attractiveness rating (weak AI), implicitly and concomitantly captures basic human psychophysical biases and demonstrates a wide range of human-level characteristics of facial attractiveness judgment (strong AI), as revealed by studying its \u201cpsychophysics\u201d. Acknowledgments We thank Dr. Bernhard Fink and the Ludwig-Boltzmann Institute for Urban Ethology at the Institute for Anthropology, University of Vienna, Austria, and Prof. Alice J. O\u2019Toole from the University of Texas at Dallas, for kindly letting us use their face databases. This work was supported by the internal research fund of The Academic College of Tel-Aviv-Yaffo. Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.visres.2007.11.007. Appendix A Supplementary data Supplementary data References Alley and Cunningham, 1991 T.R. Alley M.R. Cunningham Averaged faces are attractive but very attractive faces are not average Psychological Science 2 1991 123 125 Andersson, 1994 M. Andersson Sexual selection 1994 Princeton University Press Princeton, NJ Becker, 1999 S. Becker Implicit learning in 3D object recognition: The importance of temporal context Neural Computation 11 2 1999 347 374 Cunningham, 1986 M.R. Cunningham Measuring the physical attractiveness: Quasi-experiments on the sociobiology of female facial beauty Journal of Personality and Social Psychology 50 1986 925 935 Cunningham et al., 2002 M.R. Cunningham A.P. Barbee C.L. Philhower Dimensions of facial physical attractiveness: The intersection of biology and culture G. Rhodes L.A. Zebrowitz Advances in visual cognition, vol. 1: facial attractiveness 2002 Ablex Westport, CT Cunningham et al., 1995 M.R. Cunningham A.R. Roberts C.-H. Wu A.P. Barbee P.B. Druen Their ideas of beauty are, on the whole, the same as ours: Consistency and variability in the cross-cultural perception of female physical attractiveness Journal of Personality and Social Psychology 68 1995 261 279 Dailey et al., 2002 M.N. Dailey G.W. Cottrell C. Padgett R. Adolphs EMPATH: A neural network that categorizes facial expressions Journal of Cognitive Neuroscience 14 8 2002 1158 1173 Eisenthal et al., 2006 Y. Eisenthal G. Dror E. Ruppin Facial attractiveness: Beauty and the machine Neural Computation 18 2006 119 142 Fink et al., 2001 B. Fink K. Grammer R. Thornhill Human (homo sapiens) facial attractiveness in relation to skin texture and color Journal of Comparative Psychology 115 2001 92 99 Galton, 1878 F. Galton Composite portraits Journal of the Anthropological Institute of Great Britain and Ireland 8 1878 132 142 Graf et al., 2006 A.B.A. Graf F.A. Wichmann H.H. B\u00fclthoff B. Sch\u00f6lkopf Classification of faces in man and machine Neural Computation 18 2006 143 165 Grammer and Thornhill, 1994 K. Grammer R. Thornhill Human (Homo sapiens) facial attractiveness and sexual selection: The role of symmetry and averageness Journal of Comparative Psychology 108 1994 233 242 Grammer et al., 2002 K. Grammer B. Fink A. Juette G. Ronzal R. Thornhill Female faces and bodies: N-dimensional feature space and attractiveness G. Rhodes L.A. Zebrowitz Advances in visual cognition, vol. 1: facial attractiveness 2002 Ablex Westport, CT Halberstadt and Rhodes, 2003 J.B. Halberstadt G. Rhodes It\u2019s not just average faces that are attractive: Computer-manipulated averageness makes birds, fish, and automobiles attractive Psychonomic Bulletin and Review 10 2003 149 156 Hjelmas and Low, 2001 E. Hjelmas B.K. Low Face detection: A survey Computer Vision and Image Understanding 83 2001 236 274 Johnston and Franklin, 1993 V.S. Johnston M. Franklin Is beauty in the eye of the beholder? Ethology and Sociobiology 14 1993 183 199 Kurzweil, 2005 Kurzweil, R. (2005). The singularity is near: When humans transcend biology. Viking Penguin. Langlois and Roggman, 1990 J.H. Langlois L.A. Roggman Attractive faces are only average Psychological Science 1 1990 115 121 Langlois et al., 1987 J.H. Langlois L.A. Roggman R.J. Casey J.M. Ritter L.A. Rieser-Danner V.Y. Jenkins Infant preferences for attractive faces: Rudiments of a stereotype? Developmental Psychology 23 1987 363 369 Little et al., 2002 A.C. Little I.S. Penton-Voak D.M. Burt D.I. Perrett Evolution and individual differences in the perception of attractiveness: How cyclic hormonal changes and self-perceived attractiveness influence female preferences for male faces G. Rhodes L.A. Zebrowitz Advances in visual cognition, vol. 1: facial attractiveness 2002 Ablex Westport, CT M\u00f8ller and Swaddle, 1997 A.P. M\u00f8ller J.P. Swaddle Asymmetry, developmental stability, and evolution 1997 Oxford University Press Oxford O\u00b4Toole et al., 1999 A.J. O\u00b4Toole T. Price T. Vetter J.C. Bartlett V. Blanz 3D shape and 2D surface textures of human faces: The role of averages in attractiveness and age Image and Vision Computing 18 1999 9 19 Perrett et al., 1994 D.I. Perrett K.A. May S. Yoshikawa Facial shape and judgments of female attractiveness Nature 368 1994 239 242 Rasmussen and Williams, 2006 Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian processes for machine learning. The MIT Press. ISBN 0-262-18253-X. Reber et al., 2004 R. Reber N. Schwarz P. Winkielman Processing fluency and aesthetic pleasure: Is beauty in the perceiver\u2019s processing experience? Personality and Social Psychology Review 8 2004 364 382 Rhodes et al., 1999 G. Rhodes A. Sumich G. Byatt Are average facial configurations attractive only because of their symmetry? Psychological Science 10 1999 52 58 Rubenstein et al., 2002 A.J. Rubenstein J.H. Langlois L.A. Roggman What makes a face attractive and why: The role of averageness in defining facial beauty G. Rhodes L.A. Zebrowitz Advances in visual cognition, vol. 1: facial attractiveness 2002 Ablex Westport, CT Schaeffer and Herik, 2002 J. Schaeffer H.J. Herik Games, computers, and artificial intelligence Artificial Intelligence 134 2002 1 7 Slezak, 1991 P. Slezak Artificial experts: Essay review Social Studies of Science 22 1 1991 175 201 Suykens et al., 2002 Suykens, J. A. K., Van Gestel, T., De Brabanter, J., De Moor, B., & Vandewalle, J. (2002). Least squares support vector machines. World Scientific, Singapore. ISBN 981-238-151-1. Thornhill and Gangsted, 1999 R. Thornhill S.W. Gangsted Facial attractiveness Trends in Cognitive Sciences 3 1999 452 460 Zaidel et al., 1995 D.W. Zaidel A.C. Chen C. German She is not a beauty even when she smiles: Possible evolutionary basis for a relationship between facial attractiveness and hemispheric specialization Neuropsychologia 33 5 1995 649 655 Zebrowitz and Rhodes, 2002 L.A. Zebrowitz G. Rhodes Nature let a hundred flowers bloom: The multiple ways and wherefores of attractiveness G. Rhodes L.A. Zebrowitz Advances in visual cognition, vol. 1: facial attractiveness 2002 Ablex Westport, CT Zhao et al., 2000 Zhao, W. Y., Chellappa, R., Rosenfeld, A., & Phillips, P. J. (2000). Face recognition: A literature survey. UMD CfAR Technical Report CAR-TR-948.", "scopus-id": "38549125396", "pubmed-id": "18164363", "coredata": {"eid": "1-s2.0-S0042698907005032", "dc:description": "Abstract Recent psychological studies have strongly suggested that humans share common visual preferences for facial attractiveness. Here, we present a learning model that automatically extracts measurements of facial features from raw images and obtains human-level performance in predicting facial attractiveness ratings. The machine\u2019s ratings are highly correlated with mean human ratings, markedly improving on recent machine learning studies of this task. Simulated psychophysical experiments with virtually manipulated images reveal preferences in the machine\u2019s judgments that are remarkably similar to those of humans. Thus, a model trained explicitly to capture a specific operational performance criteria, implicitly captures basic human psychophysical characteristics.", "openArchiveArticle": "true", "prism:coverDate": "2008-01-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0042698907005032", "dc:creator": [{"@_fa": "true", "$": "Kagian, Amit"}, {"@_fa": "true", "$": "Dror, Gideon"}, {"@_fa": "true", "$": "Leyvand, Tommer"}, {"@_fa": "true", "$": "Meilijson, Isaac"}, {"@_fa": "true", "$": "Cohen-Or, Daniel"}, {"@_fa": "true", "$": "Ruppin, Eytan"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0042698907005032"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0042698907005032"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0042-6989(07)00503-2", "prism:volume": "48", "prism:publisher": "Elsevier Ltd.", "dc:title": "A machine learning predictor of facial attractiveness revealing human-like psychophysical biases", "prism:copyright": "Copyright \u00a9 2007 Elsevier Ltd. All rights reserved.", "openaccess": "1", "prism:issn": "00426989", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Face perception"}, {"@_fa": "true", "$": "Facial attractiveness"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Aesthetics"}, {"@_fa": "true", "$": "Computational neuroscience"}], "openaccessArticle": "true", "prism:publicationName": "Vision Research", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "235-243", "prism:endingPage": "243", "prism:coverDisplayDate": "January 2008", "prism:doi": "10.1016/j.visres.2007.11.007", "prism:startingPage": "235", "dc:identifier": "doi:10.1016/j.visres.2007.11.007", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-mmc1.doc?httpAccept=%2A%2F%2A", "@multimediatype": "Microsoft Word file", "@type": "APPLICATION", "@size": "311808", "@ref": "mmc1", "@mimetype": "application/word"}, {"@category": "standard", "@height": "228", "@width": "203", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "22137", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "93", "@width": "83", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4771", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "707", "@width": "630", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "84623", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "320", "@width": "380", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "20339", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "94", "@width": "111", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2123", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1090", "@width": "1294", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "74376", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "262", "@width": "539", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "35517", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "61", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2407", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "high", "@height": "891", "@width": "1834", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "161154", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "381", "@width": "572", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "54707", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "83", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2983", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1298", "@width": "1947", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "273701", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "378", "@width": "570", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "53383", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "83", "@width": "125", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2958", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1285", "@width": "1938", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0042698907005032-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "281830", "@ref": "gr5", "@mimetype": "image/jpeg"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/38549125396"}}