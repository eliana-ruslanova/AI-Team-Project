{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014001221", "dc:identifier": "doi:10.1016/j.neunet.2014.05.014", "eid": "1-s2.0-S0893608014001221", "prism:doi": "10.1016/j.neunet.2014.05.014", "pii": "S0893-6080(14)00122-1", "dc:title": "Fast Gaussian kernel learning for classification tasks based on specially structured global optimization ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "57", "prism:startingPage": "51", "prism:endingPage": "62", "prism:pageRange": "51-62", "dc:format": "application/json", "prism:coverDate": "2014-09-30", "prism:coverDisplayDate": "September 2014", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Zhong, Shangping"}, {"@_fa": "true", "$": "Chen, Tianshun"}, {"@_fa": "true", "$": "He, Fengying"}, {"@_fa": "true", "$": "Niu, Yuzhen"}], "dc:description": "\n               Abstract\n               \n                  For a practical pattern classification task solved by kernel methods, the computing time is mainly spent on kernel learning (or training). However, the current kernel learning approaches are based on local optimization techniques, and hard to have good time performances, especially for large datasets. Thus the existing algorithms cannot be easily extended to large-scale tasks. In this paper, we present a fast Gaussian kernel learning method by solving a specially structured global optimization (SSGO) problem. We optimize the Gaussian kernel function by using the formulated kernel target alignment criterion, which is a difference of increasing (d.i.) functions. Through using a power-transformation based convexification method, the objective criterion can be represented as a difference of convex (d.c.) functions with a fixed power-transformation parameter. And the objective programming problem can then be converted to a SSGO problem: globally minimizing a concave function over a convex set. The SSGO problem is classical and has good solvability. Thus, to find the global optimal solution efficiently, we can adopt the improved Hoffman\u2019s outer approximation method, which need not repeat the searching procedure with different starting points to locate the best local minimum. Also, the proposed method can be proven to converge to the global solution for any classification task. We evaluate the proposed method on twenty benchmark datasets, and compare it with four other Gaussian kernel learning methods. Experimental results show that the proposed method stably achieves both good time-efficiency performance and good classification performance.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Fast Gaussian kernel learning method"}, {"@_fa": "true", "$": "Specially structured global optimization"}, {"@_fa": "true", "$": "Kernel target alignment criterion"}, {"@_fa": "true", "$": "Difference of convex functions"}, {"@_fa": "true", "$": "Difference of increasing functions"}, {"@_fa": "true", "$": "Hoffman\u2019s outer approximation method"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014001221", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014001221", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84902319279", "scopus-eid": "2-s2.0-84902319279", "pubmed-id": "24929345", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84902319279", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20140602", "$": "2014-06-02"}}}}}