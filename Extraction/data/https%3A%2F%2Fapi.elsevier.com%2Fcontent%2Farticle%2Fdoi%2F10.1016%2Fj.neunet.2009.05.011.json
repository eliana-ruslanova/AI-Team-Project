{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608009001026", "dc:identifier": "doi:10.1016/j.neunet.2009.05.011", "eid": "1-s2.0-S0893608009001026", "prism:doi": "10.1016/j.neunet.2009.05.011", "pii": "S0893-6080(09)00102-6", "dc:title": "Real-time reinforcement learning by sequential Actor\u2013Critics and experience replay ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "22", "prism:issueIdentifier": "10", "prism:startingPage": "1484", "prism:endingPage": "1497", "prism:pageRange": "1484-1497", "prism:number": "10", "dc:format": "application/json", "prism:coverDate": "2009-12-31", "prism:coverDisplayDate": "December 2009", "prism:copyright": "Copyright \u00a9 2009 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Wawrzy\u0144ski, Pawe\u0142"}], "dc:description": "\n               Abstract\n               \n                  Actor\u2013Critics constitute an important class of reinforcement learning algorithms that can deal with continuous actions and states in an easy and natural way. This paper shows how these algorithms can be augmented by the technique of experience replay without degrading their convergence properties, by appropriately estimating the policy change direction. This is achieved by truncated importance sampling applied to the recorded past experiences. It is formally shown that the resulting estimation bias is bounded and asymptotically vanishes, which allows the experience replay-augmented algorithm to preserve the convergence properties of the original algorithm. The technique of experience replay makes it possible to utilize the available computational power to reduce the required number of interactions with the environment considerably, which is essential for real-world applications. Experimental results are presented that demonstrate that the combination of experience replay and Actor\u2013Critics yields extremely fast learning algorithms that achieve successful policies for non-trivial control tasks in considerably short time. Namely, the policies for the cart-pole swing-up [Doya, K. (2000). Reinforcement learning in continuous time and space. Neural Computation, 12(1), 219\u2013245] are obtained after as little as 20 min of the cart-pole time and the policy for Half-Cheetah (a walking 6-degree-of-freedom robot) is obtained after four hours of Half-Cheetah time.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Direct adaptive control"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Experience replay"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608009001026", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608009001026", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "71749106087", "scopus-eid": "2-s2.0-71749106087", "pubmed-id": "19523786", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/71749106087", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20090531", "$": "2009-05-31"}}}}}