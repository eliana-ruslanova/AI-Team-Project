{"scopus-eid": "2-s2.0-85079756216", "originalText": "serial JL 314095 291210 291767 291838 291871 291884 31 90 Engineering ENGINEERING 2020-01-14 2020-01-14 2020-03-10T19:40:45 1-s2.0-S2095809920300035 S2095-8099(20)30003-5 S2095809920300035 10.1016/j.eng.2019.11.012 S200 S200.2 FULL-TEXT 2020-04-07T15:07:24.095954Z 0 0 20200114 2020 2020-01-15T00:28:04.740806Z aiptxt articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb webpdf webpdfpagecount yearnav table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid primabst pubtype ref 2095-8099 20958099 In Press, Corrected Proof 0 Available online 14 January 2020 2020-01-14 2020 article rev \u00a9 2020 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. BRAINSCIENCEARTIFICIALINTELLIGENCE FAN J 1 Introduction 2 AI inspired by brain science 3 Brain projects 4 Instrumental bridges between brain science and AI References HEBB 1949 D ORGANIZATIONBEHAVIOR LECUN 1989 541 551 Y KRIZHEVSKY 2012 1097 1105 A PROCEEDINGSNEURALINFORMATIONPROCESSINGSYSTEMS20122012DEC36LAKETAHOENVUSA IMAGENETCLASSIFICATIONDEEPCONVOLUTIONALNEURALNETWORKS JAMES 1890 W PRINCIPLESPSYCHOLOGY HOCHREITER 1997 1735 1780 S KIRKPATRICK 2017 3521 3526 J RUSSELL 2010 S ARTIFICIALINTELLIGENCEAMODERNAPPROACH MILLER 2003 141 144 G TURING 1950 433 460 A MINSKY 1987 M PERCEPTRONSINTRODUCTIONCOMPUTATIONALGEOMETRY MCCARTHY 1996 J DEFENDINGAIRESEARCHACOLLECTIONESSAYSREVIEWS HINTON 1986 77 109 G PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURECOGNITIONFOUNDATIONS DISTRIBUTEDREPRESENTATIONS ROSENBLATT 1958 386 408 F HUBEL 1959 574 591 D LECUN 2015 436 444 Y RUMELHART 1986 318 362 D PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURECOGNITIONFOUNDATIONS LEARNINGINTERNALREPRESENTATIONSBYERRORPROPAGATION RUMELHART 1986 D PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURESCOGNITIONFOUNDATIONS RAICHLE 1999 656 658 M MITENCYCLOPEDIACOGNITIVESCIENCES POSITRONEMISSIONTOMOGRAPHY SCOLARI 2015 32 39 M REED 2015 1252 1260 S PROCEEDINGSNEURALINFORMATIONPROCESSINGSYSTEMS20152015DEC712MONTREALQCCANADA DEEPVISUALANALOGYMAKING ATKINSON 1968 89 195 R PSYCHOLOGYLEARNINGMOTIVATIONVOLUME2 HUMANMEMORYAPROPOSEDSYSTEMCONTROLPROCESSES BADDELEY 1974 47 89 A PSYCHOLOGYLEARNINGMOTIVATIONVOLUME8 WORKINGMEMORY GOLDMANRAKIC 1990 325 335 P MCCARTHY 1996 600 611 G JONIDES 1993 623 625 J GRAVES 2016 471 476 A DENK 1990 73 76 W NISHIYAMA 2015 63 75 J CICHON 2015 180 185 J SUTTON 1998 R INTRODUCTIONREINFORCEMENTLEARNING SUTTON 1981 135 170 R INSEL 2013 687 688 T JEONG 2019 390 393 S AMUNTS 2016 574 581 K OKANO 2016 582 590 H JABALPURWALA 2016 601 606 I ALLIANCE 2019 365 369 A DEISSEROTH 2011 26 29 K PEGARD 2017 1228 N HOCHBAUM 2014 825 833 D JI 2016 1154 1164 N WEISENBURGER 2018 431 452 S AHRENS 2013 413 420 M KIM 2016 3385 3394 T MCCONNELL 2016 e18659 G STIRMAN 2016 857 862 J CHEN 2013 336 340 J SOFRONIEW 2016 e14472 N JOESCH 2016 e15015 M FRIEDRICH 2017 e1005685 J BERENS 2018 e1006157 P PANINSKI 2018 232 241 L HOFFER 2017 1731 1741 E PROCEEDINGSNEURALINFORMATIONPROCESSINGSYSTEMS20172017DEC49LONGBEACHCAUSA TRAINLONGERGENERALIZEBETTERCLOSINGGENERALIZATIONGAPINLARGEBATCHTRAININGNEURALNETWORKS KADMON 2016 4788 4796 J PROCEEDINGSNEURALINFORMATIONPROCESSINGSYSTEMS20162016DEC510BARCELONASPAIN OPTIMALARCHITECTURESINASOLVABLEMODELDEEPNETWORKS Full 2020-01-09T01:22:48Z FundingBody Chinese Academy of Engineering http://creativecommons.org/licenses/by-nc-nd/4.0/ OA-Window This is an open access article under the CC BY-NC-ND license. \u00a9 2020 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. 2020-03-07T03:29:11.582Z http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp S2095809920300035 Beijing Municipal Science & Technology Commission Z181100003118014 Beijing Municipal Science and Technology Commission http://data.elsevier.com/vocabulary/SciValFunders/501100009592 http://sws.geonames.org/1814991/ National Natural Science Foundation of China 61327902 NSFC National Natural Science Foundation of China http://data.elsevier.com/vocabulary/SciValFunders/501100001809 http://sws.geonames.org/1814991/ Chinese Academy of Engineering 2019-XZ-9 This work is supported by the Consulting Research Project of the Chinese Academy of Engineering ( 2019-XZ-9 ), the National Natural Science Foundation of China ( 61327902 ), and the Beijing Municipal Science & Technology Commission ( Z181100003118014 ). item S2095-8099(20)30003-5 S2095809920300035 1-s2.0-S2095809920300035 10.1016/j.eng.2019.11.012 314095 2020-03-10T20:37:18.680095Z 2020-01-14 1-s2.0-S2095809920300035-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2095809920300035/MAIN/application/pdf/86d9852a4a8e8e751e0fc630cb8133fe/main.pdf main.pdf pdf true 346558 MAIN 5 1-s2.0-S2095809920300035-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2095809920300035/PREVIEW/image/png/61e6a0cd24731131d249157d79f1b0bf/main_1.png main_1.png png 60405 849 656 IMAGE-WEB-PDF 1 ENG 357 S2095-8099(20)30003-5 10.1016/j.eng.2019.11.012 Table 1 Overview of brain science research projects around the world. Project Timeline Funding amount (billion USD) Main objectives United States 2013\u20132025 ~4.5 \u2022 Discovering diversity: identify different brain cell types and determine their roles in health and disease \u2022 Maps at multiple scales: generate circuit diagrams with varying resolutions from the synapses to the whole brain \u2022 The brain in action: produce a dynamic picture of the functioning brain through large-scale monitoring of neural activity \u2022 Demonstrating causality: link brain activity to behavior with precise interventional tools that change neural circuit dynamics \u2022 Identifying fundamental principles: produce conceptual foundations for understanding mental processes by developing new theoretical and analytical tools \u2022 Advancing human neuroscience: develop innovative technologies to understand the human brain and treat its disorders, and create and support human brain research networks \u2022 From BRAIN Initiative to the brain: apply new technological/conceptual approaches to discover how neural activity patterns transform into cognitions, emotions, perceptions, and actions National Institutes of Health BRAIN Initiative [34] Korea 2018\u20132027 > 1.2 \u2022 Decipher the brain functions and mechanisms that mediate the integration and control of brain functions underlying decision-making \u2022 Map a functional connectome with searchable, multidimensional, and information-integrated features \u2022 Develop novel technologies and neuro-tools for integrated brain mapping \u2022 Enable socioeconomic ramifications that not only facilitate global collaboration in the neuroscience community, but also develop various brain science-related industrial and medical innovations Korea Brain Initiative [35] Europe 2013\u20132023 > 1 \u2022 Develop a scientific infrastructure for brain research and cognitive neuroscience \u2022 Gather and disseminate data describing the brain and related diseases \u2022 Simulate the brain. \u2022 Build theories and models of the brain \u2022 Develop brain-inspired computing, data analytics, and robots Human Brain Project [36] Japan 2014\u20132024 > 0.3 \u2022 Use the marmoset, a small primate with a short life cycle, for functional and structural brain mapping and genetic studies \u2022 Develop innovative tools to monitor and manipulate different aspects of neural activity \u2022 Establish biomarkers for brain disorders Brain/MINDS [37] Canada 2006\u2013unknown > 0.24 \u2022 Understand the brain in health and illness, improve lives, and achieve societal impact \u2022 Increase the scale and scope of funding to accelerate the pace of Canadian brain research \u2022 Create a collective commitment to brain research across the public, private, and voluntary sectors \u2022 Deliver transformative, original, and outstanding research programs Brain Canada [38] Australia 2016\u20132026 > 0.2 \u2022 Health: develop new treatments by revealing the mechanisms of brain abnormalities in neuropsychiatric diseases \u2022 Education: help improve brain growth by coding the cognitive functions of neural circuits and brain networks \u2022 New industry: develop new drugs, medical devices, and wearable technologies by promoting the combination of industrial collaborators and brain research Australian Brain Initiative [39] Research Artificial Intelligence\u2014Review From Brain Science to Artificial Intelligence Jingtao Fan a Lu Fang b Jiamin Wu a Yuchen Guo a Qionghai Dai a \u204e qhdai@mail.tsinghua.edu.cn a Department of Automation, Tsinghua University, Beijing 100084, China Department of Automation Tsinghua University Beijing 100084 China Department of Automation, Tsinghua University, Beijing 100084, China b Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen 518055, China Tsinghua-Berkeley Shenzhen Institute Tsinghua University Shenzhen 518055 China Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen 518055, China \u204e Corresponding author. Abstract Reviewing the history of the development of artificial intelligence (AI) clearly reveals that brain science has resulted in breakthroughs in AI, such as deep learning. At present, although the developmental trend in AI and its applications has surpassed expectations, an insurmountable gap remains between AI and human intelligence. It is urgent to establish a bridge between brain science and AI research, including a link from brain science to AI, and a connection from knowing the brain to simulating the brain. The first steps toward this goal are to explore the secrets of brain science by studying new brain-imaging technology; to establish a dynamic connection diagram of the brain; and to integrate neuroscience experiments with theory, models, and statistics. Based on these steps, a new generation of AI theory and methods can be studied, and a subversive model and working mode from machine perception and learning to machine thinking and decision-making can be established. This article discusses the opportunities and challenges of adapting brain science to AI. Keywords Artificial intelligence Brain science 1 Introduction The history of artificial intelligence (AI) clearly reveals the connections between brain science and AI. Many pioneer AI scientists are also brain scientists. The neural connections in the human brain that were discovered using microscopes inspired the artificial neural network [1]. The brain\u2019s convolution property and multilayer structure, which were discovered using electronic detectors, inspired the convolutional neural network and deep learning [2,3]. The attention mechanism that was discovered using a positron emission tomography (PET) imaging system inspired the attention module [4]. The working memory that was discovered from functional magnetic resonance imaging (fMRI) results inspired the memory module in machine learning models that led to the development of long short-term memory (LSTM) [5]. The changes in the spine that occur during learning, which were discovered using two-photon imaging systems, inspired the elastic weight consolidation (EWC) model for continual learning [6]. Although the AI community and the brain science community currently appear to be unconnected, the results from brain science reveal important issues related to the principles of intelligence, which lead to significant theoretical and technological breakthroughs in AI. We are now in the deep learning era, which was directly inspired by brain science. It can be seen that the increasing research findings in brain science can inspire new deep learning modes. Furthermore, the next breakthrough in AI is likely to come from brain science. 2 AI inspired by brain science The goal of AI is to investigate theories and develop computer systems that are able to conduct tasks that require biological or human intelligence, with functions such as perceptrons, recognition, decision-making, and control [7]. Conversely, the goal of brain science, which is also termed neuroscience, is to study the structures, functions, and operating mechanisms of biological brains, such as how the brain processes information, makes decisions, and interacts with the environment [8]. It is easy to see that AI can be regarded as the simulation of brain intelligence. Therefore, a straightforward way to develop AI is to combine it with brain science and related fields, such as cognition science and psychology. In fact, many pioneers of AI, such as Alan Turing [9], Marvin Minsky and Seymour Papert [10], John McCarthy [11], and Geoffrey Hinton [12], were interested in both fields and contributed a great deal to AI thanks to their solid backgrounds in brain science. Research on AI began directly after the emergence of modern computers, with the goal of building intelligent \u201cthinking\u201d machines. Since the birth of AI, there have been interactions between it and brain science. At the beginning of the 20th century, researchers were able to observe the connections between neurons in the neural system, including brains, due to the development of microscopy. Inspired by the connections between neurons, computer scientists developed the artificial neural network, which is one of the earliest and most successful models in the history of AI. In 1949, Hebbian learning was proposed [1]. This is one of the oldest learning algorithms. Hebbian learning was directly inspired by the dynamics of biological neural systems. In particular, based on the observation that a synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs, the Hebbian learning algorithm increases the connection weight between two neurons if they are highly correlated. After this development, artificial neural networks received considerable research attention from researchers. A representative work was the perceptron [13], which directly modeled the information storage and organization in the brain. The perceptron is a single-layer artificial neural network with a multidimensional input, which laid the foundation for the multilayer network. In 1959, Hubel and Wiesel[14]\u2014the recipients of the 1981 Nobel Prize in Physiology or Medicine\u2014utilized electronic signal detectors to capture the responses of neurons when a visual system saw different images. The single-cell recordings from the mammalian visual cortex revealed how visual inputs are filtered and pooled in simple and complex cells in the V1 area. This research demonstrated that the visual processing system in the brain conducted convolutional operations and had a multilayered structure. It indicated that biological systems utilized successive layers with nonlinear computations to transform raw visual inputs into an increasingly complex set of features, thereby making the vision system invariant to the transformations, such as pose and scale, in the visual inputs during the recognition task. These observations directly inspired the convolutional neural network [2,3], which was the fundamental model for the recent, ground-breaking deep learning technique [15]. Another key component of artificial neural networks and deep learning is the back-propagation algorithm [16], which addresses the problem of how to tune the parameters or weights in a network. Interestingly, the basic idea of back propagation was first proposed in the 1980s by neuroscientists and cognitive scientists [17], rather than by computer scientists or machine learning researchers. The scientists observed that the microstructures of neural systems and the neural system of the biological brain were gradually tuned using a learning procedure with the purpose of minimizing the error and maximizing the reward of the output. The attention mechanism was first introduced in the 1890s as a psychological concept, and was designed such that an intelligent agent selectively concentrated on certain important parts of the information\u2014instead of concentrating on all of the information\u2014in order to improve the cognition process [4]. In the 1990s, studies began using new medical imaging technologies, such as PET, to investigate the attention mechanism in the brain. In 1999, PET was utilized to study selective attention in the brain [18]. Then, using other imaging technologies, researchers discovered more about the attention mechanism in a biological brain [19]. Inspired by the attention mechanism in a biological brain, AI researchers began incorporating attention modules into artificial neural networks in temporal [20] or spatial [21] ways, which improved the performance of deep neural networks for natural language processing and computer vision, respectively. With an attention module, the network is able to selectively focus on important objects or words and ignore irrelevant ones, thereby making the training and inferential processes more efficient than those of a conventional deep network. A machine learning model usually forgets the information in the data that it has processed, whereas biological intelligence is able to maintain such information for a period of time. It is believed that there is working memory in a biological brain that remembers past data. The concept of working memory was first introduced in the 1970s and was summarized from cognition experiments [22,23]. Since 1990, researchers have used PET and fMRI to study the working memory in biological brains, and have found that the prefrontal cortex in the brain is a key part [24\u201326]. Inspired by the working memory research from brain science, AI researchers have attempted to incorporate a memory module into machine learning models. One representative method is LSTM [5], which laid the foundation for many sequential processing tasks, such as natural language processing, video understanding, and time-series analysis. A recent study also showed that with a working memory module, a model can perform complicated reasoning and inference tasks, such as finding the shortest path between specific points and inferring the missing links in randomly generated graphs [27]. By remembering previous knowledge, it is also possible to perform one-shot learning, which requires just a few labeled samples to learn a new concept [28]. Continual learning is a basic skill in biological intelligence that is used to learn a new task without forgetting previous ones. How a biological neural system learns multiple tasks at different times is a challenging research topic. In 1990, the two-photon microscopy technique [29] made it possible to observe the in vivo structures and functions of dendritic spines during learning at the spatial scale of single synapses [30]. With this imaging system, researchers in the 2010s studied neocortical plasticity in the brain during continual learning. The results revealed how neural systems remember previous tasks when learning new tasks by controlling the growth of neurons [31]. Inspired by the observation of biological neural systems, a learning algorithm termed EWC was proposed for deep neural networks. This algorithm controlled the changes in the network parameters when learning a new task, such that older knowledge was preserved, thereby making continual learning in deep learning possible [6]. Reinforcement learning (RL) is a widely used machine learning framework that has been utilized in many applications, such as AlphaGo. It relates to how AI agents take action and interact with the environment. In fact, RL is also strongly related to the biological learning process [32]. One important RL method\u2014which was also one of the earliest methods\u2014is temporal-difference learning (TDL). TDL learns by bootstrapping from the current estimate of the value function. This strategy is similar to the concept of second-order conditioning that has been observed in animal systems [33]. 3 Brain projects Many countries and regions have conducted projects to accelerate brain science research, as shown in Table 1 [34\u201339]. Despite different emphases and routes, the development of the next generation of AI based on discoveries in brain science is a common objective of all brain research projects. Governments and most scientists seem to have reached a consensus that advancing neural imaging and manipulating techniques can help us explore the working principles of the brain, which will allow us to design a better AI architecture, including both hardware and software. During such studies, mutual collaboration between multiple disciplines including biology, physics, informatics, and chemistry are necessary to enable new discoveries in different aspects. During the past five years, important achievements in brain research have been made with the support of brain research projects. The development of optogenetics has made it possible to manipulate neural activities at a single-cell resolution [40]. Large-scale manipulation can be further accomplished using advanced beam-modulation techniques [41,42]. In the meantime, various methods have been proposed to record large-scale neural activities in three dimensions (3D) [43\u201345]. The number of neurons that can be recorded at the same time has increased rapidly from tens to thousands, and may be increased to millions in the near future with the increasing technological developments in wide-field high-resolution imaging [46\u201348]. Such significant improvements in the field of neurophotonics provide a basis for important discoveries in neuroscience [49,50]. For example, the emphasis in the BRAIN Initiative will be gradually moved to discovery-driven science. One typical case in the BRAIN Initiative, which aims to revolutionize machine learning through neuroscience, is machine intelligence from cortical networks (MICrONS). With serial-section electron microscopy, complicated neural structures can be reconstructed in 3D at unprecedented resolutions [51]. In combination with high-throughput data analysis techniques for multiscale data [52,53], novel scientific questions can be developed to explore fundamental neuroscience problems [54]. With this improved understanding, researchers have proposed novel architectures for deep neural networks, and have tried to understand the working principles of current architectures [55,56]. In addition, the current deep learning techniques can help to accelerate the massive amount of data processing that is necessary in such research, thus forming a virtuous circle. Thanks to technological developments in recent years, it is now possible to observe neural activities in a systematic view at unprecedented spatial\u2013temporal resolutions. Many large-scale data analysis techniques have been proposed in the meantime to solve the challenges that result from the massive amount of data produced by such technologies. Following this route, various brain projects can exponentially accelerate brain research. By achieving an increasing number of discoveries, we can develop a better picture of the human brain. There is no doubt that the working principles of the brain will inspire the design of the next generation of AI, just as past discoveries in brain research have inspired today\u2019s AI achievements. 4 Instrumental bridges between brain science and AI Instrumental observations of the brain have made enormous contributions to the emergence and advancement of AI. Modern neurobiology started from the information acquisition of microstructures across the subcellular to tissue levels, and benefited from the inventions of microscopy and the biased staining of substances in cells and tissues. The renowned neuroanatomist Santiago Ram\u00f3n y Cajal was the first to use Golgi staining to observe a large number of tissue specimens of the nervous system, and put forward the fundamental theories on neurons and neural signal transduction. Cajal and Golgi shared the Nobel Prize in Physiology or Medicine in 1906. Cajal is now widely known as the father of modern neurobiology. Our ever-growing understanding of the human brain has benefitted from countless advances in neurotechnology, including the manipulation, processing, and information acquisition of neurons, neural systems, and brains; and cognitive and behavioral learning. Among these advances, the development of new technologies and instruments for high-quality imaging acquisition has been the focus of the past era and is expected to attract the most attention in the future. For example, the BRAIN Initiative, which was launched in the United States in 2013, aims to map dynamic brain images that exhibit the rapid and complex interactions between brain cells and their surrounding nerve circuits, and to unveil the multidimensional intertwined relationships between neural organizations and brain functions. Such advances are also expected to make it possible for us to understand the processes of recording, processing, applying, storing, and retrieving large amounts of information in the brain. In 2017, the BRAIN Initiative sponsored a number of interdisciplinary scientists at Harvard, who undertook to research the understanding of the relationship between neural circuits and behavior, mainly by acquiring and processing large datasets of neural systems under various conditions using high-quality imaging. Traditional neuroscience research mostly uses electrophysiological methods, such as the use of metal electrodes for nerve excitation and signal acquisition, which have the advantages of high sensitivity and high temporal resolution. However, electrophysiology is invasive and is not suitable for long-term observation. In addition, it has a low spatial resolution and limited expansion ability for the parallel observations that are required to extract the global neural activities at a single neuron resolution of the brain. In contrast, optical methods are noninvasive and have high spatial and temporal resolution and high sensitivity. These methods are capable of acquiring dynamic and static information from individual neurons, nerve activities, and interactions and expanding our analyses of the nervous system from the subcellular level to\u2014potentially\u2014the whole brain. Furthermore, optical methods have been developed as manipulating tools to control nerve activities at high spatial\u2013temporal resolutions by using optogenetics. It is very urgent to develop technology and instruments with large fields of view and high spatial\u2013temporal resolutions. On the spatial scale, imaging must span from submicron synapses and neurons that are tens of microns in size to brains that are a few millimeters across. On the temporal scale, the rate of frame acquisition should be higher than the response rate of the probing fluorescent proteins that are used. However, due to the intrinsic diffraction limit of optical imaging, there is an inherent contradiction among large fields of view, high resolution, and large depths of view. High-resolution imaging of single neurons or even smaller features usually cannot see brain tissue features that are larger than a few millimeters, and dynamic imaging is often accompanied by higher noise. Live and noninvasive imaging for real-time and long-term acquisition is, however, limited to the superficial layer due to tissue granules that scatter light. How to break through the above bottlenecks and realize a wide field of view, high spatiotemporal resolution, and large depth of view will be the biggest challenge of microscopic imaging in the coming decade. It is conclusive that exploring from the microstructure dimension may lead to a new type of neurocomputing unit, whereas exploring from the macrostructure dimension in real time may enable an understanding of trans-brain operations and reveal the comprehensive decision-making mechanisms of the brain using multiple information sources (auditory, visual, olfactory, tactile, etc.) in complex environments. The binary ability of the whole brain to explore both the micro- and macro-dimensions in real time will, beyond any doubt, promote the development of the next generation of AI. Therefore, the developmental goal of a microscopic imaging instrument is to possess broader, higher, faster, and deeper imaging from pixels to voxels and from static to dynamic. Such an instrument could establish a direct link between biological macro-cognitive decision-making and the structure and function of a neural network, lay a foundation for revealing the computational essence of cognition and intelligence, and ultimately promote human self-recognition, thereby filling the research gap between AI and human intelligence. Acknowledgements This work is supported by the Consulting Research Project of the Chinese Academy of Engineering (2019-XZ-9), the National Natural Science Foundation of China (61327902), and the Beijing Municipal Science & Technology Commission (Z181100003118014). Compliance with ethics guidelines Jingtao Fan, Lu Fang, Jiamin Wu, Yuchen Guo, and Qionghai Dai declare that they have no conflicts of interest or financial conflicts to disclose. References [1] D.O. Hebb The organization of behavior 1949 John Wiley & Sons Hoboken Hebb DO. The organization of behavior. Hoboken: John Wiley & Sons; 1949. [2] Y. LeCun B. Boser J.S. Denker D. Henderson R.E. Howard W. Hubbard Backpropagation applied to handwritten zip code recognition Neural Comput 1 4 1989 541 551 LeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, et al. Backpropagation applied to handwritten zip code recognition. Neural Comput 1989;1(4):541\u201351. [3] A. Krizhevsky I. Sutskever G. Hinton ImageNet classification with deep convolutional neural networks F. Pereira C.J.C. Burges L. Bottou K.Q. Weinberger Proceedings of the Neural Information Processing Systems 2012; 2012 Dec 3\u20136; Lake Tahoe, NV, USA 2012 1097 1105 Krizhevsky A, Sutskever I, Hinton G. ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJC, Bottou L, and Weinberger KQ, editors. Proceedings of the Neural Information Processing Systems 2012; 2012 Dec 3\u20136; Lake Tahoe, NV, USA; 2012. p. 1097\u2013105. [4] W. James F. Burkhardt F. Bowers I.K. Skrupskelis The principles of psychology 1890 Henry Holt New York James W, Burkhardt F, Bowers F, Skrupskelis IK. The principles of psychology. New York: Henry Holt; 1890. [5] S. Hochreiter J. Schmidhuber Long short-term memory Neural Comput 9 8 1997 1735 1780 Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput 1997;9(8):1735\u201380. [6] J. Kirkpatrick R. Pascanu N. Rabinowitz J. Veness G. Desjardins A.A. Rusu Overcoming catastrophic forgetting in neural networks Proc Natl Acad Sci USA 114 13 2017 3521 3526 Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu AA, et al. Overcoming catastrophic forgetting in neural networks. Proc Natl Acad Sci USA 2017;114(13):3521\u20136. [7] S.J. Russell P. Norvig Artificial intelligence: a modern approach 3rd ed. 2010 Pearson Education New York Russell SJ, Norvig P. Artificial intelligence: a modern approach. 3rd ed. New York: Pearson Education; 2010. [8] G.A. Miller The cognitive revolution: a historical perspective Trends Cogn Sci 7 3 2003 141 144 Miller GA. The cognitive revolution: a historical perspective. Trends Cogn Sci 2003;7(3):141\u20134. [9] A. Turing Computing machinery and intelligence Mind 236 1950 433 460 Turing A. Computing machinery and intelligence. Mind 1950;236:433\u201360. [10] M. Minsky S. Papert Perceptrons: an introduction to computational geometry 1987 MIT Press Cambridge Minsky M, Papert S. Perceptrons: an introduction to computational geometry. Cambridge, MA: MIT Press; 1987. [11] J. McCarthy Defending AI research: a collection of essays and reviews 1996 CSLI Publications Stanford McCarthy J. Defending AI research: a collection of essays and reviews. Stanford: CSLI Publications; 1996. [12] G.E. Hinton D.E. Rumelhart J.L. McClelland Distributed representations Parallel distributed processing: explorations in the microstructure of cognition: foundations 1986 MIT Press Cambridge 77 109 Hinton GE, Rumelhart DE, McClelland JL. Distributed representations. In: Parallel distributed processing: explorations in the microstructure of cognition: foundations. Cambridge, MA: MIT Press; 1986. p. 77\u2013109. [13] F. Rosenblatt The perceptron: a probabilistic model for information storage and organization in the brain Psychol Rev 65 6 1958 386 408 Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychol Rev 1958;65(6):386\u2013408. [14] D.H. Hubel T.N. Wiesel Receptive fields of single neurones in the cat\u2019s striate cortex J Physiol 148 3 1959 574 591 Hubel DH, Wiesel TN. Receptive fields of single neurones in the cat\u2019s striate cortex. J Physiol 1959;148(3):574\u201391. [15] Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 7553 2015 436 444 LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521(7553):436\u201344. [16] D.E. Rumelhart J.L. McClelland Learning internal representations by error propagation Parallel distributed processing: explorations in the microstructure of cognition: foundations 1986 MIT Press Cambridge 318 362 Rumelhart DE, McClelland JL. Learning internal representations by error propagation. In: Parallel distributed processing: explorations in the microstructure of cognition: foundations. Cambridge, MA: MIT Press; 1986. p. 318\u201362. [17] D.E. Rumelhart J.L. McClelland Parallel distributed processing: explorations in the microstructures of cognition: foundations 1986 MIT Press Cambridge Rumelhart DE, McClelland JL. Parallel distributed processing: explorations in the microstructures of cognition: foundations. Cambridge, MA: MIT Press; 1986. [18] M.E. Raichle Positron emission tomography R.A. Wilson L.C. Keil The MIT encyclopedia of the cognitive sciences 1999 MIT Press Cambridge 656 658 Raichle ME. Positron emission tomography. In: Wilson RA, Keil LC, editors. The MIT encyclopedia of the cognitive sciences. Cambridge, MA: MIT Press; 1999. p. 656\u20138. [19] M. Scolari K.N. Seidl-Rathkopf S. Kastner Functions of the human frontoparietal attention network: evidence from neuroimaging Curr Opin Behav Sci 1 2015 32 39 Scolari M, Seidl-Rathkopf KN, Kastner S. Functions of the human frontoparietal attention network: evidence from neuroimaging. Curr Opin Behav Sci 2015;1:32\u20139. [20] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. 2014. arXiv:1409.0473. [21] S. Reed Y. Zhang Y. Zhang H. Lee Deep visual analogy-making C. Cortes N.D. Lawrence D.D. Lee M. Sugiyama R. Garnett Proceedings of the Neural Information Processing Systems 2015; 2015 Dec 7\u201312; Montreal, QC, Canada 2015 1252 1260 Reed S, Zhang Y, Zhang Y, Lee H. Deep visual analogy-making. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, and Garnett R, editors. Proceedings of the Neural Information Processing Systems 2015; 2015 Dec 7\u201312; Montreal, QC, Canada; 2015. p. 1252\u201360. [22] R.C. Atkinson R.M. Shiffrin Human memory: a proposed system and its control processes K.W. Spence J.T. Spence Psychology of learning and motivation (volume 2) 1968 Academic Press New York 89 195 Atkinson RC, Shiffrin RM. Human memory: a proposed system and its control processes. In: Spence KW, Spence JT, editors. Psychology of learning and motivation (volume 2). New York: Academic Press; 1968. p. 89\u2013195. [23] A.D. Baddeley G. Hitch Working memory G.H. Bower Psychology of learning and motivation (volume 8) 1974 Academic Press New York 47 89 Baddeley AD, Hitch G. Working memory. In: Bower GH, editor. Psychology of learning and motivation (volume 8). New York: Academic Press; 1974. p. 47\u201389. [24] P.S. Goldman-Rakic Cellular and circuit basis of working memory in prefrontal cortex of nonhuman primates Prog Brain Res 85 1990 325 335 Goldman-Rakic PS. Cellular and circuit basis of working memory in prefrontal cortex of nonhuman primates. Prog Brain Res 1990;85:325\u201335. [25] G. McCarthy A. Puce R.T. Constable J.H. Krystal J.C. Gore P. Goldman-Rakic Activation of human prefrontal cortex during spatial and nonspatial working memory tasks measured by functional MRI Cereb Cortex 6 4 1996 600 611 McCarthy G, Puce A, Constable RT, Krystal JH, Gore JC, Goldman-Rakic P. Activation of human prefrontal cortex during spatial and nonspatial working memory tasks measured by functional MRI. Cereb Cortex 1996;6(4):600\u201311. [26] J. Jonides E.E. Smith R.A. Koeppe E. Awh S. Minoshima M.A. Mintun Spatial working memory in humans as revealed by PET Nature 363 6430 1993 623 625 Jonides J, Smith EE, Koeppe RA, Awh E, Minoshima S, Mintun MA. Spatial working memory in humans as revealed by PET. Nature 1993;363(6430):623\u20135. [27] A. Graves G. Wayne M. Reynolds T. Harley I. Danihelka A. Grabska-Barwi\u0144ska Hybrid computing using a neural network with dynamic external memory Nature 538 7626 2016 471 476 Graves A, Wayne G, Reynolds M, Harley T, Danihelka I, Grabska-Barwi\u0144ska A, et al. Hybrid computing using a neural network with dynamic external memory. Nature 2016;538(7626):471\u20136. [28] Santoro A, Bartunov S, Botvinick M, Wierstra D, Lillicrap T. One-shot learning with memory-augmented neural networks. 2016. arXiv:1605.06065. [29] W. Denk J.H. Strickler W.W. Webb Two-photon laser scanning fluorescence microscopy Science 248 4951 1990 73 76 Denk W, Strickler JH, Webb WW. Two-photon laser scanning fluorescence microscopy. Science 1990;248(4951):73\u20136. [30] J. Nishiyama R. Yasuda Biochemical computation for spine structural plasticity Neuron 87 1 2015 63 75 Nishiyama J, Yasuda R. Biochemical computation for spine structural plasticity. Neuron 2015;87(1):63\u201375. [31] J. Cichon W.B. Gan Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity Nature 520 7546 2015 180 185 Cichon J, Gan WB. Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity. Nature 2015;520(7546):180\u20135. [32] R. Sutton A. Barto Introduction to reinforcement learning 1998 MIT Press Cambridge Sutton R, Barto A. Introduction to reinforcement learning. Cambridge, MA: MIT Press; 1998. [33] R.S. Sutton A.G. Barto Toward a modern theory of adaptive networks: expectation and prediction Psychol Rev 88 2 1981 135 170 Sutton RS, Barto AG. Toward a modern theory of adaptive networks: expectation and prediction. Psychol Rev 1981;88(2):135\u201370. [34] T.R. Insel S.C. Landis F.S. Collins The NIH BRAIN Initiative Science 340 6133 2013 687 688 Insel TR, Landis SC, Collins FS. The NIH BRAIN Initiative. Science 2013;340(6133):687\u20138. [35] S. Jeong Y. Lee B. Jun Y. Ryu J. Sohn S. Kim Korea Brain Initiative: emerging issues and institutionalization of neuroethics Neuron 101 3 2019 390 393 Jeong S, Lee Y, Jun B, Ryu Y, Sohn J, Kim S, Woo C, et al. Korea Brain Initiative: emerging issues and institutionalization of neuroethics. Neuron 2019;101(3):390\u20133. [36] K. Amunts C. Ebell J. Muller M. Telefont A. Knoll T. Lippert The human brain project: creating a European research infrastructure to decode the human brain Neuron 92 3 2016 574 581 Amunts K, Ebell C, Muller J, Telefont M, Knoll A, Lippert T. The human brain project: creating a European research infrastructure to decode the human brain. Neuron 2016;92(3):574\u201381. [37] H. Okano E. Sasaki T. Yamamori A. Iriki T. Shimogori Y. Yamaguchi Brain/MINDS: a Japanese national brain project for marmoset neuroscience Neuron 92 3 2016 582 590 Okano H, Sasaki E, Yamamori T, Iriki A, Shimogori T, Yamaguchi Y, et al. Brain/MINDS: a Japanese national brain project for marmoset neuroscience. Neuron 2016;92(3):582\u201390. [38] I. Jabalpurwala Brain Canada: one brain one community Neuron 92 3 2016 601 606 Jabalpurwala I. Brain Canada: one brain one community. Neuron 2016;92(3):601\u20136. [39] A. Alliance A neuroethics framework for the Australian Brain Initiative Neuron 101 3 2019 365 369 Alliance A. A neuroethics framework for the Australian Brain Initiative. Neuron 2019;101(3):365\u20139. [40] K. Deisseroth Optogenetics Nat Methods 8 1 2011 26 29 Deisseroth K. Optogenetics. Nat Methods 2011;8(1):26\u20139. [41] N.C. P\u00e9gard A.R. Mardinly I.A. Oldenburg S. Sridharan L. Waller H. Adesnik Three-dimensional scanless holographic optogenetics with temporal focusing (3D-SHOT) Nat Commun 8 1 2017 1228 P\u00e9gard NC, Mardinly AR, Oldenburg IA, Sridharan S, Waller L, Adesnik H. Three-dimensional scanless holographic optogenetics with temporal focusing (3D-SHOT). Nat Commun 2017;8(1):1228. [42] D.R. Hochbaum Y. Zhao S.L. Farhi N. Klapoetke C.A. Werley V. Kapoor All-optical electrophysiology in mammalian neurons using engineered microbial rhodopsins Nat Methods 11 8 2014 825 833 Hochbaum DR, Zhao Y, Farhi SL, Klapoetke N, Werley CA, Kapoor V, et al. All-optical electrophysiology in mammalian neurons using engineered microbial rhodopsins. Nat Methods 2014;11(8):825\u201333. [43] N. Ji J. Freeman S.L. Smith Technologies for imaging neural activity in large volumes Nat Neurosci 19 9 2016 1154 1164 Ji N, Freeman J, Smith SL. Technologies for imaging neural activity in large volumes. Nat Neurosci 2016;19(9):1154\u201364. [44] S. Weisenburger A. Vaziri A guide to emerging technologies for large-scale and whole-brain optical imaging of neuronal activity Annu Rev Neurosci 41 1 2018 431 452 Weisenburger S, Vaziri A. A guide to emerging technologies for large-scale and whole-brain optical imaging of neuronal activity. Annu Rev Neurosci 2018;41(1):431\u201352. [45] M.B. Ahrens M.B. Orger D.N. Robson J.M. Li P.J. Keller Whole-brain functional imaging at cellular resolution using light-sheet microscopy Nat Methods 10 5 2013 413 420 Ahrens MB, Orger MB, Robson DN, Li JM, Keller PJ. Whole-brain functional imaging at cellular resolution using light-sheet microscopy. Nat Methods 2013;10(5):413\u201320. [46] T.H. Kim Y. Zhang J. Lecoq J.C. Jung J. Li H. Zeng Long-term optical access to an estimated one million neurons in the live mouse cortex Cell Rep 17 12 2016 3385 3394 Kim TH, Zhang Y, Lecoq J, Jung JC, Li J, Zeng H, et al. Long-term optical access to an estimated one million neurons in the live mouse cortex. Cell Rep 2016;17(12):3385\u201394. [47] G. McConnell J. Tr\u00e4g\u00e5rdh R. Amor J. Dempster E. Reid W.B. Amos A novel optical microscope for imaging large embryos and tissue volumes with sub-cellular resolution throughout Elife 5 2016 e18659 McConnell G, Tr\u00e4g\u00e5rdh J, Amor R, Dempster J, Reid E, Amos WB. A novel optical microscope for imaging large embryos and tissue volumes with sub-cellular resolution throughout. Elife 2016;5:e18659. [48] J.N. Stirman I.T. Smith M.W. Kudenov S.L. Smith Wide field-of-view, multi-region, two-photon imaging of neuronal activity in the mammalian brain Nat Biotechnol 34 8 2016 857 862 Stirman JN, Smith IT, Kudenov MW, Smith SL. Wide field-of-view, multi-region, two-photon imaging of neuronal activity in the mammalian brain. Nat Biotechnol 2016;34(8):857\u201362. [49] J.L. Chen S. Carta J. Soldado-Magraner B.L. Schneider F. Helmchen Behaviour-dependent recruitment of long-range projection neurons in somatosensory cortex Nature 499 2013 336 340 Chen JL, Carta S, Soldado-Magraner J, Schneider BL, Helmchen F. Behaviour-dependent recruitment of long-range projection neurons in somatosensory cortex. Nature 2013;499:336\u201340. [50] N.J. Sofroniew D. Flickinger J. King K. Svoboda A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging Elife 5 2016 e14472 Sofroniew NJ, Flickinger D, King J, Svoboda K. A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging. Elife 2016;5:e14472. [51] M. Joesch D. Mankus M. Yamagata A. Shahbazi R. Schalek A. Suissa-Peleg Reconstruction of genetically identified neurons imaged by serial-section electron microscopy Elife 5 2016 e15015 Joesch M, Mankus D, Yamagata M, Shahbazi A, Schalek R, Suissa-Peleg A, et al. Reconstruction of genetically identified neurons imaged by serial-section electron microscopy. Elife 2016;5:e15015. [52] J. Friedrich W. Yang D. Soudry Y. Mu M.B. Ahrens R. Yuste Multi-scale approaches for high-speed imaging and analysis of large neural populations PLoS Comput Biol 13 8 2017 e1005685 Friedrich J, Yang W, Soudry D, Mu Y, Ahrens MB, Yuste R, et al. Multi-scale approaches for high-speed imaging and analysis of large neural populations. PLoS Comput Biol 2017;13(8):e1005685. [53] P. Berens J. Freeman T. Deneux N. Chenkov T. McColgan A. Speiser Community-based benchmarking improves spike rate inference from two-photon calcium imaging data PLoS Comput Biol 14 5 2018 e1006157 Berens P, Freeman J, Deneux T, Chenkov N, McColgan T, Speiser A, et al. Community-based benchmarking improves spike rate inference from two-photon calcium imaging data. PLoS Comput Biol 2018;14(5):e1006157. [54] L. Paninski J.P. Cunningham Neural data science: accelerating the experiment-analysis-theory cycle in large-scale neuroscience Curr Opin Neurobiol 50 2018 232 241 Paninski L, Cunningham JP. Neural data science: accelerating the experiment-analysis-theory cycle in large-scale neuroscience. Curr Opin Neurobiol 2018;50:232\u201341. [55] E. Hoffer I. Hubara D. Soudry Train longer, generalize better: closing the generalization gap in large batch training of neural networks I. Guyon U.V. Luxburg S. Bengio H. Wallach R. Fergus S. Vishwanathan Proceedings of the Neural Information Processing Systems 2017; 2017 Dec 4\u20139; Long Beach, CA, USA 2017 1731 1741 Hoffer E, Hubara I, Soudry D. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, and Garnett R, editors. Proceedings of the Neural Information Processing Systems 2017; 2017 Dec 4\u20139; Long Beach, CA, USA; 2017. p. 1731\u201341. [56] J. Kadmon H. Sompolinsky Optimal architectures in a solvable model of deep networks D.D. Lee M. Sugiyama U.V. Luxburg I. Guyon R. Garnett Proceedings of the Neural Information Processing Systems 2016; 2016 Dec 5\u201310; Barcelona, Spain 2016 4788 4796 Kadmon J, Sompolinsky H. Optimal architectures in a solvable model of deep networks. In: Lee DD, Sugiyama M, Luxburg UV, Guyon I, and Garnett R, editors.Proceedings of the Neural Information Processing Systems 2016; 2016 Dec 5\u201310; Barcelona, Spain; 2016. p. 4788\u201396.", "scopus-id": "85079756216", "coredata": {"eid": "1-s2.0-S2095809920300035", "dc:description": "Abstract Reviewing the history of the development of artificial intelligence (AI) clearly reveals that brain science has resulted in breakthroughs in AI, such as deep learning. At present, although the developmental trend in AI and its applications has surpassed expectations, an insurmountable gap remains between AI and human intelligence. It is urgent to establish a bridge between brain science and AI research, including a link from brain science to AI, and a connection from knowing the brain to simulating the brain. The first steps toward this goal are to explore the secrets of brain science by studying new brain-imaging technology; to establish a dynamic connection diagram of the brain; and to integrate neuroscience experiments with theory, models, and statistics. Based on these steps, a new generation of AI theory and methods can be studied, and a subversive model and working mode from machine perception and learning to machine thinking and decision-making can be established. This article discusses the opportunities and challenges of adapting brain science to AI.", "openArchiveArticle": "false", "prism:coverDate": "2020-01-14", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S2095809920300035", "dc:creator": [{"@_fa": "true", "$": "Fan, Jingtao"}, {"@_fa": "true", "$": "Fang, Lu"}, {"@_fa": "true", "$": "Wu, Jiamin"}, {"@_fa": "true", "$": "Guo, Yuchen"}, {"@_fa": "true", "$": "Dai, Qionghai"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S2095809920300035"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S2095809920300035"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S2095-8099(20)30003-5", "prism:publisher": "THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company.", "dc:title": "From Brain Science to Artificial Intelligence", "prism:copyright": "\u00a9 2020 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company.", "openaccess": "1", "prism:issn": "20958099", "dcterms:subject": [{"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Brain science"}], "openaccessArticle": "true", "prism:publicationName": "Engineering", "openaccessSponsorType": "FundingBody", "pubType": "Research Artificial Intelligence\u2014Review", "prism:coverDisplayDate": "Available online 14 January 2020", "prism:doi": "10.1016/j.eng.2019.11.012", "dc:identifier": "doi:10.1016/j.eng.2019.11.012", "openaccessSponsorName": "Chinese Academy of Engineering"}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85079756216"}}