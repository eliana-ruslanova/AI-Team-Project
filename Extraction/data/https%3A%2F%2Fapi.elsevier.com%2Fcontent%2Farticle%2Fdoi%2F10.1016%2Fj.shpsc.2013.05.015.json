{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S1369848613000836", "dc:identifier": "doi:10.1016/j.shpsc.2013.05.015", "eid": "1-s2.0-S1369848613000836", "prism:doi": "10.1016/j.shpsc.2013.05.015", "pii": "S1369-8486(13)00083-6", "dc:title": "Machine wanting ", "prism:publicationName": "Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences", "prism:aggregationType": "Journal", "prism:issn": "13698486", "prism:volume": "44", "prism:issueIdentifier": "4", "prism:startingPage": "679", "prism:endingPage": "687", "prism:pageRange": "679-687", "prism:number": "4", "dc:format": "application/json", "prism:coverDate": "2013-12-31", "prism:coverDisplayDate": "December 2013", "prism:copyright": "Copyright \u00a9 2013 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "McShea, Daniel W."}], "dc:description": "\n               Abstract\n               \n                  Wants, preferences, and cares are physical things or events, not ideas or propositions, and therefore no chain of pure logic can conclude with a want, preference, or care. It follows that no pure-logic machine will ever want, prefer, or care. And its behavior will never be driven in the way that deliberate human behavior is driven, in other words, it will not be motivated or goal directed. Therefore, if we want to simulate human-style interactions with the world, we will need to first understand the physical structure of goal-directed systems. I argue that all such systems share a common nested structure, consisting of a smaller entity that moves within and is driven by a larger field that contains it. In such systems, the smaller contained entity is directed by the field, but also moves to some degree independently of it, allowing the entity to deviate and return, to show the plasticity and persistence that is characteristic of goal direction. If all this is right, then human want-driven behavior probably involves a behavior-generating mechanism that is contained within a neural field of some kind. In principle, for goal directedness generally, the containment can be virtual, raising the possibility that want-driven behavior could be simulated in standard computational systems. But there are also reasons to believe that goal-direction works better when containment is also physical, suggesting that a new kind of hardware may be necessary.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Emotion"}, {"@_fa": "true", "$": "Teleology"}, {"@_fa": "true", "$": "Purpose"}, {"@_fa": "true", "$": "Goal-directedness"}, {"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Robot"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S1369848613000836", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S1369848613000836", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84888135298", "scopus-eid": "2-s2.0-84888135298", "pubmed-id": "23792091", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84888135298", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20130620", "$": "2013-06-20"}}}}}