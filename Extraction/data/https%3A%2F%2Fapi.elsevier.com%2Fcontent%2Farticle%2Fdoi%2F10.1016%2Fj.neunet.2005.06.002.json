{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608005001280", "dc:identifier": "doi:10.1016/j.neunet.2005.06.002", "eid": "1-s2.0-S0893608005001280", "prism:doi": "10.1016/j.neunet.2005.06.002", "pii": "S0893-6080(05)00128-0", "dc:title": "Constructing Bayesian formulations of sparse kernel learning methods ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "pubType": "\n               2005 Special Issue\n            ", "prism:issn": "08936080", "prism:volume": "18", "prism:issueIdentifier": "5-6", "prism:startingPage": "674", "prism:endingPage": "683", "prism:pageRange": "674-683", "prism:number": "5-6", "dc:format": "application/json", "prism:coverDate": "2005-08-31", "prism:coverDisplayDate": "July\u2013August 2005", "prism:copyright": "Copyright \u00a9 2005 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "prism:issueName": "IJCNN 2005", "dc:creator": [{"@_fa": "true", "$": "Cawley, Gavin C."}, {"@_fa": "true", "$": "Talbot, Nicola L.C."}], "dc:description": "\n               Abstract\n               \n                  We present here a simple technique that simplifies the construction of Bayesian treatments of a variety of sparse kernel learning algorithms. An incomplete Cholesky factorisation is employed to modify the dual parameter space, such that the Gaussian prior over the dual model parameters is whitened. The regularisation term then corresponds to the usual weight-decay regulariser, allowing the Bayesian analysis to proceed via the evidence framework of MacKay. There is in addition a useful by-product associated with the incomplete Cholesky factorisation algorithm, it also identifies a subset of the training data forming an approximate basis for the entire dataset in the kernel-induced feature space, resulting in a sparse model. Bayesian treatments of the kernel ridge regression (KRR) algorithm, with both constant and heteroscedastic (input dependent) variance structures, and kernel logistic regression (KLR) are provided as illustrative examples of the proposed method, which we hope will be more widely applicable.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608005001280", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608005001280", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "27744490126", "scopus-eid": "2-s2.0-27744490126", "pubmed-id": "16085387", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/27744490126", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20050808", "$": "2005-08-08"}}}}}