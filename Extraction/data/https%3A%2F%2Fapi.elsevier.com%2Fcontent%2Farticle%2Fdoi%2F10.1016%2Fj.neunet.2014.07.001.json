{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0893608014001580", "dc:identifier": "doi:10.1016/j.neunet.2014.07.001", "eid": "1-s2.0-S0893608014001580", "prism:doi": "10.1016/j.neunet.2014.07.001", "pii": "S0893-6080(14)00158-0", "dc:title": "Ordinal regression neural networks based on concentric hyperspheres ", "prism:publicationName": "Neural Networks", "prism:aggregationType": "Journal", "prism:issn": "08936080", "prism:volume": "59", "prism:startingPage": "51", "prism:endingPage": "60", "prism:pageRange": "51-60", "dc:format": "application/json", "prism:coverDate": "2014-11-30", "prism:coverDisplayDate": "November 2014", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd. All rights reserved.", "prism:publisher": "Elsevier Ltd.", "dc:creator": [{"@_fa": "true", "$": "Guti\u00e9rrez, Pedro Antonio"}, {"@_fa": "true", "$": "Ti\u0148o, Peter"}, {"@_fa": "true", "$": "Herv\u00e1s-Mart\u00ednez, C\u00e9sar"}], "dc:description": "\n               Abstract\n               \n                  Threshold models are one of the most common approaches for ordinal regression, based on projecting patterns to the real line and dividing this real line in consecutive intervals, one interval for each class. However, finding such one-dimensional projection can be too harsh an imposition for some datasets. This paper proposes a multidimensional latent space representation with the purpose of relaxing this projection, where the different classes are arranged based on concentric hyperspheres, each class containing the previous classes in the ordinal scale. The proposal is implemented through a neural network model, each dimension being a linear combination of a common set of basis functions. The model is compared to a nominal neural network, a neural network based on the proportional odds model and to other state-of-the-art ordinal regression methods for a total of 12 datasets. The proposed latent space shows an improvement on the two performance metrics considered, and the model based on the three-dimensional latent space obtains competitive performance when compared to the other methods.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Ordinal regression"}, {"@_fa": "true", "$": "Ordinal classification"}, {"@_fa": "true", "$": "Neural networks"}, {"@_fa": "true", "$": "Latent variable"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0893608014001580", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0893608014001580", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84904878701", "scopus-eid": "2-s2.0-84904878701", "pubmed-id": "25078110", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84904878701", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20140711", "$": "2014-07-11"}}}}}