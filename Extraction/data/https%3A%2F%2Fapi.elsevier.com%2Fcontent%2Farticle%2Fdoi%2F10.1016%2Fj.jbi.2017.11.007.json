{"scopus-eid": "2-s2.0-85034622038", "originalText": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2017-11-13 2017-11-13 2017-11-21 2017-11-21 2017-12-01T16:01:23 1-s2.0-S1532046417302447 S1532-0464(17)30244-7 S1532046417302447 10.1016/j.jbi.2017.11.007 S300 S300.1 FULL-TEXT 1-s2.0-S1532046417X00129 2018-12-01T01:42:07.784721Z 0 0 20171201 20171231 2017 2017-11-13T08:41:29.499179Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref specialabst 1532-0464 15320464 true 76 76 C Volume 76 14 102 109 102 109 201712 December 2017 2017-12-01 2017-12-31 2017 Original research papers article fla \u00a9 2017 Elsevier Inc. RECURRENTNEURALNETWORKSSPECIALIZEDWORDEMBEDDINGSFORHEALTHDOMAINNAMEDENTITYRECOGNITION JAUREGIUNANUE I 1 Introduction 2 Related work 3 Methods 3.1 CRF 3.2 Bidirectional LSTM and bidirectional LSTM-CRF 4 Word features 4.1 Specialized word embeddings 4.2 Character-level embeddings 4.3 Feature augmentation 5 Results 5.1 Datasets 5.2 Evaluation metrics 5.3 Training and hyper-parameters 5.4 Results 5.4.1 CCE results over the i2b2/VA dataset 5.4.2 DNR results over the DrugBank and MedLine datasets 5.4.3 Accuracy by entity classes 6 Conclusion Conflict of interest References ABACHA 2015 122 132 A DEBRUIJN 2011 557 562 B LAMPLE 2016 G HINTON 2012 82 97 G KRIZHEVSKY 2012 1097 1105 A PENNINGTON 2014 1532 1543 J MIKOLOV 2013 3111 3119 T JOHNSON 2016 A CHALAPATHY 2016 R LIU 2015 1 9 S BOAG 2015 W LEBRET 2013 R NIKFARJAM 2015 A WU 2015 Y DERNONCOURT 2016 F COCOS 2017 A XIE 2017 J WEI 2016 Q JAGANNATHA 2016 856 A GRIDACH 2017 85 91 M LEE 2016 J LAFFERTY 2001 282 289 J BENGIO 1994 157 166 Y HOCHREITER 1997 1735 1780 S UZUNER 2011 552 556 O HERREROZAZO 2013 914 920 M NADEAU 2007 3 26 D BERGSTRA 2012 281 305 J SRIVASTAVA 2014 1929 1958 N JAUREGIUNANUEX2017X102 JAUREGIUNANUEX2017X102X109 JAUREGIUNANUEX2017X102XI JAUREGIUNANUEX2017X102X109XI Full 2018-12-01T00:55:54Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ 2018-11-21T00:00:00.000Z UnderEmbargo http://creativecommons.org/licenses/by-nc-nd/4.0/ \u00a9 2017 Elsevier Inc. This article is made available under the Elsevier license. item S1532-0464(17)30244-7 S1532046417302447 1-s2.0-S1532046417302447 10.1016/j.jbi.2017.11.007 272371 2017-12-01T15:27:32.762551-05:00 2017-12-01 2017-12-31 1-s2.0-S1532046417302447-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/MAIN/application/pdf/1120d9f338c32a4842268a4895312dfc/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/MAIN/application/pdf/1120d9f338c32a4842268a4895312dfc/main.pdf main.pdf pdf true 634936 MAIN 8 1-s2.0-S1532046417302447-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/PREVIEW/image/png/140b48f980f5f1ecd1c1e9ccca1d8ea2/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/PREVIEW/image/png/140b48f980f5f1ecd1c1e9ccca1d8ea2/main_1.png main_1.png png 51989 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046417302447-fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/fx1/THUMBNAIL/image/gif/d590daacc0d83d94d9e3209f7aaa51ac/fx1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/fx1/THUMBNAIL/image/gif/d590daacc0d83d94d9e3209f7aaa51ac/fx1.sml fx1 true fx1.sml sml 5110 130 219 IMAGE-THUMBNAIL 1-s2.0-S1532046417302447-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr1/THUMBNAIL/image/gif/b962d090a2e8175204f13a34c5db98dd/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr1/THUMBNAIL/image/gif/b962d090a2e8175204f13a34c5db98dd/gr1.sml gr1 gr1.sml sml 3753 52 219 IMAGE-THUMBNAIL 1-s2.0-S1532046417302447-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr2/THUMBNAIL/image/gif/d9b7bb98d5110a4b69e6563598f0c7cf/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr2/THUMBNAIL/image/gif/d9b7bb98d5110a4b69e6563598f0c7cf/gr2.sml gr2 gr2.sml sml 4464 127 219 IMAGE-THUMBNAIL 1-s2.0-S1532046417302447-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr3/THUMBNAIL/image/gif/241c5348bd1d9649859e8266da55ff69/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr3/THUMBNAIL/image/gif/241c5348bd1d9649859e8266da55ff69/gr3.sml gr3 gr3.sml sml 3605 46 219 IMAGE-THUMBNAIL 1-s2.0-S1532046417302447-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr4/THUMBNAIL/image/gif/797f319bc8b29377385dfddd4b90e828/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr4/THUMBNAIL/image/gif/797f319bc8b29377385dfddd4b90e828/gr4.sml gr4 gr4.sml sml 2168 28 219 IMAGE-THUMBNAIL 1-s2.0-S1532046417302447-fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/fx1/DOWNSAMPLED/image/jpeg/4df6978ec9a0870604c9cdcd6e0897ae/fx1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/fx1/DOWNSAMPLED/image/jpeg/4df6978ec9a0870604c9cdcd6e0897ae/fx1.jpg fx1 true fx1.jpg jpg 14665 200 338 IMAGE-DOWNSAMPLED 1-s2.0-S1532046417302447-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr1/DOWNSAMPLED/image/jpeg/f24ff885539931e5d226c2ffe428e82e/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr1/DOWNSAMPLED/image/jpeg/f24ff885539931e5d226c2ffe428e82e/gr1.jpg gr1 gr1.jpg jpg 25872 158 667 IMAGE-DOWNSAMPLED 1-s2.0-S1532046417302447-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr2/DOWNSAMPLED/image/jpeg/2958e41e72333968463f0c3ee8fb838f/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr2/DOWNSAMPLED/image/jpeg/2958e41e72333968463f0c3ee8fb838f/gr2.jpg gr2 gr2.jpg jpg 31756 388 667 IMAGE-DOWNSAMPLED 1-s2.0-S1532046417302447-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr3/DOWNSAMPLED/image/jpeg/c21f53f0a3af646de4914c7b21f8c561/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr3/DOWNSAMPLED/image/jpeg/c21f53f0a3af646de4914c7b21f8c561/gr3.jpg gr3 gr3.jpg jpg 32731 141 667 IMAGE-DOWNSAMPLED 1-s2.0-S1532046417302447-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr4/DOWNSAMPLED/image/jpeg/cb0faf59eb8cf095fe37180d6e2bf112/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr4/DOWNSAMPLED/image/jpeg/cb0faf59eb8cf095fe37180d6e2bf112/gr4.jpg gr4 gr4.jpg jpg 14379 85 667 IMAGE-DOWNSAMPLED 1-s2.0-S1532046417302447-fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/fx1/HIGHRES/image/jpeg/2bb87d68b89944c568acbefcbabfe575/fx1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/fx1/HIGHRES/image/jpeg/2bb87d68b89944c568acbefcbabfe575/fx1_lrg.jpg fx1 true fx1_lrg.jpg jpg 82650 886 1496 IMAGE-HIGH-RES 1-s2.0-S1532046417302447-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr1/HIGHRES/image/jpeg/22657a82c3b73c37a6f5f29dd4244ca7/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr1/HIGHRES/image/jpeg/22657a82c3b73c37a6f5f29dd4244ca7/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 155320 701 2952 IMAGE-HIGH-RES 1-s2.0-S1532046417302447-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr2/HIGHRES/image/jpeg/62db08d9d9c7a4efd336bef088dfed0b/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr2/HIGHRES/image/jpeg/62db08d9d9c7a4efd336bef088dfed0b/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 162030 1717 2953 IMAGE-HIGH-RES 1-s2.0-S1532046417302447-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr3/HIGHRES/image/jpeg/ea406104c34d2e80939e59871a8e90b2/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr3/HIGHRES/image/jpeg/ea406104c34d2e80939e59871a8e90b2/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 180671 623 2953 IMAGE-HIGH-RES 1-s2.0-S1532046417302447-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/gr4/HIGHRES/image/jpeg/8045f347e56a213dcdc512240ffc3feb/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/gr4/HIGHRES/image/jpeg/8045f347e56a213dcdc512240ffc3feb/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 87801 378 2953 IMAGE-HIGH-RES 1-s2.0-S1532046417302447-si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/bb61c4b2c25393ebc12cd0d57c94c707/si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/bb61c4b2c25393ebc12cd0d57c94c707/si1.gif si1 si1.gif gif 187 11 10 ALTIMG 1-s2.0-S1532046417302447-si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/88065fe5568dded4262aedca2dae64b5/si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/88065fe5568dded4262aedca2dae64b5/si10.gif si10 si10.gif gif 693 14 184 ALTIMG 1-s2.0-S1532046417302447-si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/ed0ba2f19367df6031f1bd74aecb852e/si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/ed0ba2f19367df6031f1bd74aecb852e/si11.gif si11 si11.gif gif 758 14 165 ALTIMG 1-s2.0-S1532046417302447-si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/21a881fd8c64c5d56f0d5c7fd2c088d4/si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/21a881fd8c64c5d56f0d5c7fd2c088d4/si12.gif si12 si12.gif gif 777 18 159 ALTIMG 1-s2.0-S1532046417302447-si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/673e076e77aad9b6ba81850aef5cbfa0/si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/673e076e77aad9b6ba81850aef5cbfa0/si13.gif si13 si13.gif gif 192 9 16 ALTIMG 1-s2.0-S1532046417302447-si14.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/5080473e980699cf2c0dd92582ea813e/si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/5080473e980699cf2c0dd92582ea813e/si14.gif si14 si14.gif gif 201 9 17 ALTIMG 1-s2.0-S1532046417302447-si15.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/efcd86ecd3a437d7f810cc4d60b709d7/si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/efcd86ecd3a437d7f810cc4d60b709d7/si15.gif si15 si15.gif gif 203 9 17 ALTIMG 1-s2.0-S1532046417302447-si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/30e9e40c42a07b619b2fafa5897182c0/si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/30e9e40c42a07b619b2fafa5897182c0/si16.gif si16 si16.gif gif 223 14 17 ALTIMG 1-s2.0-S1532046417302447-si17.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/cb56fdb1eb29adda817ab4a536a2ae91/si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/cb56fdb1eb29adda817ab4a536a2ae91/si17.gif si17 si17.gif gif 230 14 18 ALTIMG 1-s2.0-S1532046417302447-si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/00df35d538c4aacc958777d03f9f1275/si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/00df35d538c4aacc958777d03f9f1275/si18.gif si18 si18.gif gif 230 14 18 ALTIMG 1-s2.0-S1532046417302447-si19.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/34f3bd60e3cf27e5b67a7e3033cc63c2/si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/34f3bd60e3cf27e5b67a7e3033cc63c2/si19.gif si19 si19.gif gif 196 13 11 ALTIMG 1-s2.0-S1532046417302447-si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/e794ddd7ae845608b799d17293fd3961/si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/e794ddd7ae845608b799d17293fd3961/si2.gif si2 si2.gif gif 174 7 11 ALTIMG 1-s2.0-S1532046417302447-si20.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/229cbd189b28da5a5f55ea5043b0b367/si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/229cbd189b28da5a5f55ea5043b0b367/si20.gif si20 si20.gif gif 192 9 13 ALTIMG 1-s2.0-S1532046417302447-si21.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/093da6b00533cf3915791a5d28075d1b/si21.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/093da6b00533cf3915791a5d28075d1b/si21.gif si21 si21.gif gif 198 9 14 ALTIMG 1-s2.0-S1532046417302447-si22.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/2fca20522988d7d3374b1732badb44c9/si22.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/2fca20522988d7d3374b1732badb44c9/si22.gif si22 si22.gif gif 214 11 16 ALTIMG 1-s2.0-S1532046417302447-si23.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/dc237bdf78497e0f1bb19fffe9c3bfd7/si23.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/dc237bdf78497e0f1bb19fffe9c3bfd7/si23.gif si23 si23.gif gif 197 12 9 ALTIMG 1-s2.0-S1532046417302447-si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/0423429538c31384f2c5fb338a639bf6/si24.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/0423429538c31384f2c5fb338a639bf6/si24.gif si24 si24.gif gif 178 7 11 ALTIMG 1-s2.0-S1532046417302447-si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/b64d3ce8c54736f2dc5bbb47bd3a562c/si25.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/b64d3ce8c54736f2dc5bbb47bd3a562c/si25.gif si25 si25.gif gif 195 12 13 ALTIMG 1-s2.0-S1532046417302447-si26.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/4a92c5a7ee31748533c627bd35d9bb93/si26.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/4a92c5a7ee31748533c627bd35d9bb93/si26.gif si26 si26.gif gif 237 19 16 ALTIMG 1-s2.0-S1532046417302447-si27.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/bc5be5dbae86375ff93c3d1e006d2ec2/si27.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/bc5be5dbae86375ff93c3d1e006d2ec2/si27.gif si27 si27.gif gif 255 14 30 ALTIMG 1-s2.0-S1532046417302447-si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/5eac05e9dc942ea235014fa2a0c3fdde/si28.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/5eac05e9dc942ea235014fa2a0c3fdde/si28.gif si28 si28.gif gif 512 20 95 ALTIMG 1-s2.0-S1532046417302447-si29.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/1e62581ad0c125e2c681e40de689e453/si29.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/1e62581ad0c125e2c681e40de689e453/si29.gif si29 si29.gif gif 3387 74 365 ALTIMG 1-s2.0-S1532046417302447-si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/bb61e9fd90c91a273efdfe34b8470003/si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/bb61e9fd90c91a273efdfe34b8470003/si3.gif si3 si3.gif gif 399 14 62 ALTIMG 1-s2.0-S1532046417302447-si30.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/cff42e36f4e4d967eeecef86775fb150/si30.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/cff42e36f4e4d967eeecef86775fb150/si30.gif si30 si30.gif gif 550 21 135 ALTIMG 1-s2.0-S1532046417302447-si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/42cb8f35147b282c5b09839690e45d66/si31.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/42cb8f35147b282c5b09839690e45d66/si31.gif si31 si31.gif gif 490 21 109 ALTIMG 1-s2.0-S1532046417302447-si32.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/d2c1a0d8c089a1ec64f68ef4165a6134/si32.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/d2c1a0d8c089a1ec64f68ef4165a6134/si32.gif si32 si32.gif gif 722 25 143 ALTIMG 1-s2.0-S1532046417302447-si33.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/993ca496d6c37fea848624a0e153dd03/si33.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/993ca496d6c37fea848624a0e153dd03/si33.gif si33 si33.gif gif 780 15 165 ALTIMG 1-s2.0-S1532046417302447-si34.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/b85f97cceeb138a90711547e45708113/si34.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/b85f97cceeb138a90711547e45708113/si34.gif si34 si34.gif gif 912 15 198 ALTIMG 1-s2.0-S1532046417302447-si35.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/4d31e27b2ad1f2064654240740ccf4e0/si35.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/4d31e27b2ad1f2064654240740ccf4e0/si35.gif si35 si35.gif gif 606 15 119 ALTIMG 1-s2.0-S1532046417302447-si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/3cde095ac2a2ede0476eaaba69ade18f/si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/3cde095ac2a2ede0476eaaba69ade18f/si4.gif si4 si4.gif gif 185 7 12 ALTIMG 1-s2.0-S1532046417302447-si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/a4c83c62996041b0b087c010ffad0bf2/si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/a4c83c62996041b0b087c010ffad0bf2/si5.gif si5 si5.gif gif 350 14 49 ALTIMG 1-s2.0-S1532046417302447-si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/0fafe49ea2dd5c50fd1b9becf17e0853/si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/0fafe49ea2dd5c50fd1b9becf17e0853/si6.gif si6 si6.gif gif 353 14 50 ALTIMG 1-s2.0-S1532046417302447-si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046417302447/STRIPIN/image/gif/9323306e4c8a76e77c6305e1d7ccb4c7/si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046417302447/STRIPIN/image/gif/9323306e4c8a76e77c6305e1d7ccb4c7/si9.gif si9 si9.gif gif 981 40 187 ALTIMG YJBIN 2886 S1532-0464(17)30244-7 10.1016/j.jbi.2017.11.007 Elsevier Inc. Fig. 1 (a) DNR and (b) CCE tasks examples, where \u2018B\u2019 (beginning) specifies the start of a named entity, \u2018I\u2019 (inside) specifies that the word is part of the same named entity, and \u2018O\u2019 (outside) specifies that the word is not part of any predefined class. Fig. 2 The Bidirectional LSTM-CRF with word-level and character-level word embeddings. In the example, word \u2018sulfate\u2019 is assumed to be the 5th word in a sentence and its only entity;\u2018x5\u2019 represents its word-level embedding (a single embedding for the whole word); \u2018x5*\u2019 represents its character-level embedding, formed from the concatenation of the last hidden state of the forward and backward passes of a character-level Bidirectional LSTM; \u2018h1\u20135\u2019 are the hidden states of the main Bidirectional LSTM which become the inputs into a final CRF; eventually, the CRF provides the pr labeling. Fig. 3 Description of the hand-crafted features. Fig. 4 (a) An example of an incorrect tagging in the \u201cstrict\u201d evaluation method. (b) An example of a correct tagging in the \u201cstrict\u201d evaluation method. Table 1 Statistics of the training and test datasets used in the experiments. Training set Test set (a) i2b2/VA Documents 170 256 Sentences 16,315 27,626 problem 7073 12,592 test 4608 9225 treatment 4844 9344 DrugBank MedLine Training set Test set Training set Test set (b) SemEval-2013 Task 9.1 Documents 730 54 175 58 Sentences 6577 145 1627 520 drug_n 124 6 520 115 group 3832 65 234 90 brand 1770 53 36 6 drug 9715 180 1574 171 Table 2 The hyper-parameters used in the experiments. Hyper-parameter Value Word embedding dim (dw) 300 (cc)/600 (cc\u202f+\u202fmimic) Word LSTM hidden layer dim (Hw) 100 Char embedding dim (dc) 25 Char LSTM hidden layer dim (Hc) 25 Dropout 0.5 Optimization Stochastic Gradient Descend Learning rate 0.01 Concatenated hand-crafted features dim 146 Table 3 Comparison of the results between the different RNN models and the state-of-the-art systems over the CNE and DNR tasks. Model i2b2/VA F1 score (%) (a) CCE results over the i2b2/VA dataset Binarized Neural Embedding CRF [17] 82.80 CliNER [14] 80.00 Truecasing CRFSuite [37] 75.86 CRF + (random) 11.27 CRF + (features) 25.53 CRF + (cc) 53.72 CRF + (cc/mimic) 58.28 CRF + (cc/mimic) + (features) 64.09 B-LSTM + (random) 65.43 B-LSTM + (random) + (features) 69.42 B-LSTM + (cc) 75.17 B-LSTM + (cc) + (char) 76.79 B-LSTM + (cc/mimic) + (char) 77.19 B-LSTM + (cc/mimic) + (char) + (features) 77.59 B-LSTM-CRF + (random) 75.05 B-LSTM-CRF + (random) + (features) 77.81 B-LSTM-CRF + (cc) 82.85 B-LSTM-CRF + (cc) + (char) 83.35 B-LSTM-CRF + (cc/mimic) + (char) 82.70 B-LSTM-CRF + (cc/mimic) + (char) + (features) 83.29 Model DrugBank MedLine F1 score (%) F1 score (%) (b) DNR results over the DrugBank and MedLine datasets WBI-NER [3] 87.80 58.10 Hybrid-DDI [2] 80.00 37.00 Word2Vec+DINTO [1] 75.00 57.00 CRF + (random) 28.70 13.65 CRF + (features) 44.52 20.19 CRF + (cc) 43.42 32.62 CRF + (cc/mimic) 53.12 30.87 CRF + (cc/mimic) + (features) 66.45 29.36 B-LSTM + (random) 65.09 21.28 B-LSTM + (random) + (features) 75.43 30.88 B-LSTM + (cc) 71.75 42.39 B-LSTM + (cc) + (char) 84.35 43.33 B-LSTM + (cc/mimic) + (char) 83.63 44.39 B-LSTM + (cc/mimic) + (char) + (features) 84.06 45.92 B-LSTM-CRF + (random) 69.50 44.60 B-LSTM-CRF + (random) + (features) 75.78 43.36 B-LSTM-CRF + (cc) 79.03 57.87 B-LSTM-CRF + (cc) + (char) 87.87 59.02 B-LSTM-CRF + (cc/mimic) + (char) 88.38 60.66 B-LSTM-CRF + (cc/mimic) + (char) + (features) 87.42 59.75 Table 4 Percentage of words initialized with pre-trained embeddings in the train, dev and test of the respective datasets. Common Crawl (cc) Common Crawl\u202f+\u202fMIMIC-III (cc/mimic) i2b2/VA 99.99% 99.99% DrugBank 49.50% 67.02% MedLine 49.10% 61.51% Table 5 Results by class for the B-LSTM-CRF with character-level and cc/mimic embeddings. Entities i2b2/VA Precision Recall F1 score (a) i2b2/VA B-LSTM-CRF+(cc)+(char) problem 81.29 83.62 82.44 test 84.74 85.01 84.87 Entities DrugBank MedLine Precision Recall F1 score Precision Recall F1 score (b) SemEval-2013 Task 9.1 B-LSTM-CRF+(cc/mimic)+(char) group 81.69 87.88 84.67 69.14 60.22 64.37 drug 94.77 89.56 91.83 73.89 77.33 75.57 drug_n 00.00 00.00 00.00 68.18 25.57 37.19 Recurrent neural networks with specialized word embeddings for health-domain named-entity recognition I\u00f1igo Jauregi Unanue a b \u204e ijauregi@cmcrc.com Ehsan Zare Borzeshi b Massimo Piccardi a a University of Technology Sydney (UTS), Australia University of Technology Sydney (UTS) Australia b Capital Markets Cooperative Research Centre (CMCRC), Australia Capital Markets Cooperative Research Centre (CMCRC) Australia \u204e Corresponding author at: University of Technology Sydney (UTS), Australia. University of Technology Sydney (UTS) Australia Graphical abstract Highlights \u2022 Past approaches to health-domain NER have mainly used manual features and conventional classifiers. \u2022 In this paper, we explore a neural network approach (B-LSTM-CRF) that can learn the features automatically. \u2022 In addition, initializing the features with pre-trained embeddings can lead to higher accuracy. \u2022 We pre-train the features using a critical care database (MIMIC-III). \u2022 Experiments have been carried out over three contemporary datasets for health-domain NER, outperforming past systems. Abstract Background Previous state-of-the-art systems on Drug Name Recognition (DNR) and Clinical Concept Extraction (CCE) have focused on a combination of text \u201cfeature engineering\u201d and conventional machine learning algorithms such as conditional random fields and support vector machines. However, developing good features is inherently heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks (RNNs) have proved capable of automatically learning effective features from either random assignments or automated word \u201cembeddings\u201d. Objectives (i) To create a highly accurate DNR and CCE system that avoids conventional, time-consuming feature engineering. (ii) To create richer, more specialized word embeddings by using health domain datasets such as MIMIC-III. (iii) To evaluate our systems over three contemporary datasets. Methods Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning approach. The same features are used for all the models. Results We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all previously proposed systems. The specialized embeddings have helped to cover unusual words in DrugBank and MedLine, but not in the i2b2/VA dataset. Conclusions We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over datasets that are adequate for the domain, in order to adequately cover the domain-specific vocabulary. Keywords Neural networks (computer) [MeSH] Machine learning [MeSH] Artificial intelligence [MeSH] Clinical concept extraction Drug name recognition 1 Introduction In recent years, the amount of digital information generated from all sectors of society has increased rapidly, and as a result, agriculture, industry, small businesses and, of course, healthcare, are becoming more efficient and productive thanks to the insights obtained from the \u201cBig Data\u201d. However, in order to deal effectively with such large data, there is an ongoing need for novel, scalable and more accurate analytic tools. In the healthcare system, patients\u2019 medical records represent a big data source. Even though the records contain very useful information about the patients, in most cases the information consists of unstructured text such as, among others, doctors\u2019 notes, medical observations made by various physicians, and descriptions of the recommended treatments. This type of data cannot be analyzed using common statistical tools; rather, they need to be approached by Natural Language Processing (NLP) techniques. In this paper, we focus on a well-known task in NLP, namely Named-Entity Recognition (NER). The goal of NER is to automatically find \u201cnamed entities\u201d in text and classify them into predefined categories such as people, locations, companies, time expressions etc. In the case of specialized domains, NER systems focus on text with specific dictionaries and topics, together with dedicated sets of named-entities. In the health domain, the two most important NER tasks are Clinical Concept Extraction (CCE) and Drug Name Recognition (DNR). The former aims to identify mentions of clinical concepts in patients\u2019 records to help improve the organization and management of healthcare services. Named entities in CCE can include test names, treatments, problems related to individual patients, and so forth. The latter seeks to find drug mentions in unstructured biomedical texts to match drug names with their effects and discover drug-drug interactions (DDIs). DNR is a key step of pharmacovigilance (PV) which is concerned with the detection and understanding of adverse effects of drugs and other drug-related problems. Fig. 1 shows examples of both tasks. NER is a challenging learning problem because in most domains the training datasets are scarce, preventing a \u201cbrute-force\u201d approach by exhaustive dictionaries. Consequently, many systems rely on hand-crafted rules and language-specific knowledge to solve this task. To give a simple example of such rules, if the word begins with a capital letter in the middle of the sentence, it can be assumed to be a named entity in most cases. Nevertheless, these approaches are time-costly to develop, depend considerably on the language and the domain, are ineffective in the presence of informal sentences and abbreviations and, although they usually achieve high precision, suffer from low recall (i.e., they miss many entities). Conversely, machine learning (ML) approaches overcome all these limitations as they are intrinsically robust to variations. Current state-of-the-art ML methods follow a two-step process: (1) feature engineering and (2) automated classification [1\u20134]. The first step represents the text by numeric vectors using domain-specific knowledge. The second step refers to the task of classifying each word into a different named-entity class, with popular choices for the classifier being the linear-chain Conditional Random Fields (CRF), Structural Support Vector Machines (S-SVM) and maximum-entropy classifiers. The drawback of this approach is that feature engineering can be often as time-consuming as the manual design of rules. In recent years, the advent of deep learning has contributed to significantly overcome this problem [5\u20137]. The Long Short-Term Memory (LSTM) and its variants (e.g., the Bidirectional LSTM), which are a specific type of Recurrent Neural Networks (RNNs), have reported very promising results [5]. In these models, words only need to be assigned to random vectors, and during training the neural network is able to automatically learn improved representations for them, completely bypassing feature engineering. In order to further increase the performance of these systems, the input vectors can alternatively be assigned with general-purpose word embeddings learned with GloVe or Word2vec [8,9]. The aim of general-purpose word embeddings is to map every word in a dictionary to a numerical vector (the embedding) so that the distance between the vectors somehow reflects the semantic difference between the words. For example, \u2018cat\u2019 and \u2018dog\u2019 should be closer in the vector space than \u2018cat\u2019 and \u2018car\u2019. The common principle behind embedding approaches is that the meaning of a word is conveyed by the words it is used with (its surrounding words, or context). Therefore, the training of the word embeddings only requires large, general-purpose text corpora such as Wikipedia (400\u202fK unique words) or Common Crawl (2.2\u202fM unique words), without the need for any manual annotation. However, drug and clinical concept recognition are very domain-specific tasks, and many words might not appear in general-domain datasets. In order to assign word embeddings to these specialized words, the embedding algorithms need to be retrained using medical domain resources such as the MIMIC-III corpora [10]. As well as semantic word embeddings, character-level embeddings of words can also be automatically learned. Such embeddings can capture typical prefixes and suffixes, providing the classifiers with richer representations of the words [5]. Preliminary results for the work presented in this paper have obtained very promising accuracy in DNR and CCE tasks using neural networks. Chalapathy et al. [11] presented a DNR system that uses a Bidirectional LSTM-CRF architecture with random assignments of the input word vectors at the EMNLP 2016 Health Text Mining and Information Analysis workshop. The reported results were very close to the system that ranked first in the SemEval-2013 Task 9.1. In Chalapathy et al. [12], the authors leveraged the same architecture for CCE at the Clinical NLP 2016 workshop, this time using pre-trained word embeddings from GloVe, and the results outperformed previous systems over the i2b2/VA dataset. In this paper, we extend the previous research by training the deep networks with more complex and specialized word embeddings. Moreover, we explore the impact of augmenting the word embeddings with conventional feature engineering. As methods, we compare contemporary recurrent neural networks such as the Bidirectional LSTM and the Bidirectional LSTM-CRF against a conventional ML baseline (a CRF). We report state-of-the-art results in both DNR and CCE. 2 Related work Most of the research carried out in domain-specific NER has combined supervised and semi-supervised ML models with text feature engineering. For example, the WBI-NER system that ranked first in the SemEval-2013 Task 9.1 (Recognition and classification of pharmacological substances, DNR) [3], is based on a linear-chain CRF with specialized features. Other similar systems for DNR [2,13] use various general- and domain-specific features. In CCE, the same approach (feature engineering\u202f+\u202fconventional ML classifier) has achieved the best results [4,14]. In the recent years, there has been an increase in the use of deep neural networks for a variety of NLP tasks, including NER [5\u20137]. Pre-trained word embeddings [8,9,15] have been used in traditional ML methods [16,17] and in neural networks, where Deconourt et al. [18] has achieved better performance than previously published systems in de-identification of patient notes. Cocos et al. [19] have used the Bidirectional LSTM model for labelling Adverse Drug Reactions in pharmacovigilance. Xie et al. [20] have used a similar model for studying the adverse effects of e-cigarettes. Wei et al. [21] have combined the output of a Bidirectional LSTM and a CRF as input to an SVM classifier for disease name recognition. A possible drawback of this approach is that the overall prediction is not structured and may miss on useful correlation between the output variables. In a work that is more related to ours, Jaganatha and Yu [22] have employed a Bidirectional LSTM-CRF to label named entities from electronic health records of cancer patients. Their model differs in the CRF output module where the pairwise potentials are modelled using a Convolutional Neural Network (CNN) rather than the usual transition matrix. Gridach [23] has also used the Bidirectional LSTM-CRF for named-entity recognition in the biomedical domain. The main difference and contribution of the proposed approach is that it leverages specialized health-domain embeddings created from a structured database. In the experiments, these embeddings have been used jointly with general-domain embeddings and they have proved able to improve the accuracy in several cases. In addition, our work evaluates the use of hand-crafted features in the system [24]. This aims to provide a comprehensive feature comparison for health-domain named-entity recognition based on LSTM models. 3 Methods In this section we provide a description of the main methods employed. First, we describe the conditional random field (CRF), a traditional machine learning approach for the classification of sequences, which is used as a baseline in the experiments. This baseline is compared with two variants of a contemporary recurrent neural network, which are known as Bidirectional LSTM and Bidirectional LSTM-CRF, respectively. 3.1 CRF A CRF model is a well-known machine learning approach that has been widely used in NER [25]. It predicts sequences of labels ( y ) from sequences of measurements ( x ) taking into account the sequentiality of the data. A CRF model, p ( y | x , w ) , is given in Eq. (1) below, where w notes the model\u2019s parameters, \u03a8 ( x , y ) is the chosen feature vector and Z ( w , x ) is the cumulative sum of p ( y | x , w ) over all the possible y : (1) p ( y | x , w ) = exp ( w T \u03a8 ( x , y ) ) Z ( w , x ) The parameters of this model are typically learned from a training set, ( Y , X ) = { x i , y i } , i = 1 \u2026 N , with conditional maximum likelihood as in: (2) w = arg max w p ( Y | X , w ) Once the model has been trained, the prediction of a CRF is the sequence of labels maximizing the model for the given the input sequence and the learned parameters: (3) y \u2217 = arg max y p ( y | x , w ) The labels are typically predicted using a Viterbi-style algorithm which provides the optimal prediction for the measurement sequence as a whole. The model is trained by maximizing the conditional likelihood, or cross-entropy, over a given training set. For its implementation, we have used the HCRF library [26]. The features used as input are described in Section 4. In the experiments, we use the CRF as a useful baseline for performance comparison with the proposed neural networks. Note that a CRF model is also used as the output layer in the Bidirectional LSTM-CRF as explained in the next section. 3.2 Bidirectional LSTM and bidirectional LSTM-CRF RNNs are a type of neural network architecture in which connections between units form a directed cycle, creating an internal state and achieving dynamic temporal behavior. Thanks to their internal memory, RNNs can process a sequence of vectors ( x 1 , x 2 ,\u2026, x n ) as input and produce another sequence ( h 1 , h 2 ,\u2026, h n ) as output that contains some extent of sequential information about every vector in the input. However, these architectures in practice fail to learn long-term dependencies in the sequences as they tend to be biased by the most recent vectors [27]. The Long Short-Term Memory (LSTM) was therefore designed to overcome this issue by incorporating a gated memory-cell that has been shown to capture long-term dependencies [28]. Eq. (4) shows the implementations of the different gates in the LSTM [5], where i t is the \u201cinput\u201d gate, c t is the \u201ccell\u201d gate, o t is the \u201coutput\u201d gate, W are the weights of the network, b are the biases, \u03c3 is the element-wise sigmoid function, and \u2299 is the element-wise product. The bidirectional LSTM (B-LSTM) is just a variation, in which both the left-to-right ( h \u2192 t ) and the right-to-left ( h \u2190 t ) representations of the input sentence are generated, and then concatenated h t = [ h \u2192 t ; h \u2190 t ] in order to obtain the final representation. (4) i t = \u03c3 ( W xi x t + W hi h t - 1 + W ci c t - 1 + b i ) c t = ( 1 - i t ) \u2299 c t - 1 + i t \u2299 tanh ( W xc x t + W hc h t - 1 + b c ) ] o t = \u03c3 ( W xo x t + W ho h t - 1 + W co c t + b o ) h t = o t \u2299 tanh ( c t ) When applying the LSTM in NER, the words in the input sentence are first mapped to numerical vectors. These vectors can be random valued, a pre-trained word embedding, domain-specific word features or any combination of them. For each vector, the output of the network are the posterior probabilities of each named-entity class. An improvement of these networks has been presented by Lample et al. [5] using a CRF as a final output layer. This final layer provides the system with the ability to perform joint decoding of the input sequence in a Viterbi-style manner. The resulting network is known as the Bidirectional LSTM-CRF (B-LSTM-CRF). We test the LSTM models with the same features used for the CRF in order to establish the fairest-possible comparison. The features are described in detail in Section 4. Fig. 2 shows a descriptive diagram of the Bidirectional LSTM-CRF. 4 Word features As mentioned above, neural networks can learn meaningful representations from random initializations of word embeddings. However, it has been proved that pre-trained word embeddings can improve the performance of the network [5,11,18,24]. In this section, we present the pre-trained embeddings employed in lieu of the random assignments. 4.1 Specialized word embeddings A word embedding maps a word to a numerical vector in a vector space, where semantically-similar words are expected to be assigned similar vectors. To perform this mapping, we have used a well-known algorithm called GloVe [8]. This algorithm learns word embeddings by looking at the co-occurrences of the word in the training data, assuming that a word\u2019s meaning is mostly defined by its context and, therefore, words having similar contexts should have similar embeddings. GloVe can be trained from large, general-purpose datasets such as Wikipedia, Gigaword5 or Common Crawl without the need for any manual supervision. In this work, we have experimented with different general-purpose, pre-trained word embeddings from the official GloVe website [29] and noticed that the embeddings trained with Common Crawl (cc) (2.2\u202fM unique words) were giving the best results. We have employed these embeddings on their own, and also concatenated with the MIMIC-III embeddings (cc/mimic). By default, the code always initializes the word embedding of each unique word in the dictionary with a unique random vector. In alternative, we replace the random initialization with a pre-trained embedding. However, although such datasets generate good embeddings in many cases, for domain-specific tasks such as DNR and CCE they can suffer from some lack of vocabulary. As a matter of fact, in health corpora it is common to find very technical and unusual words which are specific to the health domain. If GloVe is trained only with general-purpose datasets, it is likely that such words will be missing and will still have to be assigned with random vectors. In order to solve this problem, we have generated a new word embedding by re-training GloVe with a large health domain dataset called MIMIC-III [10]. This dataset contains records of 53,423 distinct hospital admissions of adults to an intensive care unit between 2001 and 2012. The data, structured in 26 tables, include information such as vital signs, observations of care providers, diagnostic codes etc. We expect such a dataset to contain many of the technical words from the health domain that may not appear in general-domain datasets, and as the size of MIMIC-III is sufficiently large, we should be able to extract meaningful vector representations for these words. As a first step, we have selected a subset of the tables and columns, and generated a new dataset where each selected cell together with the title of the corresponding column form a pseudo-sentence. As the next step, we have used this dataset to re-train GloVe, and concatenated these specialized word embeddings with the others to create vectors that contain information from both approaches. Obviously, there are words that appear in the general dataset, but not in MIMIC-III, and the vice versa. In such cases, the corresponding embedding is still assigned randomly. If a word does not appear in either dataset, we assign its whole embedding randomly. In all cases, the embeddings are updated during training by the backpropagation step. 4.2 Character-level embeddings Following Lample et al. [5] we also add character-level embeddings of the words. Such embeddings reflect the actual sequence of characters of a word and have proven to be useful for specific-domain tasks and morphologically-rich languages. Typically, they contribute to catching prefixes and suffixes which are frequent in the domain, and correctly classifying the corresponding words. As an example, a word ending in \u201ccycline\u201d is very likely a drug name, and a character-level embedding could help classify it correctly even if the word was not present in the training vocabulary. All the characters are initialized with a random embedding, and then the embeddings are passed character-by-character to a dedicated LSTM in both forward and backward order. The final outputs in the respective directions promise to be useful encodings of the ending and the beginning of the word. These character-level embeddings are integral part of the LSTM architecture and are not available in the CRF or other models. The character embeddings, too, are updated during training with backpropagation. 4.3 Feature augmentation Conventional machine learning approaches for NER usually have a feature engineering step. Lee et al. [24] have shown that adding hand-crafted features to a neural network can contribute to increase the recall. In our work, we try this approach with features similar to those used by Lee et al. [24] Fig. 3 shows the list of features used. The distinct values of each feature are encoded onto short random vectors, for a total dimension of 146-D. During training, these encodings are updated as part of the backpropagation step. 5 Results 5.1 Datasets Hereafter, we evaluate the models on three datasets in the health domain. The first is the 2010 i2b2/VA IRB Revision (we refer to it as i2b2/VA for short in the following) and is used for evaluating CCE. This dataset is a reduced version of the original 2010 i2b2/VA dataset that is no longer distributed due to restrictions introduced by the Institutional Review Board (IRB) in 2011 [30]. The other two datasets are DrugBank and MedLine, both part of the SemEval-2013 Task 9.1 for DNR [31]. Table 1 a and b describes the basic statistics of these datasets. For the experiments, we have used the official training and test splits released with the distributions. 5.2 Evaluation metrics We report the performance of the model in terms of the F1 score. The F1 score is a very relevant measure as it considers both the precision and the recall, computing a weighted average of them. If we note as TP the number of true positives, FP the false positives and FN the false negatives, we have: \u2022 precision = TP TP + FP \u2022 recall = TP TP + FN \u2022 F 1 = 2 precision \u00b7 recall precision + recall However, it must be remarked that there are different ways of computing the precision and the recall, depending on what we consider as a correct or incorrect prediction [32]. In this work, we employ the \u201cstrict\u201d evaluation method, where both the entity class and its exact boundaries are expected to be correct. We have used the B-I-O tagging standard to annotate the text at word level. In detail, \u2018B\u2019 means the beginning (first word) of a named entity; \u2018I\u2019 stands for \u2018inside\u2019, meaning that the word is part of the same entity (for multi-word entities; e.g., \u201calbuterol sulfate\u201d); and \u2018O\u2019 stands for \u2018outside\u2019, meaning that the word is not part of any named entities. Therefore, a valid annotation of a named entity always begins with a \u2018B\u2019. An example is shown in Fig. 4 . All the models used in this paper have been trained to predict explicit \u2018B\u2019 and \u2018I\u2019 labels for each entity class. The evaluation includes a pre-processing step that converts an \u2018I\u2019 prediction to a \u2018B\u2019 if it follows directly an \u2018O\u2019 prediction, thus making all predicted entities valid. An entity is considered as correctly predicted only if all its \u2018B\u2019 and \u2018I\u2019 labels and all its classes are predicted correctly. In the example of Fig. 4 the prediction will be counted as a true positive only if all the four words \u201crecently diagnosed abdominal carcinomatosis\u201d are tagged as a single entity of the problem class. Every differing \u2018B\u2019 prediction will instead be counted as a false positive. The evaluation protocol explicitly counts only the true positives and the false positives, and derives the false negatives as (number of true entities \u2013 true positives). 5.3 Training and hyper-parameters For an unbiased evaluation, all the trained models have been tested blindly on unseen test data. In order to facilitate replication of the empirical results, we have used a publicly-available library for the implementation of the neural networks (i.e. the Theano neural network toolkit [33]) and we release our code [34]. To operate, any machine learning model requires both a set of parameters, which are learned automatically during training, and some \u201chyper-parameters\u201d, which have to be selected manually. Therefore, we have divided the training set of each dataset into two parts: a training set for learning the parameters (70%), and a validation set (30%) for selecting the best hyper-parameters [35]. The hyper-parameters of the LSTM include the number of hidden nodes (for both LSTM versions), ( H w , H c ) \u2208 { 25 , 50 , 100 } ; the word embedding dimension, dw \u2208 { 50 , 100 , 200 , 300 , 600 } ; and the character embedding dimension, d c \u2208 { 25 , 50 , 100 } . Additional hyper-parameters include the learning rate and the drop-out rate, which were left to their default values of [0.01] and [0.5] respectively [36]. All weights in the network, feature encodings and the words that do not have a pre-trained word embedding have been initialized randomly from the uniform distribution within range [\u22121, 1], and updated during training with backpropagation. The number of training \u201cepochs\u201d (i.e., iterations) was set to 100, selecting the epoch that obtained the best results on the validation set. The best model from the validation set was finally tested on the unseen, independent test set without any further tuning, and the corresponding accuracy reported in the tables. Table 2 shows all the hyper-parameters used for the experiments reported in the Results section. 5.4 Results Table3 a and b shows the results of the proposed models and the state-of-the-art systems on the CCE task (i2b2/VA dataset) and DNR task (DrugBank and MedLine datasets), respectively. In the following subsections, we discuss the results obtained for each task. 5.4.1 CCE results over the i2b2/VA dataset On the i2b2/VA dataset (Table 3a), the Bidirectional LSTM-CRF (B-LSTM-CRF) with Common Crawl embeddings (cc) and character-level embeddings (char) as features has obtained the best results (83.35% F1 score). The model has outperformed all systems from the literature (top quadrant of Table 3a) which are all based on conventional domain-specific feature engineering. It is important to note that deBruijin et al. [4] had reported a higher accuracy on 2010 i2b2/VA (85.23% F1 score), but their model was trained and tested on the original version of the dataset which is no longer available due to the restrictions introduced by the Institutional Review Board. As for what specialized embeddings are concerned, Table 4 shows that the general-domain dataset Common Crawl already contains almost all the words in the dataset. Therefore, adding the MIMIC-III embeddings (mimic) does not extend the vocabulary, and therefore it brings no improvement. On the other hand, the B-LSTM has improved by 0.3\u202fpp with the cc/mimic embeddings. Even though the mimic embeddings do not cover significant extra vocabulary, they may have enriched the feature space. Conversely, the cc/mimic embeddings have provided no improvements with the B-LSTM-CRF. For this, we need to take into account that the B-LSTM-CRF already has a high score (83.35% F1-score). Consequently, it may be more difficult to improve its results. Conversely, using conventional feature engineering has led to lower accuracy (77.81% F1-score). Eventually, concatenating both the features and the pre-trained embeddings showed no improvement over the best model. Table 3a also shows the importance of using a final CRF layer in the B-LSTM-CRF, given that the B-LSTM alone was only able to achieve a 77.59% F1 score. At its turn, the CRF baseline has only obtained a 64.09% F1 score in its best configuration, lower than any version of the LSTM. 5.4.2 DNR results over the DrugBank and MedLine datasets In the DNR task (Table 3b), the proposed B-LSTM-CRF with the concatenated word embeddings (cc/mimic) and the character-level embeddings (char) has improved over all the previous approaches on both DrugBank (88.38% F1 score) and MedLine (60.66% F1 score). Table 4 shows that only 49% of the words in the datasets have been found in the cc embeddings. However, when the concatenated embeddings (cc/mimic) are used, the percentage of found words has increased to 67% for DrugBank and 61% for MedLine, leading to better results in the classification task. Words that appear in the MIMIC-III dataset but are not contained in Common Crawl are typically very technical and domain-specific, such as drug names or treatments; examples include: pentostatin, sitagliptin, hydrobromide, organophosphate, pyhisiological and methimazole. In total, 1189 extra words have been mapped in DrugBank and 716 in MedLine thanks to the use of MIMIC-III. However, the B-LSTM has only obtained an accuracy improvement on the MedLine dataset, but not on DrugBank. This can be explained by the fact that the accuracy of the B-LSTM on MedLine is very low (44.33%) and, therefore, easy to improve. Instead, on DrugBank the accuracy of the B-LSTM is already very high (84.35% F1-score) and thus difficult to improve. With the B-LSTM-CRF, results with extra vocabulary covered by the cc/mimic embeddings have improved with both datasets. As for what concerns the hand-crafted features, their use has led to higher accuracy than with the Common Crawl embeddings on the DrugBank dataset in two cases. However, the concatenation of the features and the pre-trained embeddings has not improved the best results. As in the CCE task, the B-LSTM-CRF model has proved better than the B-LSTM alone on both DrugBank (88.38% vs 84.35% F1-score) and MedLine (60.66% vs 45.92% F1-score.) Finally, we can see that the use of the character-level embeddings has led to higher relative improvements for DrugBank than for the other two datasets. A plausible explanation for this is that this dataset contains more words with distinctive prefixes and suffixes which are more effectively captured by the character-level embeddings. In general, the CRF has significantly underperformed compared to the neural networks. We speculate that this model may require more extensive feature engineering to achieve a comparable performance, or that it may not be able to achieve it at all. In particular, we see that the CRF has performed the worst with MedLine. A possible explanation can be found in the \u201ccurse of dimensionality\u201d: MedLine is a small dataset (1627 training sentences), while the overall dimensionality of the input embeddings is 746. This makes the learning problem very sparse and seems to seriously affect a linear model such as the CRF. On the contrary, the non-linear internal architecture of the neural networks may in some cases help reduce the effective dimensionality and mollify this problem. 5.4.3 Accuracy by entity classes Table5 a and b break down the results by entity class for the best model on each dataset. With the MedLine dataset, we can notice the poor performance at detecting brand. In DrugBank, the same issue occurs with entity class drug_n. This issue is likely attributable to the small sample size. Instead, the i2b2/VA dataset all entity classes are detected with similar F1 scores, likely owing to the larger number of samples per class. However, we see that brand achieves the second best F1-score in DrugBank despite its relatively low frequency in the dataset, and that drug_n obtains a very poor performance in MedLine even if it has the second highest frequency. We identify two other main factors that may have a major impact on the accuracy: (1) the average length of the entities in each class, and (2) the number of test entities that had not been seen during the training stage. In this respect, the brand and drug entities are usually very short (average\u202f\u223c\u202f1 word), while the group and drug_n entities often have multiple words. Since shorter entities are easier to predict correctly, brand obtains better accuracy than group in DrugBank. On the other hand, the drug_n and group entities have similar length, but in MedLine drug_n obtains a very poor performance. This is most likely because no entity of type drug_n that appears in the test set had been seen during training. Conversely, a large percentage of the test group entities had been seen during training and have therefore proved easier to predict. 6 Conclusion In this paper, we have set to investigate the effectiveness of the Bidirectional LSTM and Bidirectional LSTM-CRF \u2013 two specific architectures of recurrent neural networks \u2013 for drug name recognition and clinical concept extraction, and compared them with a baseline CRF model. As input features, we have applied combinations of different word embeddings (Common Crawl and MIMIC\u2013III), character-level embedding and conventional feature engineering. We have showed that the neural network models have obtained significantly better results than the CRF, and reported state-of-the-art results over the i2b2/VA, DrugBank and MedLine datasets using the B-LSTM-CRF model. We have also provided evidence that retraining GloVe on a domain-specific dataset such as MIMIC-III can help learn vector representations for domain-specific words and increase the classification accuracy. Finally, we have showed that adding hand-crafted features does not further improve performance since the neural networks can learn useful word representations automatically from pre-trained word embeddings. Consequently, time-consuming, domain-specific feature engineering can be usefully avoided. Conflict of interest The authors of this work do not have any kind of conflict of interests. References [1] I. Segura-Bedmar, V. Suarez-Paniagua, P. Martinez, Exploring word embedding for drug name recognition, in: 6th International Workshop on Health Text Mining and Information Analysis (LOUHI), 2015, p. 64. [2] A.B. Abacha M.F.M. Chowdhury A. Karanasiou Y. Mrabet A. Lavelli P. Zweigenbaum Text mining for pharmacovigilance: using machine learning for drug name recognition and drug-drug interaction extraction and classification J. Biomed. Inform. 58 2015 122 132 [3] T. Rocktaschel, T. Huber, M. Weidlich, U. Leser, WBI-NER: the impact of domain specific features on the performance of identifying and classifying mentions of drugs, in: 7th International Workshop on Semantic Evaluation, 2013, pp. 356\u2013363. [4] B. deBruijn C. Cherry S. Kiritchenko J. Martin X. Zhu Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010 J. Am. Med. Inform. Assoc. 18 5 2011 557 562 [5] G. Lample M. Ballesteros S. Subramanian K. Kawakami C. Dyer Neural architectures for named entity recognition North American Chapter of the Association for Computational Linguistics (NAACL) 2016 [6] G. Hinton L. Deng D. Yu G.E. Dahl A.R. Mohamed N. Jaitly Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups IEEE Signal Process. Mag. 29 6 2012 82 97 [7] A. Krizhevsky I. Sutskever G.E. Hinton Imagenet classification with deep convolutional neural networks Adv. Neural Inform. Process. Syst. (NIPS) 2012 1097 1105 [8] J. Pennington R. Socher C.D. Manning Glove: global vectors for word representation Empir. Methods Nat. Lang. Process. (EMNLP) 14 2014 1532 1543 [9] T. Mikolov I. Sutskever K. Chen G.S. Corrado J. Dean Distributed representations of words and phrases and their compositionality Adv. Neural Inform. Process. Syst. (NIPS) 2013 3111 3119 [10] A.E. Johnson T.J. Pollard L. Shen L.W. Lehman M. Feng M. Ghassemi B. Moody P. Szolovits L.A. Celi R.G. Mark MIMIC-III, a freely accessible critical care database Sci. Data 3 2016 [11] R. Chalapathy, E.Z. Borzeshi, M. Piccardi, An investigation of recurrent neural architectures for drug name recognition, in: 7th International Workshop on Health Text Mining and Information Analysis (LOUHI), 2016. [12] R. Chalapathy E.Z. Borzeshi M. Piccardi Bidirectional LSTM-CRF for clinical concept extraction Clinical Natural Language Processing Workshop (ClinicalNLP) 2016 [13] S. Liu B. Tang Q. Chen X. Wang X. Fan Feature engineering for drug name recognition in biomedical texts: feature conjunction and feature selection Comput. Math. Methods Med. 2015 1 9 [14] W. Boag K. Wacome T. Naumann A. Rumshisky Cliner: a lightweight tool for clinical named entity recognition AMIA Joint Summits on Clinical Research Informatics (poster) 2015 [15] R. Lebret R. Collobert Word emdeddings through hellinger PCA European Chapter of the Association for Computational Linguistics (EACL) 2013 [16] A. Nikfarjam A. Sarker K. O\u2019Connor R. Ginn G. Gonzalez Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features J. Am. Med. Inform. Assoc. 2015 [17] Y. Wu J. Xu M. Jiang Y. Zhang H. Xu A study of neural word embeddings for named entity recognition in clinical text AMIA Ann. Symp. Proc. 2015 [18] F. Dernoncourt J.Y. Lee O. Uzuner P. Szolovits De-identification of patient notes with recurrent neural networks J. Am. Med. Inform. Assoc. 156 2016 [19] A. Cocos A.G. Fiks A.J. Masino Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twitter posts J. Am. Med. Inform. Assoc. 180 2017 [20] J. Xie X. Liu Zeng D. Dajun Mining e-cigarette adverse events in social media using Bi-LSTM recurrent neural network with word embedding representation J. Am. Med. Inform. Assoc. 45 2017 [21] Q. Wei T. Chen R. Xu Y. He L. Gui Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks Database 2016 [22] A.N. Jagannatha H. Yu Structured prediction models for RNN based sequence labeling in clinical text Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016 2016 856 [23] M. Gridach Character-level neural network for biomedical named entity recognition J. Biomed. Inform. 70 2017 85 91 [24] J.Y. Lee F. Dernoncourt O. Uzuner P. Szolovits Feature-augmented neural networks for patient note de-identification Clinical Natural Language Processing Workshop (ClinicalNLP) 2016 [25] J. Lafferty A. McCallum F. Pereira Conditional random fields: Probabilistic models for segmenting and labeling sequence data International Conference on Machine Learning (ICML) 1 2001 282 289 [26] HCRF. Available from: <http://multicomp.ict.usc.edu/?p=790>. [27] Y. Bengio P. Simard P. Frasconi Learning long-term dependencies with gradient descent is difficult IEEE Trans. Neural Networks 5 2 1994 157 166 [28] S. Hochreiter J. Schmidhuber Long short-term memory Neural Comput. 9 8 1997 1735 1780 [29] GloVe. Available from: <https://nlp.stanford.edu/projects/glove/>. [30] \u00d6. Uzuner B.R. South S. Shen S.L. DuVall 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text J. Am. Med. Inform. Assoc. 18 5 2011 552 556 [31] M. Herrero-Zazo I. Segura-Bedmar P. Mart\u00ednez T. Declerck The DDI corpus: An annotated corpus with pharmacological substances and drug\u2013drug interactions J. Biomed. Inform. 46 5 2013 914 920 [32] D. Nadeau S. Sekine A survey of named entity recognition and classification Lingvisticae Investigationes 30 1 2007 3 26 [33] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, Y. Bengio, Theano: a CPU and GPU math compiler in Python, in: Proc. 9th Python in Science Conf. 2010, pp. 1\u20137. [34] HealthNER. Available from: <https://github.com/ijauregiCMCRC/healthNER>. [35] J. Bergstra Y. Bengio Random search for hyper-parameter optimization J. Mach. Learn. Res. 13 2012 281 305 [36] N. Srivastava G.E. Hinton A. Krizhevsky I. Sutskever R. Salakhutdinov Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 15 1 2014 1929 1958 [37] X. Fu, S. Ananiadou, Improving the extraction of clinical concepts from clinical records, in: Proceedings of BioTxtM14 Workshop, 2014.", "scopus-id": "85034622038", "pubmed-id": "29146561", "coredata": {"eid": "1-s2.0-S1532046417302447", "dc:description": "Abstract Background Previous state-of-the-art systems on Drug Name Recognition (DNR) and Clinical Concept Extraction (CCE) have focused on a combination of text \u201cfeature engineering\u201d and conventional machine learning algorithms such as conditional random fields and support vector machines. However, developing good features is inherently heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks (RNNs) have proved capable of automatically learning effective features from either random assignments or automated word \u201cembeddings\u201d. Objectives (i) To create a highly accurate DNR and CCE system that avoids conventional, time-consuming feature engineering. (ii) To create richer, more specialized word embeddings by using health domain datasets such as MIMIC-III. (iii) To evaluate our systems over three contemporary datasets. Methods Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning approach. The same features are used for all the models. Results We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all previously proposed systems. The specialized embeddings have helped to cover unusual words in DrugBank and MedLine, but not in the i2b2/VA dataset. Conclusions We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over datasets that are adequate for the domain, in order to adequately cover the domain-specific vocabulary.", "openArchiveArticle": "true", "prism:coverDate": "2017-12-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046417302447", "dc:creator": [{"@_fa": "true", "$": "Jauregi Unanue, I\u00f1igo"}, {"@_fa": "true", "$": "Zare Borzeshi, Ehsan"}, {"@_fa": "true", "$": "Piccardi, Massimo"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046417302447"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046417302447"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(17)30244-7", "prism:volume": "76", "prism:publisher": "Elsevier Inc.", "dc:title": "Recurrent neural networks with specialized word embeddings for health-domain named-entity recognition", "prism:copyright": "\u00a9 2017 Elsevier Inc.", "openaccess": "1", "prism:issn": "15320464", "dcterms:subject": [{"@_fa": "true", "$": "Neural networks (computer) [MeSH]"}, {"@_fa": "true", "$": "Machine learning [MeSH]"}, {"@_fa": "true", "$": "Artificial intelligence [MeSH]"}, {"@_fa": "true", "$": "Clinical concept extraction"}, {"@_fa": "true", "$": "Drug name recognition"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "102-109", "prism:endingPage": "109", "prism:coverDisplayDate": "December 2017", "prism:doi": "10.1016/j.jbi.2017.11.007", "prism:startingPage": "102", "dc:identifier": "doi:10.1016/j.jbi.2017.11.007", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "130", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-fx1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5110", "@ref": "fx1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "52", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3753", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "127", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4464", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3605", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "28", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2168", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "200", "@width": "338", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-fx1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "14665", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "158", "@width": "667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25872", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "388", "@width": "667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31756", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "141", "@width": "667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "32731", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "85", "@width": "667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "14379", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "886", "@width": "1496", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-fx1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "82650", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "701", "@width": "2952", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "155320", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1717", "@width": "2953", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "162030", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "623", "@width": "2953", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "180671", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "378", "@width": "2953", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "87801", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "11", "@width": "10", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "187", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "184", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "693", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "165", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "758", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "159", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "777", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "9", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "192", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "9", "@width": "17", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "201", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "9", "@width": "17", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "203", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "17", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "223", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "18", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "230", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "18", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "230", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "196", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "7", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "174", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "9", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "192", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "9", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si21.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "198", "@ref": "si21", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si22.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "214", "@ref": "si22", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si23.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "197", "@ref": "si23", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "7", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si24.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "178", "@ref": "si24", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si25.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si25", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si26.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "237", "@ref": "si26", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "30", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si27.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "255", "@ref": "si27", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "95", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si28.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "512", "@ref": "si28", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "74", "@width": "365", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si29.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "3387", "@ref": "si29", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "399", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "135", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si30.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "550", "@ref": "si30", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "109", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si31.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "490", "@ref": "si31", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "25", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si32.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "722", "@ref": "si32", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "165", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si33.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "780", "@ref": "si33", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "198", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si34.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "912", "@ref": "si34", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "119", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si35.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "606", "@ref": "si35", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "7", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "185", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "49", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "350", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "50", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "353", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "40", "@width": "187", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046417302447-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "981", "@ref": "si9", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85034622038"}}