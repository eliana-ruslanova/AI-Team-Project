{"scopus-eid": "2-s2.0-84992190571", "originalText": "serial JL 272510 291210 291738 291782 291833 31 90 Neurobiology of Learning and Memory NEUROBIOLOGYLEARNINGMEMORY 2016-10-13 2016-10-13 2016-10-18 2016-10-18 2017-03-01T13:16:36 1-s2.0-S1074742716302532 S1074-7427(16)30253-2 S1074742716302532 10.1016/j.nlm.2016.10.007 S300 S300.2 FULL-TEXT 1-s2.0-S1074742716X0010X 2017-03-01T08:36:02.153089-05:00 0 0 20161201 20161231 2016 2016-10-13T21:06:08.121233Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref 1074-7427 10747427 UNLIMITED NONE true 136 136 C Volume 136 19 147 165 147 165 201612 December 2016 2016-12-01 2016-12-31 2016 Regular Articles article fla \u00a9 2016 The Author(s). Published by Elsevier Inc. NEURALNETWORKMODELDEVELOPSBORDEROWNERSHIPREPRESENTATIONTHROUGHVISUALLYGUIDEDLEARNING EGUCHI A 1 Introduction 1.1 Hypothesis 2 Materials & methods 2.1 VisNet model 2.1.1 Pre-processing of the visual input by Gabor filters 2.1.2 Activations of neurons and competition within the network 2.1.3 Modification of synaptic weights during training 2.2 Analysis techniques 2.2.1 Single-cell information 2.2.2 Multiple-cell information 3 Simulation results 3.1 Study 1: simulation of the visually-guided development of border ownership representations 3.1.1 Steady state firing properties of cells in layers 1 and 3 at the end of each stimulus presentation 3.1.2 Dynamical firing properties of cells in layer 1 during each stimulus presentation: time course of the emergence of border ownership signals 3.2 Study 2: failure of the model under more general stimulus conditions 3.2.1 Proposed mechanism by which border ownership information carried by V1/V2 neurons in the rate-coded model may be lost when the network is presented with multiple visual objects 3.2.2 Results 4 Discussion References ANGELUCCI 2003 141 154 A BAEK 2005 125 130 K BI 1998 10464 10472 G COWEY 1974 447 454 A COX 2005 1145 1147 D CRAFT 2007 4310 4326 E CUMMING 1999 5602 5618 B DANIEL 1961 203 221 P DECO 2002 G AUNIFIEDMODELSPATIALOBJECTATTENTIONBASEDINTERCORTICALBIASEDCOMPETITION DECO 2004 621 642 G EGUCHI 2015 100 A FOLDIAK 1991 194 200 P FREEMAN 2011 1195 1201 J GROSS 1969 1303 1306 C IZHIKEVICH 2006 245 282 E JEHEE 2007 1153 1165 J JONES 1987 1187 1211 J KOHONEN 1982 59 69 T LADES 1993 300 311 M LAYTON 2012 O MARKRAM 1997 213 215 H MARTIN 2015 6860 6870 A MOTTER 2009 5749 5757 B NISHIMURA 2004 843 848 H PASUPATHY 2006 293 313 A PASUPATHY 2001 2505 2519 A PASUPATHY 2002 1332 1338 A PETKOV 1997 83 96 N PETTET 1992 8366 8370 M QIU 2007 1492 1499 F RENART 1999 237 255 A ROELFSEMA 2004 982 991 P ROLLS 2000 205 218 E ROLLS 2008 E MEMORYATTENTIONDECISIONMAKINGAUNIFYINGCOMPUTATIONALNEUROSCIENCEAPPROACH ROLLS 2012 35 E ROLLS 2003 339 348 E ROLLS 2000 2547 2572 E ROLLS 1997 309 333 E ROYER 2003 518 522 S RUBIN 1915 E SYNSOPLEVEDEFIGURER RUMELHART 1985 75 112 D SUGIHARA 2011 374 385 T TRAPPENBERG 2002 293 300 T TROMANS 2011 e25616 J VONDERHEYDT 2003 281 304 R VONDERMALSBURG 1973 85 100 C WAGATSUMA 2013 N WALLIS 1997 167 194 G WASSLE 1990 1897 1911 H ZHAOPING 2005 143 153 L ZHOU 2000 6594 6611 H EGUCHIX2016X147 EGUCHIX2016X147X165 EGUCHIX2016X147XA EGUCHIX2016X147X165XA Full 2016-10-12T13:20:33Z Author http://creativecommons.org/licenses/by/4.0/ 2017-10-18T00:00:00.000Z UnderEmbargo http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY license. \u00a9 2016 The Author(s). Published by Elsevier Inc. item S1074-7427(16)30253-2 S1074742716302532 1-s2.0-S1074742716302532 10.1016/j.nlm.2016.10.007 272510 2017-03-01T08:36:02.153089-05:00 2016-12-01 2016-12-31 UNLIMITED NONE 1-s2.0-S1074742716302532-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/MAIN/application/pdf/1e05280c529597b8661d73073eb1fa7f/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/MAIN/application/pdf/1e05280c529597b8661d73073eb1fa7f/main.pdf main.pdf pdf true 1855150 MAIN 19 1-s2.0-S1074742716302532-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/PREVIEW/image/png/97f99521ac175d0fc71a822611253186/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/PREVIEW/image/png/97f99521ac175d0fc71a822611253186/main_1.png main_1.png png 60757 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1074742716302532-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr1/THUMBNAIL/image/gif/797cd7cf8e72215e80145588f80e9863/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr1/THUMBNAIL/image/gif/797cd7cf8e72215e80145588f80e9863/gr1.sml gr1 gr1.sml sml 2284 163 166 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr10/THUMBNAIL/image/gif/3d5c96e1887da9da70ad8433896a4853/gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr10/THUMBNAIL/image/gif/3d5c96e1887da9da70ad8433896a4853/gr10.sml gr10 gr10.sml sml 7405 164 186 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr11/THUMBNAIL/image/gif/1eb1802a417277cbf23c98a8ef566618/gr11.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr11/THUMBNAIL/image/gif/1eb1802a417277cbf23c98a8ef566618/gr11.sml gr11 gr11.sml sml 8693 99 219 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr12/THUMBNAIL/image/gif/f1112531617811ca105a6523260af76b/gr12.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr12/THUMBNAIL/image/gif/f1112531617811ca105a6523260af76b/gr12.sml gr12 gr12.sml sml 5310 102 219 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr2/THUMBNAIL/image/gif/12024208839cad1c6230c0b186986618/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr2/THUMBNAIL/image/gif/12024208839cad1c6230c0b186986618/gr2.sml gr2 gr2.sml sml 7315 164 173 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr3/THUMBNAIL/image/gif/84ad8ee18e23784f49d6b5635c7de5a6/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr3/THUMBNAIL/image/gif/84ad8ee18e23784f49d6b5635c7de5a6/gr3.sml gr3 gr3.sml sml 8132 82 219 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr4/THUMBNAIL/image/gif/d4ec55f8030f4f8fe6d29302c2bac47d/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr4/THUMBNAIL/image/gif/d4ec55f8030f4f8fe6d29302c2bac47d/gr4.sml gr4 gr4.sml sml 11848 147 219 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr5/THUMBNAIL/image/gif/ea2fff8812f9bd9806101de98a78fdac/gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr5/THUMBNAIL/image/gif/ea2fff8812f9bd9806101de98a78fdac/gr5.sml gr5 gr5.sml sml 6089 164 139 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr6/THUMBNAIL/image/gif/6bdb7811288e709329c3bfa1c4652b0e/gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr6/THUMBNAIL/image/gif/6bdb7811288e709329c3bfa1c4652b0e/gr6.sml gr6 gr6.sml sml 9168 164 209 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr7/THUMBNAIL/image/gif/f08060188cec9a66a9d66fb80d25d580/gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr7/THUMBNAIL/image/gif/f08060188cec9a66a9d66fb80d25d580/gr7.sml gr7 gr7.sml sml 7195 164 144 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr8/THUMBNAIL/image/gif/c0cb28f56b0fbc188c9e51a015aa4c87/gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr8/THUMBNAIL/image/gif/c0cb28f56b0fbc188c9e51a015aa4c87/gr8.sml gr8 gr8.sml sml 9591 164 211 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr9/THUMBNAIL/image/gif/1e7cf944e01ae464a73e5605a84d5e94/gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr9/THUMBNAIL/image/gif/1e7cf944e01ae464a73e5605a84d5e94/gr9.sml gr9 gr9.sml sml 6283 164 170 IMAGE-THUMBNAIL 1-s2.0-S1074742716302532-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr1/DOWNSAMPLED/image/jpeg/24ec025de2a60f4e1261971b47642662/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr1/DOWNSAMPLED/image/jpeg/24ec025de2a60f4e1261971b47642662/gr1.jpg gr1 gr1.jpg jpg 6377 306 311 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr10/DOWNSAMPLED/image/jpeg/92facb30502e321ee3e3fa5482575167/gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr10/DOWNSAMPLED/image/jpeg/92facb30502e321ee3e3fa5482575167/gr10.jpg gr10 gr10.jpg jpg 28518 329 373 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr11/DOWNSAMPLED/image/jpeg/e358404888787cb633a92e6bc4947bc0/gr11.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr11/DOWNSAMPLED/image/jpeg/e358404888787cb633a92e6bc4947bc0/gr11.jpg gr11 gr11.jpg jpg 59500 334 738 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr12/DOWNSAMPLED/image/jpeg/223f6548d236c6426463bdfb7fcdb723/gr12.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr12/DOWNSAMPLED/image/jpeg/223f6548d236c6426463bdfb7fcdb723/gr12.jpg gr12 gr12.jpg jpg 36971 291 627 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr2/DOWNSAMPLED/image/jpeg/a2555943cf2c7d98069404ff52b1add2/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr2/DOWNSAMPLED/image/jpeg/a2555943cf2c7d98069404ff52b1add2/gr2.jpg gr2 gr2.jpg jpg 99266 704 744 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr3/DOWNSAMPLED/image/jpeg/66835f83a91f940d77276b262e0eaf2d/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr3/DOWNSAMPLED/image/jpeg/66835f83a91f940d77276b262e0eaf2d/gr3.jpg gr3 gr3.jpg jpg 69557 282 756 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr4/DOWNSAMPLED/image/jpeg/db149550b9f6cbe42faa1ea39a64901c/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr4/DOWNSAMPLED/image/jpeg/db149550b9f6cbe42faa1ea39a64901c/gr4.jpg gr4 gr4.jpg jpg 96698 506 756 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr5/DOWNSAMPLED/image/jpeg/6fd1ea8d30beb4f3e07f6bde9bf867ce/gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr5/DOWNSAMPLED/image/jpeg/6fd1ea8d30beb4f3e07f6bde9bf867ce/gr5.jpg gr5 gr5.jpg jpg 96401 811 689 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr6/DOWNSAMPLED/image/jpeg/50a011a2df4822cfc8fa1c922a84f348/gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr6/DOWNSAMPLED/image/jpeg/50a011a2df4822cfc8fa1c922a84f348/gr6.jpg gr6 gr6.jpg jpg 76631 548 699 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr7/DOWNSAMPLED/image/jpeg/4f87ee36d584f6789051bdc6112244af/gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr7/DOWNSAMPLED/image/jpeg/4f87ee36d584f6789051bdc6112244af/gr7.jpg gr7 gr7.jpg jpg 109623 785 689 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr8/DOWNSAMPLED/image/jpeg/5bddf3ac9499fdd2aeaf0bd0bbc9f6ec/gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr8/DOWNSAMPLED/image/jpeg/5bddf3ac9499fdd2aeaf0bd0bbc9f6ec/gr8.jpg gr8 gr8.jpg jpg 89759 536 691 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr9/DOWNSAMPLED/image/jpeg/2a6e71e88ed21243494589ce5a478f81/gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr9/DOWNSAMPLED/image/jpeg/2a6e71e88ed21243494589ce5a478f81/gr9.jpg gr9 gr9.jpg jpg 90948 691 717 IMAGE-DOWNSAMPLED 1-s2.0-S1074742716302532-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr1/HIGHRES/image/jpeg/161d597c3a7f0cadf090103958f2942c/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr1/HIGHRES/image/jpeg/161d597c3a7f0cadf090103958f2942c/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 37441 1356 1378 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr10/HIGHRES/image/jpeg/950c1de05976ff01ce96297dc0ea9504/gr10_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr10/HIGHRES/image/jpeg/950c1de05976ff01ce96297dc0ea9504/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 217678 1456 1651 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr11/HIGHRES/image/jpeg/3d8053e49e5927e63a6629a37e837770/gr11_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr11/HIGHRES/image/jpeg/3d8053e49e5927e63a6629a37e837770/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 290643 1478 3269 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr12_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr12/HIGHRES/image/jpeg/95ac4429c5d3f4ba8d22ab91a78a5605/gr12_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr12/HIGHRES/image/jpeg/95ac4429c5d3f4ba8d22ab91a78a5605/gr12_lrg.jpg gr12 gr12_lrg.jpg jpg 253998 1290 2776 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr2/HIGHRES/image/jpeg/c961e6c07d54829e43d5dafa5d4c670d/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr2/HIGHRES/image/jpeg/c961e6c07d54829e43d5dafa5d4c670d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 755324 3114 3293 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr3/HIGHRES/image/jpeg/f6299e522f5b7179f9dee781b52c6672/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr3/HIGHRES/image/jpeg/f6299e522f5b7179f9dee781b52c6672/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 638398 1250 3346 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr4/HIGHRES/image/jpeg/c7ecd5345e572e04c3798c4416204654/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr4/HIGHRES/image/jpeg/c7ecd5345e572e04c3798c4416204654/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 558837 2240 3348 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr5/HIGHRES/image/jpeg/01ff6e60d25a9b8ef720b16f6ad754e6/gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr5/HIGHRES/image/jpeg/01ff6e60d25a9b8ef720b16f6ad754e6/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 665636 3591 3051 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr6/HIGHRES/image/jpeg/1102eff90e50fe9ad127c5b97a63de5a/gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr6/HIGHRES/image/jpeg/1102eff90e50fe9ad127c5b97a63de5a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 509174 2428 3096 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr7/HIGHRES/image/jpeg/0ea7ab1dc9406be1004f6c2367f8b0e4/gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr7/HIGHRES/image/jpeg/0ea7ab1dc9406be1004f6c2367f8b0e4/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 778756 3474 3051 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr8/HIGHRES/image/jpeg/95ebda998d1ef76bbb4aa00e90d93abc/gr8_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr8/HIGHRES/image/jpeg/95ebda998d1ef76bbb4aa00e90d93abc/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 605169 2374 3059 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/gr9/HIGHRES/image/jpeg/a15226bd287841ec86bd8b35750ebab2/gr9_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/gr9/HIGHRES/image/jpeg/a15226bd287841ec86bd8b35750ebab2/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 660411 3056 3173 IMAGE-HIGH-RES 1-s2.0-S1074742716302532-si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/25869fd6d9d28f6c2e9552db4af2da04/si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/25869fd6d9d28f6c2e9552db4af2da04/si1.gif si1 si1.gif gif 195 12 13 ALTIMG 1-s2.0-S1074742716302532-si100.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6b7ba070a18cda8c4f0bb18723d91373/si100.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6b7ba070a18cda8c4f0bb18723d91373/si100.gif si100 si100.gif gif 227 21 12 ALTIMG 1-s2.0-S1074742716302532-si101.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/079c17db7e607805e0f364c76b2e292c/si101.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/079c17db7e607805e0f364c76b2e292c/si101.gif si101 si101.gif gif 1502 19 317 ALTIMG 1-s2.0-S1074742716302532-si102.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/9682d22d736d40732c1b8a7cd799caa0/si102.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/9682d22d736d40732c1b8a7cd799caa0/si102.gif si102 si102.gif gif 216 22 12 ALTIMG 1-s2.0-S1074742716302532-si104.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/f65ee7c5f032bf5afbfe3df9db05e7ad/si104.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/f65ee7c5f032bf5afbfe3df9db05e7ad/si104.gif si104 si104.gif gif 195 13 12 ALTIMG 1-s2.0-S1074742716302532-si106.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/7d7142abe8856b94ea20a54c52ad9213/si106.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/7d7142abe8856b94ea20a54c52ad9213/si106.gif si106 si106.gif gif 1716 46 248 ALTIMG 1-s2.0-S1074742716302532-si107.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/629e546bbcaed51939c1ad659b9c7503/si107.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/629e546bbcaed51939c1ad659b9c7503/si107.gif si107 si107.gif gif 1422 36 245 ALTIMG 1-s2.0-S1074742716302532-si108.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/d8fd28d7ba1fbc93a0ccf694cd0764bc/si108.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/d8fd28d7ba1fbc93a0ccf694cd0764bc/si108.gif si108 si108.gif gif 1136 20 230 ALTIMG 1-s2.0-S1074742716302532-si109.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/4e5745e5e65f2d293e255dffa81b5d1e/si109.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/4e5745e5e65f2d293e255dffa81b5d1e/si109.gif si109 si109.gif gif 213 22 12 ALTIMG 1-s2.0-S1074742716302532-si110.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/be01678bcf48e183f4c840007d345eda/si110.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/be01678bcf48e183f4c840007d345eda/si110.gif si110 si110.gif gif 1299 17 271 ALTIMG 1-s2.0-S1074742716302532-si112.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/be5503f5794a435dd8462a6e19258fb2/si112.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/be5503f5794a435dd8462a6e19258fb2/si112.gif si112 si112.gif gif 257 21 19 ALTIMG 1-s2.0-S1074742716302532-si113.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/f3e20a8c8957165db1736dfda1fe5d3b/si113.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/f3e20a8c8957165db1736dfda1fe5d3b/si113.gif si113 si113.gif gif 464 24 71 ALTIMG 1-s2.0-S1074742716302532-si115.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/d21bccc360b1119716e6eea2b9780a8e/si115.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/d21bccc360b1119716e6eea2b9780a8e/si115.gif si115 si115.gif gif 229 23 13 ALTIMG 1-s2.0-S1074742716302532-si116.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/2f33d626d11909003150c2503a4dbf3c/si116.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/2f33d626d11909003150c2503a4dbf3c/si116.gif si116 si116.gif gif 339 17 37 ALTIMG 1-s2.0-S1074742716302532-si117.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/d9b7c19c528d68bf1f0e0a09d23dbe21/si117.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/d9b7c19c528d68bf1f0e0a09d23dbe21/si117.gif si117 si117.gif gif 213 16 12 ALTIMG 1-s2.0-S1074742716302532-si118.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/dc6d7a1e56a4ca2f1b2bef96f8183bd0/si118.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/dc6d7a1e56a4ca2f1b2bef96f8183bd0/si118.gif si118 si118.gif gif 198 11 12 ALTIMG 1-s2.0-S1074742716302532-si119.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/9a2b4e30667de2abc45e806404f1c6bf/si119.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/9a2b4e30667de2abc45e806404f1c6bf/si119.gif si119 si119.gif gif 365 17 46 ALTIMG 1-s2.0-S1074742716302532-si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/c63fd61be4e677c203a940ade2f40633/si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/c63fd61be4e677c203a940ade2f40633/si12.gif si12 si12.gif gif 570 25 63 ALTIMG 1-s2.0-S1074742716302532-si120.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/96f7f9116e0d890728b2ac25f5c5d13d/si120.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/96f7f9116e0d890728b2ac25f5c5d13d/si120.gif si120 si120.gif gif 456 17 61 ALTIMG 1-s2.0-S1074742716302532-si122.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/bc93ec3277d7e3ef8a3457cdcbbfad1c/si122.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/bc93ec3277d7e3ef8a3457cdcbbfad1c/si122.gif si122 si122.gif gif 482 14 78 ALTIMG 1-s2.0-S1074742716302532-si123.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/e2f7addfa761e9436bdb467a2b49171b/si123.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/e2f7addfa761e9436bdb467a2b49171b/si123.gif si123 si123.gif gif 268 14 42 ALTIMG 1-s2.0-S1074742716302532-si124.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6301d66620af75e9d3d0944ae7c94bb5/si124.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6301d66620af75e9d3d0944ae7c94bb5/si124.gif si124 si124.gif gif 325 17 49 ALTIMG 1-s2.0-S1074742716302532-si125.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/35c47ea9143a944957ae212adc08dd7b/si125.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/35c47ea9143a944957ae212adc08dd7b/si125.gif si125 si125.gif gif 440 17 52 ALTIMG 1-s2.0-S1074742716302532-si126.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6c5686ac9488f0195c02bd15871b0f8f/si126.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6c5686ac9488f0195c02bd15871b0f8f/si126.gif si126 si126.gif gif 511 17 84 ALTIMG 1-s2.0-S1074742716302532-si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/2bfed9b1b197ced5e750a692c23d4d9d/si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/2bfed9b1b197ced5e750a692c23d4d9d/si18.gif si18 si18.gif gif 417 23 38 ALTIMG 1-s2.0-S1074742716302532-si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/ea9e4056d4c0220e5f3c3b76085deb93/si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/ea9e4056d4c0220e5f3c3b76085deb93/si2.gif si2 si2.gif gif 519 24 54 ALTIMG 1-s2.0-S1074742716302532-si27.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/a15cdaf5cbd69f7768761261eecef58f/si27.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/a15cdaf5cbd69f7768761261eecef58f/si27.gif si27 si27.gif gif 1209 25 188 ALTIMG 1-s2.0-S1074742716302532-si29.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/a347790c10721b0db08c87027fdaeacc/si29.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/a347790c10721b0db08c87027fdaeacc/si29.gif si29 si29.gif gif 512 17 84 ALTIMG 1-s2.0-S1074742716302532-si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/dca026947d8502bdca0a3fa61c598f13/si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/dca026947d8502bdca0a3fa61c598f13/si3.gif si3 si3.gif gif 562 25 62 ALTIMG 1-s2.0-S1074742716302532-si30.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/5a6f77d3e6f1afed612734bdbaeaa1b8/si30.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/5a6f77d3e6f1afed612734bdbaeaa1b8/si30.gif si30 si30.gif gif 380 14 59 ALTIMG 1-s2.0-S1074742716302532-si33.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/2a94b4904d3db852ebe07e23c3f71bbd/si33.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/2a94b4904d3db852ebe07e23c3f71bbd/si33.gif si33 si33.gif gif 195 10 11 ALTIMG 1-s2.0-S1074742716302532-si34.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/072259c7d1623a2b0cfcaf0800a64eb7/si34.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/072259c7d1623a2b0cfcaf0800a64eb7/si34.gif si34 si34.gif gif 211 16 11 ALTIMG 1-s2.0-S1074742716302532-si35.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/11327feb8f1c34980fd0068a3d8a372f/si35.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/11327feb8f1c34980fd0068a3d8a372f/si35.gif si35 si35.gif gif 224 11 18 ALTIMG 1-s2.0-S1074742716302532-si36.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/be51463c6d2fe0aefea4a78343e8fbf9/si36.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/be51463c6d2fe0aefea4a78343e8fbf9/si36.gif si36 si36.gif gif 229 14 16 ALTIMG 1-s2.0-S1074742716302532-si37.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/8dd84be03205812d653aad47a27422de/si37.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/8dd84be03205812d653aad47a27422de/si37.gif si37 si37.gif gif 214 11 16 ALTIMG 1-s2.0-S1074742716302532-si38.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/efa577b70b0b7dfacac546a49489b74f/si38.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/efa577b70b0b7dfacac546a49489b74f/si38.gif si38 si38.gif gif 222 14 14 ALTIMG 1-s2.0-S1074742716302532-si39.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/22ae34f9906ea88eacf4d0d63428b96b/si39.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/22ae34f9906ea88eacf4d0d63428b96b/si39.gif si39 si39.gif gif 215 16 13 ALTIMG 1-s2.0-S1074742716302532-si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6031a9ecde18d506c4a1dea56f149a87/si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6031a9ecde18d506c4a1dea56f149a87/si4.gif si4 si4.gif gif 370 23 31 ALTIMG 1-s2.0-S1074742716302532-si40.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/06fce9b3c93a0b81f9eee65df2f4daa7/si40.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/06fce9b3c93a0b81f9eee65df2f4daa7/si40.gif si40 si40.gif gif 580 17 113 ALTIMG 1-s2.0-S1074742716302532-si41.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/7b493365eeb4deca322168896f0163ac/si41.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/7b493365eeb4deca322168896f0163ac/si41.gif si41 si41.gif gif 183 13 10 ALTIMG 1-s2.0-S1074742716302532-si42.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/d65320a8fcda4164e0bc707d7635dc25/si42.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/d65320a8fcda4164e0bc707d7635dc25/si42.gif si42 si42.gif gif 199 13 9 ALTIMG 1-s2.0-S1074742716302532-si43.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/8458fb798a00a33f0bf84524a1d7ddb6/si43.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/8458fb798a00a33f0bf84524a1d7ddb6/si43.gif si43 si43.gif gif 697 17 128 ALTIMG 1-s2.0-S1074742716302532-si44.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/7a9900f67d5ce27b6d9a9c95ba20e03f/si44.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/7a9900f67d5ce27b6d9a9c95ba20e03f/si44.gif si44 si44.gif gif 190 13 10 ALTIMG 1-s2.0-S1074742716302532-si45.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/9a5fc86bd807d8be4cb7498495f9b955/si45.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/9a5fc86bd807d8be4cb7498495f9b955/si45.gif si45 si45.gif gif 216 12 17 ALTIMG 1-s2.0-S1074742716302532-si46.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/c71ed4db3fb76fe5124333497cabfaff/si46.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/c71ed4db3fb76fe5124333497cabfaff/si46.gif si46 si46.gif gif 200 11 15 ALTIMG 1-s2.0-S1074742716302532-si47.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/93c46f393adafd62f9e832b3b630993d/si47.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/93c46f393adafd62f9e832b3b630993d/si47.gif si47 si47.gif gif 232 13 19 ALTIMG 1-s2.0-S1074742716302532-si48.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/786eaa8e0bbef40792f420fc89de5f0a/si48.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/786eaa8e0bbef40792f420fc89de5f0a/si48.gif si48 si48.gif gif 171 8 11 ALTIMG 1-s2.0-S1074742716302532-si52.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/fb176c5a705d434f3eb20b8acd7c61cf/si52.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/fb176c5a705d434f3eb20b8acd7c61cf/si52.gif si52 si52.gif gif 2386 44 419 ALTIMG 1-s2.0-S1074742716302532-si53.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/fe53b7be8e666aa9889ad8c10bc54706/si53.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/fe53b7be8e666aa9889ad8c10bc54706/si53.gif si53 si53.gif gif 2254 100 163 ALTIMG 1-s2.0-S1074742716302532-si55.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/c27db3de4d35a0602a842212a5cf5710/si55.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/c27db3de4d35a0602a842212a5cf5710/si55.gif si55 si55.gif gif 257 17 26 ALTIMG 1-s2.0-S1074742716302532-si56.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/1d3af5db6d8d472148f03ca0cd64ff05/si56.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/1d3af5db6d8d472148f03ca0cd64ff05/si56.gif si56 si56.gif gif 194 10 12 ALTIMG 1-s2.0-S1074742716302532-si58.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/199519ff36f2e1876992e836743714b0/si58.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/199519ff36f2e1876992e836743714b0/si58.gif si58 si58.gif gif 285 17 26 ALTIMG 1-s2.0-S1074742716302532-si62.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/85c6f1e65a88028d07d33515e48dccdf/si62.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/85c6f1e65a88028d07d33515e48dccdf/si62.gif si62 si62.gif gif 220 16 14 ALTIMG 1-s2.0-S1074742716302532-si63.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6a58dc57cbf01aef001a15092d688654/si63.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6a58dc57cbf01aef001a15092d688654/si63.gif si63 si63.gif gif 1454 48 245 ALTIMG 1-s2.0-S1074742716302532-si65.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/31378faee736cd4f21ac34754fc2defc/si65.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/31378faee736cd4f21ac34754fc2defc/si65.gif si65 si65.gif gif 202 14 12 ALTIMG 1-s2.0-S1074742716302532-si66.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/c354303e7a13e1fcd39f14420a5775c7/si66.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/c354303e7a13e1fcd39f14420a5775c7/si66.gif si66 si66.gif gif 259 15 21 ALTIMG 1-s2.0-S1074742716302532-si68.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/870465a6bbb3b4ac8fd8a9d17772b53c/si68.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/870465a6bbb3b4ac8fd8a9d17772b53c/si68.gif si68 si68.gif gif 237 14 23 ALTIMG 1-s2.0-S1074742716302532-si69.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/6350cf72cb913451e1a16dde71ba779f/si69.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/6350cf72cb913451e1a16dde71ba779f/si69.gif si69 si69.gif gif 285 14 32 ALTIMG 1-s2.0-S1074742716302532-si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/24b6ad034a705399ef6c5004b4d79a5b/si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/24b6ad034a705399ef6c5004b4d79a5b/si7.gif si7 si7.gif gif 526 24 55 ALTIMG 1-s2.0-S1074742716302532-si71.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/f7663b98b9c92a6bcf61b051eff6bafc/si71.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/f7663b98b9c92a6bcf61b051eff6bafc/si71.gif si71 si71.gif gif 2054 52 360 ALTIMG 1-s2.0-S1074742716302532-si73.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/059b158009d8a02f5bed776a20b15cbf/si73.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/059b158009d8a02f5bed776a20b15cbf/si73.gif si73 si73.gif gif 251 17 22 ALTIMG 1-s2.0-S1074742716302532-si78.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/ee8e9ad8a7073b232c38385b8a87f8b0/si78.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/ee8e9ad8a7073b232c38385b8a87f8b0/si78.gif si78 si78.gif gif 1487 45 291 ALTIMG 1-s2.0-S1074742716302532-si79.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/704feb14c641f3455cf697d8ec57be7a/si79.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/704feb14c641f3455cf697d8ec57be7a/si79.gif si79 si79.gif gif 218 16 14 ALTIMG 1-s2.0-S1074742716302532-si85.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/a590b5c14c529164baaa839fea47da2e/si85.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/a590b5c14c529164baaa839fea47da2e/si85.gif si85 si85.gif gif 275 14 23 ALTIMG 1-s2.0-S1074742716302532-si86.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/3853e813cf175948edf3b5eba4926952/si86.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/3853e813cf175948edf3b5eba4926952/si86.gif si86 si86.gif gif 1013 39 143 ALTIMG 1-s2.0-S1074742716302532-si87.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/3733db7de7d7d5be23397396f17d2b6c/si87.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/3733db7de7d7d5be23397396f17d2b6c/si87.gif si87 si87.gif gif 307 18 30 ALTIMG 1-s2.0-S1074742716302532-si88.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/ea77b921831f7c4d85c46486307bd169/si88.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/ea77b921831f7c4d85c46486307bd169/si88.gif si88 si88.gif gif 367 17 45 ALTIMG 1-s2.0-S1074742716302532-si89.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/684bed0a7cbc7680d0b72f2a1bd9dd8d/si89.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/684bed0a7cbc7680d0b72f2a1bd9dd8d/si89.gif si89 si89.gif gif 314 19 33 ALTIMG 1-s2.0-S1074742716302532-si90.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/7906042a2f56167ba0212a37ff273ce1/si90.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/7906042a2f56167ba0212a37ff273ce1/si90.gif si90 si90.gif gif 307 17 31 ALTIMG 1-s2.0-S1074742716302532-si91.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/c6cb1ba223f3332e7e7d75e000ddd739/si91.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/c6cb1ba223f3332e7e7d75e000ddd739/si91.gif si91 si91.gif gif 917 37 164 ALTIMG 1-s2.0-S1074742716302532-si92.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/ae58a8e2c3339dbe213c256229563278/si92.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/ae58a8e2c3339dbe213c256229563278/si92.gif si92 si92.gif gif 298 17 30 ALTIMG 1-s2.0-S1074742716302532-si94.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/18508c50d98148538bd907d66ece0fbd/si94.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/18508c50d98148538bd907d66ece0fbd/si94.gif si94 si94.gif gif 237 11 19 ALTIMG 1-s2.0-S1074742716302532-si95.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/85696353e81b6515bf8d273db24be2ce/si95.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/85696353e81b6515bf8d273db24be2ce/si95.gif si95 si95.gif gif 560 39 78 ALTIMG 1-s2.0-S1074742716302532-si96.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/ac9962f291b005b4aa9cbce36f6e0bb7/si96.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/ac9962f291b005b4aa9cbce36f6e0bb7/si96.gif si96 si96.gif gif 297 17 32 ALTIMG 1-s2.0-S1074742716302532-si98.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/45821815fe85dd29b91f48e9be6a095f/si98.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/45821815fe85dd29b91f48e9be6a095f/si98.gif si98 si98.gif gif 840 32 124 ALTIMG 1-s2.0-S1074742716302532-si99.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1074742716302532/STRIPIN/image/gif/2ebfac806d26f322620a72b5c217391f/si99.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1074742716302532/STRIPIN/image/gif/2ebfac806d26f322620a72b5c217391f/si99.gif si99 si99.gif gif 1527 51 208 ALTIMG YNLME 6557 S1074-7427(16)30253-2 10.1016/j.nlm.2016.10.007 The Author(s) Fig. 1 Rubin\u2019s Vase (Rubin, 1915). Fig. 2 Hypothesised modulation of edge detecting simple cells in lower layers V1/V2 by top-down signals from higher layer V4 neurons representing boundary contour elements. The figure shows the steady state activations of all neurons after sufficient time (e.g. \u2a7e 61ms) has elapsed after stimulus presentation to allow visual signals to propagate from the retina up to V4 and then back down to modulate V1/V2 responses. The following four cases are shown. (a) An object with a straight vertical border on its left is presented with this border positioned at retinal location 1. Ascending visual input initially stimulates both subsets of V1/V2 neurons, \u03a6 Left , Loc 1 V 1 / V 2 and \u03a6 Right , Loc 1 V 1 / V 2 , representing a vertical straight edge at retinal location 1. However, in layer V4, only those V4 neurons \u03a6 Left V 4 representing a vertical straight edge on the left of an object are preferentially stimulated by the current visual input. Note that these V4 neurons receive additional feedforward (bottom-up) input signals from other V1/V2 neurons (not shown in the figure) which represent local image context, and these additional context signals are required to guide the selective responses of the V4 neurons. How V4 neurons may develop such response properties through self-organisation of the feedforward connections has been previously modelled by Eguchi et al. (2015). The subset of V4 neurons \u03a6 Left V 4 then stimulates via feedback (top-down) connections those two subsets of V1/V2 neurons \u03a6 Left , Loc 1 V 1 / V 2 and \u03a6 Left , Loc 2 V 1 / V 2 which receive strengthened connections from \u03a6 Left V 4 and are consequently modulated by a straight vertical edge on the left of an object. However, only the particular subset of V1/V2 cells \u03a6 Left , Loc 1 V 1 / V 2 , which represent a vertical bar at retinal location 1 where the vertical bar forms the left hand border of an object, receive the greatest combination of bottom-up and top-down input. Consequently, these V1/V2 neurons fire maximally, representing the border ownership of the vertical edge at this location. (b) An object with a straight vertical border on its left is presented with this border positioned at retinal location 2. In this case, the subset of V1/V2 cells \u03a6 Left , Loc 2 V 1 / V 2 , which represent a vertical bar at retinal location 2 where the vertical bar forms the left hand border of an object, receive the greatest combination of bottom-up and top-down input and fire maximally. (c) An object with a straight vertical border on its right is presented with this border positioned at retinal location 1. This time the subset of V1/V2 cells \u03a6 Right , Loc 1 V 1 / V 2 , which represent a vertical bar at retinal location 1 where the vertical bar forms the right hand border of an object, receive the greatest combination of bottom-up and top-down input and fire maximally. (d) An object with a straight vertical border on its right is presented with this border positioned at retinal location 2. Now the subset of V1/V2 cells \u03a6 Right , Loc 2 V 1 / V 2 , which represent a vertical bar at retinal location 2 where the vertical bar forms the right hand border of an object, receive the greatest combination of bottom-up and top-down input and fire maximally. Fig. 3 (a) The original four-layer feedforward (bottom-up) version of the VisNet architecture. The figure shows the feedforward connectivity, where each neuron receives connections from a topologically corresponding region of the preceding layer. The convergence of feedforward connections through the network is designed to provide fourth layer neurons with information from across the entire input retina. The new VisNet architecture implemented in this paper was extended to incorporate additional feedback (top-down) connections, which have the similar topological connectivity as the feedforward connections except in the opposite direction as shown in (b). (c) Convergence in the visual system V1: visual cortex area V1;TEO: posterior inferior temporal cortex, TE: anterior inferior temporal cortex (IT). Fig. 4 The visual object stimuli used for the simulation study. (a) A set of abstract familiar shape stimuli used to both train and test the network model (shaded hexagons and semicircles). The objects were black when presented on a light grey background or light grey when presented on a black background. Each object had a vertical straight edge either on its left boundary (a1, a3) or right boundary (a2, a4). During training and testing, each object was presented in two locations on the left and right of the retina. Whenever an object was presented on the left of the retina, the vertical straight edge on its (left or right) boundary was precisely aligned with retinal Location 1 (a1, a3). Similarly, whenever the object was presented on the right of the retina, the vertical straight edge on its (left or right) boundary was aligned with retinal Location 2 (a2, a4). (b) A set of novel stimuli used to cross-validate the performance of the network after it had been trained on the familiar set of stimuli (a). The four novel stimuli were a dog\u2019s head, a penguin, and two differently shaped human heads. Each novel stimulus has a vertical straight edge on one side. The four novel objects are each presented in two retinal locations in a similar manner to the familiar shapes (a). This gives a total of eight novel stimulus presentations. Fig. 5 The steady state response properties of Layer 3 neurons at the end of each stimulus presentation of familiar shapes that were used to train the network (shown in Fig. 4(a)). (a) Information analysis: We computed the information carried by the output (3rd layer) neurons about whether the vertical straight edge was on the left or right boundary of each object presented to the network before and after training. Plot (a1) shows the maximum single cell information carried by each of the 4096 neurons in Layer 3 about which one of the two stimulus categories was presented, where all of the neurons in Layer 3 are plotted along the abscissa in rank order. The result shows that nearly all of the Layer 3 neurons learned to respond selectively to a vertical straight edge either on the left or on the right of an object boundary, regardless of the global shape, shading or retinal location of the object. Plot (a2) shows the multiple cell information carried by different sized (i.e. up to ten neurons) random ensembles of Layer 3 neurons that individually had high levels of single cell information. It is evident that training has led to an increase in the multiple cell information, which after training asymptotes to the maximum level of 1 bit with only one neuron included in the analysis. (b) Firing rate responses of two Layer 3 neurons that has maximum single cell information: plot (b1) shows the responses of two Layer 3 neurons to all eight objects with a vertical straight edge on their right boundary, and plot (b2) shows the responses of the same two Layer 3 neurons to all eight objects with a vertical straight edge on their left boundary. These results show that neuron (17, 46) learned to respond selectively to all objects with a vertical straight edge on the right, while neuron (33, 23) learned to respond to all objects with a vertical straight edge on the left. Fig. 6 Cross-validation of the developed firing properties of neurons in Layer 3 with the set of novel shapes not presented during training as shown in Fig. 4(b). Each of the four novel objects is presented in two retinal locations giving a total of eight novel stimulus presentations. This figure shows the firing rate responses of the same two Layer 3 neurons that were previously tested on familiar objects in Fig. 5(b). Plot (a) shows the responses of the two Layer 3 neurons to all eight novel stimulus presentations with a vertical straight edge on their right boundary, and plot (b) shows the responses of the same two Layer 3 neurons to all eight novel stimulus presentations with a vertical straight edge on their left boundary. These results show that neuron (17, 46) learned to respond selectively to all objects with a vertical straight edge on the right, while neuron (33, 23) learned to respond to all objects with a vertical straight edge on the left. These results confirm that the developed firing properties of the cells are not specific to the set of trained objects and generalise to the set of novel objects not presented during training. Fig. 7 Steady state response properties of Layer 1 neurons at the end of each stimulus presentation of the familiar shapes that were used to train the network (Fig. 4(a)). (a) Information analysis: Since Layer 1 neurons are not expected to develop translation invariance across different retinal locations, we computed the information carried by these neurons about whether the vertical straight edge in the object stimulus was from one of four stimulus categories: (i) Location 1/ left boundary, (ii) Location 1/ right boundary, (iii) Location 2/ left boundary, and (iv) Location 2/ right boundary. Since there are n =four stimulus categories, perfectly discriminating neurons carry a maximum of 2 bits of information. Plot (a1) shows the maximum single cell information carried by each of the 4096 neurons in Layer 1 about which one of the four stimulus categories was presented, where all of the neurons in Layer 1 are plotted along the abscissa in rank order. The result shows that these Layer 1 neurons have learned to respond with perfect selectivity to one of the four stimulus categories, thus providing the kind of border ownership representations experimentally observed in cortical visual area V1 by Zhou et al. (2000). Plot (a2) shows the multiple cell information carried by different sized (i.e. up to ten neurons) random ensembles of Layer 1 neurons that individually had high levels of single cell information. It is evident that training has led to an increase in the multiple cell information, which after training asymptotes to the maximum level of 2 bits with only two neurons included in the analysis. (b) The firing rate responses of four Layer 1 neurons with maximum single cell information: plot (b1) shows the responses of the four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their right boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. Plot (b2) shows the responses of the same four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their left boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. These results show that different Layer 1 neurons had learned to respond selectively to each of the four stimulus categories. These are the same kinds of border ownership representations found experimentally in primate visual area V1 by Zhou et al. (2000). Fig. 8 Cross-validation of the developed firing properties of neurons in Layer 1 with the set of novel shapes not presented during training as shown in Fig. 4(b). Each of the four novel objects is presented in two retinal locations giving a total of eight novel stimulus presentations. This figure shows the firing rate responses of the same four Layer 1 neurons that were previously tested on familiar objects in Fig. 7(b). Plot (a) shows the responses of the four Layer 1 neurons to all eight novel object stimuli with a vertical straight edge on their right boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. Plot (b) shows the responses of the same four Layer 1 neurons to all eight novel object stimuli with a vertical straight edge on their left boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. It is evident that each of the four Layer 1 neurons has learned to respond to one of the four stimulus categories: (i) Location 1/left boundary, (ii) Location 1/right boundary, (iii) Location 2/left boundary, and (iv) Location 2/right boundary. These results confirm that the border ownership representations developed in Layer 1 were not specific to the set of trained objects and generalise to the set of novel objects not presented during training. Fig. 9 The temporal evolution of border ownership representations conveyed by four typical Layer 1 neurons during the 300ms time course of stimulus presentations. The network was trained and tested with the objects shown in Fig. 4. Results are shown after training. Each row shows results for one of the four neurons, where each neuron is tuned to a different border ownership category as follows. Row 1 (top row): a neuron tuned to a vertical straight edge on the right object boundary aligned with retinal Location 1, Row 2: a neuron tuned to a vertical straight edge on the left object boundary aligned with retinal Location 1, Row 3: a neuron tuned to a vertical straight edge on the right object boundary aligned with retinal Location 2, and Row 4 (bottom row): a neuron tuned to a vertical straight edge on the left object boundary aligned with retinal Location 2. Column (a) shows the average firing rates of the four neurons plotted over the 300ms time courses of the stimulus presentations. Each subplot shows the average responses of the neuron to the members of its preferred stimulus category (solid line) and the members of its three non-preferred stimulus categories (dashed line). For all four neurons, it can be seen that their firing responses begin to strongly differentiate between the preferred and non-preferred stimulus categories at around 50ms. By the end of the stimulus presentation at 300ms, the neurons show complete differentiation between the preferred and non-preferred stimulus categories. Column (b) shows the average single cell information carried by the four neurons about their preferred stimulus category plotted over the 300ms time courses of the stimulus presentations. Fig. 10 Hypothesised modulation of edge detecting simple cells in lower layers V1/V2 of the rate-coded model by top-down signals from higher layer V4 neurons representing boundary contour elements when two visual object stimuli are presented simultaneously. Assume that during testing of the model, an object with a straight vertical border on its left is presented with this border positioned at retinal location 1, and another object with a straight vertical border on its right is presented with this border positioned at retinal location 2. Ascending visual input initially stimulates all subsets of V1/V2 neurons, which represent a vertical straight edge at retinal location 1 ( \u03a6 Left , Loc 1 V 1 / V 2 and \u03a6 Right , Loc 1 V 1 / V 2 ), and a vertical straight edge at retinal location 2 ( \u03a6 Left , Loc 2 V 1 / V 2 and \u03a6 Right , Loc 2 V 1 / V 2 ). In layer V4, V4 neurons that represent a vertical straight edge on the left of an object ( \u03a6 Left V 4 ) are stimulated by ascending visual signals from the object in retinal Location 1, while V4 neurons that represent a vertical straight edge on the right of an object ( \u03a6 Right V 4 ) are stimulated by ascending visual signals from the object in retinal Location 2. However, the subpopulations \u03a6 Left V 4 and \u03a6 Right V 4 have each been trained to respond with translation invariance across all trained retinal locations, and so have developed strong bi-directional (i.e. bottom-up and top-down) polysynaptic connections with subpopulations of V1/V2 simple cells representing all retinal locations. Consequently, \u03a6 Left V 4 and \u03a6 Right V 4 will top-down modulate V1/V2 simple cells representing a vertical straight edge on the left and right object boundaries, respectively, across all trained retinal locations. In this case, all of the V1/V2 cells shown in the figure end up receiving a similar amount of bottom-up and top-down excitatory input. Both subpopulations \u03a6 Left , Loc 1 V 1 / V 2 and \u03a6 Right , Loc 1 V 1 / V 2 will be active at retinal Location 1, and both subpopulations \u03a6 Left , Loc 2 V 1 / V 2 and \u03a6 Right , Loc 2 V 1 / V 2 will be active at retinal Location 2. Thus, when more than one visual object is presented to the model, the V1/V2 neurons \u03a6 Left , Loc 1 V 1 / V 2 , \u03a6 Right , Loc 1 V 1 / V 2 , \u03a6 Left , Loc 2 V 1 / V 2 and \u03a6 Right , Loc 2 V 1 / V 2 may fail to represent the border ownership (binding) information. Fig. 11 The set of visual stimuli used to test the performance of the network when two objects are presented simultaneously during testing. There are two categories of visual stimuli. The first stimulus category consists of all possible combinations two objects where one of the objects has a vertical straight edge on its right boundary which is positioned at retinal Location 1. The second stimulus category consists of all possible combinations two objects where one of the objects has a vertical straight edge on its left boundary which is positioned at retinal Location 1. Fig. 12 A quantitative comparison of the border ownership information carried by Layer 1 neurons when the network is tested with objects shown individually (solid line) or when tested on two objects presented together (dashed line). The network performance is assessed using single-cell information analysis. The information analysis is applied to the steady state firing responses of Layer 1 neurons at the end of each stimulus presentation. In both test situations, the model was initially trained with the individual objects shown in Fig. 4, as described earlier in the paper. Solid lines: the performance of the model when tested with the objects shown in Fig. 4 presented one at a time during testing. We computed the single cell information carried by each Layer 1 neuron about the four stimulus categories previously described in Fig. 7(a). In this figure the maximum single-cell information ( log 2 ( 4 ) = 2 ) has been rescaled to 1. The information carried by Layer 1 neurons about stimulus category (i) is shown in the right plot, while information carried by Layer 1 neurons about the stimulus category (ii) is shown in the left plot. It can be seen that when the network is tested on a single object at a time, many Layer 1 neurons reach the theoretical maximum level of information about border ownership. Dashed lines: the performance of the model when tested with two objects shown together during each visual presentation using the test images shown in Fig. 11. Here we computed the single cell information carried by each Layer 1 neuron about the two stimulus categories described in Fig. 11. Since there are two stimulus categories, neurons may carry up to a maximum of 1 bit of information. The information carried by Layer 1 neurons about the first stimulus category is shown in the left plot, while information carried by Layer 1 neurons about the second stimulus category is shown in the right plot. It can be seen that when the network is tested on two objects at a time, there is a large drop in the levels of single cell information carried by Layer 1 neurons about border ownership compared to when the network is tested on individual objects, with far fewer Layer 1 neurons reaching the theoretical maximum of 1 bit for this test case. Table 1 Parameters used for simulations with VisNet. Layer 1 2 3 (a) Parameters for VisNet model Dimensions 64 \u00d7 64 64 \u00d7 64 64 \u00d7 64 Number of feedforward fan-in connections 201 100 100 Fan-in Radius (feedforward) 12 12 18 Number of feedback fan-in connections 5 5 \u2013 Fan-in Radius (feedback) 12 12 \u2013 Sparseness of activations (set by adjusting sigmoid threshold \u03b1 ) 33% 33% 50% Sigmoid slope ( \u03b2 ) 31.5 46.1 1.48 Learning rate (k) 1.0 1.0 1.0 Excitatory Radius ( \u03c3 E ) 1.4 1.1 0.8 Excitatory Contrast ( \u03b4 E ) 5.35 33.15 117.57 Inhibitory Radius ( \u03c3 I ) 2.76 5.4 8.0 Inhibitory Contrast ( \u03b4 I ) 1.6 1.5 1.5 (b) Parameters for Gabor filtering Phase shift ( \u03c8 ) 0 , \u03c0 , - \u03c0 / 2 , \u03c0 / 2 Wavelength ( \u03bb ) 2 Orientation ( \u03b8 ) 0 , \u03c0 / 4 , \u03c0 / 2 , 3 \u03c0 / 4 Spatial bandwidth (b) 1.5 octaves Aspect ratio ( \u03b3 ) 0.5 (c) Parameters for differential model Activation time constant ( \u03c4 h ) [s] 0.1 Trace time constant ( \u03c4 t ) [s] 0.5 Presentation time per stimulus transform [s] 1.0 Numerical step size ( \u0394 t ) [s] 0.01 Neural network model develops border ownership representation through visually guided learning Akihiro Eguchi \u204e akihiro.eguchi@psy.ox.ac.uk Simon M. Stringer Oxford Centre for Theoretical Neuroscience and Artificial Intelligence, Department of Experimental Psychology, University of Oxford, Oxford, UK Oxford Centre for Theoretical Neuroscience and Artificial Intelligence Department of Experimental Psychology University of Oxford Oxford UK \u204e Corresponding author. Highlights \u2022 Border-ownership representations develop through visually guided learning in a model. \u2022 Unsupervised learning mechanisms of the hypothetical grouping circuits are presented. \u2022 Top-down connections from V4 guide competitive learning in the lower area in V2. Abstract As Rubin\u2019s famous vase demonstrates, our visual perception tends to assign luminance contrast borders to one or other of the adjacent image regions. Experimental evidence for the neuronal coding of such border-ownership in the primate visual system has been reported in neurophysiology. We have investigated exactly how such neural circuits may develop through visually-guided learning. More specifically, we have investigated through computer simulation how top-down connections may play a fundamental role in the development of border ownership representations in the early cortical visual layers V1/V2. Our model consists of a hierarchy of competitive neuronal layers, with both bottom-up and top-down synaptic connections between successive layers, and the synaptic connections are self-organised by a biologically plausible, temporal trace learning rule during training on differently shaped visual objects. The simulations reported in this paper have demonstrated that top-down connections may help to guide competitive learning in lower layers, thus driving the formation of lower level (border ownership) visual representations in V1/V2 that are modulated by higher level (object boundary element) representations in V4. Lastly we investigate the limitations of our model in the more general situation where multiple objects are presented to the network simultaneously. Keywords Primate vision Border ownership Neural network model 1 Introduction As Rubin\u2019s famous vase (Fig. 1 ) demonstrates, our visual perception tends to assign luminance contrast borders to one or other of the adjacent image regions, as if they serve as occluding contours (von der Heydt, Zhou, & Friedman, 2003). This is an example of feature binding in vision, in this case binding a luminance contrast border to a particular object. Representing such binding relationships between visual features is essential to the ability of the visual system to interpret and make sense of complex visual scenes. Experimental evidence for the neuronal coding of such border-ownership in the primate visual system has arisen in a neurophysiology study carried out by Zhou, Friedman, and von der Heydt (2000). Zhou et al. (2000) have shown that the responses of simple cells in earlier cortical stages of visual processing such as V1 and V2, which respond preferentially to oriented edges, are also modulated by which side of an object or figure the edge occurs on. This is the case even when the figure/background cues lie well outside the classical receptive field of the neuron, which in area V1 is approximately 1 degree in size. Such neurons are referred to as border ownership cells. Sugihara, Qiu, and von der Heydt (2011) later reported that the border ownership signal emerges with a latency of 61ms, which is about 13ms later than the onset of orientation selectivity. This suggests that the global image context specifying border ownership modulates the activity of these neurons. In other words, there must be a mechanism that enables the contextual information to be conveyed to these early stage visual neurons in V1 and V2. It has been proposed that these kinds of border ownership responses in area V1 represent a form of feature binding, and so may be important for understanding how primate vision may solve the problem of feature binding more generally. Some theoreticians have suggested that the context integration required for border ownership representations in V1 and V2 can be achieved via lateral propagation of signals within a layer via horizontal fibres (Baek & Sajda, 2005; Nishimura & Sakai, 2004; Zhaoping, 2005). However, Sugihara et al. (2011) have argued that the conduction velocity of horizontal fibres is too slow (most of them being between 0.1 and 0.4m/s (Angelucci & Bullier, 2003)) to produce the border ownership signals within the short latency observed in neurophysiology studies. Furthermore, Sugihara et al. (2011) showed that varying the distance between the target border and the visual features that carry contextual information about the \u2018owner\u2019 of the border does not in fact influence the latency before the border ownership signals arise. Therefore, they concluded that context influence by horizontal signal propagation alone is highly unlikely. On the other hand, the feedforward (bottom-up) and feedback (top-down) connections between successive visual stages have fast-conducting axons, with conduction velocities of between 2 and 6m/s, which is about ten times faster than cortical horizontal fibres (Angelucci & Bullier, 2003). Accordingly, both Craft, Schtze, Niebur, and von der Heydt (2007) and Jehee, Lamme, and Roelfsema (2007) have proposed models that involve hypothetical \u2018grouping circuits\u2019 within a higher cortical layer that capture the contextual information about local boundary elements, and these contextual signals are then relayed down through feedback connections to modulate responses in an earlier layer. They proposed that the larger receptive fields in the higher layer allow the network to employ \u2018grouping circuits\u2019 without having to rely on slow lateral propagation of signals. Nevertheless, it still remains a challenge to understand exactly how such neural circuits may be learned. The objective of the current study is to investigate the learning mechanisms that underpin the development of border ownership cells in the primate visual brain, in terms of synaptic modification guided by visual experience and consequent neural adaptation throughout a hierarchy of cortical stages. Moreover, given the proposed role of border ownership cells in feature binding, which is essential for integrating the visual features within a scene, the simulations described below provide a step towards understanding how the brain learns to make sense of the visual world. One higher visual area that might provide appropriate top-down modulatory signals is V4, which contains neurons that represent the localised boundary contour elements of objects (Layton, Mingolla, & Yazdanbakhsh, 2012). The responses of these neurons are sensitive to both the shape of the boundary element and where the element is with respect to the centre of mass of the object (Pasupathy & Connor, 2001; Pasupathy & Connor, 2002). Hence each of the neurons encodes that a specific border element belongs to a particular object - i.e. a kind of border ownership representation. A subpopulation of these neurons will provide a distributed representation of the entire boundary of the object. Furthermore, the neurons are able to respond invariantly as the object is shifted across different locations on the retina over a modest range. The visually-guided development of such V4 cells has been previously investigated in a computational modelling study with an established neural network model, VisNet, of the primate ventral visual pathway (Eguchi, Mender, Evans, Humphreys, & Stringer, 2015). The network architecture consisted of a hierarchy of cortical visual layers, with each layer modelled as a competitive neural network (Wallis & Rolls, 1997). Whenever an image was presented to the network, visual signals propagated through feedforward plastic synaptic connections between successive layers. Within each competitive layer, the excitatory cells competed with each other to respond to the current visual stimulus. In the brain, competition between excitatory cells is implemented via inhibitory interneurons. Although to save computational expense in VisNet, competition between excitatory neurons is modelled more directly using local filters. During an initial period of training with visual objects, the feedforward synaptic connections between successive layers of the network are continually modified using local, biologically plausible, associative learning rules. The competition within each layer then forces individual neurons to learn to respond selectively to a particular stimulus class, with different neurons responding to different kinds of stimulus. Competitive learning is a very simple unsupervised learning paradigm that allows neurons to discover important features of the stimulus input patterns (Rumelhart & Zipser, 1985). Eguchi et al. (2015) showed that the gradual increase in the receptive field size of neurons through successive layers of the visual system (Gross, Bender, & Rocha-Miranda, 1969; Pettet & Gilbert, 1992) allows V4 neurons access to local image information specifying how localised luminance contrast contours belong to adjacent object regions. As a result, cells in the higher layer of their hierarchical competitive neural network model developed neuronal response properties similar to those reported by Pasupathy and Connor (2001, 2002) when the model was trained on a number of real world objects. In this paper, we extend the previous purely feedforward model of Eguchi et al. (2015) by incorporating both feedforward (bottom-up) and feedback (top-down) connections. This extended model architecture is used to investigate how the edge-detecting simple cells in the earliest layer of the network, which corresponds to visual areas V1/V2 in the primate brain, may develop border ownership representations via top-down modulation from neurons in the output layer, which corresponds to visual area V4. The necessary feedforward and feedback synaptic connectivity within the network is set up by visually-guided learning using a biologically plausible, local, trace learning rule (Foldiak, 1991) as the network is trained on a collection of differently shaped visual object stimuli. We go on to show how these border ownership signals in the earliest layer evolve dynamically during the 300ms time course of a stimulus presentation, as reported by Sugihara et al. (2011) and Jehee et al. (2007). We then investigate the limitations of the model in the more general situation where multiple objects are presented to the network simultaneously. 1.1 Hypothesis Eguchi et al. (2015) have shown that when an established hierarchical neural network model of the primate ventral visual pathway, VisNet (Wallis & Rolls, 1997), is trained on 177 images of real world objects, which rotated in plane through 360\u00b0 and shifted across a 3 \u00d7 3 grid of nine different retinal locations, the neurons in the higher layers of the model learn to represent local boundary contour elements. Individual neurons are tuned to boundary elements with a specific curvature at a particular location with respect to the centre of mass of the object. Moreover, the neurons respond invariantly as an object is translated across different retinal locations. These are the same neuronal response properties as observed in area V4 of the primate visual system by Pasupathy and Connor (2002). Although they have reported that the translation invariant responses of V4 neurons are only over a modest range, we can simply suppose that the size of simulated retina in the model matches to the covered range. The version of the VisNet architecture used in the previous study incorporated only feedforward (bottom-up) connections between successive layers of the network (Eguchi et al., 2015). No feedback (top-down) connections were included in the model even though these are known to exist in the primate ventral visual pathway. It has previously been suggested that the top-down connections might implement attention to objects during visual search (Deco & Lee, 2002; Wagatsuma, Oki, & Sakai, 2013) and were incorporated into a variant of VisNet model to simulate top-down biasing effects (Deco & Rolls, 2004). However, in this previous study the top-down connections were only implemented after training, and so did not influence the visual representations that developed during visually-guided learning. In contrast, in our present paper the top-down connections are also present during training, and thus play a key role in the development of border ownership representations in the early layers. In particular, we propose that the global image context specifying border ownership is conveyed to the early stage visual neurons by top-down connections between layers in order to drive the development of border ownership cells in the early cortical areas as reported by Zhou et al. (2000). Accordingly, we hypothesised that learning in the extended VisNet architecture introduced in this paper would operate as follows. First, during visually-guided learning in which VisNet is trained on images of differently shaped objects, neurons in the later stages of visual processing such as V4 will learn to encode boundary contour elements through learning in the feedforward connections as previously demonstrated by Eguchi et al. (2015). Next, with continued visually-guided training on the same object images, we expect that strong polysynaptic feedback connections may subsequently develop from those neurons in the later stages of visual processing to neurons in earlier stages such as V1 and V2. These strengthened top-down connections might then modulate the responses of neurons in V1 and V2 according to where their preferred edge element occurs within an object. More precisely, let us consider a subset \u03a6 Left V 4 of neurons in V4 that have learned, by the visually-guided competitive learning mechanisms, to encode a vertical straight contour on the left of an object across different retinal locations. This subset of V4 neurons may also develop strengthened top-down polysynaptic connections to a subset of simple cells in V1 and V2 that originally signal the presence of any vertical straight contour within their small classical receptive field. This will force the subset of V1/V2 neurons to preferentially respond when the vertical straight contour is part of the left boundary of an object (top-down signals) at a particular retinal location (bottom-up signals). Fig. 2 (a) shows a case example in which an object with a straight vertical border on its left is presented with this border positioned at retinal location 1. The figure illustrates how the subset \u03a6 Left V 4 of V4 neurons, which represent a vertical straight edge on the left of an object, may modulate the responses of a subset of V1/V2 simple cells \u03a6 Left , Loc 1 V 1 / V 2 that represent the presence of a vertical contour at retinal location 1. Fig. 2(b)\u2013(d) shows similar case examples in which the vertical straight edge may occur on either the left or right boundary of the object, with the vertical straight edge positioned in either retinal location 1 or location 2. In summary, we hypothesise that the observations of Zhou et al. (2000), in which the responses of V1 and V2 neurons are modulated by which side of a figure the edge occurs on, may be replicated by incorporating both bottom-up and top-down associatively modifiable connections within VisNet. This will allow neurons in the early layers to develop their firing responses through visually-guided competitive learning driven by a combination of both bottom-up and top-down visual signals. The neural circuits developed after visually-guided learning in VisNet are expected to be similar to the hypothetical \u2018grouping circuits\u2019 proposed in a previous modelling study of border ownership representation with top-down connections carried out by Craft et al. (2007). However, the focus of our current study is to investigate exactly how such neural circuits may be learned when the network is trained on visual images of differently shaped objects. 2 Materials & methods 2.1 VisNet model The simulation studies presented in this paper are conducted with a modified version of an established neural network model, VisNet, of the primate ventral visual pathway, which was originally developed by Wallis and Rolls (1997). The original feedforward (bottom-up) version of the network architecture is shown in Fig. 3 (a) and (c). It is based on the following: (i) a series of hierarchical competitive networks with local graded lateral inhibition; (ii) convergent feedforward connections to each neuron from a topologically corresponding region of the preceding layer, leading to an increase in the receptive field size of neurons through the visual processing areas; and (iii) synaptic plasticity based on a local associative trace learning rule (6) and (7), which is explained below. The hierarchical series of 4 neuronal layers of VisNet have been loosely related to the following successive stages of processing in the ventral visual pathway: V2, V4, the posterior inferior temporal cortex (TEO), and the anterior inferior temporal cortex (TE) (see Rolls (2012) for a comprehensive review of studies performed using VisNet). In the current simulations reported below, the number of the layers has been reduced to three since a large number of border ownership neurons were found to develop in the third layer of VisNet, which corresponds to TEO in the earlier study (Eguchi et al., 2015). In the simulations described in this paper, the VisNet architecture was extended to incorporate additional feedback (top-down) connections, which have the similar topological connectivity as the feedforward connections except in the opposite direction (Fig. 3(b)). Both the feedforward and feedback connections to individual cells are derived from a topologically corresponding region of the preceding layer, using a Gaussian distribution of connection probabilities. These distributions are defined by a radius which will contain approximately 67% of the connections from the preceding layer. The values used in the current studies are given in Table 1 . The gradual increase in the receptive field of cells in successive layers 1\u20133 reflects the known physiology of the primate ventral visual pathway (Freeman & Simoncelli, 2011; Pasupathy, 2006; Pettet & Gilbert, 1992). Furthermore, in order to investigate the precise temporal dynamics of the top-down modulation, we have converted the original discrete time model, which has been used for past VisNet studies, into a time-continuous model with differential equations that are given below. 2.1.1 Pre-processing of the visual input by Gabor filters Before the visual images are presented to the VisNet\u2019s input layer 1, they are preprocessed by a set of Gabor filters, previously implemented by Deco and Rolls (2004), which accord with the general tuning profiles of simple cells in V1 (Cumming & Parker, 1999; Jones & Palmer, 1987; Lades et al., 1993). The filters provide a unique pattern of filter outputs for each transform of each visual object, which is passed through to the first layer of VisNet. These filters are known to provide a good fit to the firing properties of V1 simple cells, which respond to local oriented bars and edges within the visual field (Cumming & Parker, 1999; Jones & Palmer, 1987). The input filters used are computed by the following equations: (1) g ( x , y , \u03bb , \u03b8 , \u03c8 , b , \u03b3 ) = exp - x \u2032 2 + \u03b3 2 y \u2032 2 2 \u03c3 2 cos 2 \u03c0 x \u2032 \u03bb + \u03c8 with the following definitions: (2) x \u2032 = x cos \u03b8 + y sin \u03b8 y \u2032 = - x sin \u03b8 + y cos \u03b8 \u03c3 = \u03bb ( 2 b + 1 ) \u03c0 ( 2 b - 1 ) ln 2 2 where x and y specify the position of a light impulse in the visual field (Petkov & Kruizinga, 1997). The parameter \u03bb is the wavelength ( 1 / \u03bb is the spatial frequency), \u03c3 controls number of such periods inside the Gaussian window based on \u03bb and spatial bandwidth b , \u03b8 defines the orientation of the feature, \u03c8 defines the phase, and \u03b3 sets the aspect ratio that determines the shape of the receptive field. In the experiments in this paper, an array of Gabor filters is generated at each of 256 \u00d7 256 retinal locations with the parameters given in Table 1. The outputs of the Gabor filters are passed to the neurons in layer 1 of VisNet according to the synaptic connectivity given in Table 1. That is, each layer 1 neuron receives connections from 201 randomly chosen Gabor filters localised within a topologically corresponding region of the retina (this number has been used to be consistent with the original VisNet study (Wallis & Rolls, 1997)). These distributions are defined by a radius shown in Table 1. 2.1.2 Activations of neurons and competition within the network Within each of the neural layers 1\u20133 of the network, the activation h i of each neuron i is governed by the following differential equation: (3) \u03c4 h dh i ( t ) dt = - h i ( t ) + \u2211 j w ij ( t ) r j ( t ) where \u03c4 h is the time constant, r j is the firing rate of presynaptic neuron j, and w ij is the strength of the synapse from neuron j to neuron i. The value of \u03c4 h used in the simulations is 0.1 , which is larger than the typical values used for spiking network, 0.01 . However, since we do not implement spikes of the neurons and the synaptic learning rule does not depend on the precise timing like STDP, we decided to use the larger time constant for this particular model for the speed of its computation. In this paper, the full differential model, which comprises Eqs. (3), (6) and (7) given below, is numerically simulated using a Forward Euler finite difference scheme with a fixed numerical timestep \u0394 t given in Table 1. In this paper, we have run simulations with a self-organising map (SOM) (Kohonen, 1982; von der Malsburg, 1973) implemented within each layer. In the SOM architecture, short-range excitation and long-range inhibition are combined to form a Mexican-hat spatial profile and is constructed as a difference of two Gaussians as follows: (4) I a , b = - \u03b4 I exp - a 2 + b 2 \u03c3 I 2 + \u03b4 E exp - a 2 + b 2 \u03c3 E 2 Here, to implement the SOM, the activations h i of neurons within a layer are convolved with a spatial filter, I a , b , where \u03b4 I controls the inhibitory contrast and \u03b4 E controls the excitatory contrast. The width of the inhibitory radius is controlled by \u03c3 I while the width of the excitatory radius is controlled by \u03c3 E . The parameters a and b index the distance away from the centre of the filter. The lateral inhibition and excitation parameters used in the SOM architecture are given in Table 1. These values were previously found to optimize the performance of the VisNet model (Rolls, 2000; Tromans, Harris, & Stringer, 2011). Next, the contrast between the activations of neurons within each layer is enhanced by passing the activations of the neurons through a sigmoid transfer function as follows: (5) r = f sigmoid ( h \u2032 ) = 1 1 + exp - 2 \u03b2 ( h \u2032 - \u03b1 ) where h \u2032 is the activation after applying the SOM filter, r is the firing rate after contrast enhancement, and \u03b1 and \u03b2 are the sigmoid threshold and slope respectively. The parameters \u03b1 and \u03b2 are constant within each layer although \u03b1 is adjusted within each layer of neurons to control the sparseness of the firing rates. For example, to set the sparseness to 5 % , the threshold is set to the value of the 95th percentile point of the activations within the layer. The parameters for the sigmoid activation function are shown in Table 1. 2.1.3 Modification of synaptic weights during training During training with visual objects, while the connectivity pattern is fixed, the strengths of the feedforward and feedback synaptic connections between successive neuronal layers are modified by a trace learning rule (Foldiak, 1991; Wallis & Rolls, 1997), which incorporates a memory trace of recent neuronal activity: (6) dw ij ( t ) dt = k r i \u203e ( t ) r j ( t ) where r j ( t ) is the firing rate of pre-synaptic neuron j , r i \u203e ( t ) is the memory trace value of the firing rate of post-synaptic neuron i , w ij is the synaptic weight from pre-synaptic neuron j to post-synaptic neuron i, and k is the learning rate constant. The memory trace value r i \u203e ( t ) is updated according to the equation: (7) \u03c4 t r i \u203e ( t ) dt = - r i \u203e ( t ) + r i ( t ) where r i ( t ) is the firing rate of post-synaptic neuron i, and \u03c4 t is a trace time constant which is given in Table 1. The effect of the trace learning rule (6) is to encourage neurons to learn to respond to visual input patterns that tend to occur close together in time. The utility of this temporal binding is as follows. If, during training, each object is presented to the network in a sequence of different retinal locations clustered together in time before switching to the next object, then this enables neurons in higher layers to learn to respond to their preferred visual stimulus with shift invariance across different retinal locations as described in the earlier simulation study of Eguchi et al. (2015). During the numerical simulation, to prevent the same few neurons always winning the competition, the synaptic weight vector w i for each neuron i is normalised to unit length after each learning update for each training image by setting (8) w i = w i \u2016 w i \u2016 where \u2016 w i \u2016 is the length of the vector w i given by (9) \u2016 w i \u2016 = \u2211 j w ij 2 Neurophysiological evidence for synaptic weight normalisation is provided by Royer and Par\u00e9 (2003). In the original discrete-time version of VisNet, the synaptic weights are trained layer by layer (Wallis & Rolls, 1997). However, it is important to note that in the current time-continuous version of VisNet, all the synapses across the layers are trained simultaneously. This means that every time step, each neuron calculates the weighted sum of the pre-synaptic activations, at both feed-forward and top-down synapses, to update the activation h (Eq. (3)). Next the neuronal firing rates within each layer are simultaneously determined by applying the SOM filter (Eq. (4)) and then the contrast enhancement (Eq. (5)). The trace learning rule (Eqs. (6) and (7)) is then applied at all of the synapses simultaneously to update the synaptic weights. In other words, in the current VisNet model, the training of the backprojections starts at the same time as the forward projections, with the bottom-up and top-down afferent connections to all of the layers being trained simultaneously. 2.2 Analysis techniques Information theory is used to quantify how selective neurons are for members of a particular stimulus category. If a neuron responds invariantly to the members of a particular stimulus category but not to stimuli from other stimulus categories, then the neuron carries a high level of information about the presence of its preferred stimulus category. For example, we have previously used information theory to quantify how well neurons have learned to respond selectively to a particular visual stimulus with translation invariance across different retinal locations (Eguchi et al., 2015). If the responses r of a neuron carry a high level of information about the presence of a particular stimulus s across different retinal locations, then this implies that the neuron will respond selectively to the presence of that stimulus regardless of where the stimulus is presented on the retina. In this way, information theory can provide a direct measure of both the selectivity of a neuron for a particular stimulus, as well as how translation-invariant the neuronal responses are as the stimulus is shifted across the retina. In this paper, we continue to use information theory to assess the stimulus selectivity and translation invariance of neurons in the layer 3 that have learned to respond to localised object boundary elements with translation invariance, as previously investigated by Eguchi et al. (2015). However, in this new study we also apply information theory to assess how well simple cells in layer 1 have learned to represent border ownership through top-down modulation. We therefore use information theory to assess whether some layer 1 simple cells learn to respond selectively to a vertical straight edge on the left boundary of an object, while other simple cells learn to respond to a vertical straight edge on the right boundary of an object, regardless of the overall object shape. The simple cells in layer 1 have a small fan-in from the retina and are tuned to specific retinal locations, and consequently do not respond invariantly over different retinal locations. Instead, the simple cells should ideally respond invariantly over different global object shapes, as long as there is a straight vertical edge in the correct location on the object boundary. Two information measures were used to assess network performance (see Rolls, Treves, Tovee, & Panzeri (1997) and Rolls & Milward (2000)). These two measure use the responses from either individual neurons (single-cell information analysis) or small ensembles of neurons (multiple-cell information analysis), each of which will be discussed in turn. 2.2.1 Single-cell information A single cell information measure was applied to individual cells to measure how much information is available from the responses of a single cell about which stimulus category is present. For border ownership simple cells in layer 1, there are two stimulus categories presented at each of two retinal training locations 1 and 2 (i.e., in total four stimulus categories). These two categories are: (i) a vertical straight edge which is on the left hand boundary of an object and (ii) a vertical straight edge which is on the right hand boundary of an object. Therefore, to score high single cell information, a layer 1 neuron must respond selectively either to all object shapes with a vertical straight bar on the left or all object shapes with a straight vertical bar on the right, but only for one of the two retinal locations. On the other hand, we are interested in measuring translation invariant responses of the cells in layer 3 as V4 neurons. Accordingly, although the responses of neurons in layer 3 are assessed using the same two stimulus categories, we calculated how well those cells learned to respond invariantly to stimuli presented in both retinal locations 1 and 2. In other words, there are in total two stimulus categories in this case instead of four in the case of layer 1. Therefore, to score high single cell information, a layer 3 neuron must respond either to all object shapes with a vertical straight bar on the left or all object shapes with a straight vertical bar on the right, and do so for both of the two retinal locations. To be informative in the context of this study, the responses of a given neuron (r) should be specific to the presence of a straight vertical edge at a particular side (s =left/right), and independent of the remaining global shape of the object (in layers 1 and 3) or retinal location (in layer 3). The amount of stimulus specific information that a specific cell carries is calculated from the following formula with details given by Rolls and Milward (2000): (10) I ( s , R \u2192 ) = \u2211 r \u2208 R \u2192 P ( r | s ) log 2 P ( r | s ) P ( r ) Here s is a particular stimulus and R \u2192 is the set of responses of a cell to the set of stimuli. The maximum information that an ideally developed cell could carry is given by the formula: (11) Maximum cell information = log 2 ( n ) bits where n is a number of different stimulus categories. For example, in the case of translation invariant representation in Layer 3 with two stimulus categories, the maximum information possible is 1 bit. 2.2.2 Multiple-cell information While useful in assessing the tuning properties of individual neurons, the single-cell information measure cannot give a complete assessment of VisNet\u2019s performance with respect to recognition of the full set of stimulus categories. If all cells learned to respond to the same stimulus category (according to the single-cell measure) then there would be relatively little information available about the whole set stimulus categories S \u2192 . To address this issue, we also calculated a multiple-cell information measure, which assesses the amount of information that is available about the whole set of stimulus categories from a subpopulation of neurons. This measure quantifies the network\u2019s ability to tell which stimulus is currently presented to the network based on the set of responses, R \u2192 , of a subpopulation of cells. In brief, we would like to calculate the mutual information between the stimulus categories and the neuronal responses \u2013 the average amount of information obtained (across all stimuli) from the responses of the neuronal ensemble, about which stimulus category was present after a single presentation of a stimulus. However, due to the difficulty in adequately sampling this high dimensional neural response space, it is challenging to construct accurate probability distributions for directly calculating the mutual information. Instead, a decoding procedure is used to estimate which stimulus s \u2032 gave rise to the particular firing rate response vector on each trial. A probability table is then constructed between the real stimuli, s, and the decoded stimuli, s \u2032 . From this probability table, the multiple-cell information is then calculated as follows. (12) I C \u2192 ( S , S \u2032 ) = \u2211 s , s \u2032 P ( s , s \u2032 ) log 2 P ( s , s \u2032 ) P ( s ) P ( s \u2032 ) (13) P ( s \u2032 ) = \u2211 s \u2208 S P ( s \u2032 | R C \u2192 ( s ) ) \u00d7 P ( R C \u2192 ( s ) ) ) (14) P ( s , s \u2032 ) = P ( s \u2032 | R C \u2192 ( s ) ) \u00d7 P ( R C \u2192 ( s ) ) Here, S represents the set of the stimulus categories presented to the network, and C \u2192 defines the set of cells used in the analysis. For each analysis, the ensembles of cells are sampled from the pool of the cells which consists of five cells that had, as single cells, the most information about each stimulus category (i.e., the size of the pool is 5 \u00d7 number _ of _ the _ stimulus _ category ). From the set of cells C \u2192 , the firing responses R C \u2192 (R = r ( c ) | c \u2208 C \u2192 ) to each stimulus in S are used as the basis for the Bayesian decoding procedure. For a given set of cells, the probabilities generated by the decoding procedure are factored into a confusion matrix, which matches up the actual input stimulus category in S \u2192 with the predicted stimulus category in S \u2032 \u2192 . Here, P ( s i \u2032 ) represents the probability that the predicted stimulus category s i \u2032 is actually the stimulus category s i that is currently presented to the network. A higher value of P ( s , s \u2032 ) relative to P ( s ) P ( s \u2032 ) indicates a stronger relationship between s and s \u2032 . This information provides the basis for calculating the multiple-cell information analysis. More details of the decoding procedure is provided in Rolls and Milward (2000). 3 Simulation results 3.1 Study 1: simulation of the visually-guided development of border ownership representations In this simulation study, VisNet was initially trained and tested on the same abstract visual object shapes (familiar objects) shown in Fig. 4 (a). The model was then also cross-validated by testing the same trained network on the novel visual objects (novel objects) shown in Fig. 4(b), which were not presented to the network during initial training. The familiar objects were hexagons and semicircles, which were either black or light grey. Black objects were presented against a light grey background, while light grey objects were presented against a black background. Each object had a vertical straight edge either on its left boundary (Fig. 4(a2,a4)) or right boundary (Fig. 4(a1,a3)). Although in the natural environment, the object does not normally jump from one location to the other instantaneously, the region activated on the retina does constantly shifts around due to the rapid eye movement called saccades. To simulate this effect, during training and testing, each object was presented in two locations on the left (Location 1) and right (Location 2) of the 256 \u00d7 256 retina. Whenever an object was presented on the left of the retina, the vertical straight edge on its (left or right) boundary was precisely aligned with retinal Location 1 (Fig. 4(a1,a2)). This enabled us to explore the top-down modulation of the subpopulation of simple cells in Layer 1 tuned to vertical straight edges at this specific retinal location. In a similar manner, whenever the object was presented on the right of the retina, the vertical straight edge on its (left or right) boundary was aligned with retinal Location 2 (Fig. 4(a3,a4)). Again, this permitted us to explore the top-down modulation of simple cells in Layer 1 tuned to vertical straight edges at this retinal location. During training, the familiar objects shown in Fig. 4(a) were presented to the network one at a time shifting across the two retinal locations while the feedforward and feedback synaptic connections between successive layers were modified using the trace learning rule (6) and (7). The trace learning rule in the feedforward connections drives the development of neuronal responses in the higher layers that are translation invariant across different retinal locations by encouraging postsynaptic neurons to learn to respond to subsets of input patterns that tend to occur close together in time. As long as, during training, each object is presented across different retinal locations in temporal proximity, then the trace learning rule will produce output neurons that have learned to respond selectively to a particular object feature in a translation invariant manner. Therefore, during training, we selected each object in turn and presented that object in the two different retinal locations before moving on to the next object. 3.1.1 Steady state firing properties of cells in layers 1 and 3 at the end of each stimulus presentation In this section we analyse the steady state firing responses of Layer 1 and Layer 3 neurons at the end of each stimulus presentation before and after training with the same object stimuli used for training (familiar objects) shown in Fig. 4(a) as well as with the novel object stimuli shown in Fig. 4(b) to cross-validate the developed response properties. We first tested the firing properties of the output (Layer 3) neurons to investigate whether these neurons had learned to respond selectively to the presence of a vertical straight edge on either the left boundary or right boundary of an object. Such neurons had to respond invariantly across different global object shapes (i.e. hexagon or semicircle), different kinds of object shading (i.e. black or light grey), and different trained retinal locations (i.e. Location 1 or Location 2). The same set of stimuli used to train the network shown in Fig. 4(a) was presented to VisNet during testing, and the firing rate of each neuron in the output layer of the network was recorded. In order to quantify the performance, information analysis was conducted as described in Section 2.2. In this analysis, there are two different stimulus categories ( n = 2 ) as explained in Section 2.2. In Fig. 4(a), stimuli from the first category with a vertical straight edge on the left are shown in rows (b) and (d), while stimuli from the second category with a vertical straight edge on the right are shown in rows (a) and (c). Since each category member was defined by its shape (hexagon or semicircle), shading (black or light grey), and retinal location (Location 1 or Location 2), there were 2 3 = 8 members of transforms of each of the two stimulus categories. Individual Layer 3 neurons had to respond invariantly over the eight transforms of its preferred stimulus category, and not respond to any members of the other stimulus category, in order to carry maximum information about its preferred category. Fig. 5 shows the information analysis of the steady state response properties of Layer 3 neurons at the end of each stimulus presentation. Results are presented before and after training. Plot (a) shows the single cell information analysis. The maximum amount of information possible for the simulation is log 2 ( n ) where n is the number of stimulus categories=2, that is 1 bit. Before training, no neurons reached 1bit of information and in fact most neurons carried much less than 1bit. However, after training, nearly all the neurons carried 1bit of information. This result confirms that nearly all of the Layer 3 neurons had successfully learned to respond selectively to a vertical straight edge either on the left or on the right of an object boundary, regardless of the global shape, shading or retinal location of the object. Plot (b) shows the multiple-cell information analysis. It is evident that training has led to an increase in the multiple cell information, which after training asymptotes to the maximum level of 1bit with only one neuron included in the analysis. This is possible because, in the case of just two stimulus categories, the low or high firing responses of a single perfectly discriminating neuron will provide 1bit of information about both stimulus categories. However, further inspection of the responses of Layer 3 neurons confirmed that some neurons had learned to respond selectively to objects with a straight vertical edge on the left boundary, while other neurons had learned to respond to a straight vertical edge on the right object boundary. This confirmed that the population of Layer 3 neurons learned to represent both of these stimulus categories. Fig. 5(b) shows the steady state firing rate responses of two typical Layer 3 neurons (17, 46) and (33, 23) at the end of each stimulus presentation. The firing rate responses are plotted before and after training. Plot (b1) shows the responses of the two Layer 3 neurons to all eight object stimuli from the second stimulus category, i.e. objects with a vertical straight edge on their right boundary. While plot (b2) shows the responses of the same two Layer 3 neurons to all eight object stimuli from the first stimulus category, i.e. objects with a vertical straight edge on their left boundary. The white circle plotted on each stimulus gives the idea of the size of the fan-in radius of the neurons in the input layer of the network. The results show that, after training, neuron (17, 46) had learned to respond selectively to all objects with a vertical straight edge on the right, while neuron (33, 23) had learned to respond to all objects with a vertical straight edge on the left. These observed firing rate responses in Layer 3 were similar to those experimentally observed in area V4 of the primate visual system (Pasupathy & Connor, 2001, 2002) and demonstrated in the previous simulation study of Eguchi et al. (2015). These are the kind of neuronal response characteristics needed to provide top-down modulation of border ownership neurons in Layer 1 (corresponding to V1/V2). Since the study above uses exactly the same two shapes (hexagon and semicircle) for training and testing the network, there is a possibility that the responses of the developed cells are specific to the set of actual trained objects and might not generalise to novel objects not encountered during training. Therefore, in order to cross-validate the response characteristics of these neurons, the four novel shapes shown in Fig. 4(b) are presented to the same trained network and the firing rates are recorded. In other words, the network was trained with the objects shown in Fig. 4(a) and then tested with a set of four different novel shapes shown in Fig. 4(b). Fig. 6 shows the firing rate responses of the two Layer 3 neurons, which were previously tested on familiar objects in Fig. 5(b), at the end of each novel stimulus presentation before and after training. Similar to the original set of shapes used to train the network, each shape contains a vertical straight edge on either the right or left and is presented at two different retinal locations (i.e., Location 1 or Location 2). Fig. 6(a) shows the responses of two Layer 3 neurons to all eight novel object stimuli from the second stimulus category, i.e. objects with a vertical straight edge on their right boundary. Before training, neuron (17, 46) and neuron (33, 23) both responded quite erratically to the different object stimuli. However, after training, neuron (17, 46) responded to all of the objects with a vertical straight edge on their right, while neuron (33, 23) did not respond to any of these stimuli. Plot (b) shows the responses of the same two Layer 3 neurons to all eight novel object stimuli from the first stimulus category, i.e. objects with a vertical straight edge on their left boundary. Before training, neurons (17, 46) and (33, 23) responded quite erratically to the different object stimuli. However, after training, neuron (33, 23) responded to all of the objects with a vertical straight edge on their left, while neuron (17, 46) did not respond to any of these stimuli. Taken together, these results show that neuron (17, 46) learned to respond selectively to all novel objects with a vertical straight edge on the right, while neuron (33, 23) learned to respond to all novel objects with a vertical straight edge on the left. Thus, the neurons continued to respond selectively to the presence of a vertical straight edge on either the left or the right of an object even if the objects are novel. This result confirms that the representations developed in the output layer of VisNet are not specific to the set of trained objects, but are in fact more generally selective to the presence of a vertical straight edge on either the left boundary or right boundary of an object. We next tested whether Layer 1 neurons had developed the kind of border ownership representations reported by Zhou et al. (2000). In other words, we tested whether the feedback (top-down) connections newly implemented in VisNet enabled the activity in Layer 3 (corresponding to the experimentally observed neural responses in primate visual area V4) to successfully modulate the responses of neurons in Layer 1 (corresponding to visual areas V1/V2) such that the Layer 1 simple cells representing vertical straight edges at either retinal Location 1 or 2 responded selectively depending on whether the vertical straight edge was on the left or right boundary of the object. In order to quantify the performance of Layer 1 neurons, we computed the information carried by the steady state responses of these cells at the end of each stimulus presentation. The results of this analysis are presented in Fig. 7 (a), where we show the information carried by Layer 1 neurons before and after training. Layer 1 neurons are not expected to develop translation invariance across different retinal locations due to the small fan-in of connections from the retina. Therefore, we computed information that was specific to either retinal Location 1 or Location 2. Specifically, the analysis calculated the information carried by the Layer 1 neurons about whether the vertical straight edge in the object stimulus presented to the network was an example from one of four stimulus categories: (i) the vertical straight edge is positioned at retinal Location 1 and is on the left boundary of the object presented there, (ii) the vertical straight edge is positioned at retinal Location 1 and is on the right boundary of the object presented there, (iii) the vertical straight edge is positioned at retinal Location 2 and is on the left boundary of the object presented there, and (iv) the vertical straight edge is positioned at retinal Location 2 and is on the right boundary of the object presented there. Since there are n = 4 stimulus categories, perfectly discriminating neurons carry a maximum of log 2 ( n ) = 2 bits of information. Fig. 7(a1) shows the single cell information analysis. The plot shows the maximum information carried by each of the 4096 neurons in Layer 1 about which one of the four stimulus categories was presented. It can be seen that training the network has led to a large increase in the number of neurons carrying the maximum 2 bits of information. After training, 145 cells learned to carry the maximum single cell information. These Layer 1 neurons thus provide the kind of border ownership representations experimentally observed in cortical visual area V1 by Zhou et al. (2000). Plot (a2) shows the multiple-cell information analysis. Although one may be confused with the unexpectedly good decoding performance even in the untrained network, this can be explained by the topologically established synaptic connections and the feedback connections from the neurons in the higher layer which has larger size of receptive field. However, as long as there is some statistical correlations between the input pattern and the output, the multiple-cell information analysis can better capture the information than the single-cell information analysis. Therefore, it is important to see whether the performances improved after the training or not. In this case, although the change is not as obvious as the case of the layer 3, it is still evident that training has led to an increase in the multiple cell information, which after training asymptotes to the maximum level of 2bits with only two neurons included in the analysis. Fig. 7(b) shows the steady state firing rate responses of four typical Layer 1 neurons at the end of each stimulus presentation before and after training. Plot (b1) shows the responses of the four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their right boundary. After training, neuron (22, 8) responded selectively to all of the objects with a vertical straight edge on their right boundary aligned with retinal Location 1, while neuron (42, 43) responded to all of the objects with a vertical straight edge on their right boundary aligned with retinal Location 2. Plot (b2) shows the responses of the same four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their left boundary. After training, neuron (34, 53) responded selectively to all of the objects with a vertical straight edge on their left boundary aligned with retinal Location 1, while neuron (32, 12) responded to all of the objects with a vertical straight edge on their left boundary aligned with retinal Location 2. The developed firing properties were next cross-validated by testing the same trained network on the novel set of shapes shown in Fig. 4(b). Fig. 8 (b) shows the firing rate responses of the same four Layer 1 neurons that were previously tested on familiar objects in Fig. 7(b). Plot (b1) shows the responses of the four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their right boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. Plot (b2) shows the responses of the same four Layer 1 neurons to all eight object stimuli with a vertical straight edge on their left boundary. The first four stimuli 1\u20134 shown along the abscissa have the object presented in retinal Location 1, while the next four stimuli 5\u20138 have the object presented in retinal Location 2. It can be seen that each of the four Layer 1 neurons responds selectively to one of the four stimulus categories: (i) Location 1/ left boundary, (ii) Location 1/ right boundary, (iii) Location 2/ left boundary, and (iv) Location 2/ right boundary. These results confirm that the border ownership representations developed in Layer 1 are not specific to the set of trained objects, and are in fact able to generalise to the set of novel object shapes. Thus, different Layer 1 neurons had learned to respond selectively to the presence of a vertical straight edge on either the left boundary or right boundary of an object when the edge is aligned with a particular retinal location. These are the same kinds of border ownership representations reported in the neurophysiology study of primate visual area V1 carried out by Zhou et al. (2000). 3.1.2 Dynamical firing properties of cells in layer 1 during each stimulus presentation: time course of the emergence of border ownership signals Sugihara et al. (2011) reported that the representation of border ownership in primate visual area V1, i.e. the selective modulation of the responses of V1 neurons that encode vertical straight edges by whether the edge appears on the left or right boundary of an object, begins to appear at around 61ms after the presentation of the visual stimulus. We hypothesise that this gradual emergence of the border ownership signal in area V1 is due to the time it takes for visual signals to propagate up to higher visual areas such as V4, where neurons may represent a vertical straight edge on either the left or right of an object boundary across different retinal locations, and then to propagate back down to modulate the activities of neurons in area V1. We investigated this proposal computationally by recording the temporal evolution of the responses of border ownership neurons in Layer 1 of the trained VisNet model during 300ms stimulus presentations. Fig. 9 shows the dynamical evolution through time of the border ownership representations conveyed by four typical neurons in Layer 1 during the 300ms time course of stimulus presentations. The results are shown after training has established border ownership representations in Layer 1. Each row shows results for one of the four neurons, where each neuron is tuned to a different border ownership category as follows: (Row 1) the neuron is tuned to a vertical straight edge on the right object boundary aligned with retinal Location 1, (Row 2) the neuron is tuned to a vertical straight edge on the left object boundary aligned with retinal Location 1, (Row 3) the neuron is tuned to a vertical straight edge on the right object boundary aligned with retinal Location 2, and (Row 4) the neuron is tuned to a vertical straight edge on the left object boundary aligned with retinal Location 2. Column (a) shows the average responses of each neuron to the members of its preferred stimulus category (solid line) and the members of its three non-preferred stimulus categories (dashed line) plotted over the 300ms time courses of the stimulus presentations. It can be seen that the firing responses of all four neurons begin to strongly differentiate between their preferred and non-preferred stimulus categories by about 50ms after the start of stimulus presentation. By the end of the stimulus presentation at 300ms, the responses of the neurons fully differentiate between their preferred and non-preferred stimulus categories. Column (b) shows the average single cell information carried by the four neurons about their preferred stimulus category plotted over the 300ms time courses of the stimulus presentations. Consistent with the firing rate plots, there is a monotonic increase in the information carried by each of the four neurons during the 300ms time course of stimulus presentation. The simulation results show how the border ownership representations gradually emerge in Layer 1 over the time course of 300ms during stimulus presentation. Near the beginning of the stimulus presentation, the Layer 1 neurons merely represent the presence of a straight vertical edge at a particular retinal Location 1 or 2. The Layer 1 neurons have not begun to carry information about border ownership at this point. However, as the visual signals propagate up to Layer 3 and back down again to Layer 1, these top down signals from Layer 3 begin to strongly modulate the activities of Layer 1 neurons at around 50ms. The effect of this top down modulation is to drive the activity of the Layer 1 neurons to represent the border ownership categories. These simulation results are qualitatively similar to the temporal evolution of border ownership representations reported by Sugihara et al. (2011) and Jehee et al. (2007). 3.2 Study 2: failure of the model under more general stimulus conditions In the above simulations, we have tested the model by presenting a single object to the network at a time. However, the primate visual system is usually presented with multiple objects simultaneously in real world scenes. This more realistic situation actually exposes a weakness in our current rate-coded model. As we explained earlier, Pasupathy and Connor (2002) have reported that the local boundary representations observed in area V4 such as \u03a6 Left V 4 and \u03a6 Right V 4 are translation invariant across different retinal positions over a modest range. This may lead to a lack of specificity with respect to retinal location in the contextual information that is back-projected to the earlier layers of the network. This will be problematic, for example, when two objects that contain a straight vertical contour on different object sides (left or right) are presented to the network simultaneously. In this case, both \u03a6 Left V 4 and \u03a6 Right V 4 will be activated in the higher V4 layer. However, \u03a6 Left V 4 and \u03a6 Right V 4 will top-down modulate V1/V2 simple cells representing a vertical straight edge on the left and right object boundaries, respectively, across all trained retinal locations. Thus, the top-down modulation of V1/V2 neuronal firing rates is not specific to retinal location. This effectively destroys the local border ownership (binding) information carried by the V1/V2 neurons. We elaborate this important argument in more detail next. 3.2.1 Proposed mechanism by which border ownership information carried by V1/V2 neurons in the rate-coded model may be lost when the network is presented with multiple visual objects Suppose that, during testing of the model, an object that contains a straight vertical contour on the left is presented with that contour positioned at a retinal Location 1, and another object that contains a straight vertical contour on the right is presented with that contour at a retinal Location 2 as shown in Fig. 10 . In this case, as explained in the figure, both \u03a6 Left V 4 and \u03a6 Right V 4 should become highly activated at the same time. However, during training, the subpopulations \u03a6 Left V 4 and \u03a6 Right V 4 are trained to respond invariantly as an object is translated across different retinal locations. This means that both \u03a6 Left V 4 and \u03a6 Right V 4 each end up with strong bi-directional polysynaptic connections with subpopulations of V1/V2 simple cells representing a straight vertical contour at all trained retinal locations. In this case, the top-down signals from \u03a6 Left V 4 and \u03a6 Right V 4 each modulate the responses of V1/V2 simple cells across both retinal Locations 1 and 2. In this situation, as we have explained earlier, \u03a6 Left , Loc 1 V 1 / V 2 will become strongly activated by receiving both the feedforward signals that indicate that a straight vertical contour is present at retinal Location 1 and the feedback signals from \u03a6 Left V 4 that indicate that the straight vertical contour is on the left side of the object. Similarly, \u03a6 Right , Loc 2 V 1 / V 2 will become strongly activated by receiving both the feedforward signals that indicate that a straight vertical contour is present at retinal Location 2 and the feedback signals from \u03a6 Right V 4 that indicate that the straight vertical contour is on the right side of the object. However, the problem is that the other sets of neurons, \u03a6 Left , Loc 2 V 1 / V 2 and \u03a6 Right , Loc 1 V 1 / V 2 may also be strongly activated. This is because both \u03a6 Left V 4 and \u03a6 Right V 4 have strong bi-directional polysynaptic connections with subpopulations of V1/V2 simple cells representing a straight vertical contour at both trained retinal Locations 1 and 2. More specifically, \u03a6 Left , Loc 2 V 1 / V 2 may receive not only the feedforward signals that indicate that a straight vertical contour is present at the retinal location 2, but also the feedback signals from \u03a6 Left V 4 which are actually activated by the presence of the other object with a straight vertical contour on the left at retinal Location 1. As a result, \u03a6 Left , Loc 2 V 1 / V 2 may become activated even though no object with a straight vertical contour on the left is ever presented at retinal Location 2. Similarly, \u03a6 Right , Loc 1 V 1 / V 2 may receive not only the feedforward signals that indicate that the straight vertical contour is present at retinal Location 1, but also the feedback signals from \u03a6 Right V 4 which are activated by the presence of the other object with a straight vertical contour on the right at retinal Location 2. As a result, \u03a6 Right , Loc 1 V 1 / V 2 may become activated even though no object with a straight vertical contour on the right is ever presented at retinal Location 1. The upshot of this is that when the two objects are presented to the model simultaneously, all of the V1/V2 subpopulations \u03a6 Left , Loc 1 V 1 / V 2 , \u03a6 Right , Loc 1 V 1 / V 2 , \u03a6 Left , Loc 2 V 1 / V 2 and \u03a6 Right , Loc 2 V 1 / V 2 may become active. In this case, these subpopulations of V1/V2 neurons will fail to represent the border ownership (binding) information. This will be a general problem for the current rate-coded formulation of the model when presented with visual input from more realistic scenes containing multiple objects. 3.2.2 Results In this section, the model was trained with the set of objects shown in Fig. 4, where these objects were presented to the network one at a time during training as described in the simulations above. However, the network was then tested with two objects shown together during each visual presentation, where we used the set of test images shown in Fig. 11 . We analysed the steady state firing responses of Layer 1 neurons at the end of each such visual presentation. We compared these results with those reported above in which only a single object was presented to the network at a time during testing. In order to facilitate comparison of the results for the two test situations, in each case we analysed how much information Layer 1 neurons carried about border ownership stimulus categories (straight vertical edges on the left or right object boundaries) that were associated with retinal Location 1. The set of images used for testing the network with two objects at a time are shown in Fig. 11. There are two different stimulus categories. The first stimulus category, shown in Fig. 11(a), consists of all possible combinations two objects where one of the objects has a vertical straight edge on its right boundary which is positioned at retinal Location 1. On the other hand, the second stimulus category, shown in Fig. 11(b), consists of all possible combinations two objects where one of the objects has a vertical straight edge on its left boundary which is positioned at retinal Location 1. Each of the two stimulus categories undergoes 16 transforms, which are due to variations in the following four stimulus features: 2 different shapes (semicircle or hexagon) at retinal Location 1 \u00d7 2 different shapes (semicircle or hexagon) at retinal Location 2 \u00d7 2 sides of an object (left or right) on which a straight vertical edge may occur at retinal Location 2 \u00d7 2 kinds of shading contrast between objects and background. Fig. 12 compares the border ownership information carried by Layer 1 neurons (corresponding to visual areas V1/V2) when the network is tested with objects shown individually (solid line) or when tested on two objects presented together (dashed line). We assess the performance of the network using single-cell information analysis. The information analysis is applied to the steady state firing responses of Layer 1 neurons at the end of each stimulus presentation. The solid lines in Fig. 12 shows the performance of the model when tested with the objects shown in Fig. 4 presented one at a time during testing. We computed the single cell information carried by each Layer 1 neuron about one of the four stimulus categories separately while we previously plotted them altogether in Fig. 7(a). That is, we computed the information about whether the vertical straight edge in the object stimulus was from one of the following four stimulus categories: (i) a vertical straight edge on the left object boundary positioned at retinal Location 1, (ii) a vertical straight edge on the right object boundary positioned at retinal Location 1, (iii) a vertical straight edge on the left object boundary positioned at retinal Location 2, and (iv) a vertical straight edge on the right object boundary positioned at retinal Location 2. Since there are four stimulus categories, perfectly discriminating neurons carry a maximum of 2 bits of information. However, in this figure the maximum single-cell information has been rescaled to 1. We plot results for the two stimulus categories (i) and (ii), which are associated with retinal Location 1. The information carried by Layer 1 neurons about stimulus category (i) is shown in the right plot, while information carried by Layer 1 neurons about the stimulus category (ii) is shown in the left plot. It can be seen that when the network is tested on a single object at a time, a large number of Layer 1 neurons (i.e. 55 neurons and 71 neurons for the stimulus category (i) and (ii), respectively) reach the theoretical maximum level of information about border ownership. The dashed lines in Fig. 12 shows the performance of the model when tested with two objects shown together during each visual presentation using the test images shown in Fig. 11. Here we computed the single cell information carried by the Layer 1 neurons about the two stimulus categories described in Fig. 11, which are both associated with retinal Location 1. The first stimulus category includes all combinations of two objects where one of the objects has a vertical straight edge on its right boundary which is positioned at retinal Location 1, while the second stimulus category includes all combinations of two objects where one of the objects has a vertical straight edge on its left boundary positioned at retinal Location 1. Since there are two stimulus categories, neurons may carry up to a maximum of 1 bit of information. The information carried by Layer 1 neurons about the first stimulus category is shown in the left plot, while information carried by Layer 1 neurons about the second stimulus category is shown in the right plot. It can be seen that when the network is tested on two objects at a time, there is a large drop in the levels of single cell information carried by Layer 1 neurons about border ownership compared to when the network is tested on individual objects, with far fewer Layer 1 neurons reaching the theoretical maximum of 1 bit for this test case. This result supported our above prediction that border ownership information carried by Layer 1 (V1/V2) neurons in the rate-coded model may be lost when the network is presented with multiple visual objects during testing. It can be seen in Fig. 12, however, that a small number of Layer 1 neurons did still reach the maximum of 1 bit of information (19 neurons for both stimulus categories (i) and (ii)). How might this happen if both of the Layer 3 subpopulations \u03a6 Left V 4 and \u03a6 Right V 4 were completely translation invariant with strong bi-directional (bottom-up and top-down) polysynaptic connections with subpopulations of Layer 1 (V1/V2) simple cells representing a straight vertical contour at both trained retinal Locations 1 and 2? To understand this, we investigated the firing properties of neurons in Layer 2 after training. Although not shown here, it was found that, due to the limited feedforward fan-in of synaptic connections from the input \u2018retina\u2019, some of the Layer 2 neurons had learned to respond to a vertical straight edge either on the left object boundary or right object boundary at only a single retinal location. These location-specific Layer 2 neurons were then able to directly modulate the Layer 1 neurons representing that particular retinal location. This would allow these Layer 1 neurons to continue to respond selectively to whether a vertical straight edge was on either the left or right boundary of an object presented at that retinal location regardless of the presence of another object simultaneously presented elsewhere. However, this effect was rather minor given that the great majority of Layer 1 neurons lost their border ownership selectivity when two objects were presented during testing. 4 Discussion We have investigated through computer simulation how top-down connections may play a fundamental role in the development of border ownership representations in the early cortical visual layers V1/V2. In terms of the novelty, this work is different from previous modelling studies that have already proposed hypothetical neural circuits for such coding in that we investigated how such circuits may develop using a biologically plausible, local, trace learning rule to modify the synaptic connectivity during visual experience. A number of modelling studies have previously considered the role of top-down signals in visual information processing. For example, as discussed in Section 1.1, some authors have proposed that top-down connections might implement attention to objects during visual search (Deco & Lee, 2002; Deco & Rolls, 2004). However, in these previous modelling studies the top-down connections were only introduced after the initial training phase was completed, and hence the self-organisation of the synaptic connections throughout the network relied on purely feedforward visual processing. Consequently, the top-down connections did not affect the visual representations that developed in the network during visually-guided learning. In another modelling study carried out by Renart, Parga, and Rolls (1999), top-down connections were able to influence the recall of visual representations in a linked attractor network comprised of multiple cortical modules (Rolls, 2008). However, the representations in this attractor network were hand specified during an initial stage of supervised learning, and did not self-organise using unsupervised competitive learning. Thus, again, the top-down connections were not able to influence the nature of the visual representations that developed. In our own model presented in this paper, the top-down connections are present during both training and testing. Consequently, the top-down connections played a critical role in the self-organisation of border ownership representations in Layer 1 during the initial unsupervised competitive learning. In this case, each neuron receives signals from both afferent bottom-up and top-down connections, which self-organise simultaneously during learning. This allows the network to develop representations that depend on a precise learned combination of bottom-up and top-down signals. The simulations reported here have demonstrated how top-down connections may help to guide competitive learning in lower layers, thus driving the formation of lower level (border ownership) visual representations in V1/V2 that are modulated by higher level (object boundary element) representations in V4. More precisely, we have shown that simple cells in area V1 representing a vertical straight edge at a particular retinal location can learn to be modulated by top-down connections from higher level representations of object shape in, for example, area V4 (Pasupathy & Connor, 2001; Pasupathy & Connor, 2002). However, more importantly, we also identified the limitation of the mechanism within a rate-coded model when trying to simulate the results of the neurophysiological studies that have shown that border-ownership selective neurons for single-figure displays generally are so also for multi-figure displays (Martin & von der Heydt, 2015; Qiu, Sugihara, & von der Heydt, 2007). In the second half of the simulation studies, we have investigated how the rate-coded model presented in this paper fails under more general stimulus conditions, in which more than one object stimulus is presented to the network at the same time after training. The result suggests that the incorporation of additional top-down connections, although necessary, is not sufficient by itself to allow the network to develop robust border ownership representations in the early layers and thus solve this kind of feature binding problem. Our model failed because the current model of the network is not able to specify which features are part of which objects. Therefore, we propose that it is important to have a form of binding neuron (e.g., border ownership neuron in V1/V2) that responds if and only if the neurons representing the low-level feature such as simple oriented bars are actually participating in driving the neurons representing the high-level feature. The binding neuron should not respond if the neurons representing the low-level feature and the neurons representing the high-level feature just happen to be co-active, where the former are not actually driving the latter. Such unrelated co-activation of low and high-level features might occur, for example, because of the presence of multiple similar objects within a complex natural scene. Then, the question is what further biological details is needed to be incorporated into the model to allow it to form such robust border ownership representations under more general stimulus conditions. A biological detail that is not implemented in the current model is cortical magnification. It is known that mammalian brains process visual input in a highly non-uniform manner. Specifically, the Ganglion cells in the retina sample the visual input at a higher resolution in the fovea than the periphery (Wassle, Grunert, Rohrenbeck, & Boycott, 1990), which gives rise to a distorted visual field representation in V1 where the fovea has a higher \u201ccortical magnification factor\u201d, i.e. more V1 neurons processing foveal input than the peripheral visual field (Cowey & Rolls, 1974; Daniel & Whitteridge, 1961). Subsequent neural processing, with a simple Gaussian sampling of the representation that is laid out across the surface of area V1, results in an asymmetry of central V4 receptive fields as well (Motter, 2009). The question is whether cortical magnification may play any role in the development of border ownership representations. Our laboratory has previously investigated the effects of implementing a cortical magnification factor within a purely feedforward neural network model of primate visual object recognition (Trappenberg, Rolls, & Stringer, 2002). It was found that when the objects were presented against a simple blank background then neurons in the upper cortical layer responded to their preferred objects across a wide region of the retina. In this scenario, trace learning can continue to operate normally as an object translates across different locations on the retina. Neurophysiological evidence for trace learning has been reported by Cox, Meier, Oertelt, and DiCarlo (2005). Moreover, past simulation studies have found that the trace learning mechanism is quite robust to the way in which the eyes saccade around the visual scene, and is in fact enhanced by more randomised exploration of a scene (Rolls & Milward, 2000). Consequently, we would not expect the introduction of a cortical magnification factor into the border ownership simulations reported in this paper to prevent the model from operating in the same qualitative manner as described above. However, in the simulation study of (Trappenberg et al., 2002), it was also found that, with a cortical magnification factor, if the objects were presented against cluttered backgrounds then the receptive fields of neurons in the upper layer shrunk down around the fovea due to competition from the background features. These simulation results reflected what had been previously observed in a primate neurophysiology study carried out by Rolls, Aggelopoulos, and Zheng (2003), in which the receptive fields of object-selective neurons in the primate temporal visual cortex reduced down to approximately the size of the object when it was presented against a natural scene. It should also be noted that the neurophysiology studies of V4 shape selective neurons (Pasupathy & Connor, 2001) investigated the responses of these neurons to shapes that were presented in isolation. Our border ownership model sought to replicate the development of these V4 shape selective firing properties in Layer 3 under similar viewing conditions - that is, the network was trained on one shape at a time presented against a blank background. It remains to be seen how the firing properties of these shape selective neurons in area V4 of the primate brain might be affected when the shapes are presented within natural scenes. The cortical magnification factor may play an important role in this situation. Addressing these issues will require a combination of further neurophysiology and modelling studies. Another biological detail that is not implemented in the current model is the spike dynamics of neurons. We hypothesise that extending the model with spiking neural network would solve the issue. The current rate-coded model only represents the average firing rate of each neuron, and not the actual timings of the electrical pulses emitted by neurons in the brain. The architecture and operation of neural tissue in the visual cortex of primates differs from the VisNet model implemented in this paper in the following important ways. Firstly, real neurons in the brain communicate by emitting and receiving electrical pulses called action potentials or \u2018spikes\u2019. Secondly, the way in which synapses are strengthened and weakened during learning is dependent on the timings of the spikes emitted by the pre- and post-synaptic neurons (Bi & Poo, 1998; Markram, Lbke, Frotscher, & Sakmann, 1997). For example, in the brain, a synapse may be strengthened if the pre-synaptic spike occurs about 20ms before the post-synaptic spike, but weakened if the pre-synaptic spike occurs about 20ms after the post-synaptic spike. This is known as spike time dependent plasticity (STDP). Thirdly, the electrical pulses can take several milliseconds to travel along an axon from one neuron to the next, with different axonal connections having different time delays. Physiological studies have shown that neural synchrony is unrelated, or at best weakly related, to contour grouping (Martin & von der Heydt, 2015; Roelfsema, Lamme, & Spekreijse, 2004). On the other hand, if distributions of axonal delays between neurons are incorporated into a model, then this can give rise to a phenomenon known as \u2018polychronization\u2019 (Izhikevich, 2006). This phenomenon involves the network learning many memory patterns, each of which takes the form of a repeating temporal loop of neuronal firings. These temporal memory loops self-organise automatically when STDP is used to modify the strengths of synapses in a recurrently connected spiking network with randomised distributions of axonal conduction delays between neurons. Polychronization can dramatically increase the selectivity of neurons and increase the memory capacity of a network. We hypothesise that such a spiking model may develop border ownership neurons in layer 1 (corresponding to V1/V2) that respond selectively to a vertical straight edge on either the left or right boundary of an object at the neuron\u2019s preferred retinal location, regardless of the presence of other objects at different retinal locations. More generally, we propose that these biological elements will be needed to model how the primate visual system solves \u2018the binding problem\u2019 in vision. Consequently, in future work we will explore how border ownership representations may develop in a new spiking neural network version of the VisNet model, which incorporates bottom-up and top-down connections, distributions of axonal transmission delays, and spike time dependent plasticity (STDP). References Angelucci and Bullier, 2003 A. Angelucci J. Bullier Reaching beyond the classical receptive field of V1 neurons: Horizontal or feedback axons? Journal of Physiology-Paris 97 23 2003 141 154 Baek and Sajda, 2005 K. Baek P. Sajda Inferring figure-ground using a recurrent integrate-and-fire neural circuit IEEE Transactions on Neural Systems and Rehabilitation Engineering 13 2 2005 125 130 Bi and Poo, 1998 G.-q. Bi M.-m. Poo Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type The Journal of Neuroscience 18 24 1998 10464 10472 Cowey and Rolls, 1974 A. Cowey E.T. Rolls Human cortical magnification factor and its relation to visual acuity Experimental Brain Research 21 5 1974 447 454 Cox et al., 2005 D.D. Cox P. Meier N. Oertelt J.J. DiCarlo \u2019Breaking\u2019 position-invariant object recognition Nature Neuroscience 8 9 2005 1145 1147 PMID: 16116453 Craft et al., 2007 E. Craft H. Schtze E. Niebur R. von der Heydt A neural model of figure-ground organization Journal of Neurophysiology 97 6 2007 4310 4326 Cumming and Parker, 1999 B.G. Cumming A.J. Parker Binocular neurons in v1 of awake monkeys are selective for absolute, not relative, disparity The Journal of Neuroscience 19 13 1999 5602 5618 PMID: 10377367 Daniel and Whitteridge, 1961 P.M. Daniel D. Whitteridge The representation of the visual field on the cerebral cortex in monkeys The Journal of Physiology 159 2 1961 203 221 Deco and Lee, 2002 G. Deco T. Lee A unified model of spatial and object attention based on inter-cortical biased competition 2002 Computer Science Department Deco and Rolls, 2004 G. Deco E.T. Rolls A neurodynamical cortical model of visual attention and invariant object recognition Vision Research 44 6 2004 621 642 Eguchi et al., 2015 A. Eguchi B.M. W. Mender B. Evans G. Humphreys S. Stringer Computational modeling of the neural representation of object shape in the primate ventral visual system Frontiers in Computational Neuroscience 9 100 2015 100 Foldiak, 1991 P. Foldiak Learning invariance from transformation sequences Neural Computation 3 2 1991 194 200 Freeman and Simoncelli, 2011 J. Freeman E.P. Simoncelli Metamers of the ventral stream Nature Neuroscience 14 9 2011 1195 1201 Gross et al., 1969 C.G. Gross D.B. Bender C.E. Rocha-Miranda Visual receptive fields of neurons in inferotemporal cortex of the monkey Science 166 3910 1969 1303 1306 PMID: 4982685 Izhikevich, 2006 E.M. Izhikevich Polychronization: Computation with spikes Neural Computation 18 2 2006 245 282 Jehee et al., 2007 J.F. M. Jehee V.A. F. Lamme P.R. Roelfsema Boundary assignment in a recurrent network architecture Vision Research 47 9 2007 1153 1165 Jones and Palmer, 1987 J.P. Jones L.A. Palmer The two-dimensional spatial structure of simple receptive fields in cat striate cortex Journal of Neurophysiology 58 6 1987 1187 1211 PMID: 3437330 Kohonen, 1982 T. Kohonen Self-organized formation of topologically correct feature maps Biological Cybernetics 43 1 1982 59 69 Lades et al., 1993 M. Lades J. Vorbruggen J. Buhmann J. Lange C. von der Malsburg R. Wurtz W. Konen Distortion invariant object recognition in the dynamic link architecture IEEE Transactions on Computers 42 3 1993 300 311 Layton et al., 2012 O.W. Layton E. Mingolla A. Yazdanbakhsh Dynamic coding of border-ownership in visual cortex Journal of Vision 12 13 2012 8\u20138 Markram et al., 1997 H. Markram J. Lbke M. Frotscher B. Sakmann Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs Science (New York, N.Y.) 275 5297 1997 213 215 Martin and von der Heydt, 2015 A.B. Martin R. von der Heydt Spike synchrony reveals emergence of proto-objects in visual cortex The Journal of Neuroscience 35 17 2015 6860 6870 Motter, 2009 B.C. Motter Central V4 receptive fields are scaled by the V1 cortical magnification and correspond to a constant sized sampling of the V1 surface The Journal of neuroscience: The official journal of the Society for Neuroscience 29 18 2009 5749 5757 Nishimura and Sakai, 2004 H. Nishimura K. Sakai Determination of border ownership based on the surround context of contrast Neurocomputing 5860 2004 843 848 Pasupathy, 2006 A. Pasupathy Neural basis of shape representation in the primate brain Progress in Brain Research 154 2006 293 313 PMID: 17010719 Pasupathy and Connor, 2001 A. Pasupathy C.E. Connor Shape representation in area v4: Position-specific tuning for boundary conformation Journal of Neurophysiology 86 5 2001 2505 2519 Pasupathy and Connor, 2002 A. Pasupathy C.E. Connor Population coding of shape in area v4 Nature Neuroscience 5 12 2002 1332 1338 Petkov and Kruizinga, 1997 N. Petkov P. Kruizinga Computational models of visual neurons specialised in the detection of periodic and aperiodic oriented visual stimuli: Bar and grating cells Biological cybernetics 76 2 1997 83 96 PMID: 9116079 Pettet and Gilbert, 1992 M.W. Pettet C.D. Gilbert Dynamic changes in receptive-field size in cat primary visual cortex Proceedings of the National Academy of Sciences 89 17 1992 8366 8370 PMID: 1518870 Qiu et al., 2007 F.T. Qiu T. Sugihara R. von der Heydt Figure-ground mechanisms provide structure for selective attention Nature Neuroscience 10 11 2007 1492 1499 Renart et al., 1999 A. Renart N. Parga E.T. Rolls Associative memory properties of multiple cortical modules Network: Computation in Neural Systems 10 3 1999 237 255 Roelfsema et al., 2004 P.R. Roelfsema V.A. F. Lamme H. Spekreijse Synchrony and covariation of firing rates in the primary visual cortex during contour grouping Nature Neuroscience 7 9 2004 982 991 Rolls, 2000 E.T. Rolls Functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition Neuron 27 2 2000 205 218 PMID: 10985342 Rolls, 2008 E. Rolls Memory, attention, and decision-making: A unifying computational neuroscience approach 1st ed. 2008 Oxford University Press Rolls, 2012 E.T. Rolls Invariant visual object and face recognition: Neural and computational bases, and a model, VisNet Frontiers in Computational Neuroscience 6 2012 35 Rolls et al., 2003 E.T. Rolls N.C. Aggelopoulos F. Zheng The receptive fields of inferior temporal cortex neurons in natural scenes The Journal of Neuroscience 23 1 2003 339 348 Rolls and Milward, 2000 E.T. Rolls T. Milward A model of invariant object recognition in the visual system: Learning rules, activation functions, lateral inhibition, and information-based performance measures Neural Computation 12 11 2000 2547 2572 PMID: 11110127 Rolls et al., 1997 E.T. Rolls A. Treves M.J. Tovee S. Panzeri Information in the neuronal representation of individual stimuli in the primate temporal visual cortex Journal of computational neuroscience 4 4 1997 309 333 Royer and Par\u00e9, 2003 S. Royer D. Par\u00e9 Conservation of total synaptic weight through balanced synaptic depression and potentiation Nature 422 6931 2003 518 522 PMID: 12673250 Rubin, 1915 E. Rubin Synsoplevede figurer (PhD thesis) 1915 University of Copenhagen Copenhagen Rumelhart and Zipser, 1985 D.E. Rumelhart D. Zipser Feature discovery by competitive learning Cognitive Science 9 1 1985 75 112 Sugihara et al., 2011 T. Sugihara F.T. Qiu R. von der Heydt The speed of context integration in the visual cortex Journal of Neurophysiology 106 1 2011 374 385 Trappenberg et al., 2002 T.P. Trappenberg E.T. Rolls S.M. Stringer Effective size of receptive fields of inferior temporal visual cortex neurons in natural scenes Advances in Neural Information Processing Systems 1 2002 293 300 Tromans et al., 2011 J.M. Tromans M. Harris S.M. Stringer A computational model of the development of separate representations of facial identity and expression in the primate visual system PLoS ONE 6 10 2011 e25616 von der Heydt et al., 2003 R. von der Heydt H. Zhou H.S. Friedman Neural coding of border ownership: Implications for the theory of figure-ground perception Perceptual organization in vision: Behavioral and neural perspectives 2003 281 304 von der Malsburg, 1973 C. von der Malsburg Self-organization of orientation sensitive cells in the striate cortex Kybernetik 14 2 1973 85 100 Wagatsuma et al., 2013 N. Wagatsuma M. Oki K. Sakai Feature-based attention in early vision for the modulation of figure-ground segregation Frontiers in Psychology 4 2013 Wallis and Rolls, 1997 G. Wallis E.T. Rolls Invariant face and object recognition in the visual system Progress in Neurobiology 51 2 1997 167 194 Wassle et al., 1990 H. Wassle U. Grunert J. Rohrenbeck B.B. Boycott Retinal ganglion cell density and cortical magnification factor in the primate Vision Research 30 11 1990 1897 1911 Zhaoping, 2005 L. Zhaoping Border ownership from intracortical interactions in visual area V2 Neuron 47 1 2005 143 153 Zhou et al., 2000 H. Zhou H.S. Friedman R. von der Heydt Coding of border ownership in monkey visual cortex The Journal of Neuroscience 20 17 2000 6594 6611", "scopus-id": "84992190571", "pubmed-id": "27743879", "coredata": {"eid": "1-s2.0-S1074742716302532", "dc:description": "Abstract As Rubin\u2019s famous vase demonstrates, our visual perception tends to assign luminance contrast borders to one or other of the adjacent image regions. Experimental evidence for the neuronal coding of such border-ownership in the primate visual system has been reported in neurophysiology. We have investigated exactly how such neural circuits may develop through visually-guided learning. More specifically, we have investigated through computer simulation how top-down connections may play a fundamental role in the development of border ownership representations in the early cortical visual layers V1/V2. Our model consists of a hierarchy of competitive neuronal layers, with both bottom-up and top-down synaptic connections between successive layers, and the synaptic connections are self-organised by a biologically plausible, temporal trace learning rule during training on differently shaped visual objects. The simulations reported in this paper have demonstrated that top-down connections may help to guide competitive learning in lower layers, thus driving the formation of lower level (border ownership) visual representations in V1/V2 that are modulated by higher level (object boundary element) representations in V4. Lastly we investigate the limitations of our model in the more general situation where multiple objects are presented to the network simultaneously.", "openArchiveArticle": "false", "prism:coverDate": "2016-12-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1074742716302532", "dc:creator": [{"@_fa": "true", "$": "Eguchi, Akihiro"}, {"@_fa": "true", "$": "Stringer, Simon M."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1074742716302532"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1074742716302532"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1074-7427(16)30253-2", "prism:volume": "136", "prism:publisher": "The Author(s). Published by Elsevier Inc.", "dc:title": "Neural network model develops border ownership representation through visually guided learning", "prism:copyright": "\u00a9 2016 The Author(s). Published by Elsevier Inc.", "openaccess": "1", "prism:issn": "10747427", "dcterms:subject": [{"@_fa": "true", "$": "Primate vision"}, {"@_fa": "true", "$": "Border ownership"}, {"@_fa": "true", "$": "Neural network model"}], "openaccessArticle": "true", "prism:publicationName": "Neurobiology of Learning and Memory", "openaccessSponsorType": "Author", "prism:pageRange": "147-165", "prism:endingPage": "165", "prism:coverDisplayDate": "December 2016", "prism:doi": "10.1016/j.nlm.2016.10.007", "prism:startingPage": "147", "dc:identifier": "doi:10.1016/j.nlm.2016.10.007", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "163", "@width": "166", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2284", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "186", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7405", "@ref": "gr10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "99", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr11.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8693", "@ref": "gr11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "102", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr12.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5310", "@ref": "gr12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "173", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7315", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "82", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8132", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "147", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11848", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "139", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6089", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "209", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9168", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "144", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7195", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "211", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9591", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "170", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6283", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "306", "@width": "311", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "6377", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "329", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28518", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "334", "@width": "738", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr11.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "59500", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "291", "@width": "627", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr12.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "36971", "@ref": "gr12", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "704", "@width": "744", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "99266", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "282", "@width": "756", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "69557", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "506", "@width": "756", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "96698", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "811", "@width": "689", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "96401", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "548", "@width": "699", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "76631", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "785", "@width": "689", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "109623", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "536", "@width": "691", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "89759", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "691", "@width": "717", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "90948", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1356", "@width": "1378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "37441", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1456", "@width": "1651", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr10_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "217678", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1478", "@width": "3269", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr11_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "290643", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1290", "@width": "2776", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr12_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "253998", "@ref": "gr12", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3114", "@width": "3293", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "755324", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1250", "@width": "3346", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "638398", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2240", "@width": "3348", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "558837", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3591", "@width": "3051", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "665636", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2428", "@width": "3096", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "509174", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3474", "@width": "3051", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "778756", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2374", "@width": "3059", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr8_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "605169", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3056", "@width": "3173", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-gr9_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "660411", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "12", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si100.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "227", "@ref": "si100", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "317", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si101.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1502", "@ref": "si101", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "22", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si102.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "216", "@ref": "si102", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si104.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si104", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "248", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si106.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1716", "@ref": "si106", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "36", "@width": "245", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si107.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1422", "@ref": "si107", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "230", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si108.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1136", "@ref": "si108", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "22", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si109.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "213", "@ref": "si109", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "271", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si110.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1299", "@ref": "si110", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "21", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si112.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "257", "@ref": "si112", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "24", "@width": "71", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si113.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "464", "@ref": "si113", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "23", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si115.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "229", "@ref": "si115", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "37", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si116.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "339", "@ref": "si116", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si117.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "213", "@ref": "si117", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si118.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "198", "@ref": "si118", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "46", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si119.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "365", "@ref": "si119", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "25", "@width": "63", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "570", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "61", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si120.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "456", "@ref": "si120", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "78", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si122.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "482", "@ref": "si122", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "42", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si123.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "268", "@ref": "si123", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "49", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si124.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "325", "@ref": "si124", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "52", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si125.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "440", "@ref": "si125", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "84", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si126.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "511", "@ref": "si126", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "23", "@width": "38", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "417", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "24", "@width": "54", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "519", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "25", "@width": "188", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si27.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1209", "@ref": "si27", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "84", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si29.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "512", "@ref": "si29", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "25", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "562", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "59", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si30.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "380", "@ref": "si30", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "10", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si33.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si33", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si34.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "211", "@ref": "si34", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "18", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si35.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "224", "@ref": "si35", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si36.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "229", "@ref": "si36", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "16", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si37.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "214", "@ref": "si37", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si38.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "222", "@ref": "si38", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "13", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si39.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "215", "@ref": "si39", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "23", "@width": "31", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "370", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "113", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si40.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "580", "@ref": "si40", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "10", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si41.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "183", "@ref": "si41", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si42.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "199", "@ref": "si42", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "128", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si43.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "697", "@ref": "si43", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "10", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si44.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "190", "@ref": "si44", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "17", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si45.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "216", "@ref": "si45", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si46.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "200", "@ref": "si46", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si47.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "232", "@ref": "si47", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "8", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si48.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "171", "@ref": "si48", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "44", "@width": "419", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si52.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2386", "@ref": "si52", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "100", "@width": "163", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si53.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2254", "@ref": "si53", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "26", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si55.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "257", "@ref": "si55", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "10", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si56.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "194", "@ref": "si56", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "26", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si58.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "285", "@ref": "si58", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si62.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "220", "@ref": "si62", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "48", "@width": "245", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si63.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1454", "@ref": "si63", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "12", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si65.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "202", "@ref": "si65", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "21", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si66.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "259", "@ref": "si66", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si68.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "237", "@ref": "si68", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "32", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si69.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "285", "@ref": "si69", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "24", "@width": "55", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "526", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "52", "@width": "360", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si71.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2054", "@ref": "si71", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "22", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si73.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "251", "@ref": "si73", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "45", "@width": "291", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si78.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1487", "@ref": "si78", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si79.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "218", "@ref": "si79", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si85.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "275", "@ref": "si85", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "39", "@width": "143", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si86.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1013", "@ref": "si86", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "18", "@width": "30", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si87.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "307", "@ref": "si87", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "45", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si88.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "367", "@ref": "si88", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "19", "@width": "33", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si89.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "314", "@ref": "si89", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "31", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si90.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "307", "@ref": "si90", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "37", "@width": "164", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si91.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "917", "@ref": "si91", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "30", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si92.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "298", "@ref": "si92", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si94.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "237", "@ref": "si94", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "39", "@width": "78", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si95.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "560", "@ref": "si95", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "17", "@width": "32", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si96.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "297", "@ref": "si96", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "32", "@width": "124", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si98.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "840", "@ref": "si98", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "51", "@width": "208", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1074742716302532-si99.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1527", "@ref": "si99", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84992190571"}}