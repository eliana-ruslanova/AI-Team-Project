{"scopus-eid": "2-s2.0-85045189457", "originalText": "serial JL 278549 291210 291703 31 90 Canadian Association of Radiologists Journal CANADIANASSOCIATIONRADIOLOGISTSJOURNAL 2018-04-11 2018-04-11 2018-04-26 2018-04-26 2018-04-26T23:02:19 1-s2.0-S0846537118300305 S0846-5371(18)30030-5 S0846537118300305 10.1016/j.carj.2018.02.002 S300 S300.1 FULL-TEXT 1-s2.0-S0846537118X00035 2018-04-27T00:14:05.44197Z 0 0 20180501 20180531 2018 2018-04-11T12:49:18.449428Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body acknowledge affil articletitle auth auth authfirstini authfull authkeywords authlast nonengabst orcid primabst pubtype ref 0846-5371 08465371 UNLIMITED NONE true 69 69 2 2 Volume 69, Issue 2 2 120 135 120 135 201805 May 2018 2018-05-01 2018-05-31 2018 Health Policy and Practice / Sant\u00e9: politique et pratique m\u00e9dicale article rev \u00a9 2018 The Authors. Published by Elsevier Inc. on behalf of Canadian Association of Radiologists. CANADIANASSOCIATIONRADIOLOGISTSWHITEPAPERARTIFICIALINTELLIGENCEINRADIOLOGY TANG A Objectives AI Terminology Hierarchy of AI Fields Neural Networks Learning Process Recommendations Education Education for Image Analysis AI Research and Development Education for Commercialization Education for Radiologists Using AI Products Evidence-Based Medicine Recommendations Research and Development Opportunities Access to Appropriate Data Type DICOM Anonymization and Deidentification Data Sharing Common Computing Framework Bridging the Valley of Death Academic and Industrial Collaborations Recommendations Clinical Applications Clinical Workflow Types of Applications Use Cases Separate normal from not normal Improved CAD Radiomics Workflow optimization and quality assurance Grading and classification Natural language processing, computer-assisted reporting, and knowledge management Recommendations Implementation Data Interoperability Framework People Ethical, Legal, and Social Implications Recommendations Structure and Governance Recommendation Role of Radiologists Recommendations Impact of AI on Radiology in Canada Recommendations Conclusions Acknowledgements References GOODFELLOW 2016 I DEEPLEARNING ARTHUR 1959 535 554 S MCCARTHY 2006 12 J LECUN 2015 436 444 Y CHARTRAND 2017 2113 2131 G LAMBIN 2012 441 446 P SU 2017 J EVTIMOV 2017 I TRABOULSEE 2016 394 401 A BOSSUYT 2015 826 832 P COLLINS 2015 55 63 G KAMNITSAS 2017 K UNSUPERVISEDDOMAINADAPTATIONINBRAINLESIONSEGMENTATIONADVERSARIALNETWORKS BROSCH 2016 69 96 T MACHINELEARNINGMEDICALIMAGING DEEPLEARNINGBRAINIMAGESITSAPPLICATIONMULTIPLESCLEROSIS CHANNIN 2009 590 592 D CARDOSO 2017 M INTRAVASCULARIMAGINGCOMPUTERASSISTEDSTENTINGLARGESCALEANNOTATIONBIOMEDICALDATAEXPERTLABELSYNTHESIS MERKEL 2014 2 D BOSSUYT 2006 1089 1092 P BECKER 2017 434 440 A GILLIES 2016 563 577 R PONS 2016 329 343 E ALKASAB 2017 1184 1189 T WANG 2017 X ZALIS 2010 625 633 M PRIOR 2013 1282 1285 F WOLSTENCROFT 2013 W557 W561 K DUMONTIER 2014 14 M LANGLOTZ 2006 1595 1597 C KOOI 2017 303 312 T DROZDZAL 2018 1 13 M ANTHIMOPOULOS 2016 1207 1216 M SHIN 2016 1 31 H LITJENS 2017 G WANG 2016 D LAKHANI 2017 574 582 P TANGX2018X120 TANGX2018X120X135 TANGX2018X120XA TANGX2018X120X135XA Full 2018-03-07T14:11:11Z Author http://creativecommons.org/licenses/by-nc-nd/4.0/ 2019-04-26T00:00:00.000Z 2019-04-26T00:00:00.000Z UnderEmbargo http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license. \u00a9 2018 The Authors. Published by Elsevier Inc. on behalf of Canadian Association of Radiologists. item S0846-5371(18)30030-5 S0846537118300305 1-s2.0-S0846537118300305 10.1016/j.carj.2018.02.002 278549 2018-04-26T23:21:17.829081Z 2018-05-01 2018-05-31 UNLIMITED NONE 1-s2.0-S0846537118300305-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/MAIN/application/pdf/029f8007a5020315d3252783aefcc4ea/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/MAIN/application/pdf/029f8007a5020315d3252783aefcc4ea/main.pdf main.pdf pdf true 1003215 MAIN 16 1-s2.0-S0846537118300305-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/PREVIEW/image/png/3dcc15638b8c1b90d26a246f52da91e0/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/PREVIEW/image/png/3dcc15638b8c1b90d26a246f52da91e0/main_1.png main_1.png png 74716 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0846537118300305-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr4/THUMBNAIL/image/gif/ad33f0cddf47e959dbf23313785b5764/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr4/THUMBNAIL/image/gif/ad33f0cddf47e959dbf23313785b5764/gr4.sml gr4 gr4.sml sml 8224 54 219 IMAGE-THUMBNAIL 1-s2.0-S0846537118300305-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr5/THUMBNAIL/image/gif/bd00f017ed4eaaec230f40e11b2261d3/gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr5/THUMBNAIL/image/gif/bd00f017ed4eaaec230f40e11b2261d3/gr5.sml gr5 gr5.sml sml 14379 110 219 IMAGE-THUMBNAIL 1-s2.0-S0846537118300305-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr2/THUMBNAIL/image/gif/10a6df063c01d2d4f397900f976744f3/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr2/THUMBNAIL/image/gif/10a6df063c01d2d4f397900f976744f3/gr2.sml gr2 gr2.sml sml 11914 146 219 IMAGE-THUMBNAIL 1-s2.0-S0846537118300305-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr3/THUMBNAIL/image/gif/7fd8f7eff34628f2bfbb5c37ab299440/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr3/THUMBNAIL/image/gif/7fd8f7eff34628f2bfbb5c37ab299440/gr3.sml gr3 gr3.sml sml 10189 101 219 IMAGE-THUMBNAIL 1-s2.0-S0846537118300305-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr1/THUMBNAIL/image/gif/9c5cd52655d654ca1da74c461fda8f3b/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr1/THUMBNAIL/image/gif/9c5cd52655d654ca1da74c461fda8f3b/gr1.sml gr1 gr1.sml sml 14958 164 179 IMAGE-THUMBNAIL 1-s2.0-S0846537118300305-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr4/DOWNSAMPLED/image/jpeg/a9fd8adf6e2200fab5bde25e041d5588/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr4/DOWNSAMPLED/image/jpeg/a9fd8adf6e2200fab5bde25e041d5588/gr4.jpg gr4 gr4.jpg jpg 33292 162 658 IMAGE-DOWNSAMPLED 1-s2.0-S0846537118300305-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr5/DOWNSAMPLED/image/jpeg/8dcbe8388281d584ba9116b1d045e63d/gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr5/DOWNSAMPLED/image/jpeg/8dcbe8388281d584ba9116b1d045e63d/gr5.jpg gr5 gr5.jpg jpg 58800 357 713 IMAGE-DOWNSAMPLED 1-s2.0-S0846537118300305-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr2/DOWNSAMPLED/image/jpeg/580c77f39d1e9669271d2a88de00bc30/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr2/DOWNSAMPLED/image/jpeg/580c77f39d1e9669271d2a88de00bc30/gr2.jpg gr2 gr2.jpg jpg 31431 237 356 IMAGE-DOWNSAMPLED 1-s2.0-S0846537118300305-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr3/DOWNSAMPLED/image/jpeg/9d37fdcfa7bb65a6755881ffef18d863/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr3/DOWNSAMPLED/image/jpeg/9d37fdcfa7bb65a6755881ffef18d863/gr3.jpg gr3 gr3.jpg jpg 47748 276 600 IMAGE-DOWNSAMPLED 1-s2.0-S0846537118300305-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr1/DOWNSAMPLED/image/jpeg/9de50432c2dbe2b3c829e88347d17b58/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr1/DOWNSAMPLED/image/jpeg/9de50432c2dbe2b3c829e88347d17b58/gr1.jpg gr1 gr1.jpg jpg 34287 291 318 IMAGE-DOWNSAMPLED 1-s2.0-S0846537118300305-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr4/HIGHRES/image/jpeg/7cc6aacf65af44d755e39954dbcca169/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr4/HIGHRES/image/jpeg/7cc6aacf65af44d755e39954dbcca169/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 193519 717 2914 IMAGE-HIGH-RES 1-s2.0-S0846537118300305-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr5/HIGHRES/image/jpeg/d0a1f5ed46e663070df9bef9ef1a0189/gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr5/HIGHRES/image/jpeg/d0a1f5ed46e663070df9bef9ef1a0189/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 457305 1580 3155 IMAGE-HIGH-RES 1-s2.0-S0846537118300305-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr2/HIGHRES/image/jpeg/587c3cfee7bbfd1878869c41a8eeea6d/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr2/HIGHRES/image/jpeg/587c3cfee7bbfd1878869c41a8eeea6d/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 143452 1048 1577 IMAGE-HIGH-RES 1-s2.0-S0846537118300305-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr3/HIGHRES/image/jpeg/e5e121b476fb1b05e1b55b982c9a75db/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr3/HIGHRES/image/jpeg/e5e121b476fb1b05e1b55b982c9a75db/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 267884 1223 2657 IMAGE-HIGH-RES 1-s2.0-S0846537118300305-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0846537118300305/gr1/HIGHRES/image/jpeg/add4410feae672994a18767f100aa4ea/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0846537118300305/gr1/HIGHRES/image/jpeg/add4410feae672994a18767f100aa4ea/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 148802 1291 1411 IMAGE-HIGH-RES CARJ 606 S0846-5371(18)30030-5 10.1016/j.carj.2018.02.002 The Authors Figure 1 Venn diagram illustrating the hierarchy of artificial intelligence fields defined in the text. Adapted from Goodfellow et al [1] with permission from MIT Press. Figure 2 Graph illustrating the impact of data available on performance of traditional machine learning algorithms (that rely on hand-crafted features) and neural networks with few (shallow), moderate (medium), or large (deep) numbers of layers. Adapted from Easy Solutions, Inc [9] with permission (http://blog.easysol.net/building-ai-applications/). This figure is available in colour online at http://carjonline.org/. Figure 3 Artificial intelligence (AI) in the clinical workflow. Adapted from Bossuyt et al [39] with permission. Figure 4 Proposed structure and governance of Canadian Association of Radiologists Artificial Intelligence Working Groups. ACR = American College of Radiology; AI = artificial intelligence; CAR = Canadian Association of Radiologists; WG = working group. Figure 5 Tasks vs work. Diagram illustrating the role of radiologists in patient care. Grey boxes indicate classes of tasks accomplished by radiologists. The red box indicates the areas of focus of artificial intelligence techniques in radiology. This figure is available in colour online at http://carjonline.org/. Table 1 A glossary of commonly used terms in artificial intelligence applied to radiology and health care Term Definition Artificial intelligence Capability of a machine to imitate intelligent human behavior [2] Computer-aided detection The computer highlights areas of concern for further evaluation without providing a diagnosis [3] Computer-aided diagnosis The computer provides a diagnosis or differential diagnosis for physician review [3] Computer-aided triage Computer reviews the study and classifies it, either prioritizing it for radiologist review or providing the diagnosis without further radiologist review [3] Classification Action of identifying a feature category without localization on an image Deep learning Subfield of representation learning that relies on artificial neural networks with multiple processing layers to learn representations of data with multiple levels of abstraction Detection Action of identifying and localizing a finding in an image Machine learning Subfield of artificial intelligence that provides machines the ability to learn from data without being explicitly programmed [4] Model Structure and state of a neural network that allows the transformation of input data into a calculated output Neural network A model composed of layers consisting of connected nodes inspired by neurons in a biological nervous system Represention learning Subtype of machine learning where algorithms learn the features required to classify the data Segmentation Process of delineating the boundaries of a lesion or organ in an image Testing Process of evaluating the performance of a model Training Process of selecting the ideal parameters of a model after iterative adjustments Validation Process of using a subset of the dataset (distinct from the training set) to adjust the parameters of a model Table 2 Overview of proposed Canadian Association of Radiologists AI Working Groups Working group Mandate(s) Research and development - Define a research and development framework - Oversee the creation of annotated data - Organize competitions - Develop training, validation, and test datasets for clinically relevant applications Technique and applications - Evaluate and compare datasets, algorithms, and applications that have a high potential of being used in clinical practice. Education - Contribute to outreach and education - Propose content on AI at the annual scientific meeting - Provide guidance on introductory pieces - Suggest objectives and content for radiology residency curriculum Legal and ethics - Define scope of access to health care data for training of AI models - Advise on best practices in data management - Inform on anonymization requirements - Clarify scope of medico-legal responsibilities of AI-supported clinical decisions Writing - Write a white paper updated on a regular basis - Development of practice guidelines - Definition of clinical use scenarios Industrial partners - Liaison with industrial partners in technology and AI applications to radiology - Interoperability between AI systems AI = artificial intelligence. Health Policy and Practice / Sant\u00e9: politique et pratique m\u00e9dicale Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology An Tang MD, MSc a b \u2217 an.tang@umontreal.ca Roger Tam PhD c d Alexandre Cadrin-Ch\u00eanevert MD, BIng e Will Guest MD, PhD c Jaron Chong MD f Joseph Barfett BESc, MSc, MD g Leonid Chepelev MD PhD h Robyn Cairns MSc, MD i J. Ross Mitchell PhD j Mark D. Cicero MD, BESc, FRCPC g Manuel Gaudreau Poudrette MD k Jacob L. Jaremko MD, PhD l Caroline Reinhold MD, MSc f Benoit Gallix MD f Bruce Gray MD, FRCPC g Raym Geis MD, FACR m for the Canadian Association of Radiologists (CAR) Artificial Intelligence Working Group Timothy O'Connell Paul Babyn David Koff Darren Ferguson Sheldon Derkatch Alexander Bilbily Wael Shabana a Department of Radiology, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada Department of Radiology Universit\u00e9 de Montr\u00e9al Montr\u00e9al Qu\u00e9bec Canada b Centre de recherche du Centre hospitalier de l'Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada Centre de recherche du Centre hospitalier de l'Universit\u00e9 de Montr\u00e9al Montr\u00e9al Qu\u00e9bec Canada c Department of Radiology, University of British Columbia, Vancouver, British Columbia, Canada Department of Radiology University of British Columbia Vancouver British Columbia Canada d School of Biomedical Engineering, University of British Columbia, Vancouver, British Columbia, Canada School of Biomedical Engineering University of British Columbia Vancouver British Columbia Canada e Department of Medical Imaging, CISSS Lanaudi\u00e8re, Universit\u00e9 Laval, Joliette, Qu\u00e9bec, Canada Department of Medical Imaging CISSS Lanaudi\u00e8re Universit\u00e9 Laval Joliette Qu\u00e9bec Canada f Department of Radiology, McGill University Health Center, Montr\u00e9al, Qu\u00e9bec, Canada Department of Radiology McGill University Health Center Montr\u00e9al Qu\u00e9bec Canada g Department of Medical Imaging, St. Michael's Hospital, University of Toronto, Toronto, Ontario, Canada Department of Medical Imaging St. Michael's Hospital University of Toronto Toronto Ontario Canada h Department of Radiology, University of Ottawa, Ottawa, Ontario, Canada Department of Radiology University of Ottawa Ottawa Ontario Canada i Department of Radiology, British Columbia's Children's Hospital, University of British Columbia, Vancouver, British Columbia, Canada Department of Radiology British Columbia's Children's Hospital University of British Columbia Vancouver British Columbia Canada j Department of Research, Mayo Clinic, Phoenix, Arizona, USA Department of Research Mayo Clinic Phoenix Arizona USA k Department of Radiology, Universit\u00e9 de Sherbrooke, Sherbrooke, Qu\u00e9bec, Canada Department of Radiology Universit\u00e9 de Sherbrooke Sherbrooke Qu\u00e9bec Canada l Department of Radiology and Diagnostic Imaging, University of Alberta, Edmonton, Alberta, Canada Department of Radiology and Diagnostic Imaging University of Alberta Edmonton Alberta Canada m Department of Radiology, National Jewish Health, Denver, Colorado, USA Department of Radiology National Jewish Health Denver Colorado USA \u2217 Address for correspondence: An Tang, MD, MSc, 1000, rue Saint-Denis, Department of Radiology, Centre hospitalier de l'Universit\u00e9 de Montr\u00e9al (CHUM), Montreal, Qu\u00e9bec H2X 0C2, Canada. 1000, rue Saint-Denis Department of Radiology Centre hospitalier de l'Universit\u00e9 de Montr\u00e9al (CHUM) Montreal Qu\u00e9bec H2X 0C2 Canada Abstract Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada. R\u00e9sum\u00e9 L'intelligence artificielle progresse rapidement de la phase exp\u00e9rimentale \u00e0 la phase de mise en \u0153uvre dans de nombreux domaines, notamment la m\u00e9decine. L'acc\u00e8s \u00e0 de grands ensembles de donn\u00e9es, la puissance croissante des ordinateurs et les avanc\u00e9es en mati\u00e8re d'algorithmes d'apprentissage ont permis de faire des pas de g\u00e9ant au chapitre du d\u00e9veloppement des applications d'intelligence artificielle. Au cours des cinq derni\u00e8res ann\u00e9es, des techniques comme l'apprentissage profond ont permis d'am\u00e9liorer rapidement les capacit\u00e9s de reconnaissance d'images, de production de l\u00e9gendes d'images et de reconnaissance vocale. La radiologie est un domaine tout indiqu\u00e9 pour l'adoption pr\u00e9coce de ces techniques. L'int\u00e9gration d'applications d'intelligence artificielle en radiologie au cours de la prochaine d\u00e9cennie devrait grandement am\u00e9liorer la qualit\u00e9, la valeur et la port\u00e9e de la contribution de la radiologie aux soins des patients et \u00e0 la sant\u00e9 de la population, en plus de r\u00e9volutionner le travail des radiologistes. En sa qualit\u00e9 de porte-parole de la profession au Canada, l\u2019Association canadienne des radiologistes (CAR) d\u00e9fend des normes de pratique \u00e9lev\u00e9es en imagerie centr\u00e9e sur les patients, en apprentissage continu et en recherche. La CAR a mis sur pied un groupe de travail sur l'intelligence artificielle qui a pour mandat de discuter des enjeux li\u00e9s \u00e0 la pratique, aux politiques et \u00e0 la prestation de soins relativement \u00e0 l'introduction et \u00e0 la mise en \u0153uvre d'outils d'intelligence artificielle en radiologie. Le pr\u00e9sent livre blanc formule \u00e0 l'intention de la CAR des recommandations issues des d\u00e9lib\u00e9rations des membres du groupe de travail. Il renseigne les membres de la CAR et les responsables de l\u2019\u00e9laboration des politiques sur la terminologie \u00e0 employer, les besoins en mati\u00e8re de formation, la recherche-d\u00e9veloppement, les partenariats, les applications cliniques potentielles, la mise en \u0153uvre, la structure et la gouvernance, le r\u00f4le des radiologistes et sur les retomb\u00e9es potentielles de l'intelligence artificielle en radiologie au Canada. Key Words Artificial intelligence Machine learning Deep learning Radiology Imaging Medicine Healthcare Quality improvement Artificial intelligence (AI) is permeating our personal and work environments. AI applications used every day include voice-powered personal assistants, behavioral algorithms applied to real-time phone conversations, shopping recommendations powered by predictive analytics, and self-driving vehicles. It is predicted that AI applications will become faster, smarter, and more convenient to implement and use. Breakthroughs in medical imaging technology and research have led to the exponential growth of medical imaging data stored in digital format over the past 2 decades. These machine-consumable data must be curated so that they can be used with AI to optimize patient outcomes, ensure appropriateness, and enhance the efficiency and accessibility of the health care system. As clinical experts on the use of imaging to diagnose and treat disease, it is critical that radiologists participate and lead in the implementation of data-driven systems that will interface with clinical workflows to improve patient care. In May 2017, the Canadian Association of Radiologists (CAR) established an AI Working Group with a mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This mandate will ensure that the CAR remains at the forefront of the discussion about the use of AI in imaging in Canada, so that radiologists can shape the way that the technology influences and impacts their work and role as physicians. The working group includes members from a range of subspecialties (covering adult and pediatric radiology) and backgrounds (including imaging informatics, engineering, biophysics, and research). In this white paper, we summarize the objectives of this working group as follows: 1. Essential AI terminology 2. Key issues and best practices pertaining to educational needs of CAR members 3. Importance of critical assessment of AI literature in compliance with principles of evidence-based medicine 4. Research and development 5. Clinical applications and implementation 6. Structure and governance 7. Role of radiologists and potential impact of AI in the context of radiology in Canada Based on the current state of the art, we summarize key issues and provide recommendations for each topic. Considering the rapid rate of technological advances in this field and the large impact they will have on radiology, it is intended that this white paper (and the policy and recommendations provided herein) will be reviewed and updated on a regular basis. It is the intent of the CAR that these AI-focused publications will be helpful to radiologists and policymakers as they work to integrate AI advances into practice in a manner that is beneficial to patients and the health care system. Objectives The mandates of the CAR AI Working Group are to do the following: 1. Research and examine the potential impact of AI on radiology in Canada, in various practice models. 2. Guide the development of CAR policy regarding the use and deployment of AI in radiology. 3. Promote and facilitate research and development in AI applications led by imaging experts in collaboration with different stakeholders across Canada. 4. Provide guidance and support for the membership and help members integrate AI advances into their practices in a manner that is beneficial to our patients and health care system under the leadership of radiology. 5. At the appropriate time, lead in the development of practical solutions for implementation of AI into routine clinical practice in collaboration with different stakeholders. 6. Oversee the formation and work of specific working groups that may be formed in the future to address specific challenges or issues of interest in AI. 7. Advise the CAR Board of Directors in their engagement with corporate partners working in the AI field. 8. Act as CAR spokespeople regarding the use of AI in imaging, in an effort to inform and assist other CAR members. This white paper was written to inform readers on the current state of the art of AI in radiology, provide a conceptual framework for future publications, and make recommendations for key issues pertaining to AI in radiology. AI Terminology Radiologists use a specialized vocabulary to describe findings precisely and communicate them to other radiologists, referring physicians, and patients. As AI technology expands and eventually becomes a part of clinical workflow, radiologists will need to become familiar with the underlying concepts and terminology. The hierarchy of AI fields is illustrated in Figure 1 . Definitions of commonly used terms in AI are provided in the following section and summarized in Table 1 . Interested readers may refer to online glossaries for further details [5,6]. Hierarchy of AI Fields The term artificial intelligence first appeared in print in 1955 [7]. AI refers to the branch of computer science dedicated to the development of computer algorithms to accomplish tasks traditionally associated with human intelligence, such as the ability to learn and solve problems. AI is a broad term that designates a variety of fields and techniques. Machine learning (ML) refers to the part of research on AI that seeks to provide knowledge to computers through data and observations without being explicitly programmed [8]. This is accomplished by tuning of parameters within the algorithm to optimize the goodness of fit between the input (ie, text, image, or video data fed into the algorithm) and output (ie, classification). The acquired knowledge allows computers to correctly generalize to new settings. ML algorithms evolve as they are exposed to more data. Nearly all ML algorithms used to analyze the pixel data of radiology examinations \u201clearn\u201d to give a specific answer by evaluating a large number of exams that have been hand-labeled. For example, a ML algorithm to detect lung nodules on chest radiographics will be trained by analyzing thousands of chest radiographs that humans have labeled as being normal, or as having nodules in the lungs. Representation learning refers to a subtype of ML in which no \u201chand-crafted\u201d features are provided. Instead, the computer algorithm learns the features required to classify the provided data. The amount of training data has an impact on the performance of ML algorithms: adding data generally improves performance (Figure 2 ). If provided enough training data, systems based on representation learning may achieve better performance than traditional ML systems that incorporate \u201chand-crafted\u201d features. Deep learning refers to a subfield of representation learning which relies on multiple processing layers (hence, deep) to learn representations of data with multiple layers of abstraction [10]. The various layers in these algorithms are used to detect features ranging from simple (eg, lines, edges, textures, intensity) to complex (eg, shapes, lesions, or whole organs) in a hierarchical structure [11]. Neural Networks Neural networks are the algorithms that are most commonly used for image analysis today. The name refers to their design inspired by neurons in a brain. These neural networks are composed of layers of connected nodes (or neurons) and may contain thousands to millions of nodes. Each node receives information from some pattern of other nodes. If the information that node receives crosses a threshold, that node then sends a signal out to other groups of nodes. These outputs are weighted, in that they send a small signal out when they are weakly stimulated, and a strong signal when they receive appropriate input. The ultimate goal of the overall network is to come up at the end with an answer for each exam that matches that exam's labels. Mathematically, the algorithm tries to maximize the number of right answers. Initially, it randomly sets each nodes weights and inputs, and thus randomly guesses the answer. It then compares the guess with the real \u201cground truth\u201d label provided by a human. Then it changes the patterns of the neural connections and the weights it assigns each node and uses that to produce another guess. It repeats this process thousands or millions of times, each time tweaking the nodes to improve the chance that its next guesses will better match the ground truth labels. Training a neural network involves prompting the algorithm to guess, compare, change weights for a better guess, and compare again, for thousands or millions of incrementally better guesses, finally reaching a point where more guesses either cease to improve results, or the change in improvement becomes too small to matter. Many types of neural nets exist, each representing different ways to cluster these nodes, how nodes change their weights, how they combine and from where they receive inputs, and to which other nodes their outputs go. Common types of neural networks include convolutional neural nets, recurrent neural nets, long- or short-term memory, and generative adversarial networks. In practice, these types of networks can be combined. A discussion on the design and function of neural networks is beyond the scope of this article. Hence, we direct interested readers to introductory texts on AI, ML, and deep learning linked on the CAR website [12]. Learning Process The learning process may occur either via supervised learning, in which a training set of data contains annotations by humans to match the desired output of the algorithm, or unsupervised learning, in which the training data do not contain annotations and the algorithm seeks to cluster or organize the data to reveal underlying patterns. Almost all ML for analysis of radiology exams is currently performed via supervised learning which requires appropriately labeled training data. This highlights 2 challenges: 1) adequate labeling of key imaging findings, a tedious and time-consuming process, and 2) appropriate definition of ground truth (eg, radiology report, pathology report, clinical outcomes). Proper training of ML algorithms will require new ways to label data or to deal with loosely labeled data. Ground truth, which is often found on a continuum (eg, ranging from normal, probably normal, indeterminate, probably abnormal to definitely abnormal) may require artificial clustering into normal vs abnormal. One approach, used by the American College of Radiology (ACR), is to define use cases for common problems [13]. The power of these programs is their ability to find patterns in complex data, including medical images. For image analysis, ML algorithms can identify subtle patterns beyond the threshold of human detection, with the potential to extract valuable new information [14]. ML algorithms often require a large amount of data to \u201clearn\u201d to provide useful answers, and processing these data requires significant computing power. The rapid increase in power of graphical processing units (GPUs), initially created for accelerating computer graphics, such as used in gaming, have provided flexible hardware for ML purpose. The access to computational power and large training datasets has made these algorithms cost effective. Recommendations \u2022 Radiologists should be familiar with different AI techniques. \u2022 Radiologists should understand the challenges related to preparation of training datasets for supervised learning. \u2022 Radiologists should be familiar with AI terminology and hierarchy. Education The CAR sees the need to provide expert, unbiased education to multiple different audiences, including practicing radiologists, radiology residents, medical students, radiology and medical imaging researchers, the overall data science or ML research community, industry partners, hospital and health care systems administration, other clinicians, regulators, and the general public. Goals for this education initiative include the following: \u2022 Explain and standardize the terminology. To have a realistic, useful framework, all involved should use a common and universally understood lexicon. \u2022 Describe the opportunities, and challenges, in a clear manner, and avoid either excessive hype or fear. AI will soon provide radiologists with valuable new tools to extract more information from radiology examinations, enabling imaging specialists to add greater value in the circle of care. \u2022 Understand current issues with AI and radiology. High-level medical imaging data science is neither child's play nor impossible. Good ML is powerful, helpful, and valuable. Bad or unethical ML is dangerous, and patients, radiologists, and the government must work together to prevent it. In particular, caregivers and governing bodies should be wary of grandiose claims originating from entities with a strong profit motive in the commercialization of AI technology. Education for Image Analysis AI Research and Development AI research and development requires skill in statistics, coding, data structures, and domain-specific data mining, in addition to new frameworks for data plumbing and computation. Building a medical image analysis tool may be as easy as learning the Python programming language and relying on existing software libraries. However, understanding the underlying math, statistics, data structures, and algorithms is significantly more challenging. Being able to generalize an algorithm to work on disparate input data, from multiple different imaging machines using a myriad of protocols, is very difficult. On the other side, data scientists without experience in day-to-day radiology practice need to be aware of the depth and idiosyncrasies of radiology workflows. It is common at conferences to see scientific presentations and posters of AI projects that on the surface may appear to address a clinical setting, but miss critical parts that render the algorithm useless in practice. For example, fully automated segmentation of liver tumours may appear spectacular [15], but does not provide information\u2014such as type of tumour, distribution of tumours, and relationship to vessels and staging\u2014required by surgeons and oncologists for clinical management. In addition, how things are described in medical textbooks may differ from how they are implemented in practice. Education for Commercialization The CAR may collaborate with other societies such as the ACR to develop clinical use cases or develop use cases independently. The CAR has a primary role to educate and work with regulatory bodies on how to handle products that potentially change their outputs over time. Algorithms are both amoral, and at their core are built to optimize some function. As such, they are prone to bias, and potentially produce significant ethical issues. CAR has the responsibility both to research and educate all stakeholders, and particularly the public, on what types of ethical and bias issues may arise, and how to detect and manage them. For example, a ML model for stratifying the risk of cancer in pulmonary nodules detected on computed tomography (CT) scan achieved high performance on the training dataset that included patients from the U.S. National Lung Screening Trial, but much lower performance once applied to patients at Oxford University Hospitals [16]. This suggests that a ML model incorporates implicit selection biases from the demographics of the population used for its training which may not be representative of the target population in which it will be applied. If this discrepancy in diagnostic performance is not recognized, there is a risk of misdiagnosis and potential harm to patients. Commercializing an AI image analysis product requires understanding the clinical need, or use case; the business case; and new methods of product regulation, verification, and monitoring. The computer vision literature provides countless examples of automated segmentation and computer-aided detection (CAD) tools that are not used in clinical practice despite decades of refinement. To overcome barriers to clinical adoption, AI image analysis products must be integrated seamlessly in the clinical workflow and be able to interface with picture archiving and communication system (PACS) software, which may otherwise act as a gatekeeper in the value chain. Education for Radiologists Using AI Products Practicing radiologists need to understand both the value, and the pitfalls, weaknesses, and potential errors that may occur when an AI product performs image analysis. While these algorithms are powerful, they are often brittle, and may give inappropriate answers when presented with images outside of their knowledge set [17,18]. This includes images with technical artifacts such as movement or beam hardening, or images obtained with inappropriate techniques. For example, an algorithm-evaluating brain CTs may work perfectly for long stretches, but then a new software upgrade to the CT occurs, or a new CT machine comes online, and all of a sudden, the algorithm produces faulty results. To alleviate this concern, in many modalities, new protocols for standardized imaging should be adopted with AI in mind, similarly to guidelines that have been proposed for image acquisition to enable quantitative analysis [19]. AI is used for more than image analysis. It is a powerful tool to identify patterns, predict behavior or events, or categorize objects. In the near future, these non\u2013image-analysis tools may dramatically affect radiology. For example, these tools have the potential to improve radiology departmental workflow through precision scheduling, identify patients most at risk of missing appointments, and empower individually tailored exam protocols. Perhaps most anxiety-provoking for radiologists, AI may enable programs that use radiologists and their work as data, identifying details of each radiologists\u2019 practice pattern, and even categorizing them: enabling the creation of a sophisticated radiology report card. Evidence-Based Medicine The ongoing development and impending deployment of AI applications in radiology must be tempered with a consideration of the principles of evidence-based medicine. While currently concentrated in the research domain, it is anticipated that significant human and financial resources will be committed to AI deployment. Before clinical adoption, it is imperative that AI applications in radiology should be held to the same degree of accountability and effectiveness as for new medication or medical device due to the potential ethical, medico-legal, and bias that may arise from the use of AI tools in radiology. More than most conventional information technology tools, AI applications may directly influence the diagnosis and management of a patient. Hence, this professional responsibility should be taken seriously and cautiously by radiologists and computer scientists working in this field. Critical assessment of AI solutions should be based on widely recognized principles of evidence-based medicine. A distinction must be made between preprint articles released on online repositories (eg, arXiv.org), peer-reviewed articles that describe technical developments (typically in computer science and engineering journals), and peer-reviewed articles that report diagnostic performance in radiology journals. Certain use cases, especially those that model patient diagnosis or prognosis, should respect existing consensus statements. For instance, AI tools developed for diagnostic purposes should follow the STARD (Standards for Reporting Diagnostic Accuracy) statement [20], which provides a list of essential items for reporting in diagnostic accuracy studies to improve the completeness and transparency of these studies. Similarly, studies reporting predictive models should be compliant with the TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement [21], which emphasizes the transparent reporting of study settings, outcome follow-up intervals, or precise definitions of how outcomes were defined and measured, and study design elements. Although these requirements are not always given as much attention in technique papers, they have paramount importance in clinical studies and applications. This is important because AI applications in radiology may be sensitive to characteristics of the study population (eg, prevalence and distribution of disease spectrum in the selected population), variations in equipment (eg, different scanners and different manufacturers) and imaging protocols with different imaging characteristics (eg, spatial resolution, temporal resolution, contrast, signal-to-noise ratio), and choice of reference standard (eg, radiologist's interpretation, pathology, clinical outcomes) [22]. Thus, comprehensive and transparent communication of patient cohort criteria will be required to ensure that the performance on training datasets is generalizable to target hospital sites and to ensure the safe and responsible application of AI algorithms. Whenever possible, a high standard for validation and reproducibility of techniques must be encouraged. This may occur in a variety of ways: through the development and dissemination of standardized training and validation datasets, the promotion of open data science competitions, or the creation of federated networks to permit institutions to validate tools locally and to ensure the safe generalizability of algorithms. Algorithms should undergo rigorous validation demonstrating robustness impervious to variations in equipment and imaging protocols. Recommendations \u2022 The radiology community must be educated on how to critically analyze the opportunities, pitfalls, and challenges associated with the introduction of new AI tools. \u2022 The CAR must engage with regulatory agencies on the ethical and medico-legal issues concerning AI and play an active role in the advocacy of evidence-based principles in the evaluation and certification of radiology AI tools. \u2022 The AI research and development community must address issues surrounding dynamic adaptive and active learning systems that may vary in prediction or performance over time and embed radiologist oversight over dynamic systems due to the potential impact on patient care. \u2022 The CAR should support and develop common standards for validation and testing of AI tools, emphasizing stability of performance over varying settings, equipment, and protocols to certification for clinical use. Research and Development In this section, we discuss opportunities in AI research and development in Canada. We also address needs in terms of access to clinical imaging data from different imaging modalities for training and validation purpose, sharing of data, standardization of software framework, clinical translation, and open questions pertaining to partnerships between academic research laboratories and industrial partners. Opportunities Canada hosts several leading AI research laboratories with strong connections between academic research and industrial innovation [23]. The early investment efforts of the Canadian government through organizations such as the Canadian Institute for Advanced Research and the Natural Sciences and Engineering Research Council have supported the foundational work of deep learning pioneers such as Geoffrey Hinton at the University of Toronto, Yoshua Bengio at University of Montreal, and Richard Sutton at the University of Alberta [24]. There is a clear opportunity to leverage the strength in fundamental research and apply this technical know-how to develop clinically useful AI applications with the input of Canadian radiologists. The country has a publicly funded health care system managed at the provincial level. Over the past 2 decades, hospitals have migrated from film- and paper-based diagnostic imaging to digital formats. Further, all Canadian provinces have regional or provincial diagnostic imaging repositories. While these imaging repositories were created to reduce unnecessary patient transfers and duplication of tests, the large dataset of medical imaging examinations\u2014each accompanied by a report\u2014could in theory be used to create cohorts of patients with similar diseases to train AI algorithms. Since 1997, the Government of Canada has invested in a modern system of information and communications technologies for the health care system [25]. In so doing, it has promoted standards governing shared data to ensure compatibility of health information systems. For applications that require large training datasets and validation at a population level, these prior initiatives should facilitate interoperability between provincial health information systems and across imaging repositories. However, ongoing efforts by the Canada Health Infoway Standards initiative continue to be required to achieve full interoperability [26]. Access to Appropriate Data Type Although AI technology is meant to be broadly applicable, each modality of imaging data (eg, radiographs, ultrasound, CT, and magnetic resonance imaging [MRI]) and disease area will require development of specific strategies for optimal performance. Optimal neural network design and training parameters can vary greatly between data types. For example, convolutional neural networks that have been developed for image recognition of photographs have been adapted for recognition of 2-dimensional (2D) medical images such as radiographs and ultrasound. However, the further development of 3-dimensional (3D) convolutional neural networks, which have been used successfully in a number of academic projects using 3D brain MRIs [27], may be required to take full advantage of multi-planar imaging. Therefore, it would be necessary to create freely available, modality-specific sandboxes for computer scientists to get a \u201cfeel\u201d for the data, which will inform the design process. For example, in the computer vision community, the publicly available ImageNet dataset has been used to produce numerous breakthroughs in machine learning for image recognition, but many of these advances are not directly applicable to medical imaging [28]. Once proof of concept for an application is established, validation can proceed to larger standardized datasets, preferably by an independent party using a setup similar to that used by public image recognition challenges in which the reference standard (ie, \u201cground truth\u201d) data curated by the organizers are never released (to provide honest performance benchmarks). Depending on applications and sample size required, the validation dataset may in the future require data from another hospital, province, or pooling of data at a national level. DICOM Anonymization and Deidentification The DICOM (Digital Imaging and Communications in Medicine) file format is the standard for digital image storage in medical imaging [29]. In addition to pixel information, it contains ancillary information (metadata often referred to as DICOM headers) such as patient details, image technical parameters, and institutional variables among others [30]. Any ML project aimed at image recognition will typically involve the collection of image and report data which contains protected health information (PHI). Institutional review boards require deidentification or anonymization of this data to analysis to mitigate risks of PHI data breach. Deidentification implies that all PHI is removed; however, with appropriate approval, patients can be reidentified with the use of a data linking log, which relates a study or patient identifier to a newly assigned study or patient identifier. Linking logs must be kept secure, ideally encrypted, and separate from the deidentified data. Anonymization refers to the process of deidentification wherein the linkage to original identification is irreversibly broken [31]. Technical details of DICOM anonymization is a complex topic beyond the scope of this white paper and may serve as a topic for future discussion. Data Sharing Ethics and data ownership issues are well-known challenges, especially for in the context of clinical deployment of AI tools, which will require intervention and advocacy at the level of policymakers [32]. Collaboration and sharing of data will require cultural adjustments between different research fields and jurisdictions. For example, sharing of imaging datasets so that other teams can replicate and improve on the state of the art is a common practice in the field of computer science, but uncommon in radiology where clinical and imaging data are closely stored, in part out of concern that it may contain sensitive confidential information that may be reidentified. Similarly, we anticipate that research may focus on large datasets released publicly or made available for collaboration by researchers from jurisdictions with larger patient populations and permissive rules of access for research (eg, Stanford's Medical ImageNet) [28]. For many applications, expert annotations (ie, measurements, contouring, and descriptions) for training AI algorithms are very expensive to obtain [33]. A direction of research should be to make the annotation process more efficient, which can also be done with AI. The Medical Image Computing and Computer Assisted Intervention Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis is an example of an academic venue with this research focus [34]. Common Computing Framework A common computing framework with an integrated data warehouse will be required to efficiently share AI models, experimental setups, and training and test data. For CAR initiatives, reproducibility of new methods must receive equal priority to diagnostic accuracy. Standardization of a software framework will be a challenge (each technical group will have its own preferences), but is essential for sharing and codevelopment of models. A number of established deep learning frameworks are publicly available (eg, TensorFlow, Microsoft Cognitive Toolkit) [35,36], but most are only lightly tested with 3D images (eg, CTs and MRIs). For research purposes, the selected framework must be customizable and extensible until the algorithms are sufficiently mature for deployment. Because these software frameworks are rapidly evolving, data engineering should also be standardized so that models can be updated. For deep learning, sharing of network architectures, hyperparameters (eg, learning rate), and training strategies is vital to replicate results. There should be a consensus on the minimal set of variables that should be reported. Container technology such as Docker [37] and Jupyter Notebooks [37] may be very helpful for packaging models and experiments across different software environments and computer platforms. Specialized hardware capable of highly parallel processing, such as GPUs and more recently tensor processing units (TPUs), is required. Shared public resources (eg, Compute Canada) [38] are available and are particularly beneficial for large-scale projects, as it is generally not cost effective for individual research groups to build large specialized servers that can become obsolete quickly. Resource allocation grants are available through Compute Canada and should be explored. Individual groups can still benefit from having their own local servers for rapid prototyping and more flexible hardware upgrade cycles, while keeping in mind compatibility with the selected larger public framework. Bridging the Valley of Death The valley of death is a metaphor for the lack of resources and expertise often encountered in translational research, which often prevents innovative methods from going beyond the proof-of-concept stage. In this case, many of the advances in AI may take many years to become useful for radiological practice, and many will never get there. One main cause of the valley of death is that academics must publish at top conferences and journals in their respective fields, and the lack of researchers focusing on translating the technical to the clinical creates a gap in the middle. For example, new methods are being published at an unprecedented rate in ML conferences such as the Conference on Neural Information Processing Systems, but most are not applied to medical images, and as a result medicine is benefiting at a much slower pace. Conferences such as Medical Image Computing and Computer Assisted Intervention try to fill the gap, but the most novel methods (rather than the most validated ones) still garner the most attention. Ultimately, the driver of clinical adoption may reside in the implementation and availability of AI applications integrated into the PACS system at the reading station (ie, the reading platform). Integrating concise and standardized AI interpretation results directly in the DICOM standard could allow easier portability with different PACS systems. Current AI frameworks are not accessible by non\u2013computer scientists. This is fine for interdisciplinary groups that have both clinical and technical people, but for primarily clinical sites and for the purposes of deployment, substantial effort will be required to improve the transparency of the methods and access to the frameworks. Academic and Industrial Collaborations Development and validation of AI applications for radiology will require new thinking and approaches as it relates to collaborations and intellectual property between academic research laboratories and industrial partners. Several questions may arise in the process: 1) Who owns the data and intellectual property on the models developed jointly? 2) Should data sharing agreements be signed with individual sites or with the research consortium in the case of multicentre studies? 3) Should the technical \u201cheavy lifting\u201d be done by industrial partners or by academic research laboratories in the interest of openly sharing the results of trained models made possible with datasets obtained through public funding? 4) Should the CAR focus on the clinical use scenarios and outcomes, with involvement from technical members only to provide technical oversight? and 5) Would there be sufficient synergy between the goals of CAR and those of industry? Recommendations \u2022 The CAR should promote and facilitate research and development of AI applications led by imaging experts in collaboration with AI researchers and other stakeholders. \u2022 The CAR should consider hosting medical imaging datasets for training of AI models and testing of performance. \u2022 Researchers should develop a systematic approach to identify the novel AI methods that are most applicable to medical images. \u2022 A CAR-driven initiative should coordinate a strong contingent of scientific and programming staff that can focus on the translational aspects of the work, such as software engineering and maintenance, integration of improvements, and robustness testing. \u2022 Publication of CAR-driven initiatives should benefit all members. Clinical Applications To be adopted in clinical practice, AI applications must address unmet needs or improve on existing solutions. Conceptually, there are at least 3 ways to classify clinical applications of AI in radiology: clinical workflow, types of applications, or classes of use cases. Clinical Workflow Clinical AI applications may be conceived as diagnostic tests inserted in existing clinical pathways. In the existing situation, an imaging test performed in a given population is performed by radiologists. AI applications as alternative triage, replacement, or add-on in clinical workflows are illustrated in Figure 3 . This terminology and concept is adapted from the conceptual framework developed by Bossuyt et al [39] for new diagnostic tests. In a triage scenario, AI may be used as a screening tool to sort examinations based on the probability of disease (eg, positive or negative result according to AI). An example would be the triage of unread x-rays based on the highest probability of disease determined by an AI algorithm according to the content of images or other data available. Such an application may determine which examination should be interpreted first. In a replacement scenario, AI may replace radiologists if results are consistently more accurate, rapid, reproducible, and easier to obtain. An example would be estimation of bone age by an AI software found to consistently provide better performance than a radiologist. In an add-on scenario, AI may be used in a subgroup of patients after the existing clinical pathway which relies on interpretation by a radiologist. Add-on tools may be applied only if the imaging findings warrant a time-consuming application best left to ML algorithms. An example would be automated lesion segmentation to calculate total tumour volume for prioritization of liver transplant candidates with hepatocellular carcinoma. Types of Applications The applications of ML in radiology generally fall into 1 of the following categories: detection, in which the goal is to identify an anomaly within an image (eg, a lung nodule); segmentation, in which a structure of interest is isolated from the remainder of the study (eg, defining the boundary of an organ); and classification, in which an image or lesion within an image is assigned to a category (eg, is pulmonary embolism present or not on this CT scan?). Use Cases A third way to approach clinical applications is based on classes of use cases. Separate normal from not normal The basis of almost any radiologic interpretation includes an implicit perceptual task done by the expert radiologist of classifying an image or a series of images as normal or abnormal. This very simple use case class can be applied to almost any medical imaging exam. Defining the boundary between a normal and abnormal image in a formal way is very complex and multifactorial. In this context, deep learning can potentially excel by learning a hierarchical normal representation of a specific type of image from a large number of normal exams. Improved CAD CAD software has been present for a long time in radiology for different types of exams and more commonly for screening exams. In many cases, CAD sensitivity can potentially be improved at the cost of a lower CAD specificity, explaining the variable penetration in clinical workflow. Preliminary results suggest that CAD based on deep learning algorithms can improve the area under the receiver-operating characteristic curve observed for many detection problems compared with the previous generations of CAD [40]. Radiomics Radiomics is a process that extracts a large number of quantitative features from medical images. It can potentially be applied to any medical condition, but it is currently applied mostly in oncology for quantification of tumour phenotype and for development of decision support tools [41]. Deep learning and convolutional neural networks have the potential to automatically extract the significant features from images to help predict an important outcome (eg, cancer-specific mortality). Workflow optimization and quality assurance AI will potentially provide a way to automatically detect critical findings to change the level of reading priority of a specific exam. For example, automated detection of small intracranial hemorrhage could change the outcome of some patients by improving the speed of reporting. For unusual or complex cases, content-based image retrieval software could retrieve prior imaging examinations of other patients with similar imaging findings to assist the radiologists' diagnostic task. With proper integration, the image retrieval could be seen directly in the PACS viewer. Technical quality assurance programs could use AI to automatically evaluate image quality and technical protocol conformity in a radiology department. Medical quality assurance programs could also help improve conformity to standard of care reporting for the benefit of patients. Grading and classification The ACR Reporting and Data Systems (RADS) provide assessment structure and classification for reporting in patient imaging [42]. Many other grading systems with similar objectives are often published in peer-reviewed literature. A deep neural network based on a convolutional neural network can be trained as an image classifier where the classes are defined by the different grading categories. With enough properly labelled images, the network can learn the objective grading rules directly from the images. This may lead to the development of automated grading applications. Natural language processing, computer-assisted reporting, and knowledge management An often-underappreciated application of AI in radiology is the usage of natural language processing (NLP). While distinct from the more obvious workflow impacts of imaging findings detection and characterization, the wide-scale usage of usage of NLP could have significant impacts on clinical care, quality improvement, and health policy. The large amount of unstructured information in full-text radiology reports is a potentially invaluable source of information for clinical care quality improvement and research, but presents its own challenges with analysis, given the varied and individual reporting styles of narrative reports. NLP is commonly defined as the conversion of unstructured text into a structured form to allow for the automated extraction of information, synonymous with text mining or information extraction. Broadly speaking, these systems are divided into rules-based systems, which rely upon linguistic expert crafted rules, vs ML methods, which rely on algorithms to classify text or extract information. NLP offers a variety of applications and use cases, with applications in diagnostic surveillance, identifying cases for research studies, and assessment of radiologic practice quality [43]. A key limitation has been the limited generalizability of NLP systems, which have often required extensive local expert customization to accommodate local nuances in reporting syntax to improve performance. Future avenues of research include the classification of disease progression using temporal reasoning, identifying relationships between anatomic locations and findings, validating and improving administrative coding of radiology reports, accelerating electronic medical record chart review, offering differential diagnoses from described findings, or the automated structuring of narrative reports. Advancements in NLP have suggested the possibility of computer-assisted reporting, which is the application of NLP in the creation of higher quality reports which combine findings made in narrative or structured reports, with evidence-based clinical guidelines, respecting the radiologist's natural workflow, while supporting consistent recommendations and the customization of recommendations. Successfully applied, these technologies promise to decrease unnecessary variation in reports and assist in clinical decision making to improve the quality and efficiency of patient care [44]. NLP is also playing a critical role in the cohort curation of large datasets necessary for ML and the development of deep learning algorithms. The largest open public set of chest x-rays, \u201cChestX-ray8\u201d, composed of 108,948 frontal-view x-rays from 32,717 patients, was derived from a combination of keyword search and removal of negation and uncertainty [45]. New future functionality promises to further optimize clinical workflow through the background monitoring of reporting and the seamless integration of contextual information to radiologists derived from the electronic medical record [46]. Recommendations \u2022 AI applications should integrate and be interoperable with existing clinical radiology workflows. \u2022 AI applications in radiology should be clearly defined according to their role, type, and use cases in the clinical workflow. \u2022 To prioritize the development of applications that provide information not achievable with human vision. \u2022 To prioritize the development of applications that provide knowledge not widely available in the Canadian radiology workforce. \u2022 AI applications in radiology should be developed to improve acquisition efficiency, image quality, and production of structured reports. Implementation The 3 key components in the development of practical solutions for implementation of AI in clinical radiology practice include: 1) availability of training datasets, 2) interoperable frameworks, and 3) collaboration between different stakeholders to ensure ethical, responsible, and clinically useful applications. Data Paucity of large volumes of representative data constitutes one of the major obstacles to the development and benchmarking of AI applications in radiology. Furthermore, inappropriate selection of benchmarking datasets or application of heterogeneous datasets for benchmarking may result in inaccurate estimates of performance. Over the coming years, the creation of standard clinical and image datasets with proven diagnoses across imaging modalities will foster algorithm development and support objective evaluation of performance. To do so, datasets must be divided into larger training sets with known diagnoses and smaller validation or testing sets of unknowns. These datasets should be updated at regular intervals to ensure that commercial solutions maintain the level of generalizability that is being marketed. These data will need to be anonymized, including through removal of identifiable anatomic features (eg, face). This could be conducted in a manner similar to that of the Cancer Imaging Archive, which at the time of writing contains images of 40,913 patients across modalities and body regions [47]. By analogy with medical biochemistry laboratories, algorithm benchmarking and calibration should be conducted using the local data of the implementing institution, at regular intervals, and with every substantial modification of the algorithm and imaging techniques used. This benchmarking should be conducted as part of hospital-based quality assurance in a manner that adheres to a set of standards acceptable to all stakeholders listed below. Interoperability Framework Widespread clinical adoption of AI applications in radiology will require interoperability between disparate software used in radiology departments. Ensuring interoperability is essential to enable local evaluation and benchmarking of AI applications, especially in cases where a radiological workflow could potentially consist of contributions from multiple software vendors. For instance, algorithms for protocolling, study prioritization, feature analysis and extraction, and automated report generation could conceivably each be a product of individual specialized vendors. Productive collaboration of all involved stakeholders is necessary to arrive at a set of standards aimed at ensuring seamless integration of such algorithms in research and clinical systems to support a broad range of institutional hardware and software solutions, while respecting ethical considerations regarding patient data privacy. Discussions on a common software interoperability framework should be initiated early in the development process to avoid imaging informatics issues encountered in bio- and cheminformatics. In these domains, numerous vendors developed hundreds of unique file formats, data representations, and database architectures, which resulted in barriers to research efforts by placing the onus of ensuring interoperability on the users. Fortunately, the experience of these fields in breaking down silos can be used as a basis for interoperability measures for AI-based applications in radiology. Semantic Web Technologies and RESTful web service frameworks may be harnessed for this purpose in a manner similar to that implemented in existing integrative biomedical informatics frameworks [48]. The flexibility of these frameworks allows the creation of separate globally-distributed research implementations and institutionally-distributed clinical implementations. Interoperability efforts should adhere to the evolving DICOM Workgroup 23 recommendations on Application Hosting standards [30]. Analogous to the current specification for imaging study details, all relevant information for AI solutions will have to be stored in the DICOM of the study to which they are applied. Algorithm results have to be adequately traceable to the generating algorithms, the order in which the algorithms were executed in a workflow, the version of these algorithms, and any variable parameters used, to enable reproducibility and for medicolegal purposes. Adoption of a common AI-based result representation framework would be essential to ensure adequate data provenance. Analogous with biomedical informatics, such annotations should be backed by common ontologies to ensure further interoperability [49]. Existing ontologies or derivatives of established ones, like RadLex, may be considered for this role [50]. People Actively involving all stakeholders, including the patients, radiologists, ordering physicians, imaging technologists, hospital administration, regulatory bodies, industry, and academia is critical to widespread clinical implementation of AI in radiology. It is imperative that all involved stakeholders can benefit from education regarding the benefits and limitations of AI in radiology. In collaboration with stakeholders, CAR may facilitate the development of best practices pertaining to ethics, patient consent, enforcement of confidentiality, data ownership, and the development of interoperable, big data-ready and AI-friendly infrastructure at local, regional, and national levels. Critical aspects such as source of funding for development, implementation, and delivery of clinical services using AI, as well as the level of collaboration between stakeholders will have to be carefully considered. Ethical, Legal, and Social Implications The new paradigm for the management of health care promised by AI hinges on researchers gaining access to large sets of health data from thousands of patients. In this context, classic expectations regarding individual control over personal information are challenged. We need to develop new approaches to assess the ethical acceptability and legitimacy of data science in health care: institutions must ensure that the massive datasets they hold are properly used; novel approaches are required for qualifying benefits (health and economic) and assessing research risks and how to mitigate them. Social acceptability must be a primary concern, with patients involved as partners in the development and implementation of new tools. New approaches and ethical reasoning should be developed to serve as guides for future provincial or national projects that would need to combine massive amounts of health data with AI in a private-public, multi-institution, transdisciplinary environment of Canada. To do that we also need to integrate ethic, data privacy policies, and cybersecurity in the management of big dataset within a technology-driven platform to scale up our approaches across institutions. Recommendations \u2022 The CAR should encourage the development of representative training datasets across a range of applications, from triaging to diagnosis, for developing and testing AI applications. \u2022 The CAR should encourage the adoption of a common, interoperable software framework for research and clinical purposes to promote collaboration, and support adequate medico-legal data provenance. \u2022 The CAR should advocate for a standardized approach for benchmarking and implementation of AI applications in radiology. \u2022 The CAR should promote collaboration between all relevant stakeholders to ensure ethical, responsible, and clinically useful implementation of AI in radiology. Structure and Governance One of the mandates of the CAR AI Working Group is to \u201coversee the formation and work of specific working groups that may be formed in the future to address specific challenges or issues of interest in AI\u201d. To fulfill its mandate, the working group liaises with the CAR Board of Directors and CAR staff, which it may advise on their engagement with corporate partners working in the AI field. It may also liaise with other societies sharing similar missions, such as the ACR's Data Science Institute [13]. The working group may propose to the CAR Board the creation of new sub\u2013working groups and changes in their leadership. It may guide sub\u2013working groups and approve the content produced by these sub\u2013working groups. A list of potential sub\u2013working groups and their proposed mandates is summarized in Table 2 . Recommendation \u2022 To create specific working groups dedicated to research and development, technique and applications, education, legal and ethics, writing group, and industrial partners to address challenges or issues pertinent to AI in Radiology (Figure 4 ). Role of Radiologists Radiologists are primarily known for their image interpretation skills. As a result, recent breakthroughs in image recognition introduced by deep learning techniques have been equated in the media with the imminent demise of radiologists. This misconception has been amplified by bold statements made by prominent researchers in AI [51,52]. These statements are best understood from the perspective of advances in a subset of tasks accomplished by radiologists that require specialized intelligence, mainly detection of anomalies [40,53], segmentation [54], and image classification [55]. However, the complex work performed by radiologists includes many other tasks that require common sense and general intelligence for problem solving\u2013tasks that cannot be achieved through AI. Understanding a case may require integration of medical concepts from different scientific fields (eg, anatomy, physiology, medical physics) and clinical specialties (eg, surgery, pathology, oncology) to provide plausible explanations for imaging findings. Such tasks accomplished by radiologists on a daily basis include consultation, protocoling, review of prior examinations, quality control, identification and dismissal of imaging artifacts, cancer staging, disease monitoring, interventional procedures for diagnostic or therapeutic purpose, reporting, management guidance, expertise in multidisciplinary discussions, and patient reassurance (Figure 5 ). Additional tasks include education and the development of departmental policy. With technological advances in computer science, it is anticipated that an increasing number of repetitive tasks will be automated over time. The PACS of all hospitals contain large imaging datasets with matching descriptions within radiology reports that can be used to perform ML on very large scale. The interactions between radiology images and their reports have been used to train ML for automated detection of disease in images [56]. Of note, a recent review of deep learning revealed that many recent applications in medical image analysis focus on 2D convolutional neural networks which do not directly leverage 3D information [57]. While 3D convolutional neural networks are emerging for analysis of multiplanar imaging (eg, CT), further research will be required to analyze multiparametric imaging examinations (eg, MRI). Historically, residency programs have successfully integrated basic sciences (eg, biomedical physics and radiation protection) in their teaching curriculum. Because it is anticipated that the nature of the work accomplished by radiologists will evolve in the future, it follows that the radiology programs should begin to integrate health informatics, computer science and statistics courses in their curriculum. Recommendations \u2022 The CAR should inform the AI community of the role played by radiologists as consultants, experts, diagnosticians, interventionalists, educators, and policymakers involved in patient care. \u2022 The radiology community should be prepared for automation of image interpretation tasks that will transform the nature of their work, particularly in 2D modalities. \u2022 Residency programs should integrate health informatics, computer science, and statistics courses in AI in their curriculum. Impact of AI on Radiology in Canada AI is currently having an immense impact on the research landscape of radiology departments across Canada. The availability of parallel computing hardware and ease of the open-source software tools have helped spark a movement towards AI research in Canadian academic departments that has frequently involved leadership from residents, fellows, and junior staff who are comfortable with the technology. AI research, which can be done on data exports or even open source data, is significantly less expensive than traditional imaging research involving patient recruitment, research coordinators, and prospective scanning. In the next 5 years, Canadian radiologists will see more competent AI application incorporated into PACS workflows, especially for laborious tasks prone to human error such as detection of lung nodules on x-rays or bone metastases on CT. Currently, there is no evidence in the literature that AI can replace radiologists in day-to-day clinical practice. However, there is evidence that AI can improve the performance of clinicians and that both clinicians and AI working together are better than either alone [58,59]. For example, Lakhani and Sundaram [59] showed that a radiologist-augmented approach could improve the performance of 2 deep neural networks by resolving their disagreements. Canadian health care budgets will need to include funding and support for new AI tools that potentially improve disease detection. In Canada, where PACS upgrades and optimizations are budget limited, Radiology leaders will need to strongly advocate for solutions that incorporate AI into image interpretation workflows to optimize patient care, reduce detection errors and increase hospital efficiencies. To remain current Canadian radiologists will need to follow and contribute to health care AI research and development, embrace the changes in workflow that will be required to support the implementation of clinical AI and adapt to changes in their practice that will improve care of their patients. Recommendations \u2022 Radiologists should partner with the computer science and engineering departments of their affiliated universities to ensure that the problems under examination have maximum clinical benefit. \u2022 The CAR must educate government policymakers on the complexities of radiology and the consequences of misses, including associated morbidity and litigation costs. \u2022 The CAR should provide a pathway for the implementation of AI tools in Canadian PACS environments, establishing minimum performance metrics for critical abnormalities. Conclusions AI techniques have been steadily developed since 1955 but recently have undergone a resurgence due to breakthrough performance arising from a combination of factors: wide availability of labeled data, advances in neural network architectures, and availability of parallel computing hardware. In radiology, AI applications currently focus on anomaly detection, segmentation, and classification of images. Familiarity with the terminology and key concepts in this field will allow the radiology community to critically analyze the opportunities, pitfalls, and challenges associated with the introduction of these new tools. Radiologists should become actively involved in research and development in collaboration with key stakeholders, scientists, and industrial partners to ensure radiologist oversight in the definition of use cases and validation process, and in the clinical application for patient care. Residency programs should integrate health informatics and computer science courses in AI in their curriculum. Acknowledgements Additional members of this Working Group include Timothy O'Connell, Paul Babyn, David Koff, Darren Ferguson, Sheldon Derkatch, Alexander Bilbily, and Wael Shabana. The work was supported by a Clinical Research Scholarship Salary Award from the Fonds de recherche du Qu\u00e9bec en Sant\u00e9 and Fondation de l'association des Radiologistes du Qu\u00e9bec (FRQR-ARQ #34939) to An Tang. Mark Cicero holds shares in 16 Bit Inc, a company that develops diagnostic medical imaging software, and serves as the company's Chief Operating Officer. References [1] I. Goodfellow Y. Bengio A. Courville Deep Learning 1st ed. 2016 MIT Press Cambridge, MA [2] Merriam-Webster definition of artificial intelligence. Available at: https://www.merriam-webster.com/dictionary/artificial intelligence. Accessed January 28, 2018. [3] Guidance for Industry and Food and Drug Administration Staff - Computer-Assisted Detection Devices Applied to Radiology Images and Radiology Device Data - Premarket Notification [510(k)] Submissions. 2017. Available at: https://www.fda.gov/MedicalDevices/ucm187249.htm. Accessed January 28, 2018. [4] S. Arthur Some Studies in Machine Learning Using the Game of Checkers IBM J Res Dev 3 1959 535 554 [5] Google. Machine learning. Glossary. Available at: https://developers.google.com/machine-learning/glossary/#r. Accessed January 28, 2018. [6] Minka T. A statistical learning/pattern recognition glossary. Available at: http://alumni.media.mit.edu/\u223ctpminka/statlearn/glossary/. Accessed January 28, 2018. [7] J. McCarthy M.L. Minsky N. Rochester C.E. Shannon A proposal for the dartmouth summer research project on artificial intelligence, August 31, 1955 AI Magazine 27 2006 12 [8] Faggella D. What is machine learning? Techemergence. 2017. Available at: https://www.techemergence.com/what-is-machine-learning/. Accessed January 22, 2018. [9] Bahnsen AC. Easy Solutions, Inc. Building AI applications using deep learning. Available at: http://blog.easysol.net/building-ai-applications. Accessed January 28, 2018. [10] Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 2015 436 444 [11] G. Chartrand P.M. Cheng E. Vorontsov Deep learning: a primer for radiologists Radiographics 37 2017 2113 2131 [12] Canadian Association of Radiologists. Resources on AI, machine learning, and deep learning. Available at: https://car.ca/innovation/artificial-intelligence/ai-resources/. Accessed January 28, 2018. [13] American College of Radiology Data Science Institute. Use case development. Available at: http://acrdsi.org/use-case-development.html. Accessed January 28, 2018. [14] P. Lambin E. Rios-Velazquez R. Leijenaar Radiomics: extracting more information from medical images using advanced feature analysis Eur J Cancer 48 2012 441 446 [15] Christ PF. LiTS: liver tumor segmentation challenge. 2017. Available at: https://competitions.codalab.org/competitions/17094. Accessed January 28, 2018. [16] Pickup LC, Gleeson F, Talwar A, Kadir T. Lung nodule risk stratification using CNNs: can we generalize from screening training data? Conference on Machine Intelligence in Medical Imaging. September 26\u201327, 2017; Baltimore, MD. [17] J. Su D.V. Vargas S. Kouichi One pixel attack for fooling deep neural networks arXiv 2017 [Epub ahead of print] [18] I. Evtimov K. Eykholt E. Fernandes Robust physical-world attacks on deep learning models arXiv 2017 [Epub ahead of print] [19] A. Traboulsee J.H. Simon L. Stone Revised recommendations of the Consortium of MS Centers task force for a standardized MRI protocol and clinical guidelines for the diagnosis and follow-up of multiple sclerosis AJNR Am J Neuroradiol 37 2016 394 401 [20] P.M. Bossuyt J.B. Reitsma D.E. Bruns STARD 2015: an updated list of essential items for reporting diagnostic accuracy studies Radiology 277 2015 826 832 [21] G.S. Collins J.B. Reitsma D.G. Altman K.G. Moons Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD statement Ann Intern Med 162 2015 55 63 [22] K. Kamnitsas C. Baumgartner C. Ledig Unsupervised Domain Adaptation in Brain Lesion Segmentation with Adversarial Networks 2017 Springer International Cham, Switzerland [23] Canada needs to nurture local tech champions and protect research, says AI pioneer. National Post. 2017. Available at: http://nationalpost.com/pmn/news-pmn/canada-news-pmn/canada-needs-to-nurture-local-tech-champions-and-protect-research-says-ai-pioneer. Accessed January 28, 2018. [24] Deveau D. Revolution AI: Canada's early start in artificial intelligence set it up to be today's global powerhouse. Financial Post. 2017. Available at: http://business.financialpost.com/entrepreneur/0123-biz-dd-intelligence. Accessed January 28, 2018. [25] Government of Canada. Canada's health infostructure. 2004. Available at: https://www.canada.ca/en/health-canada/services/health-care-system/ehealth/canada-health-infostructure.html. Accessed January 28, 2018. [26] Canada Health Infoway. Standards centre. Available at: https://infocentral.infoway-inforoute.ca/en/standards/standards-selection-framework/standards-ssf. Accessed February 6, 2018. [27] T. Brosch Y. Yoo L. Tang R. Tam Deep learning of brain images and its application to multiple sclerosis G. Wu D. Shen M. Sabuncu Machine Learning and Medical Imaging 2016 Elsevier New York 69 96 [28] Radiology Informatics Lab at Stanford Medicine. Medical ImageNet. 2016. Available at: http://langlotzlab.stanford.edu/projects/medical-image-net/. Accessed January 28, 2018. [29] DICOM. Digital Imaging and Communications in Medicine. Available at: https://www.dicomstandard.org/. Accessed January 28, 2018. [30] DICOM PS3.6 2018a - data dictionary. 2018. Available at: http://dicom.nema.org/medical/dicom/current/output/html/part06.html. Accessed January 28, 2018. [31] Nelson GS. Practical implications of sharing data: a primer on data privacy, anonymization, and de-identification. Paper 1884\u20132015. SAS Global Forum Proceedings 2015. Available at: https://support.sas.com/resources/papers/proceedings15/1884-2015.pdf. Accessed January 28, 2018. [32] Council of Canadian Academies. Accessing Health and Health-Related Data in Canada: The Expert Panel on Timely Access to Health and Social Data for Health Research and Health System Innovation. 2015. Available at: http://www.scienceadvice.ca/uploads/eng/assessments%20and%20publications%20and%20news%20releases/Health-data/HealthDataFullReportEn.pdf. Accessed January 28, 2018. [33] D.S. Channin P. Mongkolwat V. Kleper D.L. Rubin The annotation and image mark-up project Radiology 253 2009 590 592 [34] M.J. Cardoso T. Arbel S.L. Lee Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis 2017 Springer New York [35] Comparison of deep learning software. Available at: https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software. Accessed January 28, 2018. [36] Varangaonkar A. Top 10 deep learning frameworks. Packt Hub. 2017. Available at: https://datahub.packtpub.com/deep-learning/top-10-deep-learning-frameworks/. Accessed January 28, 2018. [37] D. Merkel Docker: lightweight linux containers for consistent development and deployment Linux J 2014 2014 2 [38] Compute Canada. Available at: https://www.computecanada.ca/research-showcase/. Accessed February 6, 2018. [39] P.M. Bossuyt L. Irwig J. Craig P. Glasziou Comparative accuracy: assessing new tests against existing diagnostic pathways BMJ 332 2006 1089 1092 [40] A.S. Becker M. Marcon S. Ghafoor M.C. Wurnig T. Frauenfelder A. Boss Deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer Invest Radiol 52 2017 434 440 [41] R.J. Gillies P.E. Kinahan H. Hricak Radiomics: images are more than pictures, they are data Radiology 278 2016 563 577 [42] American College of Radiology. Reporting and data systems. Available at: https://www.acr.org/Clinical-Resources/Reporting-and-Data-Systems. Accessed January 28, 2018. [43] E. Pons L.M. Braun M.G. Hunink J.A. Kors Natural language processing in radiology: a systematic review Radiology 279 2016 329 343 [44] T.K. Alkasab B.C. Bizzo L.L. Berland S. Nair P.V. Pandharipande H.B. Harvey Creation of an open framework for point-of-care computer-assisted reporting and decision support tools for radiologists J Am Coll Radiol 14 2017 1184 1189 [45] X. Wang Y. Peng L. Lu Z. Lu M. Bagheri R.M. Summers ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases arXiv 2017 [Epub ahead of print] [46] M. Zalis M. Harris Advanced search of the electronic medical record: augmenting safety and efficiency in radiology J Am Coll Radiol 7 2010 625 633 [47] F.W. Prior K. Clark P. Commean TCIA: an information resource to enable open science Conf Proc IEEE Eng Med Biol Soc 2013 2013 1282 1285 [48] K. Wolstencroft R. Haines D. Fellows The Taverna workflow suite: designing and executing workflows of web services on the desktop, web or in the cloud Nucleic Acids Res 41 2013 W557 W561 [49] M. Dumontier C.J. Baker J. Baran The Semanticscience Integrated Ontology (SIO) for biomedical research and knowledge discovery J Biomed Semantics 5 2014 14 [50] C.P. Langlotz RadLex: a new method for indexing online educational materials Radiographics 26 2006 1595 1597 [51] Creative Destruction Lab. Geoff Hinton: on radiology. 2016. Available at: https://www.youtube.com/watch?v=2HMPRXstSvQ. Accessed January 28, 2018. [52] Ng A. Should radiologists be worried about their jobs? Breaking news: We can now diagnose pneumonia from chest X-rays better than radiologists. 2017. Available at: https://twitter.com/andrewyng/status/930938692310482944?lang=en. Accessed January 28, 2018. [53] T. Kooi G. Litjens B. van Ginneken Large scale deep learning for computer aided detection of mammographic lesions Med Image Anal 35 2017 303 312 [54] M. Drozdzal G. Chartrand E. Vorontsov Learning normalized inputs for iterative estimation in medical image segmentation Med Image Anal 44 2018 1 13 [55] M. Anthimopoulos S. Christodoulidis L. Ebner A. Christe S. Mougiakakou Lung pattern classification for interstitial lung diseases using a deep convolutional neural network IEEE Trans Med Imaging 35 2016 1207 1216 [56] H.-C. Shin L. Lu L. Kim A. Seff J. Yao R.M. Summers Interleaved text/image deep mining on a large-scale radiology database for automated image interpretation J Mach Learn Res 17 2016 1 31 [57] G. Litjens T. Kooi B.E. Bejnordi A survey on deep learning in medical image analysis arXiv 2017 [Epub ahead of print] [58] D. Wang A. Khosla R. Gargeya H. Irshad A.H. Beck Deep learning for identifying metastatic breast cancer arXiv 2016 [Epub ahead of print] [59] P. Lakhani B. Sundaram Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks Radiology 284 2017 574 582", "scopus-id": "85045189457", "pubmed-id": "29655580", "coredata": {"eid": "1-s2.0-S0846537118300305", "dc:description": "Abstract Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada.", "openArchiveArticle": "false", "prism:coverDate": "2018-05-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0846537118300305", "dc:creator": [{"@_fa": "true", "$": "Tang, An"}, {"@_fa": "true", "$": "Tam, Roger"}, {"@_fa": "true", "$": "Cadrin-Ch\u00eanevert, Alexandre"}, {"@_fa": "true", "$": "Guest, Will"}, {"@_fa": "true", "$": "Chong, Jaron"}, {"@_fa": "true", "$": "Barfett, Joseph"}, {"@_fa": "true", "$": "Chepelev, Leonid"}, {"@_fa": "true", "$": "Cairns, Robyn"}, {"@_fa": "true", "$": "Mitchell, J. Ross"}, {"@_fa": "true", "$": "Cicero, Mark D."}, {"@_fa": "true", "$": "Poudrette, Manuel Gaudreau"}, {"@_fa": "true", "$": "Jaremko, Jacob L."}, {"@_fa": "true", "$": "Reinhold, Caroline"}, {"@_fa": "true", "$": "Gallix, Benoit"}, {"@_fa": "true", "$": "Gray, Bruce"}, {"@_fa": "true", "$": "Geis, Raym"}, {"@_fa": "true", "$": "O'Connell, Timothy"}, {"@_fa": "true", "$": "Babyn, Paul"}, {"@_fa": "true", "$": "Koff, David"}, {"@_fa": "true", "$": "Ferguson, Darren"}, {"@_fa": "true", "$": "Derkatch, Sheldon"}, {"@_fa": "true", "$": "Bilbily, Alexander"}, {"@_fa": "true", "$": "Shabana, Wael"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0846537118300305"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0846537118300305"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0846-5371(18)30030-5", "prism:volume": "69", "prism:publisher": "The Authors. Published by Elsevier Inc. on behalf of Canadian Association of Radiologists.", "dc:title": "Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology", "prism:copyright": "\u00a9 2018 The Authors. Published by Elsevier Inc. on behalf of Canadian Association of Radiologists.", "openaccess": "1", "prism:issn": "08465371", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Artificial intelligence"}, {"@_fa": "true", "$": "Machine learning"}, {"@_fa": "true", "$": "Deep learning"}, {"@_fa": "true", "$": "Radiology"}, {"@_fa": "true", "$": "Imaging"}, {"@_fa": "true", "$": "Medicine"}, {"@_fa": "true", "$": "Healthcare"}, {"@_fa": "true", "$": "Quality improvement"}], "openaccessArticle": "true", "prism:publicationName": "Canadian Association of Radiologists Journal", "prism:number": "2", "openaccessSponsorType": "Author", "prism:pageRange": "120-135", "prism:endingPage": "135", "pubType": "Health Policy and Practice / Sant\u00e9: politique et pratique m\u00e9dicale", "prism:coverDisplayDate": "May 2018", "prism:doi": "10.1016/j.carj.2018.02.002", "prism:startingPage": "120", "dc:identifier": "doi:10.1016/j.carj.2018.02.002", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "54", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8224", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "110", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14379", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "146", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11914", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "101", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "10189", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "179", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14958", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "162", "@width": "658", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33292", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "357", "@width": "713", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "58800", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "237", "@width": "356", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31431", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "276", "@width": "600", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "47748", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "291", "@width": "318", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "34287", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "717", "@width": "2914", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "193519", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1580", "@width": "3155", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "457305", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1048", "@width": "1577", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "143452", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1223", "@width": "2657", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "267884", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1291", "@width": "1411", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0846537118300305-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "148802", "@ref": "gr1", "@mimetype": "image/jpeg"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85045189457"}}