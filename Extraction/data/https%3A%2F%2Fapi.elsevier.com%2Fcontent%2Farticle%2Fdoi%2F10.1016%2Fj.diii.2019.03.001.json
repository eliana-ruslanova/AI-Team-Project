{"scopus-eid": "2-s2.0-85063338929", "originalText": "serial JL 280852 291210 291703 291901 291926 31 Diagnostic and Interventional Imaging DIAGNOSTICINTERVENTIONALIMAGING 2019-03-27 2019-03-27 2019-04-04 2019-04-04 2019-04-04T11:55:42 1-s2.0-S2211568419300579 S2211-5684(19)30057-9 S2211568419300579 10.1016/j.diii.2019.03.001 S300 S300.1 FULL-TEXT 1-s2.0-S2211568419X00042 2020-04-30T04:08:11.995365Z 0 0 20190401 20190430 2019 2019-03-27T15:35:07.281314Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast orcid primabst pubtype ref 2211-5684 22115684 true 100 100 4 4 Volume 100, Issue 4 4 211 217 211 217 201904 April 2019 2019-04-01 2019-04-30 2019 Original articles Computer developments article fla \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. KIDNEYCORTEXSEGMENTATIONIN2DCTUNETSENSEMBLEAGGREGATION COUTEAUX V Method Kidney cortex segmentation challenge Network architecture Weight initialization and pre-training Post-processing and ensemble aggregation Results Discussion Human and animal rights Informed consent and patient details Funding Author contributions Credit author statement Disclosure of interest References VANDENDOOL 2005 189 195 S GANDY 2007 12 20 S GRANTHAM 2006 2122 2130 J CHEN 2012 562 570 X HALLECK 2013 1208 1216 F JIN 2016 1395 1407 C POHLE 2001 238 246 R TORIMOTO 2015 98 104 I AKKUS 2017 449 459 Z AVENDI 2017 2439 2448 M SHELHAMER 2017 640 651 E CHEN 2017 192 196 Y HIPPOCAMPUSSEGMENTATIONTHROUGHMULTIVIEWENSEMBLECONVNETS CICEK 2016 O 3DUNETLEARNINGDENSEVOLUMETRICSEGMENTATIONSPARSEANNOTATIONMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTIONMICCAI DONG 2017 H MEDICALIMAGEUNDERSTANDINGANALYSISMIUACOMMUNICATIONSINCOMPUTERINFORMATIONSCIENCE AUTOMATICBRAINTUMORDETECTIONSEGMENTATIONUSINGUNETBASEDFULLYCONVOLUTIONALNETWORKS ERDEN 2017 B 3DCONVOLUTIONALNEURALNETWORKFORBRAINTUMORSEGMENTATION RONNEBERGER 2015 234 241 O UNETCONVOLUTIONALNETWORKSFORBIOMEDICALIMAGESEGMENTATIONMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTION BERTRAND 2017 H HYPERPARAMETEROPTIMIZATIONDEEPNEURALNETWORKSCOMBININGHYPERBANDBAYESIANMODELSELECTION BERTRAND 2017 745 748 H CLASSIFICATIONMRIDATAUSINGDEEPLEARNINGGAUSSIANPROCESSBASEDMODELSELECTION OQUAB 2014 1717 1724 M LEARNINGTRANSFERRINGMIDLEVELIMAGEREPRESENTATIONSUSINGCONVOLUTIONALNEURALNETWORKS ROKACH 2009 1 39 L MARMANIS 2016 473 480 D HE 2016 770 778 K DEEPRESIDUALLEARNINGFORIMAGERECOGNITION PENG 2017 1743 1751 C LARGEKERNELMATTERSIMPROVESEMANTICSEGMENTATIONBYGLOBALCONVOLUTIONALNETWORK LIN 2014 740 755 T MICROSOFTCOCOCOMMONOBJECTSINCONTEXT GARCIAGARCIA 2017 A AREVIEWDEEPLEARNINGTECHNIQUESAPPLIEDSEMANTICSEGMENTATIONCOMPUTERVISIONPATTERNRECOGNITION LECUN 2015 436 444 Y COUTEAUXX2019X211 COUTEAUXX2019X211X217 COUTEAUXX2019X211XV COUTEAUXX2019X211X217XV Full 2020-04-01T00:01:35Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ 2020-04-04T00:00:00Z http://www.elsevier.com/open-access/userlicense/1.0/ https://vtw.elsevier.com/content/oragreement/10178 NR_HAL publishAcceptedManuscriptIndexable http://creativecommons.org/licenses/by-nc-nd/4.0/ 2020-04-04T00:00:00Z \u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS. This article is made available under the Elsevier license. item S2211-5684(19)30057-9 S2211568419300579 1-s2.0-S2211568419300579 10.1016/j.diii.2019.03.001 280852 2020-04-30T04:08:11.995365Z 2019-04-01 2019-04-30 1-s2.0-S2211568419300579-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/MAIN/application/pdf/496b7e34526001033a5ffee6763cc590/main.pdf main.pdf pdf true 1399080 MAIN 7 1-s2.0-S2211568419300579-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/PREVIEW/image/png/849e98fa500b9634cef46cac4c1b6604/main_1.png main_1.png png 47226 849 656 IMAGE-WEB-PDF 1 1-s2.0-S2211568419300579-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr1/THUMBNAIL/image/gif/a918c86efa470b8ffff1bb6547cc3328/gr1.sml gr1 gr1.sml sml 30518 163 150 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr2/THUMBNAIL/image/gif/f98414c3aa0dc32f681fae85755b18b6/gr2.sml gr2 gr2.sml sml 20750 88 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr3/THUMBNAIL/image/gif/f52d0d3b1fe0588aa459a6b245f70160/gr3.sml gr3 gr3.sml sml 21394 89 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr4/THUMBNAIL/image/gif/8f1fa2b2434c9c2894f71addf084ed9b/gr4.sml gr4 gr4.sml sml 27602 145 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr5/THUMBNAIL/image/gif/2c42fa74e20a3e30a89d47ee9faaef61/gr5.sml gr5 gr5.sml sml 24318 72 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr6/THUMBNAIL/image/gif/76410d3fd5dbd9dc935a8f7140b1e1cb/gr6.sml gr6 gr6.sml sml 26903 120 219 IMAGE-THUMBNAIL 1-s2.0-S2211568419300579-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr1/DOWNSAMPLED/image/jpeg/8ba5a0fdb9485a48fa78a9d308c209c1/gr1.jpg gr1 gr1.jpg jpg 86260 596 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr2/DOWNSAMPLED/image/jpeg/2d180f1bc05f113e027dd85793dc2a53/gr2.jpg gr2 gr2.jpg jpg 67883 220 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr3/DOWNSAMPLED/image/jpeg/5b17e92c5fc5758cd23af52e7bc85cff/gr3.jpg gr3 gr3.jpg jpg 51812 222 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr4/DOWNSAMPLED/image/jpeg/6b75af8953761495c20a4db4aee5d3d7/gr4.jpg gr4 gr4.jpg jpg 68115 362 547 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr5/DOWNSAMPLED/image/jpeg/16da0808b0a5ca8daed952020e5c33cd/gr5.jpg gr5 gr5.jpg jpg 52344 175 533 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr6/DOWNSAMPLED/image/jpeg/34caeaec4bf05dd0635c38f4e280a3d0/gr6.jpg gr6 gr6.jpg jpg 58976 293 533 IMAGE-DOWNSAMPLED 1-s2.0-S2211568419300579-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr1/HIGHRES/image/jpeg/8bd2e62cca29cca3befbfecac4b390e9/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 256243 1583 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr2/HIGHRES/image/jpeg/c05cf9ed968b1ebdfc037d97f1f8dd04/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 283292 972 2421 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr3/HIGHRES/image/jpeg/d5400f4730beb04b5d1e6c2ba04e7b13/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 253423 981 2421 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr4/HIGHRES/image/jpeg/bd78bd14214af2643003b76af24ff907/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 186179 962 1453 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr5/HIGHRES/image/jpeg/c269e907f34801c991ff3030431da9df/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 132641 464 1417 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/gr6/HIGHRES/image/jpeg/e733b840d5f7b0b2a0d6ccda29be7e26/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 178870 778 1417 IMAGE-HIGH-RES 1-s2.0-S2211568419300579-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2211568419300579/STRIPIN/image/gif/ebc2b808cd67ec6338fc995f1786515a/si1.gif si1 si1.gif gif 409 27 69 ALTIMG 1-s2.0-S2211568419300579-am.pdf am am.pdf pdf 1839993 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:1007MBV5TN5/MAIN/application/pdf/37518f93f28ebfaf08fd4acbbe0b2dbd/am.pdf DIII 1166 S2211-5684(19)30057-9 10.1016/j.diii.2019.03.001 Soci show\u00e9t show\u00e9 fran\u00e7aises de radiologie Figure 1 CT images of the kidney from the training set provided by the data challenge organizers. The reference segmentation is overlapped in blue; a: image only; b: correct segmentation; c: inaccurate segmentation and renal column clusters (arrow); d: blood vessels included in the segmentation (arrow). Figure 2 Selected network architecture to achieve the segmentation task. Green boxes are residual blocks, blue boxes are simple convolutional layers with ReLU activation. Batch normalization is applied after convolution and before activation. Figure 3 Impact of pre-training on the training procedure: a: evolution of the Dice score on the validation set during training (red is pre-trained, green is not); b: evolution of the binary cross-entropy on the training set (blue is pre-trained, pink is not). The x-axis represents the number of training steps. Figure 4 Top line: segmentation achieved by three networks trained on three different folds of the training database (each output is displayed on a different color channel, so that white represents a consensus for positively-labeled regions. We observe inconsistencies on the inner parts of the renal columns, and to a lesser extent on the outermost edge of the renal cortex). Bottom line: corresponding input CT images. Figure 5 Illustration of automatic segmentation results obtained with the proposed approach (overlapped in blue on the input CT image); a: correct segmentation; b: cluster of renal columns; c: overextended segmentation. Figure 6 Illustration of test cases where the automatic segmentation results (blue) seem more accurate than the provided reference segmentation (red). Intersection in pink; a: vessels included in the reference mask but not in automatic segmentation result; b: reference segmentation obviously too wide. Original article Computer developments Kidney cortex segmentation in 2D CT with U-Nets ensemble aggregation V. Couteaux a b \u204e vincent.couteaux@telecom-paristech.fr S. Si-Mohamed c d R. Renard-Penna e O. Nempont a T. Lefevre a A. Popoff a G. Pizaine a N. Villain a I. Bloch b J. Behr f M.-F. Bellin g C. Roy h O. Rouvi\u00e8re i S. Montagne j N. Lassau k L. Boussel c d a Philips Research France, 33, rue de Verdun, 92150 Suresnes, France Philips Research France 33, rue de Verdun Suresnes 92150 France b LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay, 75013 Paris, France LTCI, T\u00e9l\u00e9com ParisTech, Universit\u00e9 Paris-Saclay Paris 75013 France c CREATIS, CNRS UMR 5220, Inserm U1206, INSA-Lyon, Claude Bernard Lyon 1 University, 69100 Villeurbanne, France CREATIS, CNRS UMR 5220, Inserm U1206, INSA-Lyon, Claude Bernard Lyon 1 University Villeurbanne 69100 France d Department of Radiology, Hospices Civils de Lyon, 69002 Lyon, France Department of Radiology, Hospices Civils de Lyon Lyon 69002 France e Department of Radiology, H\u00f4pital Tenon, AP\u2013HP, GRC-UPMC n\u00b05 Oncotype-URO, Sorbonne universit\u00e9s, 75020 Paris, France Department of Radiology, H\u00f4pital Tenon, AP\u2013HP, GRC-UPMC n\u00b05 Oncotype-URO, Sorbonne universit\u00e9s Paris 75020 France f Department of Radiology, CHRU de Besan\u00e7on, 25000 Besan\u00e7on, France Department of Radiology, CHRU de Besan\u00e7on Besan\u00e7on 25000 France g Department of Radiology, H\u00f4pitaux Universitaires Paris Sud, 94270 Le Kremlin Bic\u00eatre, France Department of Radiology, H\u00f4pitaux Universitaires Paris Sud Le Kremlin Bic\u00eatre 94270 France h Department of Radiology, CHU de Strasbourg, Nouvel H\u00f4pital Civil, 67000 Strasbourg, France Department of Radiology, CHU de Strasbourg, Nouvel H\u00f4pital civil Strasbourg 67000 France i Department of Uroradiology, Hospices Civils de Lyon, Facult\u00e9 de M\u00e9decine Lyon Est, 69002 Lyon, France Department of Uroradiology, Hospices Civils de Lyon, facult\u00e9 de m\u00e9decine Lyon Est Lyon 69002 France j Department of Radiology, H\u00f4pital Piti\u00e9 Salp\u00e9tri\u00e8re, AP\u2013HP, 75013 Paris, France Department of Radiology, H\u00f4pital Piti\u00e9 Salp\u00e9tri\u00e8re, AP\u2013HP Paris 75013 France k Department of Radiology, Gustave Roussy, IR4M, UMR8081, CNRS, Universit\u00e9 Paris-Sud, Universit\u00e9 Paris-Saclay, 94805 Villejuif, France Department of Radiology, Gustave Roussy, IR4M, UMR8081, CNRS, Universit\u00e9 Paris-Sud, Universit\u00e9 Paris-Saclay Villejuif 94805 France \u204e Corresponding author. Philips Research France, 33, rue de Verdun, 92150 Suresnes, France. Philips Research France 33, rue de Verdun Suresnes 92150 France Abstract Purpose This work presents our contribution to one of the data challenges organized by the French Radiology Society during the Journ\u00e9es Francophones de Radiologie. This challenge consisted in segmenting the kidney cortex from coronal computed tomography (CT) images, cropped around the cortex. Materials and methods We chose to train an ensemble of fully-convolutional networks and to aggregate their prediction at test time to perform the segmentation. An image database was made available in 3 batches. A first training batch of 250 images with segmentation masks was provided by the challenge organizers one month before the conference. An additional training batch of 247 pairs was shared when the conference began. Participants were ranked using a Dice score. Results The segmentation results of our algorithm match the renal cortex with a good precision. Our strategy yielded a Dice score of 0.867, ranking us first in the data challenge. Conclusion The proposed solution provides robust and accurate automatic segmentations of the renal cortex in CT images although the precision of the provided reference segmentations seemed to set a low upper bound on the numerical performance. However, this process should be applied in 3D to quantify the renal cortex volume, which would require a marked labelling effort to train the networks. Keywords Renal cortex Image segmentation Artificial intelligence (AI) Computed tomography (CT) Renal diseases are often associated with cortical morphological changes, such as volume reduction or notch defect. All these features are considered as surrogate markers of renal diseases and can be visible on imaging examinations, such as ultrasound, magnetic resonance imaging (MRI), or computed tomography (CT) [1,2]. Despite a well-established qualitative assessment of the renal cortex with these modalities, a quantitative approach helps improve the diagnostic work-up of renal diseases [3]. However, to date quantitative assessment of renal cortex is hampered by complex and time-consuming analyses such as semi-automated segmentations based on a pixel value threshold algorithm, region growing, appearance models combined with graph cuts or random forests [4\u20138]. The recent development of convolutional neural networks (CNN), as well as the access to very large imaging databases, could help overcome these limitations. Very promising results have recently been obtained in several applications such as the segmentation of cardiac chambers, and the brain [9,10]. However, the appropriate artificial intelligence (AI) tools for kidney analysis still need to be developed. Fully-convolutional networks have drastically improved the state-of-the-art in image segmentation [11]. U-Nets are currently a standard approach for two-dimensional (2D) or three-dimensional (3D) medical image segmentation problems [12\u201318]. The Journ\u00e9es Francophones de Radiologie was held in Paris in October 2018. For the first time this year, the French Society of Radiology organized an AI competition. Teams of industrial researchers, students, and radiologists were invited to take part in five data challenges. In this paper, we present our approach to address the kidney cortex segmentation challenge aiming at segmenting the renal cortex on 2D coronal CT images. Method Kidney cortex segmentation challenge An image database was made available in 3 batches. A first training batch of 250 images with segmentation masks was provided by the challenge organizers one month before the conference. An additional training batch of 247 pairs was shared when the conference began. Two days later, the teams were ranked on a test batch of 299 images. CT images in the coronal plane, cropped and resized around the kidney (192\u00d7192 pixels with a pixel size of 1\u00d71mm and intensity in Hounsfield units [HU]) were provided (Fig. 1 ). The reference segmentation was provided as a binary mask for each image of the training set. Due to the usual difficulties of manual segmentation, in particular for irregularly shaped objects such as the renal cortex, several reference segmentations were debatable or even erroneous. We observed that a proportion of the pixels at the edge of the cortex were either left out when they should not have been, or mislabeled as cortex while clearly outside (Fig. 1c). Moreover, blood vessels inside the kidney were occasionally included in the reference segmentation, but this was inconsistent throughout the dataset (Fig. 1d). In fact, it can be hard to distinguish actual renal columns from some blood vessels. We clipped the image intensity values between \u2212150HU and 200HU and rescaled them between 0 and 1. This range has been chosen manually to contain all the renal cortex dynamic and limit the influence of high values in the image, corresponding to bones, and very low values, corresponding to air. To address the specific difficulties of this challenge, such as the imprecision of the reference segmentations, we adopted several popular strategies such as artificial data augmentation, meta parameter optimization, pre-training and post-processing with connected components analysis [19\u201322]. We also used ensemble aggregation, a standard machine learning technique frequently applied to deep learning [12,22,23]. Network architecture We chose a U-Net architecture with 5 levels of depth, residual blocks, and rectified linear units (ReLU) activation functions, and added convolutions on the skip connections (Fig. 2 ) [18,24\u201326]. We set the meta-parameters using a Bayesian optimization approach [19,20]. We used artificial data augmentation during training to limit overfitting, by randomly applying translations, rotations, zooms, noise, brightness and contrast shifts to the input samples. The training was performed until convergence and lasted between one and two hours. We used Adam optimizer with a learning rate of 1.10\u22124 on batches of 10 images. Weight initialization and pre-training Considering the low amount of data available for training following the popular practice initiated in [21], we considered that pre-training the network on a large and publicly-available dataset would be advantageous. We therefore pre-trained our U-Nets to segment persons, the common objects in context (COCO) dataset [26]. We compared training experiments using randomly initialized weights or pre-training (Fig. 3 ). Although the final score was similar, the training converges faster using a pre-trained network, and was more stable overall. Therefore, we used pre-trained networks. Post-processing and ensemble aggregation We noticed that networks trained on different folds of the training database behave differently, especially on ambiguous pixels (Fig. 4 ). To improve the robustness and reduce the variability, we used ensemble aggregation. We trained five networks on random folds of the training dataset, and two others on the complete training dataset. For each image at test time, we thus obtained seven segmentation masks taking pixel values in the interval \u201c[0,1]\u201d. In each mask we only kept the largest connected component in order to remove obvious false positives (see, for instance Fig. 4, top middle: a blob is falsely labeled positively by one of the networks). Finally, we aggregated the results by taking the median value for each pixel, as it has shown to produce better results than the mean, by reducing the influence of extreme or outlier values. Results Participants were ranked using a Dice score: S = 2 P \u2229 T P + T , where P is the predicted mask and T is the reference mask. We obtained a score of 0.867 on the test dataset and won the challenge by a narrow margin. The slight improvement obtained by the ensemble aggregation enabled us to win this challenge, as the second ranked team scored higher than our best network. The segmentation results of our algorithm match the renal cortex with a good precision (Fig. 5 ). However, some of the flaws of the provided reference segmentations remain, such as the large clusters of renal columns, or when parts of the cortex are too widely segmented and join each other. Nonetheless, our algorithm seems to be less imprecise than the provided annotation, especially at the boundary of the cortex (Fig. 6 ). Discussion The state-of-the-art in image segmentation has improved greatly during the past five years, thanks to the progress accomplished in Deep Learning, to the point that some segmentation problems, which would have been considered a challenge ten years ago, now seem easy [27,28]. This is the case of renal cortex segmentation, where one can quickly achieve good results by training a UNet with any recent architecture found in the literature [18]. To the best of our knowledge, all the contestants chose a deep learning approach and the gap between participants was less than 0.03 Dice points. The precision of the reference segmentations provided for this challenge seemed to set a low upper bound on the performance, as corroborated by the narrow gap between the first and second place (<0.003 Dice points), and the gap between all the candidates (<0.03 Dice points). As a consequence, the performance gain achieved by each of our algorithm details (image intensity scaling, data augmentation, pre-training, meta-parameter optimization, connected components analysis and ensemble aggregation) was difficult to quantify and barely significant if at all when considered alone, but enabled us, when added together, to improve the overall performance and win the challenge. In conclusion, although 3D segmentation is useful clinically, the choice of 2D makes sense for a data challenge as it simplifies data collection, annotation, and storage [13,15\u201317]. Future research is needed to address the problem of renal cortex segmentation in 3D volumes. Human and animal rights The authors declare that the work described has been carried out in accordance with the Declaration of Helsinki of the World Medical Association revised in 2013 for experiments involving humans as well as in accordance with the EU Directive 2010/63/EU for animal experiments. Informed consent and patient details The authors declare that this report does not contain any personal information that could lead to the identification of the patient(s). The authors declare that they obtained a written informed consent from the patients and/or volunteers included in the article. The authors also confirm that the personal details of the patients and/or volunteers have been removed. Funding This work received funding from Association Nationale de la Recherche et de la Technologie (Contract 2018/2439) Author contributions All authors attest that they meet the current International Committee of Medical Journal Editors (ICMJE) criteria for Authorship. Credit author statement Vincent Couteaux: conceptualization and design; data curation; writing-original draft preparation; review & editing. Salim Si-Mohamed: conceptualization and design; data curation; supervision; resources; writing \u2013 original draft preparation; review & editing. Raphaele Renard-Penna: conceptualization and design; resources; data curation; writing \u2013 original draft preparation; review & editing. Olivier Nempont: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Thierry Lefevre: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Alexandre Popoff: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Guillaume Pizaine: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Nicolas Villain: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Isabelle Bloch: conceptualization and design; data curation; writing \u2013 original draft preparation; review & editing. Julien Behr: conceptualization and design; resources; data curation; writing \u2013 original draft preparation; review & editing. Marie-France Bellin: conceptualization and design; resources; data curation. Catherine Roy: conceptualization and design; resources; data curation; writing \u2013 original draft preparation; review & editing. Olivier Rouviere: conceptualization and design; resources; data curation; writing \u2013 original draft preparation; review & editing. Sarah Montagne: conceptualization and design; resources; data curation. Nathalie Lassau: conceptualization and design; resources; data curation; writing \u2013 original draft preparation; review & editing. Anne Cotten: conceptualization and design; data curation; resources; review & editing. Lo\u00efc Boussel: conceptualization and design; supervision; writing \u2013 original draft preparation; review & editing. Disclosure of interest The authors declare that they have no competing interest. References [1] S.W. van den Dool M.N. Wasser J.W. de Fijter J. Hoekstra R.J. van der Geest Functional renal volume: quantitative analysis at gadolinium-enhanced MR angiography--feasibility study in healthy potential kidney donors Radiology 236 2005 189 195 [2] S.J. Gandy K. Armoogum R.S. Nicholas T.B. McLeay J.G. Houston A clinical MRI investigation of the relationship between kidney volume measurements and renal function in patients with renovascular disease Br J Radiol 80 2007 12 20 [3] J.J. Grantham V.E. Torres A.B. Chapman L.M. Guay-Woodford K.T. Bae B.F. King Jr. Volume progression in polycystic kidney disease N Engl J Med 354 2006 2122 2130 [4] X. Chen R.M. Summers M. Cho U. Bagci J. Yao An automatic method for renal cortex segmentation on CT images: evaluation on kidney donors Acad Radiol 19 2012 562 570 [5] F. Halleck G. Diederichs T. Koehlitz T. Slowinski F. Engelken L. Liefeldt Volume matters: CT-based renal cortex volume measurement in the evaluation of living kidney donors Transpl Int 26 2013 1208 1216 [6] C. Jin F. Shi D. Xiang X. Jiang B. Zhang X. Wang 3D fast automatic segmentation of kidney based on modified AAM and random forest Trans Med Imaging. 35 2016 1395 1407 [7] R. Pohle K.D. Toennies A new approach for model-based adaptive region growing in medical image analysis Computer Analysis of Images and Patterns Springer 2124 2001 238 246 [8] I. Torimoto S. Takebayashi Z. Sekikawa J. Teranishi K. Uchida T. Inoue Renal perfusional cortex volume for arterial input function measured by semiautomatic segmentation technique using MDCT angiographic data with 0.5-mm collimation AJR Am J Roentgenol 204 2015 98 104 [9] Z. Akkus A. Galimzianova A. Hoogi D.L. Rubin B.J. Erickson Deep learning for brain MRI segmentation: state of the art and future directions J Digit Imaging 30 2017 449 459 [10] M.R. Avendi A. Kheradvar H. Jafarkhani Automatic segmentation of the right ventricle from cardiac MRI using a learning-based approach Magn Reson Med 78 2017 2439 2448 [11] E. Shelhamer J. Long T. Darrell Fully convolutional networks for semantic segmentation IEEE Trans Pattern Anal Mach Intell 39 2017 640 651 [12] Y. Chen B. Shi Z. Wang P. Zhang C.D. Smith J. Liu. Hippocampus segmentation through multi-view ensemble ConvNets International Symposium on Biomedical Imaging. Melbourne, VIC, Australia: IEEE 2017 192 196 [13] P.F. Christ, F. Ettlinger, F. Gr\u00fcn, M.E.A. Elshaera, J. Lipkova, S. Schlecht, et al. Automatic liver and tumor segmentation of CT and MRI volumes using cascaded fully convolutional neural networks. https://arxiv.org/abs/1702.05970 [Accessed on March 20, 2019]. [14] \u00d6. \u00c7i\u00e7ek A. Abdulkadir S.S. Lienkamp T. Brox O. Ronneberger 3D U-Net: learning dense volumetric segmentation from sparse annotation. Medical Image Computing and Computer-Assisted Intervention. MICCAI 9901 2016 Springer Cham [Lecture Notes in Computer Science] [15] H. Dong G. Yang F. Liu Y. Mo Y. Guo Automatic brain tumor detection and segmentation using U-Net based fully convolutional networks M. Vald\u00e9s Hern\u00e1ndez V. Gonz\u00e1lez-Castro Medical Image Understanding and Analysis. MIUA. Communications in Computer and Information Science 723 2017 Springer Cham [16] B. Erden N. Gamboa S. Wood 3D convolutional neural network for brain tumor segmentation 2017 Computer Science. Stanford University http://cs231n.stanford.edu/reports/2017/pdfs/526.pdf [17] F. Milletari, N. Navab, SA. Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. 3D Vision. IEEE 2016:565-71 [Accessed on March 20, 2019]. [18] O. Ronneberger P. Fischer T. Brox U-Net: convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention 2015 234 241 [19] H. Bertrand R. Ardon M. Perrot I. Bloch Hyperparameter optimization of deep neural networks: combining hyperband with bayesian model selection 2017 CAP France [20] H. Bertrand M. Perrot R. Ardon I. Bloch Classification of MRI data using deep learning and Gaussian process-based model selection 2017 Biomedical Imaging. IEEE 745 748 [21] M. Oquab L. Bottou I. Laptev J. Sivic Learning and transferring mid-level image representations using convolutional neural networks 2014 Computer Vision and Pattern Recognition. IEEE 1717 1724 [22] L. Rokach Ensemble-based classifiers Artificial Intelligence Review 33 2009 1 39 [23] D. Marmanis J.D. Wegner S. Galliani K. Schindler M. Datcu U. Stilla Semantic segmentation of aerial images with an ensemble of CNNs, ISPRS Annals of the Photogrammetry Remote Sens Spatial Info Sci III 2016 473 480 [24] K. He X. Zhang S. Ren J. Sun Deep residual learning for image recognition 2016 Computer Vision and Pattern Recognition. IEEE 770 778 [25] C. Peng X. Zhang G. Yu G. Luo J. Sun Large kernel matters improve semantic segmentation by global convolutional network 2017 Computer Vision and Pattern Recognition. IEEE 1743 1751 [26] T.Y. Lin M. Maire S.J. Belongie L.D. Bourdev R.B. Girshick J. Hays Microsoft COCO: common objects in context 2014 European Conference on Computer Vision 740 755 [27] A. Garcia-Garcia S. Orts S. Oprea V. Villena-Martinez J.G. Rodr\u00edguez A review on deep learning techniques applied to semantic segmentation. Computer Vision and Pattern Recognition 2017 Cornell University [28] Y. LeCun Y. Bengio G. Hinton Deep learning Nature 521 2015 436 444", "scopus-id": "85063338929", "pubmed-id": "30926445", "coredata": {"eid": "1-s2.0-S2211568419300579", "dc:description": "Abstract Purpose This work presents our contribution to one of the data challenges organized by the French Radiology Society during the Journ\u00e9es Francophones de Radiologie. This challenge consisted in segmenting the kidney cortex from coronal computed tomography (CT) images, cropped around the cortex. Materials and methods We chose to train an ensemble of fully-convolutional networks and to aggregate their prediction at test time to perform the segmentation. An image database was made available in 3 batches. A first training batch of 250 images with segmentation masks was provided by the challenge organizers one month before the conference. An additional training batch of 247 pairs was shared when the conference began. Participants were ranked using a Dice score. Results The segmentation results of our algorithm match the renal cortex with a good precision. Our strategy yielded a Dice score of 0.867, ranking us first in the data challenge. Conclusion The proposed solution provides robust and accurate automatic segmentations of the renal cortex in CT images although the precision of the provided reference segmentations seemed to set a low upper bound on the numerical performance. However, this process should be applied in 3D to quantify the renal cortex volume, which would require a marked labelling effort to train the networks.", "openArchiveArticle": "true", "prism:coverDate": "2019-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S2211568419300579", "dc:creator": [{"@_fa": "true", "$": "Couteaux, V."}, {"@_fa": "true", "$": "Si-Mohamed, S."}, {"@_fa": "true", "$": "Renard-Penna, R."}, {"@_fa": "true", "$": "Nempont, O."}, {"@_fa": "true", "$": "Lefevre, T."}, {"@_fa": "true", "$": "Popoff, A."}, {"@_fa": "true", "$": "Pizaine, G."}, {"@_fa": "true", "$": "Villain, N."}, {"@_fa": "true", "$": "Bloch, I."}, {"@_fa": "true", "$": "Behr, J."}, {"@_fa": "true", "$": "Bellin, M.-F."}, {"@_fa": "true", "$": "Roy, C."}, {"@_fa": "true", "$": "Rouvi\u00e8re, O."}, {"@_fa": "true", "$": "Montagne, S."}, {"@_fa": "true", "$": "Lassau, N."}, {"@_fa": "true", "$": "Boussel, L."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S2211568419300579"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S2211568419300579"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S2211-5684(19)30057-9", "prism:volume": "100", "prism:publisher": "Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "dc:title": "Kidney cortex segmentation in 2D CT with U-Nets ensemble aggregation", "prism:copyright": "\u00a9 2019 Soci\u00e9t\u00e9 fran\u00e7aise de radiologie. Published by Elsevier Masson SAS.", "openaccess": "1", "prism:issn": "22115684", "prism:issueIdentifier": "4", "dcterms:subject": [{"@_fa": "true", "$": "Renal cortex"}, {"@_fa": "true", "$": "Image segmentation"}, {"@_fa": "true", "$": "Artificial intelligence (AI)"}, {"@_fa": "true", "$": "Computed tomography (CT)"}], "openaccessArticle": "true", "prism:publicationName": "Diagnostic and Interventional Imaging", "prism:number": "4", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "211-217", "prism:endingPage": "217", "pubType": "Original article Computer developments", "prism:coverDisplayDate": "April 2019", "prism:doi": "10.1016/j.diii.2019.03.001", "prism:startingPage": "211", "dc:identifier": "doi:10.1016/j.diii.2019.03.001", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "163", "@width": "150", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "30518", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "88", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "20750", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "89", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "21394", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "145", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "27602", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "72", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "24318", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "120", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "26903", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "596", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "86260", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "220", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "67883", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "222", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "51812", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "362", "@width": "547", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "68115", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "175", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "52344", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "293", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "58976", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1583", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "256243", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "972", "@width": "2421", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "283292", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "981", "@width": "2421", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "253423", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "962", "@width": "1453", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "186179", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "464", "@width": "1417", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "132641", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "778", "@width": "1417", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "178870", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "27", "@width": "69", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "409", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S2211568419300579-am.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "AAM-PDF", "@size": "1839993", "@ref": "am", "@mimetype": "application/pdf"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85063338929"}}