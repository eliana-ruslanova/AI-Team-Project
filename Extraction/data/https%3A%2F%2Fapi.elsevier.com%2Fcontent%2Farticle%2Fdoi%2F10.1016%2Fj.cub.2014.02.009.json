{"scopus-eid": "2-s2.0-84898057935", "originalText": "serial JL 272099 291210 291735 291838 291840 291848 31 Current Biology CURRENTBIOLOGY 2014-03-20 2014-03-20 2014-11-30T13:03:15 1-s2.0-S096098221400147X S0960-9822(14)00147-X S096098221400147X 10.1016/j.cub.2014.02.009 S300 S300.3 FULL-TEXT 1-s2.0-S0960982214X00072 2015-05-15T04:03:07.206406-04:00 0 0 20140331 2014 2014-03-20T00:00:00Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure e-component body acknowledge affil appendices articletitle auth authfirstini authfull authlast highlightsabst misctext primabst pubtype ref specialabst teaserabst 0960-9822 09609822 true 24 24 7 7 Volume 24, Issue 7 20 738 743 738 743 20140331 31 March 2014 2014-03-31 2014 Reports article sco Copyright \u00a9 2014 Elsevier Ltd. AUTOMATICDECODINGFACIALMOVEMENTSREVEALSDECEPTIVEPAINEXPRESSIONS BARTLETT M Results Human Experiments Machine Learning In-Depth Analysis of Computer Vision Accuracy: Static versus Dynamic In-Depth Analysis of Computer Vision Accuracy: Most Important AUs Discussion Main Findings and Implications Acknowledgments Supplemental Information References DARWIN 1872 C EXPRESSIONEMOTIONSINMANANIMALS EKMAN 1989 297 332 P PSYCHOLOGICALMETHODSINCRIMINALINVESTIGATIONEVIDENCE ARGUMENTEVIDENCEABOUTUNIVERSALSINFACIALEXPRESSIONSEMOTION FRANK 1993 83 93 M RINN 1984 52 77 W KUNZ 2011 8730 8738 M BRODAL 1981 A NEUROLOGICALANATOMYINRELATIONCLINICALMEDICINE TSCHIASSNY 1953 677 691 K EKMAN 1982 238 252 P DEPAULO 1996 979 995 B BOND 2006 214 234 C FRANK 1997 1429 1439 M HADJISTAVROPOULOS 1996 251 258 H HILL 2004 415 422 M HILL 2002 135 144 M MCCRYSTAL 2011 1083 1089 K EKMAN 1978 P FACIALACTIONCODINGSYSTEMATECHNIQUEFORMEASUREMENTFACIALMOVEMENT BARTLETT 2010 M HANDBOOKFACEPERCEPTION AUTOMATEDFACIALEXPRESSIONMEASUREMENTRECENTAPPLICATIONSBASICRESEARCHINHUMANBEHAVIORLEARNINGEDUCATION EKMAN 2001 P TELLINGLIESCLUESDECEITINMARKETPLACEPOLITICSMARRIAGE JUNG 2007 433 437 B POOLE 1992 797 805 G EKMAN 1997 331 341 P FACEREVEALS FACIALEXPRESSIONINAFFECTIVEDISORDERS TURK 1999 1784 1788 D BARTLETTX2014X738 BARTLETTX2014X738X743 BARTLETTX2014X738XM BARTLETTX2014X738X743XM Full 2015-04-01T00:29:35Z OA-Window ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ item S0960-9822(14)00147-X S096098221400147X 1-s2.0-S096098221400147X 10.1016/j.cub.2014.02.009 272099 2014-11-30T14:18:38.041635-05:00 2014-03-31 1-s2.0-S096098221400147X-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/MAIN/application/pdf/2aa5c34e4f64ea1559a74d81801efa4b/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/MAIN/application/pdf/2aa5c34e4f64ea1559a74d81801efa4b/main.pdf main.pdf pdf true 820955 MAIN 6 1-s2.0-S096098221400147X-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/PREVIEW/image/png/df96de473365dfdf0eac636d914d50bb/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/PREVIEW/image/png/df96de473365dfdf0eac636d914d50bb/main_1.png main_1.png png 101979 849 656 IMAGE-WEB-PDF 1 1-s2.0-S096098221400147X-gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr4/HIGHRES/image/jpeg/819ee54cfcdb3dab5344cf4f1d969a63/gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr4/HIGHRES/image/jpeg/819ee54cfcdb3dab5344cf4f1d969a63/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 134975 1224 1652 IMAGE-HIGH-RES 1-s2.0-S096098221400147X-gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr3/HIGHRES/image/jpeg/a89315ed35fcc8433585f97ecd13e455/gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr3/HIGHRES/image/jpeg/a89315ed35fcc8433585f97ecd13e455/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 413560 2084 2872 IMAGE-HIGH-RES 1-s2.0-S096098221400147X-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr2/HIGHRES/image/jpeg/71a044807375f57754ea1752181dccfb/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr2/HIGHRES/image/jpeg/71a044807375f57754ea1752181dccfb/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 331898 1618 2872 IMAGE-HIGH-RES 1-s2.0-S096098221400147X-gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr1/HIGHRES/image/jpeg/330589f191035c5dda4c70bd29ed8f82/gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr1/HIGHRES/image/jpeg/330589f191035c5dda4c70bd29ed8f82/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 131653 993 1652 IMAGE-HIGH-RES 1-s2.0-S096098221400147X-fx1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/fx1/HIGHRES/image/jpeg/e11ed7a6b5e81974a8d2f74ddfa3f233/fx1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/fx1/HIGHRES/image/jpeg/e11ed7a6b5e81974a8d2f74ddfa3f233/fx1_lrg.jpg fx1 true fx1_lrg.jpg jpg 269488 1652 1652 IMAGE-HIGH-RES 1-s2.0-S096098221400147X-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr4/DOWNSAMPLED/image/jpeg/d86688d6f3e4eee0c8591105ff5085b9/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr4/DOWNSAMPLED/image/jpeg/d86688d6f3e4eee0c8591105ff5085b9/gr4.jpg gr4 gr4.jpg jpg 25218 276 373 IMAGE-DOWNSAMPLED 1-s2.0-S096098221400147X-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr3/DOWNSAMPLED/image/jpeg/6e50614972bd9be483b0d3a64e77e615/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr3/DOWNSAMPLED/image/jpeg/6e50614972bd9be483b0d3a64e77e615/gr3.jpg gr3 gr3.jpg jpg 71378 471 649 IMAGE-DOWNSAMPLED 1-s2.0-S096098221400147X-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr2/DOWNSAMPLED/image/jpeg/dbcbb3c55e9095556db76781b93f3b63/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr2/DOWNSAMPLED/image/jpeg/dbcbb3c55e9095556db76781b93f3b63/gr2.jpg gr2 gr2.jpg jpg 52694 366 649 IMAGE-DOWNSAMPLED 1-s2.0-S096098221400147X-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr1/DOWNSAMPLED/image/jpeg/c47de233c9c690b9ed8b0aa35484b121/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr1/DOWNSAMPLED/image/jpeg/c47de233c9c690b9ed8b0aa35484b121/gr1.jpg gr1 gr1.jpg jpg 23755 224 373 IMAGE-DOWNSAMPLED 1-s2.0-S096098221400147X-fx1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/fx1/DOWNSAMPLED/image/jpeg/5b299274832398ef786173552ac94350/fx1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/fx1/DOWNSAMPLED/image/jpeg/5b299274832398ef786173552ac94350/fx1.jpg fx1 true fx1.jpg jpg 50065 373 373 IMAGE-DOWNSAMPLED 1-s2.0-S096098221400147X-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr4/THUMBNAIL/image/gif/2ee1bf442a98aee945be9ee65270d754/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr4/THUMBNAIL/image/gif/2ee1bf442a98aee945be9ee65270d754/gr4.sml gr4 gr4.sml sml 4881 162 219 IMAGE-THUMBNAIL 1-s2.0-S096098221400147X-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr3/THUMBNAIL/image/gif/dcec58111885029179702c832c1679ff/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr3/THUMBNAIL/image/gif/dcec58111885029179702c832c1679ff/gr3.sml gr3 gr3.sml sml 6667 159 219 IMAGE-THUMBNAIL 1-s2.0-S096098221400147X-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr2/THUMBNAIL/image/gif/041edda2596d8af0efd7db2ce7390798/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr2/THUMBNAIL/image/gif/041edda2596d8af0efd7db2ce7390798/gr2.sml gr2 gr2.sml sml 6752 123 219 IMAGE-THUMBNAIL 1-s2.0-S096098221400147X-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/gr1/THUMBNAIL/image/gif/ae245ed8cbe42b857d5f678c093421d8/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/gr1/THUMBNAIL/image/gif/ae245ed8cbe42b857d5f678c093421d8/gr1.sml gr1 gr1.sml sml 7414 132 219 IMAGE-THUMBNAIL 1-s2.0-S096098221400147X-fx1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/fx1/THUMBNAIL/image/gif/9eeba5f36c3a52e5a2e95d6270550e44/fx1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/fx1/THUMBNAIL/image/gif/9eeba5f36c3a52e5a2e95d6270550e44/fx1.sml fx1 true fx1.sml sml 11866 164 164 IMAGE-THUMBNAIL 1-s2.0-S096098221400147X-mmc1.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S096098221400147X/mmc1/MAIN/application/pdf/5dec4e31064895c73ec0b8e12db52abc/mmc1.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S096098221400147X/mmc1/MAIN/application/pdf/5dec4e31064895c73ec0b8e12db52abc/mmc1.pdf mmc1 mmc1.pdf pdf 158705 APPLICATION CURBIO 10957 S0960-9822(14)00147-X 10.1016/j.cub.2014.02.009 Elsevier Ltd Figure 1 Example of Facial Action Coding Here, a facial expression of pain is coded in terms of eight component facial actions based on FACS. Figure 2 System Overview Face video is processed by the computer vision system, CERT, to measure the magnitude of 20 facial actions over time. The CERT output at the top is a sample of real pain, whereas the sample on the bottom shows the same three actions for faked pain from the same subject. Note that these facial actions are present in both real and faked pain, but their dynamics differ. Expression dynamics were measured with a bank of eight temporal Gabor filters and expressed in terms of bags of temporal features. These measures were passed to a machine learning system (nonlinear SVM) to classify real versus faked pain. The classification parameters were learned from the 24 1 min examples of real and faked pain. Figure 3 Bags of Temporal Features Here, we illustrate an exemplar of one stimulus as it is processed by each step. (A) Sample CERT signals from one subject (black circles indicate the time point of the face image shown in Figure 2). Three seconds of data are illustrated, but processing is performed on the full 60 s of video. (B) The CERT signals were filtered by temporal Gabor filters at eight frequency bands. (C) Filter outputs for one facial action (brow lower) and one temporal frequency band (the highest frequency). (D) Zero crossings are detected, and area under the curve and area over the curve are calculated. The descriptor consists of histograms of area under the curve for positive regions and separate histograms for area over the curve for negative regions. (Negative output is where evidence indicates absence of the facial action.) (E) Full bag of temporal features for one action (brow lower). Consists of eight pairs of histograms, one per filter. Figure 4 Contribution of Temporal Information Classification performance (A\u2032) is shown for temporal integration window sizes ranging from 10 s to 60 s. Samples were pooled across temporal position for training an SVM. The region above the shaded region is statistically significant at the p < 0.05 level. Error bars show 1 SE of the mean. Report Automatic Decoding of Facial Movements Reveals Deceptive Pain Expressions Marian Stewart Bartlett 1 \u2217 mbartlett@ucsd.edu Gwen C. Littlewort 1 Mark G. Frank 2 Kang Lee 3 \u2217\u2217 kang.lee@utoronto.ca 1 Institute for Neural Computation, University of California, 9500 Gilman Drive, MC 0440, La Jolla, San Diego, CA 92093-0440, USA 2 Department of Communication, University at Buffalo, State University of New York, Baldy Hall, Buffalo, NY 14260-1020, USA 3 Dr. Erick Jackman Institute of Child Study, University of Toronto, 45 Walmer Road, Toronto, ON M5R 2X2, Canada \u2217 Corresponding author \u2217\u2217 Corresponding author Published: March 20, 2014 Summary In highly social species such as humans, faces have evolved to convey rich information for social interaction, including expressions of emotions and pain [1\u20133]. Two motor pathways control facial movement [4\u20137]: a subcortical extrapyramidal motor system drives spontaneous facial expressions of felt emotions, and a cortical pyramidal motor system controls voluntary facial expressions. The pyramidal system enables humans to simulate facial expressions of emotions not actually experienced. Their simulation is so successful that they can deceive most observers [8\u201311]. However, machine vision may be able to distinguish deceptive facial signals from genuine facial signals by identifying the subtle differences between pyramidally and extrapyramidally driven movements. Here, we show that human observers could not discriminate real expressions of pain from faked expressions of pain better than chance, and after training human observers, we improved accuracy to a modest 55%. However, a computer vision system that automatically measures facial movements and performs pattern recognition on those movements attained 85% accuracy. The machine system\u2019s superiority is attributable to its ability to differentiate the dynamics of genuine expressions from faked expressions. Thus, by revealing the dynamics of facial action through machine vision systems, our approach has the potential to elucidate behavioral fingerprints of neural control systems involved in emotional signaling. Graphical Abstract Highlights \u2022 Untrained human observers cannot differentiate faked from genuine pain expressions \u2022 With training, human performance is above chance but remains poor \u2022 A computer vision system distinguishes faked from genuine pain better than humans \u2022 The system detected distinctive dynamic features of expression missed by humans Bartlett et al. show that humans can fake expressions of pain such that others cannot detect their deceit at rates much better than guessing; however, a novel machine vision system that automatically decodes the dynamics of facial expression can successfully differentiate between genuine and faked pain expressions with 85% accuracy. Results Human Experiments To test both human observers\u2019 and our computer vision system\u2019s abilities to discriminate real versus faked emotional expressions, we created two sets of videos. One set contained faces of individuals experiencing genuine pain as induced through a \u201ccold pressor\u201d method [12], whereas the other set contained faces of the same individuals pretending to be in pain. Expressions of pain were chosen because pain is a universally experienced emotive physiological state [12\u201315]. Additionally, both genuine and faked expressions of pain can be readily elicited using the cold pressor method, which is a routine experimental procedure used to induce pain for research purposes [12]. Stimulus subjects either experienced genuine pain while submerging their arm in ice water (5\u00b0C) for 1 min or were instructed to fake pain while submerging their arm in warm water (20\u00b0C) for 1 min. Facial expressions in the two conditions were video recorded. In experiment 1, we showed 170 human observers video clips of the stimulus subjects individually in a randomized order. The observers judged whether the expression shown in the video clip was real or faked. The observers distinguished genuine pain from faked pain at rates no greater than guessing (accuracy = 51.9%; SD = 14.6; chance accuracy = 50%). Experiment 2 examined whether training could improve human observers\u2019 detection accuracy. We showed 35 new participants 24 video pairs in a training procedure to match the cross-validation training of the computer vision system described below. Observers were presented with two videos of the same person shown sequentially. In one video the individual was expressing genuine pain, and in the other the individual was expressing faked pain. Observers then judged which video of the pair showed the individual expressing genuine pain and which video showed the individual expressing faked pain. Observers received feedback about their accuracy immediately. After being trained on all 24 pairs, participants saw, in random order, 20 new videos of 20 new stimulus subjects for the test phase. Half of these new videos displayed stimulus subjects with faked pain, and the other half displayed stimulus subjects with real pain. Observers judged whether the expression shown in each of the 20 videos was real or faked, with no feedback offered. This test phase assessed whether human observers could generalize what they had learned to detect new exemplars of genuine or faked pain expressions. In the first third of the training trials (eight trials), the accuracy was 49.6% (SD = 11%). The accuracy rate for the last third of the training trials was 58.6% (SD = 8.5%), which was significantly above chance (t[34] = 2.45, p < 0.01) and showed a significant, albeit small, improvement over earlier training trial blocks (t[34] = 2.22, p < 0.02). In the test phase, detection accuracy remained just above chance level at 54.6% (SD = 15.5%) (t[34] = 1.76, p < 0.05). Thus, results from the two human experiments together suggest that human observers are generally unsuccessful at detecting differences between real and faked pain. There was a small improvement with training, but performance remained below 60%. This result is highly consistent with prior research [14]. Machine Learning We then presented these videos to a computer vision system called the Computer Expression Recognition Toolbox (CERT). CERT is a fully automated system that analyzes facial muscle movements from video in real time [16]. It automatically detects frontal faces in video and codes each frame with respect to a set of continuous dimensions, including facial muscular actions from the Facial Action Coding System (FACS) [17]. FACS is a system for objectively scoring facial expressions in terms of elemental facial movements, called action units (AUs). This makes FACS fully comprehensive, given its basis in functional neuroanatomical actions. CERT can identify 20 out of a complete set of 46 AUs, each with its own movement and appearance characteristics (Figure 1 ). FACS was originally developed for manual coding by human experts. Manual coding is laborious and can take up to 3 hr to manually code 1 min of behavior, but CERT instantaneously outputs facial-movement information in real time (i.e., every 1/30th s). Furthermore, the frame-by-frame CERT output provides information on facial-expression intensity and dynamics at temporal resolutions that were previously impractical with human coding. CERT was developed at the University of California, San Diego, and is presently available at Emotient. We used a pattern-recognition approach to assess CERT\u2019s ability to detect falsified pain expressions (Figure 2 ). The 60 s videos were input into the computer vision system one at a time. A set of dynamic descriptors was extracted from the output for each of the 20 AUs. The descriptors consisted of \u201cbags of temporal features\u201d (Figure 3 ). See Supplemental Experimental Procedures. Two sets of descriptors were employed: one set that describes the dynamics of facial-movement events (event descriptors) and another set that describes the intervals between events (interval descriptors). Our methods for constructing bags of temporal features represent a novel approach that can be applied generally to describe signal dynamics for other pattern-recognition problems. Bags of temporal features build upon the concept of bags of features to provide sensitivity to some aspects of the signal (such as edges or peaks at different scales) while providing invariance to aspects of the signal across which we wish to generalize, such as the specific location or time point at which the signal occurs. Next, a classifier was trained to discriminate real pain from faked pain using these descriptors. The classifier was a support vector machine (SVM). The SVM combined information from multiple AUs. This was accomplished with a sequential feature selection procedure. In this procedure, the model began with the AU that gave the best individual classification accuracy. We then added to the model the AU that gave the best performance when combined with the previously selected AUs. This process was repeated until detection performance stopped improving. The selected AUs were the event descriptors for brow lower (AU 4), lip raise (AU 10), lip corner depress (AU 15), and lower lid tighten (AU 7) and the combined event and interval descriptors for mouth opening (AU 26). Because of the post hoc nature of sequential feature selection, this model was tested with double cross-validation (see Supplemental Experimental Procedures). Like the human judges, the computer system was tested on each video individually, without the ability to compare real and faked data from a given stimulus subject. The system achieved detection accuracy as measured by area under the ROC curve (A\u2032) of 0.91 for detection of real versus faked pain (see Supplemental Experimental Procedures regarding A\u2032). In terms of percent correct (PC), this corresponds to 85% correct classification at equal error rate and is significantly better than chance (Z = 3.7, p < 0.0001). Most importantly, the system outperforms human judges even after training (Z = 3.3, p < 0.001, two-tailed Z test). In-Depth Analysis of Computer Vision Accuracy: Static versus Dynamic To test the contribution of temporal information to classification accuracy, we examined the effect of the size of the time window on classification performance. We retrained the above five-AU system on sliding time windows of varying length. Classification performance was tested for temporal integration window sizes ranging from 3.3 s (100 frames) to 60 s. Figure 4 shows that the classification accuracy increased systematically with the size of the time window. To ascertain whether this time-related increase in accuracy was due to the nature of the temporal dynamics rather than the simple accumulation of information, we had an SVM classifier with the five selected AUs trained on the same frame-by-frame CERT outputs but with the order of the frames randomized. It did not perform at above-chance levels (A\u2032 = 0.58, PC = 56%, Z = 0.6, p = 0.27). A static system that did not extract temporal features but instead measured facial actions in each video frame, histogrammed them, and then passed them to an SVM was also tested. The static system was able to detect faked expressions of pain marginally above chance levels (A\u2032 = 0.72, PC = 66%, Z = 1.6, p < 0.05). However, this 66% accuracy rate for the static system was significantly poorer than the accuracy rate for the dynamic system reported above (85%; Z = 2.2, p < 0.05). These findings show that dynamic temporal information contributes substantially to the system\u2019s performance. In-Depth Analysis of Computer Vision Accuracy: Most Important AUs Next, we attempted to identify the key features for differentiating real pain from faked pain. The feature selection procedure identified which facial actions, in combination, provided the most information for detecting faked pain. We then examined which individual action units can reliably differentiate real pain from faked pain. Twenty SVM classifiers, one per action unit, were individually trained to differentiate genuine pain from faked pain using the dynamic descriptors of a single action unit. The most informative AU for differentiating real pain from faked pain was mouth opening (AU 26) using both the event and interval descriptors (A\u2032 = 0.85, PC = 72%, Z = 2.3, p < 0.01).Three other AUs individually differentiated genuine pain from faked pain at above-chance levels using just the event descriptors: lip raise (AU 10), lip press (AU 24), and brow lower (corrugator muscle; AU 4) (A\u2032 = 0.75, 0.73, and 0.73, respectively; PC = 68%, 66%, and 66%, respectively; Z = 1.8, 1.7, and 1.7, respectively; p < 0.05). All other AUs were at chance levels. Mouth opening was the single most informative feature for discriminating genuine expressions of pain from faked expressions of pain. This feature contained dynamic information about mouth openings as well as the intervals between mouth openings. This finding led us to explore how mouth-opening dynamics differ in genuine versus faked expressions of pain using some simple statistics on the unfiltered CERT output. First, there was no difference in the overall mean CERT output for mouth opening between real versus faked expressions of pain (t[24] = 0.006, p = 0.99), implying that the crucial information was in the dynamics. A measure of the duration of mouth openings (\u03c4), as well as an estimate of the temporal intervals between mouth openings, was then extracted (see Supplemental Experimental Procedures). There was a difference in the mean duration of mouth openings for genuine and faked expressions, with faked expressions being 5.4 frames shorter than genuine expressions on average and with the interval between mouth openings lasting 11.5 frames less for faked expressions on average (t[24] = 2.23 and t[24] = 2.19, respectively; both p < 0.05). The variance of \u03c4 was then computed for faked and genuine expressions. A within-subjects comparison showed that the variance of \u03c4 was 55% less for faked than for genuine expressions of pain (t[24] = 2.7, p < 0.01). Similarly, the variance of the interval length between mouth openings was 56% less for faked than for genuine expressions of pain (t[24] = 2.11, p < 0.05). Discussion We show for the first time that a fully automated computer vision system can be trained to detect a deceptive emotional physiological state\u2014faked expressions of pain\u2014from facial cues alone. The rate of accurate discrimination by the computer vision system was 85%. This is far superior to the accuracy of human observers, regardless of whether they have received training, which is consistently below 60% accuracy. This is a significant milestone for machine vision systems [18] because although computers have long outperformed humans at logic processes (such as playing chess), they have significantly underperformed compared to humans at perceptual processes, rarely reaching even the level of a human child. Furthermore, our computer vision approach has led to the discovery of new information about facial behavior in falsified pain expressions. The single most predictive feature of falsified expressions of pain is the dynamics of the mouth opening, which alone could differentiate genuine expressions of pain from deceptive expressions of pain at a detection rate of 72%. Faked expressions were associated with a reduction in variance in terms of both the duration of mouth openings and the duration of the interval between mouth openings. In other words, the action was repeated at intervals that were too regular. The critical feature for faked pain may be this over-regularity of the dynamics of the mouth-opening action. Further investigations will explore whether over-regularity is a general feature of faked expressions. Our findings further support previous research on human facial expressions that has shown that the dynamics of expression are important distinguishing characteristics of emotional expressions, such as a genuine smile versus a faked smile [8]. This difference stems from the fact that expressions are mediated by two distinct neural systems, each one originating in a different area of the brain [4\u20137]. A genuinely felt or experienced emotion originates in the subcortical areas of the brain and is involuntarily propelled onto the face via the extrapyramidal motor system [4\u20137]. In contrast, posed or faked expressions originate in the cortical motor strip and are voluntarily expressed in the face via the pyramidal motor system. Research documenting these differences was sufficiently reliable to become the primary diagnostic criteria for certain brain lesions prior to modern imaging methods (e.g., [4, 6, 7]). These two systems may correspond to the distinction between biologically driven versus socially learned facial behaviors [8]. The facial expressions mediated by these two pathways have been shown to differ in some dynamic properties. Extrapyramidal motor system-based expressions have been associated with synchronized, smooth, symmetrical, and ballistic-like facial muscle movements, whereas pyramidal motor system-based expressions are subject to volitional real-time control and tend to be less smooth, less synchronized, and less symmetric [3]. Accordingly, smiles that were spontaneously generated have been shown to have smoother dynamics, as well as faster onset and offset velocity, than smiles that are posed or faked [3, 8]. Here, we show a new difference in variance between the two systems. Pyramidally driven expression of falsified pain showed a reduced variance in the timing of mouth openings relative to the spontaneous expressions of pain driven by the extrapyramidal system. A survey study revealed that humans have an intuitive knowledge of differences between controlled and automatic responses to pain [15]. However, our findings show that despite this understanding, people could not detect differences between controlled and automatic facial responses when presented with them visually. In highly social species such as humans and other primates, the face has evolved to convey a rich array of information for social interaction. Although facial expressions are mainly evolved as cooperative social signals to communicate one\u2019s genuinely felt emotions to others, and hence behavioral intentions [1], sometimes individuals may wish to control their expressions to mislead others. Indeed, deceptions are a part of everyday life [8, 9, 19], and there are considerable adaptive advantages, including social acceptance, to deliberately manipulating, suppressing, and dissembling emotional expressions [9]. Such voluntary facial control may have been refined (for adaptive purposes or for the purposes of being polite and facilitating interaction) so much that it made it very difficult for observers to discern honest signals from controlled or falsified ones. In studies of deception, untrained human judges are typically only accurate at or near chance levels when detecting deceptive facial behaviors [11]. This inaccuracy persists despite the fact that (albeit imperfect) diagnostic signals exist [12]. In some domains, genuine and faked expressions of emotion have shown not only morphological differences but also dynamic differences. Although human judges were better than chance at detecting these morphological markers, they were unable to detect spatiotemporal dynamic markers [8]. Specifically, with regard to pain, lay adults and even experienced physicians cannot reliably differentiate real expressions of pain from faked expressions of pain [13, 14, 20, 21] (D.M. Siaw-Asamoah, 2011, North American Primary Care Physicians Research Group, conference). As shown in experiment 2 and by others [13], immediate feedback might enable perceptual learning and improve detection accuracy to above-chance levels. However, accuracy remains modest. Previous research using a laborious manual coding method has shown that there is no telltale facial action that can indicate faked pain by its presence or absence because real and faked expressions of pain include the same set of facial actions. However, these earlier studies hinted at differences in the dynamics of facial expression [14]. In the current study, the computer vision system was able to analyze facial-expression dynamics at a much higher temporal resolution and with richer description than was feasible with manual coding methods. Thus, it revealed aspects of a pain expression that have been previously unavailable to observers. Taken together, our findings suggest that in spite of the pyramidal motor system\u2019s sophisticated voluntary control over facial expressions, its control is imperfect; the system cannot fully replicate the genuine expressions of pain driven by the extrapyramidal motor system, particularly in their dynamics. Thus, our findings support the hypothesis that information that can differentiate experienced spontaneous expressions of emotion driven by the extrapyramidal motor system from posed or falsified expressions controlled by the pyramidal motor system exists, particularly in facial dynamics [4]. Although the present study addressed one psychophysiological state\u2014pain\u2014, the approach presented here may be generalizable to the comparison of other genuine and faked emotional states, which may differentially activate the cortical and subcortical facial motor pathways. Thus, our automated facial-movement coding system provides a new paradigm for the study of facial dynamics and has the potential to elucidate behavioral fingerprints of neural control systems involved in emotional signaling. There are some practical implications of the present findings. Falsified pain can be a lie told by patients to their physicians so that they can commit insurance fraud or receive unneeded prescription narcotics. Some healthcare professionals perceive such lies to be common (D.M. Siaw-Asamoah, 2011, North American Primary Care Physicians Research Group, conference), whereas others perceive them to be relatively rare. Our findings suggest that it might be possible to train physicians to specifically attend to mouth-opening dynamics to improve physicians\u2019 ability to differentiate real pain from faked pain. In addition to detecting pain malingering, our computer vision approach may be used to detect other real-world deceptive actions in the realm of homeland security, psychopathology, job screening, medicine, and law. Like pain, these scenarios also generate strong emotions, along with attempts to minimize, mask, and fake such emotions [19], which may involve dual control of the face. In addition, our computer vision system can be applied to detect states in which the human face may provide important clues about health, physiology, emotion, or thought, such as drivers\u2019 expressions of sleepiness [16, 18] and students\u2019 expressions of attention and comprehension of lectures [18], or to track response to treatment of affective disorders [22]. Several limitations to our findings should be noted. First, the pain manipulation task (cold pressor) is a good, but not perfect, analog to all varieties of clinical pain. Thus, future research will be needed in order to explore expression of other types of pain (e.g., back pain) collected in clinical settings. Second, pain is a complicated concept involving attitudes, movements elsewhere in the body, and so forth [23]. This paper addresses just one element (facial expression) and shows proof of principle. Future studies will need to address the other elements of the pain phenomenon. Main Findings and Implications In summary, the present study demonstrated the effectiveness of a computer vision and pattern-recognition system for detecting faked pain from genuine expressions. The computer system outperformed human observers, achieving significantly better accuracy. Moreover, the automated system revealed new information about facial dynamics that differentiate real expressions of pain from faked expressions of pain. Our findings demonstrate the ability of the computer system to extract information from spatiotemporal facial-expression signals that humans either cannot or do not extract. Automated systems such as CERT may bring about a paradigm shift by analyzing facial behavior at temporal resolutions previously not feasible with manual coding methods. This novel approach has succeeded in illuminating basic questions pertaining to the many social situations in which the behavioral fingerprint of neural control systems may be relevant. Acknowledgments The human subject experiments were approved by the Research Ethics Committee of the University of Toronto. Support for this work was provided by NSF grants CNS-0454233, SMA 1041755, NSF ADVANCE award 0340851, and NIH grant R01 HD047290. M.S.B. and G.L. are founders, employees, and shareholders of Emotient, a company that may potentially benefit from the research results. The terms of this arrangement have been reviewed and approved by the University of California, San Diego, in accordance with its conflict-of-interest policies. M.G.F. and the University at Buffalo are minority owners of the software used in this paper and may potentially benefit from this research. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or National Institutes of Health. Supplemental Information Supplemental Information includes Supplemental Experimental Procedures and can be found with this article online at http://dx.doi.org/10.1016/j.cub.2014.02.009. Supplemental Information Document S1. Supplemental Experimental Procedures References 1 C. Darwin The Expression of the Emotions in Man and Animals 1872 Murray London 2 P. Ekman The argument and evidence about universals in facial expressions of emotion D.C. Raskin Psychological Methods in Criminal Investigation and Evidence 1989 Springer Publishing Company New York 297 332 3 M.G. Frank P. Ekman W.V. Friesen Behavioral markers and recognizability of the smile of enjoyment J. Pers. Soc. Psychol. 64 1993 83 93 4 W.E. Rinn The neuropsychology of facial expression: a review of the neurological and psychological mechanisms for producing facial expressions Psychol. Bull. 95 1984 52 77 5 M. Kunz J.I. Chen S. Lautenbacher E. Vachon-Presseau P. Rainville Cerebral regulation of facial expressions of pain J. Neurosci. 31 2011 8730 8738 6 A. Brodal Neurological Anatomy: In Relation to Clinical Medicine 1981 Oxford University Press New York 7 K. Tschiassny Eight syndromes of facial paralysis and their significance in locating the lesion Ann. Otol. Rhinol. Laryngol. 62 1953 677 691 8 P. Ekman W. Friesen Felt, false, and miserable smiles J. Nonverbal Behav. 6 1982 238 252 9 B.M. DePaulo D.A. Kashy S.E. Kirkendol M.M. Wyer J.A. Epstein Lying in everyday life J. Pers. Soc. Psychol. 70 1996 979 995 10 C.F. Bond Jr. B.M. DePaulo Accuracy of deception judgments Pers. Soc. Psychol. Rev. 10 2006 214 234 11 M.G. Frank P. Ekman The ability to detect deceit generalizes across different types of high-stake lies J. Pers. Soc. Psychol. 72 1997 1429 1439 12 H.D. Hadjistavropoulos K.D. Craig T. Hadjistavropoulos G.D. Poole Subjective judgments of deception in pain expression: accuracy and errors Pain 65 1996 251 258 13 M.L. Hill K.D. Craig Detecting deception in facial expressions of pain: accuracy and training Clin. J. Pain 20 2004 415 422 14 M.L. Hill K.D. Craig Detecting deception in pain expressions: the structure of genuine and deceptive facial displays Pain 98 2002 135 144 15 K.N. McCrystal K.D. Craig J. Versloot S.R. Fashler D.N. Jones Perceiving pain in others: validation of a dual processing model Pain 152 2011 1083 1089 16 Bartlett, M.S., Littlewort, G., Frank, M., Lainscsek, C., Fasel, I., and Movellan, J. (2005). Recognizing Facial Expression: Machine Learning and Application to Spontaneous Behavior. IEEE International Conference on Computer Vision and Pattern Recognition, 568\u2013573. 17 P. Ekman W. Friesen Facial Action Coding System: A Technique for the Measurement of Facial Movement 1978 Consulting Psychologists Press Palo Alto, CA 18 M. Bartlett J. Whitehill Automated facial expression measurement: recent applications to basic research in human behavior, learning, and education A.J. Calder G. Rhodes J.V. Haxby M.H. Johnson Handbook of Face Perception 2010 Oxford University Press New York 19 P. Ekman Telling Lies: Clues to Deceit in the Marketplace, Politics, and Marriage 2001 W.W. Norton New York 20 B. Jung M.M. Reidenberg Physicians being deceived Pain Med. 8 2007 433 437 21 G.D. Poole K.D. Craig Judgments of genuine, suppressed, and faked facial expressions of pain J. Pers. Soc. Psychol. 63 1992 797 805 22 P. Ekman D. Matsumoto W. Friesen Facial expression in affective disorders P. Ekman E.L. Rosenberg What the Face Reveals 1997 Oxford University Press New York 331 341 23 D.C. Turk A. Okifuji Assessment of patients\u2019 reporting of pain: an integrated perspective Lancet 353 1999 1784 1788", "scopus-id": "84898057935", "pubmed-id": "24656830", "coredata": {"eid": "1-s2.0-S096098221400147X", "dc:description": "Summary In highly social species such as humans, faces have evolved to convey rich information for social interaction, including expressions of emotions and pain [1\u20133]. Two motor pathways control facial movement [4\u20137]: a subcortical extrapyramidal motor system drives spontaneous facial expressions of felt emotions, and a cortical pyramidal motor system controls voluntary facial expressions. The pyramidal system enables humans to simulate facial expressions of emotions not actually experienced. Their simulation is so successful that they can deceive most observers [8\u201311]. However, machine vision may be able to distinguish deceptive facial signals from genuine facial signals by identifying the subtle differences between pyramidally and extrapyramidally driven movements. Here, we show that human observers could not discriminate real expressions of pain from faked expressions of pain better than chance, and after training human observers, we improved accuracy to a modest 55%. However, a computer vision system that automatically measures facial movements and performs pattern recognition on those movements attained 85% accuracy. The machine system\u2019s superiority is attributable to its ability to differentiate the dynamics of genuine expressions from faked expressions. Thus, by revealing the dynamics of facial action through machine vision systems, our approach has the potential to elucidate behavioral fingerprints of neural control systems involved in emotional signaling.", "openArchiveArticle": "true", "prism:coverDate": "2014-03-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S096098221400147X", "dc:creator": [{"@_fa": "true", "$": "Bartlett, Marian Stewart"}, {"@_fa": "true", "$": "Littlewort, Gwen C."}, {"@_fa": "true", "$": "Frank, Mark G."}, {"@_fa": "true", "$": "Lee, Kang"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S096098221400147X"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S096098221400147X"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0960-9822(14)00147-X", "prism:volume": "24", "prism:publisher": "Elsevier Ltd.", "dc:title": "Automatic Decoding of Facial Movements Reveals Deceptive Pain Expressions", "prism:copyright": "Copyright \u00a9 2014 Elsevier Ltd.", "openaccess": "1", "prism:issn": "09609822", "prism:issueIdentifier": "7", "openaccessArticle": "true", "prism:publicationName": "Current Biology", "prism:number": "7", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "738-743", "prism:endingPage": "743", "pubType": "Report", "prism:coverDisplayDate": "31 March 2014", "prism:doi": "10.1016/j.cub.2014.02.009", "prism:startingPage": "738", "dc:identifier": "doi:10.1016/j.cub.2014.02.009", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "high", "@height": "1224", "@width": "1652", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "134975", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2084", "@width": "2872", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "413560", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1618", "@width": "2872", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "331898", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "993", "@width": "1652", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "131653", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1652", "@width": "1652", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-fx1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "269488", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "276", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25218", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "471", "@width": "649", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "71378", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "366", "@width": "649", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "52694", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "224", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "23755", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "373", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-fx1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "50065", "@ref": "fx1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "162", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4881", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "159", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6667", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "123", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6752", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "132", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7414", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "164", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-fx1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11866", "@ref": "fx1", "@mimetype": "image/gif"}, {"@category": "standard", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S096098221400147X-mmc1.pdf?httpAccept=%2A%2F%2A", "@multimediatype": "Acrobat PDF file", "@type": "APPLICATION", "@size": "158705", "@ref": "mmc1", "@mimetype": "application/pdf"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84898057935"}}