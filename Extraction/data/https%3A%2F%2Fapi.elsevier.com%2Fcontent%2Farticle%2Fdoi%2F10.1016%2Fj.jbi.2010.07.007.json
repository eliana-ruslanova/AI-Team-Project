{"scopus-eid": "2-s2.0-78649332322", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2010-07-27 2010-07-27 2010-11-18T22:11:02 1-s2.0-S1532046410001061 S1532-0464(10)00106-1 S1532046410001061 10.1016/j.jbi.2010.07.007 S300 S300.1 FULL-TEXT 1-s2.0-S1532046410X00089 2015-05-15T06:30:58.184067-04:00 0 0 20101201 20101231 2010 2010-07-27T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content oa subj ssids 1532-0464 15320464 43 43 6 6 Volume 43, Issue 6 14 962 971 962 971 201012 December 2010 2010-12-01 2010-12-31 2010 Regular Articles article fla Copyright \u00a9 2010 Elsevier Inc. Published by Elsevier Inc. All rights reserved. AUTOMATICALLYEXTRACTINGINFORMATIONNEEDSCOMPLEXCLINICALQUESTIONS CAO Y 1 Introduction 2 Background 3 Data 4 Methods 4.1 General topics identification 4.1.1 Supervised machine-learning systems 4.1.2 Machine-learning features 4.1.3 Training and testing 4.1.4 Evaluation metrics 4.2 Keyword identification 4.2.1 Unsupervised approaches 4.2.2 Supervised machine-learning 4.2.2.1 Machine-learning algorithms 4.2.2.2 Learning features 4.2.2.3 Evaluation 4.3 Error analysis 5 Results 6 Discussion 7 Conclusions and future work Acknowledgments References ELY 1999 358 361 J TIMPKA 1990 23 29 T BERGUS 2000 541 547 G ELY 1992 265 269 J OSHEROFF 1991 576 581 J COVELL 1985 596 599 D SMITH 1996 1062 1068 R HERSH 1998 1347 1352 W HERSH 1996 A14 A16 W WESTBROOK 2004 113 120 J WESTBROOK 2005 315 321 J GOSLING 2004 391 401 A ELY 2002 710 713 J ELY 2005 217 224 J HOOGENDAM 2008 e29 A CULLEN 2002 370 379 R STEPHENS 2009 259 264 M TANG 2006 1143 1145 H KITCHIN 2007 1113 1120 D PURCELL 2002 557 558 G JADAD 1998 611 614 A SILBERG 1997 1244 1245 W GLENNIE 2006 25 33 E CHILDS 2005 80 96 S GRIFFITHS 2000 1511 1515 K CLINE 2001 671 692 R BENIGERI 2003 381 386 M WYATT 1997 1879 1881 J MCCLUNG 1998 E2 H DELEO 2006 902 G FREEMAN 2009 478 484 M FLETCHER 1997 S5 S14 R HERSH 2002 283 293 W YU 2006 834 838 H LEE 2006 469 473 M YU 2007 236 251 H YU 2008 96 100 H KIM 2009 327 331 D AGARWAL 2009 6 10 S YU 2007 328 339 H CIMINO 2008 116 120 J HUANG 2006 359 363 X DEMNERFUSHMAN 2007 63 103 D CIMINO 2008 1203 1204 J CIMINO 2004 277 281 J CIMINO 2003 815 J CIMINO 2003 175 179 J CIMINO 2002 170 174 J CIMINO 1997 528 532 J WILCZYNSKI 2001 393 N SNEIDERMAN 2007 772 780 C SRINIVASAN 2002 722 726 P IDE 2007 253 263 N CIMINO 1993 195 206 J SEOL 2004 306 310 Y ELY 2000 429 432 J ELY 1997 382 388 J DALESSANDRO 2004 64 69 D LIN 2007 393 414 J FRIEDMAN 1999 76 C SCHWARTZ 2003 451 462 A YU 2002 262 272 H JONES 1972 11 21 S KLEINBAUM 2005 D LOGISTICREGRESSION BAGLEY 2001 979 985 S RISTAD 1998 522 532 E JOHNSON 1993 294 S CAOX2010X962 CAOX2010X962X971 CAOX2010X962XY CAOX2010X962X971XY 2013-08-22T00:00:27Z Full http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window ElsevierBranded item S1532-0464(10)00106-1 S1532046410001061 1-s2.0-S1532046410001061 10.1016/j.jbi.2010.07.007 272371 2010-11-23T22:32:07.319068-05:00 2010-12-01 2010-12-31 1-s2.0-S1532046410001061-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/MAIN/application/pdf/3663d96257c7fe5eb3dc8162f2429da2/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/MAIN/application/pdf/3663d96257c7fe5eb3dc8162f2429da2/main.pdf main.pdf pdf true 570677 MAIN 10 1-s2.0-S1532046410001061-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/PREVIEW/image/png/eb3f7c5f755023ba6bdf0057be185a2a/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/PREVIEW/image/png/eb3f7c5f755023ba6bdf0057be185a2a/main_1.png main_1.png png 85067 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046410001061-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/STRIPIN/image/gif/8d2ed98a8fc16a6a120ace1621baf923/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/STRIPIN/image/gif/8d2ed98a8fc16a6a120ace1621baf923/si4.gif si4 si4.gif gif 1653 65 306 ALTIMG 1-s2.0-S1532046410001061-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/STRIPIN/image/gif/585d86e70f603e0342f998d6efc281b9/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/STRIPIN/image/gif/585d86e70f603e0342f998d6efc281b9/si3.gif si3 si3.gif gif 2622 42 480 ALTIMG 1-s2.0-S1532046410001061-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/STRIPIN/image/gif/21d5906280a159640309eaf329f7771c/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/STRIPIN/image/gif/21d5906280a159640309eaf329f7771c/si2.gif si2 si2.gif gif 1270 43 250 ALTIMG 1-s2.0-S1532046410001061-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/STRIPIN/image/gif/c4c601db8314bb0f45020e3ae267a373/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/STRIPIN/image/gif/c4c601db8314bb0f45020e3ae267a373/si1.gif si1 si1.gif gif 1208 42 181 ALTIMG 1-s2.0-S1532046410001061-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr2/DOWNSAMPLED/image/jpeg/da650239019fc662d505849513143538/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr2/DOWNSAMPLED/image/jpeg/da650239019fc662d505849513143538/gr2.jpg gr2 gr2.jpg jpg 45030 212 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046410001061-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr2/THUMBNAIL/image/gif/b9e5c76ce5a0c101bf86d04792647f4d/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr2/THUMBNAIL/image/gif/b9e5c76ce5a0c101bf86d04792647f4d/gr2.sml gr2 gr2.sml sml 4222 95 219 IMAGE-THUMBNAIL 1-s2.0-S1532046410001061-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr3/DOWNSAMPLED/image/jpeg/b766a18988bc3c57b80ab2e9e22765e3/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr3/DOWNSAMPLED/image/jpeg/b766a18988bc3c57b80ab2e9e22765e3/gr3.jpg gr3 gr3.jpg jpg 27662 197 576 IMAGE-DOWNSAMPLED 1-s2.0-S1532046410001061-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr3/THUMBNAIL/image/gif/6d32daa93d6e152c74bfefcf099ae5ac/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr3/THUMBNAIL/image/gif/6d32daa93d6e152c74bfefcf099ae5ac/gr3.sml gr3 gr3.sml sml 4334 75 219 IMAGE-THUMBNAIL 1-s2.0-S1532046410001061-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr4/DOWNSAMPLED/image/jpeg/758b96d6f9b9cdb3270063c57132d1f5/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr4/DOWNSAMPLED/image/jpeg/758b96d6f9b9cdb3270063c57132d1f5/gr4.jpg gr4 gr4.jpg jpg 31527 201 734 IMAGE-DOWNSAMPLED 1-s2.0-S1532046410001061-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr4/THUMBNAIL/image/gif/9915da3cf53c6fb5e013717eb0104388/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr4/THUMBNAIL/image/gif/9915da3cf53c6fb5e013717eb0104388/gr4.sml gr4 gr4.sml sml 2605 60 219 IMAGE-THUMBNAIL 1-s2.0-S1532046410001061-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr5/DOWNSAMPLED/image/jpeg/d972dee363ed47c5f855e37bd99a9b0a/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr5/DOWNSAMPLED/image/jpeg/d972dee363ed47c5f855e37bd99a9b0a/gr5.jpg gr5 gr5.jpg jpg 41589 292 380 IMAGE-DOWNSAMPLED 1-s2.0-S1532046410001061-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr5/THUMBNAIL/image/gif/810ecf3c9ac187114257deb84ae0bed7/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr5/THUMBNAIL/image/gif/810ecf3c9ac187114257deb84ae0bed7/gr5.sml gr5 gr5.sml sml 7647 164 213 IMAGE-THUMBNAIL 1-s2.0-S1532046410001061-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr1/DOWNSAMPLED/image/jpeg/450e00f6a16e441183e716d30547a5ab/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr1/DOWNSAMPLED/image/jpeg/450e00f6a16e441183e716d30547a5ab/gr1.jpg gr1 gr1.jpg jpg 13670 40 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046410001061-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046410001061/gr1/THUMBNAIL/image/gif/23a94295edf538bb3f32f0c388bd7384/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046410001061/gr1/THUMBNAIL/image/gif/23a94295edf538bb3f32f0c388bd7384/gr1.sml gr1 gr1.sml sml 1205 18 219 IMAGE-THUMBNAIL YJBIN 1684 S1532-0464(10)00106-1 10.1016/j.jbi.2010.07.007 Elsevier Inc. Fig. 1 AskHERMES\u2019 system architecture. AskHERMES takes as input a question posed by a clinician. Question Analysis automatically extracts information needs. Document Retrieval retrieves relevant documents (MEDLINE and WWW). Answer Extraction automatically identifies the sentences that provide answers to questions. Summarization condenses the text by removing the redundant sentences and by generating a coherent summary. Answer Presentation presents the summary to the user who posed the question. Fig. 2 Three types of indicative keywords and examples. Fig. 3 The classification performance of topic assignment and the corresponding training size (training size=10*Ln (number of training questions)). Fig. 4 (a) Number of questions as a function of number of categories assigned to questions of the ClinicalQuestions Collection; (b) classification performance of topic assignment as a function of number of categories assigned to a question. Fig. 5 (a) Recall, precision, and F1 score of keyword extraction (using CRFs) as a function of question length. (b) Number of questions as a function of question length. Table 1 A typology of question types with representative samples of clinical questions collected by Ely and his associates (1). The left column represents generic question proportions. For example, of the 4654 clinical questions, \u201cWhat\u201d, \u201cHow\u201d, \u201cDo\u201d, and \u201cCan\u201d account for 2231 (or 48%), 697 (or 15%), 320 (or 7%), and 187 (or 4%) of the questions, respectively. Question examples (1\u20136) are in the right column. General question type and (percentage) Sample questions \u201cWhat \u2026\u201d (48%) 1. What is the cause and treatment of this old man\u2019s stomatitis? 2. What should you do with someone who is not getting better from epicondylitis after physical therapy and non-steroidal anti-inflammatory drugs have not worked? \u201cHow \u2026\u201d (15%) 3. How long should you leave a patient on Coumadin and heparin? \u201cDo \u2026\u201d (7%) 4. Do angiotensin II inhibitors work like regular angiotensin converting enzyme inhibitors to preserve kidney function in mild diabetes? \u201cCan \u2026\u201d (4%) 5. Can Lorabid cause headaches? Others (25%) 6. His last (and first) hepatitis B vaccine was last August (6months ago). I\u2019m going to bunch the series in reverse; i.e., give second injection today and 3rd in a month or two. Is that okay to bunch them in reverse? Table 2 The general topics and the number of questions assigned to the 4654 clinical questions. General topics Number of questions (percentage of the total questions) Device 37 (0.8%) Diagnosis 994 (21.4%) Epidemiology 104 (2.2%) Etiology 173 (3.7%) History 42 (0.9%) Management 1403 (30.1%) Pharmacology a 1594 (34.3%) Physical finding 271 (5.8%) Procedure 122 (2.6%) Prognosis 53 (1.1%) Test 746 (16.0%) Treatment and prevention 868 (18.7%) Unspecified 0 (0%) a The original category is \u201cPharmacological\u201d in the ClinicalQuestions Collection. Table 3 Binary classification results\u2212precision (top) and F1 score (bottom), on 10-fold cross-validation for applying support vector machines to automatically assign general topics to ad hoc clinical questions. We explored different combinations of features: bag-of-words (W), stemming (S), top 2000 features (T2000), bag-of-words+bigrams (Bi), bag-of-words+bigrams+part-of-speech (W+Bi+POS), bag-of-words+bigrams+UMLS concepts and semantic types (W+Bi+CSTY), and bag-of-words+bigrams+part-of-speech+UMLS concepts and semantic types (W+Bi+CSTY+POS). The baseline system that randomly assigns a topic is 50%. General topics W (%) S (%) T2000 (%) Words+bigrams (%) W+Bi+POS (%) W+Bi+CSTY (%) W+Bi+CSTY+POS (%) Device 57.8 67.4 61.9 64.8 62.4 71.4 71.2 56.9 65.0 62.4 61.7 61.1 73.0 71.2 Diagnosis 73.1 73.0 73.8 75.6 75.2 76.0 76.4 73.7 73.8 75.2 75.9 75.2 76.8 77.2 Epidemiology 71.8 68.8 65.8 69.2 68.2 72.8 69.7 70.6 68.5 65.8 67.9 68.0 72.2 70.3 Etiology 80.6 84.5 78.6 87.0 82.4 84.8 86.4 79.2 81.6 78.2 82.4 79.7 80.4 82.6 History 57.3 60.1 54.6 55.3 59.9 68.2 63.5 54.3 59.2 51.3 53.8 57.9 67.7 61.7 Management 69.4 68.8 69.9 73.8 73.2 73.1 72.5 68.4 68.1 68.0 71.4 71.4 71.1 71.0 Pharmacology 83.4 83.7 83.6 84.5 84.5 89.7 89.0 82.6 83.0 82.9 84.0 83.8 89.3 88.7 Physical finding 71.9 71.5 69.8 72.1 69.7 77.6 76.2 71.7 72.4 72.7 71.1 69.6 77.8 76.7 Procedure 69.2 70.2 68.2 67.1 66.2 80.4 80.1 70.4 71.3 69.2 66.6 65.4 80.5 80.3 Prognosis 72.8 73.9 66.8 68.9 72.6 73.5 74.3 73.0 74.4 68.4 69.2 73.8 74.3 74.3 Test 79.5 81.3 79.5 81.3 78.7 84.4 83.4 79.1 80.6 79.0 79.2 78.2 83.0 82.4 Treatment and prevention 67.8 68.6 68.8 71.1 70.1 71.8 70.5 68.0 68.8 69.8 70.3 69.6 71.6 70.5 Average 71.2 72.7 70.1 72.6 71.9 77.0 76.1 70.7 72.2 70.2 71.1 71.1 76.5 75.6 Bold indicates the highest performance. Table 4 Performance of keyword extraction. \u201cIDF\u201d indicates keyword prioritization with IDF value; otherwise a random selection of keywords is applied. \u201cUMLS concept\u201d indicates that we only use the text as a keyword if the text can be mapped by MMTx to a UMLS concept. We experimented with selecting all UMLS concepts and then prioritizing with its IDF value. We also report the results of logistic regression and conditional random fields. Method Precision (%) Recall (%) F1 score (%) Random words 11.2 11.6 11.4 Noun phrase 16.5 18.9 17.6 a Noun phrase+IDF 28.0 31.4 29.6 a All UMLS concepts 17.5 95.0 29.5 UMLS concept+IDF 44.3 68.6 53.8 a Logistic regression 68.7 46.0 55.1 Conditional random fields 67.6 50.8 58.0 a F1 score is statistically significant (p <0.01, t-test) compared with the score in the previous method. The F1 score of conditional random fields is statistically significant compared with the UMLS concept+IDF. The difference between logistic regression and conditional random field and between UMLS concept+IDF and logistic regression is not statistically significant. Automatically extracting information needs from complex clinical questions Yong-gang Cao a James J. Cimino b John Ely c Hong Yu a \u204e hongyu@uwm.edu a University of Wisconsin-Milwaukee, 2400 Hartford Avenue, Milwaukee, WI 53201, USA b National Institutes of Health, 10-CRC - Hatfield Clinical Research Center, 6-225110 Center Dr Bethesda, MD USA c Department of Family Medicine, 01291-D PFP, University of Iowa Carver College of Medicine, 200 Hawkins Drive, Iowa City, IA 52242-1097, USA \u204e Corresponding author. Address: 2240 Hartford Avenue, Enderis Hall 939, University of Wisconsin-Milwaukee, Milwaukee, WI 53211, USA. Fax: +1 414 810 0065. Abstract Objective Clinicians pose complex clinical questions when seeing patients, and identifying the answers to those questions in a timely manner helps improve the quality of patient care. We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Our study is motivated in the context of developing the larger clinical question answering system AskHERMES (Help clinicians to Extract and aRrticulate Multimedia information for answering clinical quEstionS). Design and measurements We developed supervised machine-learning systems to automatically assign predefined general categories (e.g. etiology, procedure, and diagnosis) to a question. We also explored both supervised and unsupervised systems to automatically identify keywords that capture the main content of the question. Results We evaluated our systems on 4654 annotated clinical questions that were collected in practice. We achieved an F1 score of 76.0% for the task of general topic classification and 58.0% for keyword extraction. Our systems have been implemented into the larger question answering system AskHERMES. Our error analyses suggested that inconsistent annotation in our training data have hurt both question analysis tasks. Conclusion Our systems, available at http://www.askhermes.org, can automatically extract information needs from both short (the number of word tokens <20) and long questions (the number of word tokens >20), and from both well-structured and ill-formed questions. We speculate that the performance of general topic classification and keyword extraction can be further improved if consistently annotated data are made available. Keywords Natural language processing Question answering Question analysis Keyword extraction 1 Introduction Clinicians have many questions when seeing patients [1], with reports of up to six questions for every patient encounter [2\u20137]. Identifying answers to such questions will support the practice of evidence-based medicine [8,9] and, as a result, will improve the quality of patient care [10\u201312]. Although clinicians have information needs when seeing patients, studies have concluded that clinicians\u2019 information needs were often unmet (e.g. [1,8]) and that a lack of time was the most common obstacle preventing them from meeting these needs [13,14]. Many clinical databases (e.g. UpToDate) provide high quality summaries for answering important medical questions related to patient care. The summaries, however, are written by domain experts who manually review the literature related to specific medical topics. As such, the database may be limited by its scope and timeliness. In addition, accessing clinical databases still requires a significant amount of time. For example, one evaluation study [15] has shown that it takes over 4min to search for answers to clinical questions in UpToDate. Many clinicians use the Internet (e.g. Google) to search for answers to their questions [16\u201319]. However, the Internet poses challenges in information content relatedness and quality [20\u201329]. A survey study [30] found that 92% of physicians preferred a target site rather than a search engine (e.g. Google). A recent study published in 2009 [31] concluded that PubMed appeared to be better than Google Scholar for locating relevant primary literature articles to answer specific drug-related questions. Another recent study [17] reported that for military physicians, open-domain sites (PubMed) are more commonly used by military surgeons. Therefore, searching answers from the published biomedical literature is important for clinicians. Although scientifically strong, clinically relevant, original research articles occur in the highest concentrations in only a few select journals (e.g. The New England Journal of Medicine, Annals of Internal Medicine, JAMA, and Archives of Internal Medicine), much clinical evidence appears in a wide range of other biomedical journals [32]. Without a search engine, it is unlikely for a clinician to keep up with the most recent clinical evidence as reported in an ever-increasing number of volumes in the medical literature. Literature search engines, including PubMed and Google Scholar, do not return answers in response to specific questions; rather, such search engines frequently return a large number of articles in response to a specific user query. Clinicians, however, have limited time for browsing the articles retrieved, and a study observed that clinicians spent on average 2min or less seeking an answer to a question, and that if a search took longer, it was likely to be abandoned [1,33]. To address the aforementioned obstacles, we are building a fully automated system AskHERMES \u2013 Help clinicians Extract and aRticulate Multimedia information from literature to answer their ad hoc clinical quEstionS [34\u201345]. Although it is still at the preliminary stage, AskHERMES is currently the only online system that automatically retrieves, extracts, and integrates information from the literature and other information resources and attempts to formulate this information as answers in response to ad hoc medical questions posed by clinicians, all of which can be achieved within a time-frame that meets their demands. Fletcher [30] identified three basic skills necessary for clinicians to manage their information needs: (1) find potentially relevant information, (2) judge the best from the much larger volume of less credible information, and (3) judge whether the best information retrieved provides sufficient evidence for clinical decisions. AskHERMES attempts to complete the first two tasks by finding and filtering clinical information. We have previously found that AskHERMES advances several other baseline information retrieval systems (e.g. PubMed) for answering definitional questions [39,46]. Currently, AskHERMES attempts to answer all types of clinical questions, with a preliminary capacity. One of the a key difference between AskHERMES and other clinical question-answering related work (e.g. [47\u201350]) is its computational approaches for automatically extracting information needs from the questions, and that is the focus of this study. 2 Background Question answering can be considered an advanced form of information retrieval. A variety of approaches have addressed question answering in the biomedical domain. Zweigenbaum [51,52] surveyed the feasibility of question answering in the biomedical domain. Rinaldi and colleagues [53] adapted an open-domain question answering system to answer genomic questions (e.g. \u201cwhere was spontaneous apoptosis observed?\u201d). The EpoCare project (Evidence at Point of Care) proposed a framework that aimed to provide physicians with the best available medical information from both literature and clinical databases [47]. Infobuttons [48,54\u201363] served as a medical portal to external information retrieval systems (e.g. PubMed) and databases (e.g. UpToDate). A related project is Wilczynski et al. [60] in which a biomedical article can be classified into clinically useful but distinguishing formats (e.g. Original Study and Case Report) and purposes (e.g. Diagnosis, and Treatment) and such classifications have been incorporated into the PubMed. Other approaches related to question answering include SemRep [61,62], which maps biomedical text to the UMLS concepts and represents concept relations with the UMLS semantic relationships (e.g. TREATS, Co-OCCURS_WITH, and OCCURS_IN), and then condenses the concepts and their semantic relations to generate a short summary. Essie is an information retrieval engine developed and used at the NLM that incorporates knowledge-based query expansion and heuristic ranking [63]. CQA-1.0 [61] attempts to capture elements related to EBM (e.g. strength of evidence). In their study, Sneiderman et al. [61] integrated the three systems (SemRep, Essie, and CQA-1.0) to achieve the best information retrieval system (that outperformed each of the three systems) in response to clinical questions. Most systems described above, however, are not available online. To our knowledge, AskHERMES (http://www.askhermes) is the only medical search engine available online that can provide answers in response to ad hoc, complex clinical questions. Fig. 1 shows AskHERMES\u2019 architecture. As shown in Fig. 1, automatically analyzing clinical questions is the first step towards answering clinical questions. Clinicians typically ask complex questions, and there is a wealth of research proposing ways for structuring those ad hoc questions. Ely and colleagues [1] studied the 1396 medical questions they collected in one study (1) to manually map to a set of 69 question types (e.g. \u201cWhat is the cause of symptom X?\u201d and \u201cWhat is the dose of drug X?\u201d) and 63 medical topics (e.g. drug or cardiology). Cimino and associates [64] predefined a set of generic questions (e.g. \u201cWhat is treatment for disease?\u201d) and then mapped ad hoc clinical questions to those generic questions. Seol and associates [65] identified four question types: treatment, diagnosis, etiology, and prognosis. Niu and colleagues [47,66] applied the PICO framework (Problem/Population, Intervention, Comparison, Outcome) to analyze clinical questions. Demner-Fushman and colleagues [48,56\u201361] extracted the PICO components from texts for question answering. Previously, we developed supervised machine-learning techniques to automatically classify medical questions into the evidence taxonomy constructed by Ely and associates [67], and we reported a \u223c50% F1 score for classifying a clinical question into five categories defined by the evidence taxonomy [34,35]. In this paper, we report on models for computationally identifying both the general topics and keywords incorporated in these questions. 3 Data Ely and associates (1) collected thousands of clinical questions from more than 100 family doctors. Until 2009, the National Library of Medicine (NLM) maintained the 4654 questions collected in four studies [1,14,68,69]. Furthermore, each question was annotated by the investigator who recorded the question [1,14,72,73]; a subset of those questions are shown in Table 1 . When the annotated data was released by NLM, each question was assigned one or more general topics, and Table 2 shows the 13 general topics and the number of questions assigned to the 4654 clinical questions. As shown in Table 2, three highest assigned topics are pharmacology (1594), management (1403), and diagnosis (994). Of the total 4654 questions, 3484 were assigned one general topic, 386 were assigned two topics, 700 were assigned three topics, 4 were assigned four topics and 5 were assigned five topics. Seventy-five questions were not assigned any topics. An example of a question that was not assigned any topics is \u201cWhat are some general facts on inflammatory bowel disease?\u201d One question assigned five topics (treatment and prevention, test, diagnosis, pharmacology, and management) was \u201cWhat is the right interval for checking the thyroid stimulating hormone level on patients on thyroid replacement, and if you make a change in dose, when should you check the thyroid stimulating hormone?\u201d In addition, each clinical question was assigned from one to three keywords: 4169 questions were assigned one keyword, 471 were assigned two keywords and 14 were assigned three keywords. For example, the question as shown above was assigned two keywords: thyroid function and tests. 4 Methods The information needs of ad hoc clinical questions can be represented by two means. First, clinical questions can be classified by general topics including etiology, procedure, diagnosis, prognosis, and treatment and prevention. Automatic topic assignment may improve information retrieval. For example, we may return a pre-classified treatment article in response to a treatment question. Secondly, each clinical question incorporates specific topics (or keywords) that indicate the main content of the question. For example, the question \u201cIn this patient with back pain, how do you make a diagnosis of arachnoiditis and how do you treat it?\u201d concerns treatment and diagnosis as general topics, and its keywords are back pain and arachnoiditis. The keywords can be used as query terms for retrieving relevant documents. They can also be used as the anchor terms for answer extraction. We distinguish between extractive keywords and indicative keywords. Extractive keywords are those that appear explicitly in a question. The two extractive keywords for the question \u201cIn this patient with back pain, how do you make a diagnosis of arachnoiditis and how do you treat it?\u201d are back pain and arachnoiditis. Indicative keywords are those that do not appear explicitly in the original question but are implied by other words in the question. For example, insomnia is the assigned keyword for the question \u201cIs Melatonin good for anything? I don\u2019t know anything about Melatonin. I need to know the dose\u201d. In this case, the assignment was based on the knowledge that Melatonin is a medication for insomnia. Fig. 2 shows three types of indicative keywords. In this work, we report on two computational models for identifying general topics and explicit keywords, two types of information needs from ad hoc clinical questions. Automatically identifying indicative keywords is a challenging but crucial task and will comprise much of our future research. We are currently focused on the simpler task of identifying extractive keywords, and we have developed both unsupervised and supervised approaches to automatically identify extractive keywords from complex clinical questions. 4.1 General topics identification We explored supervised machine-learning (ML) approaches to automatically assign a question one or more general topics shown in Table 2. All ML models were trained on the 4654 annotated clinical questions. Since a question can be assigned to multiple topics, a multi-category classifier would prohibit such a multi-category question in inclusion. We therefore developed a binary ML classifier (Yes or No) for each of the 12 topics; we excluded the category unspecified because it was empty. 4.1.1 Supervised machine-learning systems We experimented with several commonly used machine-learning algorithms for question classification, including na\u00efve Bayes, decision tree, and support vector machines (SVMs), and the results of 10-fold cross-validation showed that SVMs performed the best. The results were consistent with our previous studies [34,35]. We therefore report the results for SVMs, the best classifier. For comparison, we also report the results for na\u00efve Bayes. 4.1.2 Machine-learning features We explored different features for machine-learning, including words and n-grams, part-of-speech (POS) and stemming. We used the Stanford Parser (http://nlp.stanford.edu/downloads/lex-parser.shtml) for the POS tagging, as recent research demonstrates a good performance of the Stanford Parser in the biomedical domain [70]. In our previous work, we found that adding the UMLS concepts and semantic types as additional features led to enhanced performance in question classification [34,35]; we therefore mapped terms in questions to the UMLS concepts and semantic types and explored them as additional features. We applied the tool MMTx [71] to identify appropriate UMLS concepts and semantic types in a question string. We also applied mutual information to select top features for question classification. 4.1.3 Training and testing As shown in Table 2, the distribution of clinical questions to different topics is skewed, with a large majority of questions assigned to the top three topics (pharmacology, management, and diagnosis). To compare the performance of different binary classifiers, we arranged a baseline of 50% for each classifier, so that each classifier was trained on the same number of \u201cpositive\u201d and \u201cnegative\u201d questions. For example, when we trained a binary classifier for diagnosis, we had 994 questions that were assigned to this topic (Table 2). This set of 994 questions represents the \u201cpositive\u201d training data. To generate \u201cnegative\u201d training data, we randomly selected 994 questions from among the remaining topics. We report the classification performance by 10-fold cross-validation, in which we divided data into 10-folds and used 9 randomly selected folds for training and the remaining fold for testing. We repeated the classification 10 times and here report the average and standard deviation. 4.1.4 Evaluation metrics We report the performance by recall, precision, and F1 score, all of which are commonly used as evaluation metrics for text categorization, and we report the average F1 scores. Each F1 score (F) is calculated by F =(2*Precision*Recall)/(Precision+Recall), where recall is the number of correctly predicted medical questions divided by the total number of annotated questions in the same category, and precision is the number of correctly predicted medical questions divided by the total number of predicted questions in the same category. 4.2 Keyword identification In the biomedical domain, unsupervised approaches (e.g. pattern matching, or syntactic and semantic grammar-based) have been the foundation of many successful clinical natural language processing systems (e.g. [72,73]). Supervised machine-learning approaches have also shown success (e.g. [74]). However, the success depends upon the quality and the quantity of annotated data (e.g. [75]). Supervised machine-learning approaches may under-perform unsupervised one. For example, term variations (e.g. abbreviations [76]) allow data used for training to be sparse. In this study, we therefore explored both unsupervised and supervised approaches for keyword identification. The unsupervised approaches explored shallow parsing, noun phrase identification, domain-knowledge, and corpus statistics, while the supervised approaches explored the machine-learning models logistic regression and conditional random fields. 4.2.1 Unsupervised approaches A simple baseline system entails extracting every word in a question as a keyword. For unsupervised keyword identification, we first parsed each question to extract noun phrases as candidate keywords. For this task, we applied MMTx, which incorporates a biomedical, domain-specific shallow parser. We then ranked the noun phrases based on Inverse Document Frequency (IDF), which is commonly used in information retrieval [77]. The principle idea of the IDF model is that a noun phrase (e.g. hypertension) has a more important semantic role if the noun phrase is less frequently used across documents than those (e.g. the stop words, including a and the) that commonly appear in documents. The IDF of a single word noun phrase Wi is: (1) IDF ( W i ) = log N N ( W i ) where N is the total number of documents \u2212 a total of 17 million MEDLINE records (1966\u20132008), and N(Wi ) is the number of documents in which Wi appears. If a noun phrase incorporates a sequence of words, W 1 W 2 \u2026 Wn , we adapted the formula in [78] to calculate the IDF value for that noun phrase as the sum of the IDF values of each word: (2) IDF ( W 1 W 2 \u2026 W n ) = \u2211 i = 1 n IDF ( W i ) We consider that the higher the IDF value, the higher priority the keyword. A baseline model is used to randomly select noun phrases as keywords. We speculated that medical, domain-specific terms are likely to be keywords. For example, hypertension is clearly a more important term than the personal last name Wilbur despite Wilbur having a higher IDF value based on its number of Google hits. We therefore built a domain-filtering model, which means that a term is only included as a keyword if it can be mapped to a UMLS concept. We integrated domain-filtering with the IDF model. For the integrated UMLS+IDF model, we first applied the MMTx to map a question string to the corresponding UMLS concepts and then ranked the mapped concepts based on the IDF model. The IDF and domain-filtering methods rank the candidate keywords, but they do not make decisions on how many top-ranked keywords will be included. To determine the number of top-ranked keywords that should be included, we used a heuristic formula (3) that assigns the number of keywords as a function of the total number of word tokens in the question. The formula was based on our observation that the number of keywords increases when the question length increases. We found that this formula \u2013 although simple and heuristic \u2013 performed well for automatic keyword extraction: (3) N = Number of words in the question 6 + 1 3 , if the number of words in the question is more than 12 4.2.2 Supervised machine-learning We applied two representative, but different supervised machine-learning methods \u2013 logistic regression [79] and conditional random fields (CRFs) [80] \u2013 to identify automatically keywords in a medical question. Logistic regression (LR) is a multivariable method for modeling dichotomous outcomes \u2013 in our application, keyword or non-keyword. LR has been widely used in medicine [81]. Conditional random fields are relatively new [80] but has shown to be the best ML algorithm (surpassing SVM) for named entity recognition in the biomedical domain [82]. In our study, we treated keyword as a named entity. 4.2.2.1 Machine-learning algorithms The logistic regression model [79] predicts the probability of occurrence of an event by fitting data to a logistic curve. The posterior probability Prob(X) is the logistic of a linear function of the feature vector Y 1, Y 2,\u2026, Yn : (4) Prob ( X ) = 1 1 + e - y , y = \u03b1 0 + \u03b1 1 Y 1 + \u03b1 2 Y 2 + \u03b1 3 Y 3 + \u22ef + \u03b1 n Y n Conditional random fields (CRFs) [80] are generative probabilistic models used to segment and label sequence data and offer advantages over hidden Markov models because they are able to relax the strong independence assumption. To apply the CRFs model, we consider a clinical question as a sequence of word tokens and then identify its keywords based on the sequence of tokens that appear in the question. 4.2.2.2 Learning features For this study, we explored the learning features that have been described in Section 4.1.2. In addition, we added word length (i.e., the number of characters in every word) as a feature because domain-specific words (e.g. \u201cgastrectomy\u201d) tend to be lengthy when compared to common English words, and there is a correlation between the length of a word and its IDF value. We also added the position of a word in a question string as an additional feature, because we have observed that an important term sometimes appears toward the end of a clinical question. For example, \u201ccorneal foreign body\u201d and \u201cimmune deficiency\u201d appear towards the end of the questions \u201cWhat is a good protocol for diagnosis and treatment of corneal foreign body?\u201d and \u201cWhat tests should be done to screen for immune deficiency?\u201d Note that each feature on its own will not be sufficient enough for discrimination; however, combining multiple features will improve performance in keyword identification. 4.2.2.3 Evaluation We used the manually assigned keywords as the gold standard for evaluating our automatic keyword identification approach. In addition to the types of indicative keywords shown in Fig. 2, we found that extractive keywords also have variations. These variations include simple morphosyntactic variation (Marfan syndrome and Marfan\u2019s syndrome), orthographic variation (obsessive\u2013compulsive disorder and obsessive compulsive disorder), synonyms (bladder infections and urinary tract infections), and knowledge inferencing (lead and lead poisoning). To map a question string to its varied keywords, we used a simple approximation-matching approach that was built upon edit distance [83] with an empirical cut-off threshold. Note that keywords derived from synonyms and knowledge inferencing can also be identified as \u201cindicative keywords\u201d. However, in this application, we separate extractive keywords from indicative keywords if there is a single word in the assigned keyword that is in common with a word in the clinical question. A total of 3155 questions that have a total of 3353 associated keywords assigned to them were used to evaluate our keyword identification task. The question collection incorporates a total of 55,129 words and 13,060 noun phrases identified by MMTx. The evaluation reflects Recall, Precision, and F1 score. Recall is the number of correctly predicted keywords divided by the total number of assigned keywords, and Precision is the number of correctly predicted keywords divided by the total number of predicted keywords. We calculated the F1 score which is 2*Recall*Precision/(Recall+Precision). 4.3 Error analysis We performed multiple error analysis steps to understand the source of errors in question classification and keyword extraction. For question classification, we first plotted the classification performance on the basis of the training size. Our hypothesis is that question classification performance increases with an increase in training size. We also plotted the classification performance as a function of the number of assigned categories. As described in Section 3, our annotated collection has assigned 1\u20135 categories to each question. If Question A is assigned more categories than Question B, common wisdom suggests that the performance of the question classification of A will be worse than the performance of B. We will test this hypothesis. We plotted the performance of keyword extraction as a function of question length. We also manually examined the cases for both question classification and keyword extraction. 5 Results Table 3 shows the SVM results for automatically classifying an original clinical question into general topics. The results show that the best system was trained on bag-of-words, bigrams, POS, and the UMLS concepts and semantic types, which led to an average of 76.1% recall, 77.0% precision, and 76.5% F1 score. Using bag-of-words as features, the average F1 score was 70.7%. We found that other features have an impact on this performance. Stemming enhanced the performance (an absolute increase of 1.6%). Bigrams also slightly enhanced the performance (an absolute increase of 0.5%), with a slight further enhancement from POS (an absolute increase of 0.01%). UMLS concepts and semantic types significantly improved the performance to its highest value \u2013 a 76.47% F1 score \u2013 which is statistically significant compared to bag-of-words (p <0.0001, t-test). When POS was also added, the overall performance decreased to 75.6%, although the decrease was not statistically significant. We found that feature selection also slightly decreased the performance (an absolute decrease of 0.4%); however, the decrease was not statistically significant. Fig. 3 shows the classification performance of topic assignment as a function of training size. The category history has the lowest classification performance (67.7% F1 score), and pharmacology has the highest classification performance (89.3% F1 score). The Pearson\u2019s correlation shows a p-value of 0.07 (one tail), which is not statistically significant. Fig. 4 a shows the number of questions as a function of the number of topic assignments; the relation resembles a power law distribution, with the highest percentage (76.5%) being questions that were assigned only one topic and much fewer being questions that were assigned two labels or more. Specifically, 8.3% of the questions were assigned two topics, 15.0% questions were assigned three, four questions were assigned four, and five questions were assigned five topics. The questions that were assigned five topics include \u201cWhat are the indications for getting a digoxin level?\u201d, which was assigned Treatment and Prevention, Test, Diagnosis, Pharmacology, and Management, and \u201cCan this rash be a drug reaction to 1% hydrocortisone or some inert ingredient in the preparation?\u201d, which was assigned Treatment and Prevention, Diagnosis, Physical Finding, Pharmacology, and Management. Fig. 4b shows the classification performance of topic assignment as a function of number of topics assigned to a question. Table 4 shows the performance of keyword extraction with both supervised and unsupervised approaches. The results show that the baseline of a random word achieves an 11.4% F1 score. Limiting to noun phrase increases the performance to 17.6%. IDF has further boosted the performance to a still very poor 29.6%. When the UMLS concepts were used for filtering keywords, the performance increased to 53.8%, the highest in the unsupervised machine-learning approaches. For comparison, if all the UMLS concepts that appeared in a question were considered keywords, the F1 score was only 29.5%, comparable to the method of noun phrase+IDF. Statistical analysis shows that all performance increases were significant (p <0.01, t-test). Table 4 also shows that both supervised approaches outperformed unsupervised ones. Logistic regression increased the F1 score 2.4% when compared to the best unsupervised approach (UMLS concept+IDF), yielding a 55.1% F1 score in keyword identification, although statistical analysis shows that the increase is not statistically significant (t-test). Conditional random fields performed the best with a 58.0% F1 score, which showed an absolute increase of 7.8% over the UMLS concept+IDF approach. The results show that although the difference in keyword identification between the CRF model and the logistic regression model is not statistically significant (t-test), the difference between the CRF model and the UMLS+IDF is statistically significant (p <0.01, t-test). Fig. 5 (a) shows recall, precision and F1 score of keyword extraction as a function of question length. The results show that the recall is mostly high (>80%) and mostly independent of question length. In fact, when the number of word tokens of a question is more than 60, the recall increases with question length. On the other hand, precision decreases with question length, and as a result, the F1 score decreases as well. Fig. 5(b) shows the distribution of questions as a function of number of word tokens in a question. As shown, the number of questions decreases when the number of word tokens increases, and a majority of questions (92%) have a number of word tokens below 40. The longest question is 114 words, as shown here: \u201cIn a patient with infectious mononucleosis, if the spleen is going to get enlarged, how long does it take to do that? That is, how long should they stay out of sports? And then the other question is, how long does it take to go back to normal. Also, how big is a 10-year-old\u2019s spleen supposed to be on ultrasound? Do you have to wait until it goes back to normal size to go back to sports or just wait 4\u20136weeks and then go back regardless? Also, is feeling the spleen good enough or do you need to do an ultrasound to see if someone should stay out of sports?\u201d We manually analyzed inconsistent assignments and found many inconsistent annotations. For example, Management can refer to patient management, the scope of which can include Diagnosis, Treatment and Prevention, Pharmacology, or Test. If a question was assigned to those categories, it seems that Management might also be co-assigned. In fact, we found that many annotated questions followed this rule. For example, \u201cWhat is the dose of neurontin?\u201d was assigned three categories: Management, Treatment and Prevention, and Pharmacology. However, there was inconsistency, as some questions did not follow this rule. For example, \u201cWhat is the dose of aspirin needed to prevent TIA\u2019s (transient ischemic attacks)?\u201d was assigned to only one category: Pharmacology. Similar to the inconsistent annotation of Management, we found that inconsistency appeared in many other category assignments. For example, the question \u201c35-year-old female with \u2018throat feels funny\u2019 possible allergies. She couldn\u2019t describe the feeling other than her throat feels funny. The question is what is going on? Could it be related to her beta blocker?\u201d was assigned the category \u201cHistory\u201d by the NLM, but we believe it should be assigned two categories: Diagnosis and Management. We also found inconsistency in keyword assignment. For example, \u201cIs cow\u2019s milk a risk for mad cow disease?\u201d was assigned only the keyword \u201cmilk\u201d, and we believed that \u201cmad cow disease\u201d should be added. Our system has correctly identified both \u201cmilk\u201d and \u201cmad cow disease\u201d as keywords. Some inconsistency is due to keyword composition. For example, the question \u201cWhat are the clinical signs of neonatal myasthenia gravis?\u201d was assigned two keywords \u2013 myasthenia gravis and neonatal, we consider that one keyword neonatal myasthenia gravis is a better model for this question, and our system correctly identified this as the keyword. 6 Discussion Overall, as shown in Table 3, the average performance for automatically assigning a category to a question was a 76.5% F1 score, as opposed to the baseline of 50% attained by random guessing. Our results show that feature selection impacted question classification performance. While stemming and bigrams improved the F1 score slightly (the absolute increase of 0.5\u20131.6%), the UMLS concepts and semantic types had the highest F1 score increase (the absolute increase of 5.3%). The results were consistent with our previous work in question classification [45,46]. Our results show that neither POS nor feature selection improved the performance of question classification, and we speculate that data sparseness is the cause. For the task of keyword identification with unsupervised machine-learning approaches, we assigned the number of top-ranked keywords as a function of the total number of word tokens in the question. Despite the simplicity, we found the formula performs quite well. Our results show the baseline of randomly selecting words achieved an F1 score of only 11.4%; the results indicate that keyword identification is a challenging research task. Our results show that limiting keyword candidates to noun phrases only helped increase the performance to an F1 score of 17.6%, which is still very poor performance. We found a significant increase (an absolute increase of 12%) in performance when IDF prioritization was introduced. Our results support the method of query prioritization with the IDF value, a commonly used technique in open-domain question answering (e.g. [78]). After domain-filtering, we obtained the highest performance (53.8% F1 score) in unsupervised approaches, which was an 81.8% increase over noun phrase+IDF. Our results show the importance of domain-specific knowledge in keyword identification in medical questions. As stated earlier, in the clinical domain, unsupervised approaches (e.g. pattern matching, or syntactic and semantic grammar-based) have been the foundation of many successful clinical natural language processing systems (e.g. [74,75]). Our unsupervised approaches are competitive approaches that were built upon previous work [78,84]. Nevertheless, both supervised machine-learning approaches (i.e., logistic regression and CRFs) outperformed unsupervised ones. Specifically, CRFs outperformed the best unsupervised approach \u2013 UMLS+IDF (p <0.01) \u2013 increasing its F1 score by an absolute value of 4.2%, to 58.0%. Several factors may contribute to the results. Our clinical question collection has shown that many questions were ill-formed with grammatical errors, which has made linguistically-driven and rule-based approaches a challenge. Supervised learning has the ability to learn the relations from multiple features and was therefore robust in this specific task. Another advantage of supervised machine-learning approaches is that the number of keywords is automatically predicted. This is in contrast to our unsupervised machine-learning approaches in which the number of keywords is based on the question length. Fig. 5(a) shows that the recall of keyword prediction is high (in most questions, it is >80%). Furthermore, the recall remains high or increases when question length increases. The results further demonstrated the robustness of supervised machine-learning approaches for keyword extraction. Our results show that the use of CRFs outperformed logistic regression, although not at a statistically significant level (t-test). The results suggest that sequential information (or word order) may contribute to the performance difference. As described in Section 5, we found inconsistent topic and keyword assignments. We speculate that the inconsistency is caused by the fact that there has not been an annotation guideline for the annotated clinical question collection and there isn\u2019t any report for annotation agreement. We found ambiguity in the scope and scope overlap between different categories. For example, Management might or might not refer to patient management, the scope of which may include Diagnosis, Treatment and Prevention, Pharmacology, or Test. A lack of annotation guideline also leads to inconsistency in keyword composition as shown in the example of \u201cWhat are the clinical signs of neonatal myasthenia gravis?\u201d Inconsistency in topic assignment may be responsible for the relation between the training size and the topic classification performance, as shown in Fig. 3. Typically, there is a positive relation between a training size and a classification performance: the larger the training size, the better a classifier performs. Our Pearson\u2019s correlation analysis, however, concluded that there is only a weak correlation (p =0.07) between the training size and the topics classification performance. Although the results show that best performing category, pharmacological, had the largest number of question instances (1594), and the worst performing category history (67.7% F1 score) had the least number of question instances (43), Management did not perform well (71.4% F1 score, comparing with the highest 89.3% F1 score for pharmacological) even though the number of instances available for training was high (1403). Procedure, on the other hand, has only 122 instances, but our classifier achieved an 80.5% F1 score, the 3rd highest classification performance. We speculate that procedure is an unambiguous category for assignment, and therefore it has a highly consistent annotation. We examined the topic assignment performance by the number of topics assigned to a question. Intuitively, we speculate that there is an inverse relation between the number of topics that are assigned to a question and the classification performance for topics assignment: when the number of topics assigned to a question is higher, the classification performance of that question can be lower, and vice versa. However, our results, as shown in Fig. 4, show that the classification performance of a question did not correlate with the number of categories assigned to the question. In addition to inconsistent topics assignment, difference in training data may additionally explain this result. As described earlier, the number of questions that corresponds to the number of categories assigned to questions shows a power law distribution: a large number of questions were assigned to one topic only, and a much fewer number of questions were assigned two or more topics. As a result, our observation may not be generalizable. Nevertheless, a lack of correlation between the number of topics and the classification performance in our question data collection supports our binary classification strategy \u2013 to select a negative set of data randomly from all other categories other than the classification category \u2013 because the binary classification performance does not depend upon the number of assigned categories. Typically, the performance of a multi-classifier decreases when the number of categories increases. Despite the noisy data, our results show a good and reliable performance for question classification and keyword identification. In fact, we observe that in some cases our system outperformed the original annotation data to automatically assign the correct topics for question classification and to automatically identify the correct keywords for keyword identification. Question classification and keyword extraction can improve information retrieval and question answering, because resource searches are based on content (topic areas and keywords), not the simpler bag-of-words that treat each word mostly equally. Although validation of this assertion remains a future work, we have performed a pilot evaluation as reported in [85]. In our study, we showed whether a simple model in which we increased the weight of keywords in a question may lead to improved information retrieval. Since there is no evaluation data available for clinical information retrieval and question answering, we evaluated our approach using the text collection of the Genomics Track of the Text REtrieval Conference (TREC), which incorporates more than 160,000 full-text biomedical articles [86]. The 2006 and 2007 tasks focused on information retrieval for question answering [86,87]; a sample question from the tasks is \u201cWhat is the role of IDE in Alzheimer\u2019s disease?\u201d We employed a simple model in which we increased the weight of keywords and the results showed an improvement in information retrieval that was statistically significant [85]. 7 Conclusions and future work We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Both models can be accessed from the AskHERMES system (http://www.AskHERMES.org). The first model automatically assigns general topics (e.g. Diagnosis and Treatment and Prevention) and the second model automatically extracts keywords or semantic content. Our evaluation of 4654 annotated clinical questions has shown an average performance of 76.5% F1 score for the first model and 58.0% F1 score for the second model. We found that a significant amount of inconsistent annotation lowered the performance for both models, and we anticipate improved models if consistent annotations can be achieved. Future work will focus on the annotation and will investigate and evaluate how the two models improve clinical question answering. For example, we may integrate the work of [60] to categorize the MEDLINE articles as question type specific \u2212 etiology, diagnosis, and treatment, to improve information retrieval. We may also explore probabilistic models [88] to incorporate automatic keyword identification for improving clinical question answering. Acknowledgments The authors acknowledge support from the National Library of Medicine, Grant No. 1R01LM009836, and from the University of Wisconsin-Milwaukee, a MiTAG grant. Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect the views of the NIH and UWM. References [1] J.W. Ely J.A. Osheroff M.H. Ebell G.R. Bergus B.T. Levy M.L. Chambliss Analysis of questions asked by family doctors regarding patient care BMJ 319 1999 358 361 [2] T. Timpka E. Arborelius The GP\u2019s dilemmas: a study of knowledge need and use during health care consultations Methods Inf Med 29 1990 23 29 [3] G.R. Bergus C.S. Randall S.D. Sinift D.M. Rosenthal Does the structure of clinical questions affect the outcome of curbside consultations with specialty colleagues? Arch Fam Med 9 2000 541 547 [4] J.W. Ely R.J. Burch D.C. Vinson The information needs of family physicians: case-specific clinical questions J Fam Pract 35 1992 265 269 [5] J.A. Osheroff D.E. Forsythe B.G. Buchanan R.A. Bankowitz B.H. Blumenfeld R.A. Miller Physicians\u2019 information needs: analysis of questions posed during clinical teaching Ann Intern Med 114 1991 576 581 [6] D.G. Covell G.C. Uman P.R. Manning Information needs in office practice: are they being met? Ann Intern Med 103 1985 596 599 [7] R. Smith What clinical information do doctors need? BMJ 313 1996 1062 1068 [8] W.R. Hersh D.H. Hickam How well do physicians use electronic information retrieval systems? A framework for investigation and systematic review JAMA 280 1998 1347 1352 [9] W. Hersh Evidence-based medicine and the Internet ACP J Club 125 1996 A14 A16 [10] J.I. Westbrook A.S. Gosling E. Coiera Do clinicians use online evidence to support patient care? A study of 55,000 clinicians J Am Med Inform Assoc 11 2004 113 120 [11] J.I. Westbrook E.W. Coiera A.S. Gosling Do online information retrieval systems help experienced clinicians answer clinical questions? J Am Med Inform Assoc 12 2005 315 321 [12] A.S. Gosling J.I. Westbrook Allied health professionals\u2019 use of online evidence: a survey of 790 staff working in the Australian public hospital system Int J Med Inform 73 2004 391 401 [13] J.W. Ely J.A. Osheroff M.H. Ebell M.L. Chambliss D.C. Vinson J.J. Stevermer Obstacles to answering doctors\u2019 questions about patient care with evidence: qualitative study BMJ 324 2002 710 713 [14] J.W. Ely J.A. Osheroff M.L. Chambliss M.H. Ebell M.E. Rosenbaum Answering physicians\u2019 clinical questions: obstacles and potential solutions J Am Med Inform Assoc 12 2005 217 224 [15] A. Hoogendam A.F. Stalenhoef P.F. Robbe A.J. Overbeke Answers to questions posed during daily patient care are more likely to be answered by UpToDate than PubMed J Med Internet Res 10 2008 e29 [16] R.J. Cullen In search of evidence: family practitioners\u2019 use of the Internet for clinical information J Med Libr Assoc 90 2002 370 379 [17] M.B. Stephens A.M. Von Thun Military medical informatics: accessing information in the deployed environment Mil Med 174 2009 259 264 [18] H. Tang J.H. Ng Googling for a diagnosis \u2013 use of Google as a diagnostic aid: Internet based study BMJ 333 2006 1143 1145 [19] D.R. Kitchin K.E. Applegate Learning radiology a survey investigating radiology resident use of textbooks, journals, and the Internet Acad Radiol 14 2007 1113 1120 [20] G.P. Purcell P. Wilson T. Delamothe The quality of health information on the Internet BMJ 324 2002 557 558 [21] A.R. Jadad A. Gagliardi Rating health information on the Internet: navigating to knowledge or to Babel? JAMA 279 1998 611 614 [22] W.M. Silberg G.D. Lundberg R.A. Musacchio Assessing, controlling, and assuring the quality of medical information on the Internet: Caveant lector et viewor \u2013 let the reader and viewer beware JAMA 277 1997 1244 1245 [23] E. Glennie A. Kirby The career of radiography: information on the web J Diagn Radiogr Imaging 6 2006 25 33 [24] S. Childs Judging the quality of Internet-based health information Perform Meas Metrics 6 2005 80 96 [25] K.M. Griffiths H. Christensen Quality of web based information on treatment of depression: cross sectional survey BMJ 321 2000 1511 1515 [26] R.J. Cline K.M. Haynes Consumer health information seeking on the Internet: the state of the art Health Educ Res 16 2001 671 692 [27] M. Benigeri P. Pluye Shortcomings of health information on the Internet Health Promot Int 18 2003 381 386 [28] J.C. Wyatt Commentary: measuring quality and impact of the World Wide Web BMJ 314 1997 1879 1881 [29] H.J. McClung R.D. Murray L.A. Heitlinger The Internet as a source for current patient information Pediatrics 101 1998 E2 [30] G. De Leo C. LeRouge C. Ceriani F. Niederman Websites most frequently used by physician for gathering medical information AMIA Annu Symp Proc 2006 902 [31] M.K. Freeman S.A. Lauderdale M.G. Kendrach T.W. Woolley Google Scholar versus PubMed in locating primary literature to answer drug-related questions Ann Pharmacother 43 2009 478 484 [32] R.H. Fletcher S.W. Fletcher Evidence-based approach to the medical literature J Gen Intern Med 12 Suppl. 2 1997 S5 S14 [33] W.R. Hersh M.K. Crabtree D.H. Hickam L. Sacherek C.P. Friedman P. Tidmarsh Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions J Am Med Inform Assoc 9 2002 283 293 [34] Yu H, Sable C. Being Erlang Shen: identifying answerable questions. In: Proceedings of the nineteenth international joint conference on artificial intelligence on knowledge and reasoning for answering questions; 2005. [35] Yu H, Sable C, Zhu HR. Classifying medical questions based on an evidence taxonomy. In: Proceedings of the AAAI 2005 workshop on question answering in restricted domains; 2005. [36] Yu H, Wei Y. The semantics of a definiendum constrains both the lexical semantics and the lexicosyntactic patterns in the definiens. In: Proceedings of the BioNLP workshop on linking natural language processing and biology at HLT-NAACL, New York, USA; 2006. p. 1\u20138. [37] H. Yu Towards answering biological questions with experimental evidence: automatically identifying text that summarize image content in full-text articles AMIA Annu Symp Proc 2006 834 838 [38] M. Lee J. Cimino H.R. Zhu C. Sable V. Shanker J. Ely Beyond information retrieval\u2013medical question answering AMIA Annu Symp Proc 2006 469 473 [39] H. Yu M. Lee D. Kaufman J. Ely J. Osheroff G. Hripcsak Development, implementation, and a cognitive evaluation of a definitional question answering system for physicians J Biomed Inform 40 2007 236 251 [40] H. Yu Y.G. Cao Automatically extracting information needs from ad hoc clinical questions AMIA Annu Symp Proc 2008 96 100 [41] Yu H, Cao YG. Using the weighted keyword models to improve clinical question answering. In: IEEE international conference on bioinformatics & biomedicine workshop NLP approaches for unmet information needs in health care; 2009. [42] D. Kim H. Yu Hierarchical image classification in the bioscience literature AMIA Annu Symp Proc. 2009 2009 327 331 [43] Cao YG, Ely J, Antieau L, Yu H. Evaluation of the clinical question answering presentation. In: Proceedings of the Workshop on BioNLP, Boulder, Colorado, 171\u20138. [44] Cao YG, Ely J, Yu H. Using weighted keywords to improve clinical question answering. In: Proceeding of IEEE BIBM workshop in NLP approaches in unmet information needs in health care; 2009. [45] S. Agarwal H. Yu FigSum: automatically generating structured text summaries for figures in biomedical literature AMIA Annu Symp Proc 2009 2009 6 10 [46] H. Yu K. Kaufman A cognitive evaluation of four online search engines for answering definitional questions posed by physicians Pac Symp Biocomput 2007 328 339 [47] Niu Y, Hirst G, McArthur G, Rodriguez-Gianolli P. Answering clinical questions with role identification. In: ACL workshop on natural language processing in biomedicine; 2003. [48] J.J. Cimino D.V. Borotsov Leading a horse to water: using automated reminders to increase use of online decision support AMIA Annu Symp Proc 2008 116 120 [49] X. Huang J. Lin D. Demner-Fushman Evaluation of PICO as a knowledge representation for clinical questions AMIA Annu Symp Proc 2006 359 363 [50] D. Demner-Fushman J. Lin Answering clinical questions with knowledge-based and statistical techniques Comput Linguist 33 2007 63 103 [51] Zweigenbaum P. Question-answering for biomedicine: methods and state of the art. In: MIE 2005 workshop; 2005. [52] Zweigenbaum P. Question answering in biomedicine. In: EACL workshop on natural language processing for question answering, Budapest; 2003. p. 1\u20134. [53] Rinaldi F, Dowdall J, Schneider G, Persidis A. Answering questions in the genomics domain. In: ACL 2004 workshop on question answering in restricted domain; 2004. [54] J. Cimino Infobuttons: anticipatory passive decision support AMIA Annu Symp Proc 2008 1203 1204 [55] J.J. Cimino J. Li M. Allen L.M. Currie M. Graham V. Janetzki Practical considerations for exploiting the world wide web to create infobuttons Medinfo 11 2004 277 281 [56] J.J. Cimino J. Li Sharing infobuttons to resolve clinicians\u2019 information needs AMIA Annu Symp Proc 2003 815 [57] J.J. Cimino J. Li M. Graham L.M. Currie M. Allen S. Bakken Use of online resources while using a clinical information system AMIA Annu Symp Proc 2003 175 179 [58] J.J. Cimino J. Li S. Bakken V.L. Patel Theoretical, empirical and practical approaches to resolving the unmet information needs of clinical information system users Proc AMIA Symp 2002 170 174 [59] J.J. Cimino G. Elhanan Q. Zeng Supporting infobuttons with terminological knowledge Proc AMIA Annu Fall Symp 1997 528 532 [60] N.L. Wilczynski K.A. McKibbon R.B. Haynes Enhancing retrieval of best evidence for health care from bibliographic databases: calibration of the hand search of the literature Medinfo 2001 393 [61] C.A. Sneiderman D. Demner-Fushman M. Fiszman N.C. Ide T.C. Rindflesch Knowledge-based methods to help clinicians find answers in MEDLINE J Am Med Inform Assoc 14 2007 772 780 [62] P. Srinivasan T. Rindflesch Exploring text mining from MEDLINE Proc AMIA Symp 2002 722 726 [63] N.C. Ide R.F. Loane D. Demner-Fushman Essie: a concept-based search engine for structured biomedical text J Am Med Inform Assoc 14 2007 253 263 [64] J.J. Cimino A. Aguirre S.B. Johnson P. Peng Generic queries for meeting clinical information needs Bull Med Libr Assoc 81 1993 195 206 [65] Y.H. Seol D.R. Kaufman E.A. Mendonca J.J. Cimino S.B. Johnson Scenario-based assessment of physicians\u2019 information needs Medinfo 11 2004 306 310 [66] Niu Y, Hirst G. Analysis of semantic classes in medical text for question answering. In: ACL 2004 workshop on question answering in restricted domains; 2004. [67] J.W. Ely J.A. Osheroff P.N. Gorman M.H. Ebell M.L. Chambliss E.A. Pifer A taxonomy of generic clinical questions: classification study BMJ 321 2000 429 432 [68] J.W. Ely J.A. Osheroff K.J. Ferguson M.L. Chambliss D.C. Vinson J.L. Moore Lifelong self-directed learning using a computer database of clinical questions J Fam Pract 45 1997 382 388 [69] D.M. D\u2019Alessandro C.D. Kreiter M.W. Peterson An evaluation of information-seeking behaviors of general pediatricians Pediatrics 113 2004 64 69 [70] J. Lin W.J. Wilbur Syntactic sentence compression in the biomedical domain: facilitating access to related articles Inform Retrieval 10 2007 393 414 [71] MMTx; 2005. Available from: http://mmtx.nlm.nih.gov/docs.shtml. [72] C. Friedman G. Hripcsak L. Shagina H. Liu Representing information in patient reports using natural language processing and the extensible markup language J Am Med Inform Assoc 6 1999 76 [73] A.S. Schwartz M.A. Hearst A simple algorithm for identifying abbreviation definitions in biomedical text Pac Symp Biocomput 2003 451 462 [74] Patrick J, Li M, A cascade approach to extracting medication events. In: Proceedings of the Australasian Language Technology Association Workshop 2009;99\u2013103. [75] Li Z, Cao Y, Antieau L, Agarwal S, Zhang Q, Yu H. A hybrid approach to extract medication information from medical discharge summaries. J Am Med Inform Assoc, in press. [76] H. Yu G. Hripcsak C. Friedman Mapping abbreviations to full forms in biomedical articles J Am Med Inform Assoc 9 2002 262 272 [77] S.K. Jones A statistical interpretation of term specificity and its application in retrieval J Doc 28 1972 11 21 [78] Pradhan SS, Illouz G, Blair-Goldensohn SJ, Schlaikjer AH, Krugler V, Filatova E et al. Building a foundation system for producing short answers to factual questions. In: Eleventh text retrieval conference (TREC-11), Washington, DC; 2002. [79] D.G. Kleinbaum M. Klein Logistic regression 2nd ed. 2005 Springer [80] Lafferty J, McCallum A, Pereira F. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In: Proceedings of the ICML-01; 2001. p. 282\u20139. [81] S.C. Bagley H. White B.A. Golomb Logistic regression in the medical literature: standards for use and reporting, with particular attention to one medical domain J Clin Epidemiol 54 2001 979 985 [82] Settles B. Biomedical named entity recognition using conditional random fields and rich feature sets. In: Proceedings of the international joint workshop on natural language processing in biomedicine and its applications (NLPBA), Geneva, Switzerland; 2004. p. 104\u20137. [83] E.S. Ristad P.N. Yianilos M.T. Inc N.J. Princeton Learning string-edit distance IEEE Trans Pattern Anal Mach Intell 20 1998 522 532 [84] S.B. Johnson A. Aguirre P. Peng J. Cimino Interpreting natural language queries using the UMLS Proc Annu Symp Comput Appl Med Care 1993 294 [85] Yu H, Cao YG. Using the weighted keyword models to improve biomedical information retrieval. In: AMIA summit on translational bioinformatics, San Francisco, USA; 2009. [86] Hersh W, Cohen AM, Roberts P, Rekapalli HK. TREC 2006 genomics track overview. In: TREC genomics track conference; 2006. [87] Hersh W, Cohen A, Ruslen L, Roberts P. TREC 2007 genomics track overview. In: The TREC genomics track conference; 2007. [88] Bendersky M, Croft WB. Discovering key concepts in verbose queries. In: Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval; 2008. p. 491\u20138.", "scopus-id": "78649332322", "pubmed-id": "20670693", "coredata": {"eid": "1-s2.0-S1532046410001061", "dc:description": "Abstract Objective Clinicians pose complex clinical questions when seeing patients, and identifying the answers to those questions in a timely manner helps improve the quality of patient care. We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Our study is motivated in the context of developing the larger clinical question answering system AskHERMES (Help clinicians to Extract and aRrticulate Multimedia information for answering clinical quEstionS). Design and measurements We developed supervised machine-learning systems to automatically assign predefined general categories (e.g. etiology, procedure, and diagnosis) to a question. We also explored both supervised and unsupervised systems to automatically identify keywords that capture the main content of the question. Results We evaluated our systems on 4654 annotated clinical questions that were collected in practice. We achieved an F1 score of 76.0% for the task of general topic classification and 58.0% for keyword extraction. Our systems have been implemented into the larger question answering system AskHERMES. Our error analyses suggested that inconsistent annotation in our training data have hurt both question analysis tasks. Conclusion Our systems, available at http://www.askhermes.org, can automatically extract information needs from both short (the number of word tokens <20) and long questions (the number of word tokens >20), and from both well-structured and ill-formed questions. We speculate that the performance of general topic classification and keyword extraction can be further improved if consistently annotated data are made available.", "openArchiveArticle": "true", "prism:coverDate": "2010-12-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046410001061", "dc:creator": [{"@_fa": "true", "$": "Cao, Yong-gang"}, {"@_fa": "true", "$": "Cimino, James J."}, {"@_fa": "true", "$": "Ely, John"}, {"@_fa": "true", "$": "Yu, Hong"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046410001061"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046410001061"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(10)00106-1", "prism:volume": "43", "prism:publisher": "Elsevier Inc. Published by Elsevier Inc.", "dc:title": "Automatically extracting information needs from complex clinical questions", "prism:copyright": "Copyright \u00a9 2010 Elsevier Inc. Published by Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "6", "dcterms:subject": [{"@_fa": "true", "$": "Natural language processing"}, {"@_fa": "true", "$": "Question answering"}, {"@_fa": "true", "$": "Question analysis"}, {"@_fa": "true", "$": "Keyword extraction"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "6", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "962-971", "prism:endingPage": "971", "prism:coverDisplayDate": "December 2010", "prism:doi": "10.1016/j.jbi.2010.07.007", "prism:startingPage": "962", "dc:identifier": "doi:10.1016/j.jbi.2010.07.007", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "65", "@width": "306", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1653", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "480", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2622", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "43", "@width": "250", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1270", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "42", "@width": "181", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1208", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "212", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "45030", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "95", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4222", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "197", "@width": "576", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27662", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "75", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4334", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "201", "@width": "734", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31527", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "60", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2605", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "292", "@width": "380", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "41589", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "213", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7647", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "40", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "13670", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "18", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046410001061-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1205", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/78649332322"}}