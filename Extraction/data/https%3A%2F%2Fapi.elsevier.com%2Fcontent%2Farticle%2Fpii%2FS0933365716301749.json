{"scopus-eid": "2-s2.0-85009080898", "originalText": "serial JL 271219 291210 291682 291866 31 90 Artificial Intelligence in Medicine ARTIFICIALINTELLIGENCEINMEDICINE 2017-01-02 2017-01-02 2017-01-12 2017-01-12 2017-03-29T04:56:44 1-s2.0-S0933365716301749 S0933-3657(16)30174-9 S0933365716301749 10.1016/j.artmed.2016.12.003 S300 S300.1 FULL-TEXT 1-s2.0-S0933365716X00115 2017-03-29T00:17:23.542947-04:00 0 0 20170101 20170131 2017 2017-01-02T08:18:48.608586Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst primabst ref 0933-3657 09333657 UNLIMITED EPSRC true 75 75 C Volume 75 6 51 63 51 63 201701 January 2017 2017-01-01 2017-01-31 2017 Research Articles article fla \u00a9 2017 The Authors. Published by Elsevier B.V. HANDLINGLIMITEDDATASETSNEURALNETWORKSINMEDICALAPPLICATIONSASMALLDATAAPPROACH SHAIKHINA T 1 Introduction 2 Methodology 2.1 Porous solids: data 2.1.1 Compressive strength of trabecular bone 2.1.2 Compressive strength of concrete 2.2 NN design for CS prediction in porous solids 2.3 Method of multiple runs 2.4 Surrogate data test 2.5 Summary of the proposed framework 2.6 Assessing NN generalisation 2.7 Performance criteria 2.8 Alternative model: NN ensemble methods 2.9 Statistical analysis 3 Investigations of the effect of data size on NN performance: concrete CS models 3.1 Collective NN performance (per run) 3.2 Surrogate data test: interpretation for various dataset sizes 3.3 Individual NN performance 3.4 Generalising performance of the small-dataset NN 3.5 Comparison of the small-dataset NN with the ensemble model for the concrete CS data 4 Results: bone CS model 4.1 NN design configuration 4.2 Surrogate data test 4.3 Optimal bone CS model 4.4 Comparison with ensemble NN 5 Discussion 5.1 Significance of the proposed methodology 5.2 Practical significance of the bone CS model Acknowledgement Appendix A Trabecular bone data: real vs surrogate samples Appendix B NN design parameter estimation for bone CS data Effects of the number of neurons in hidden layer Effects of the training duration Appendix C Values of weights and biases of the final NN model for trabecular bone data References CAMPBELL 2014 185 206 C SPRINGERHANDBOOKBIONEUROINFORMATICS MACHINELEARNINGMETHODOLOGYINBIOINFORMATICS FORMAN 2004 161 172 G INZA 2017 25 48 B BIOINFORMATICSMETHODSINCLINICALRESEARCH MACHINELEARNINGINDISPENSABLETOOLINBIOINFORMATICS JOHNSON 2011 J PROBABILITYSTATISTICSFORCOMPUTERSCIENCE WOOLSON 2002 R STATISTICALMETHODSFORANALYSISBIOMEDICALDATA HAYKIN 1999 S NEURALNETWORKSACOMPREHENSIVEFOUNDATION HORNIK 1989 359 366 K AMATO 2013 47 58 F HUDSON 2000 D NEURALNETWORKSARTIFICIALINTELLIGENCEFORBIOMEDICALENGINEERING GROSSI 2011 139 150 E ARTIFICIALNEURALNETWORKSPREDICTIVEMEDICINEAREVOLUTIONARYPARADIGMSHIFT KHOVANOVA 2015 90 100 N LEBARON 1998 213 220 B BOWDEN 2002 1 11 G WASSERMAN 1989 P NEURALCOMPUTINGTHEORYPRACTICE CUNNINGHAM 2000 217 225 P SHAIKHINA 2014 622 625 T IEEEEMBSINTCONFBIOMEDHEALINFORMATICS ARTIFICIALNEURALNETWORKSINHARDTISSUEENGINEERINGANOTHERLOOKAGEDEPENDENCETRABECULARBONEPROPERTIESINOSTEOARTHRITIS PERILLI 2007 760 768 E SINUSAS 2012 49 56 K STEWART 2000 464 467 A ZIVKOVIC 2010 135 141 V CHAN 2014 1251 1258 M BESSHO 2004 545 550 M KEYAK 1997 125 133 J CARTER 1976 1174 1176 D HELGASON 2008 135 146 B GERIS 2013 L COMPUTATIONALMODELINGINTISSUEENGINEERING SINUSAS 2012 49 56 K HIRATA 2008 312 322 Y SCHREIBER 1996 635 638 T THEILER 1992 77 94 J YEH 1998 1797 1808 I YEH 2007 I UCIMACHINELEARNINGREPOSITORYCONCRETECOMPRESSIVESTRENGTHDATASET YONABA 2010 275 283 H NGUYEN 1990 21 26 D IEEEINTJTCONFNEURALNETWORKS IMPROVINGLEARNINGSPEED2LAYERNEURALNETWORKSBYCHOOSINGINITIALVALUESADAPTIVEWEIGHTS LEVENBERG 1944 164 168 K MARQUARDT 1963 431 441 D MORE 1978 105 116 J FUSHIKI 2009 137 146 T TIMMER 1998 5153 5156 J LI 2007 966 982 D GOMEZ 2014 9 I ZHANG 2003 1259 1263 S PROC2003INTCONFMACHLEARNCYBERN SURVEYINGMETHODSIMPROVINGANNGENERALIZATIONCAPABILITY OPITZ 1999 169 198 D DIETTERICH 2000 139 157 T AHMAD 2002 828 833 Z PROC2002INTJOINTCONFNEURALNETWORKS ACOMPARISONDIFFERENTMETHODSFORCOMBININGMULTIPLENEURALNETWORKSMODELS HOLLANDER 1999 M NONPARAMETRICSTATISTICALMETHODS GIBSON 2010 L CELLULARMATERIALSINNATUREMEDICINE ELLERVAINICHER 2011 2186 2191 C PETEIROBARRAL 2013 2807 2816 D SHAIKHINAX2017X51 SHAIKHINAX2017X51X63 SHAIKHINAX2017X51XT SHAIKHINAX2017X51X63XT Full 2017-01-06T13:21:27Z FundingBody Engineering and Physical Sciences Research Council http://creativecommons.org/licenses/by-nc-nd/4.0/ 2018-01-12T00:00:00.000Z UnderEmbargo http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license. \u00a9 2017 The Authors. Published by Elsevier B.V. item S0933-3657(16)30174-9 S0933365716301749 1-s2.0-S0933365716301749 10.1016/j.artmed.2016.12.003 271219 2017-03-29T00:17:23.542947-04:00 2017-01-01 2017-01-31 UNLIMITED EPSRC 1-s2.0-S0933365716301749-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/MAIN/application/pdf/1dda9d1f96e483c5b6de86a9ffebf9de/main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/MAIN/application/pdf/1dda9d1f96e483c5b6de86a9ffebf9de/main.pdf main.pdf pdf true 3113334 MAIN 13 1-s2.0-S0933365716301749-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/PREVIEW/image/png/6540e4baace6905994e5b9b23e1c4a85/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/PREVIEW/image/png/6540e4baace6905994e5b9b23e1c4a85/main_1.png main_1.png png 56335 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0933365716301749-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr1/THUMBNAIL/image/gif/1213038d6e563887b3b53df5c30aabc5/gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr1/THUMBNAIL/image/gif/1213038d6e563887b3b53df5c30aabc5/gr1.sml gr1 gr1.sml sml 14963 77 219 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr10/THUMBNAIL/image/gif/3c869dbd8d3f697f6aa7fc6a328902a5/gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr10/THUMBNAIL/image/gif/3c869dbd8d3f697f6aa7fc6a328902a5/gr10.sml gr10 gr10.sml sml 13558 148 219 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr11/THUMBNAIL/image/gif/c717ee30cc72f3dd3070d3d3ec5e86a6/gr11.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr11/THUMBNAIL/image/gif/c717ee30cc72f3dd3070d3d3ec5e86a6/gr11.sml gr11 gr11.sml sml 15293 163 189 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr2/THUMBNAIL/image/gif/db4a1bfb9aa29195f230cb4d47788d83/gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr2/THUMBNAIL/image/gif/db4a1bfb9aa29195f230cb4d47788d83/gr2.sml gr2 gr2.sml sml 12979 104 219 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr3/THUMBNAIL/image/gif/b582b906208d555c8431818e6fd1cdfc/gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr3/THUMBNAIL/image/gif/b582b906208d555c8431818e6fd1cdfc/gr3.sml gr3 gr3.sml sml 13203 163 75 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr4/THUMBNAIL/image/gif/9ad3c1cabfa86aa867dfa0c0c066df95/gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr4/THUMBNAIL/image/gif/9ad3c1cabfa86aa867dfa0c0c066df95/gr4.sml gr4 gr4.sml sml 11197 163 63 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr5/THUMBNAIL/image/gif/d9cada272f454f7d71a1aad803f4e6f9/gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr5/THUMBNAIL/image/gif/d9cada272f454f7d71a1aad803f4e6f9/gr5.sml gr5 gr5.sml sml 20580 164 152 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr6/THUMBNAIL/image/gif/33c4654cda2100bbbb66bb8a064a76d2/gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr6/THUMBNAIL/image/gif/33c4654cda2100bbbb66bb8a064a76d2/gr6.sml gr6 gr6.sml sml 16874 144 219 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr7/THUMBNAIL/image/gif/d0b04c78089ade35c226d4869b399d8f/gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr7/THUMBNAIL/image/gif/d0b04c78089ade35c226d4869b399d8f/gr7.sml gr7 gr7.sml sml 20812 163 162 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr8/THUMBNAIL/image/gif/9ad20ced263778448a51279b5ffb479b/gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr8/THUMBNAIL/image/gif/9ad20ced263778448a51279b5ffb479b/gr8.sml gr8 gr8.sml sml 17823 164 151 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr9/THUMBNAIL/image/gif/743a7d08634b629e2c20e04f87fcae42/gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr9/THUMBNAIL/image/gif/743a7d08634b629e2c20e04f87fcae42/gr9.sml gr9 gr9.sml sml 18900 145 219 IMAGE-THUMBNAIL 1-s2.0-S0933365716301749-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr1/DOWNSAMPLED/image/jpeg/097680837526f81f4318ec22faac3766/gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr1/DOWNSAMPLED/image/jpeg/097680837526f81f4318ec22faac3766/gr1.jpg gr1 gr1.jpg jpg 43043 131 376 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr10/DOWNSAMPLED/image/jpeg/92d26162f535d3bb258ef7b03fc92ca1/gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr10/DOWNSAMPLED/image/jpeg/92d26162f535d3bb258ef7b03fc92ca1/gr10.jpg gr10 gr10.jpg jpg 57535 317 470 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr11/DOWNSAMPLED/image/jpeg/539546e9698c1db4fa3218566f6daa03/gr11.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr11/DOWNSAMPLED/image/jpeg/539546e9698c1db4fa3218566f6daa03/gr11.jpg gr11 gr11.jpg jpg 53278 325 376 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr2/DOWNSAMPLED/image/jpeg/e6fa051d69a88fbee20f0b4e9e38c698/gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr2/DOWNSAMPLED/image/jpeg/e6fa051d69a88fbee20f0b4e9e38c698/gr2.jpg gr2 gr2.jpg jpg 32310 178 376 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr3/DOWNSAMPLED/image/jpeg/39b50c8f259c074c4828fbdd4aa89d00/gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr3/DOWNSAMPLED/image/jpeg/39b50c8f259c074c4828fbdd4aa89d00/gr3.jpg gr3 gr3.jpg jpg 71098 613 282 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr4/DOWNSAMPLED/image/jpeg/564ce8eb15429270e9ad63f7096491a1/gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr4/DOWNSAMPLED/image/jpeg/564ce8eb15429270e9ad63f7096491a1/gr4.jpg gr4 gr4.jpg jpg 72515 730 282 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr5/DOWNSAMPLED/image/jpeg/9b8d5e8b29519528b0c2cc85fa27dca2/gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr5/DOWNSAMPLED/image/jpeg/9b8d5e8b29519528b0c2cc85fa27dca2/gr5.jpg gr5 gr5.jpg jpg 100476 527 489 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr6/DOWNSAMPLED/image/jpeg/bcaa8b0b5940672025f9bbc4793e1200/gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr6/DOWNSAMPLED/image/jpeg/bcaa8b0b5940672025f9bbc4793e1200/gr6.jpg gr6 gr6.jpg jpg 100325 497 753 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr7/DOWNSAMPLED/image/jpeg/d46922c737e319251e68b4695a8fc6b4/gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr7/DOWNSAMPLED/image/jpeg/d46922c737e319251e68b4695a8fc6b4/gr7.jpg gr7 gr7.jpg jpg 90982 494 489 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr8/DOWNSAMPLED/image/jpeg/9d0c8d97b905669243270fd1eaab36a8/gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr8/DOWNSAMPLED/image/jpeg/9d0c8d97b905669243270fd1eaab36a8/gr8.jpg gr8 gr8.jpg jpg 93135 531 489 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr9/DOWNSAMPLED/image/jpeg/77e155b5f6489ac9bb29527dc34b831a/gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr9/DOWNSAMPLED/image/jpeg/77e155b5f6489ac9bb29527dc34b831a/gr9.jpg gr9 gr9.jpg jpg 65353 312 470 IMAGE-DOWNSAMPLED 1-s2.0-S0933365716301749-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr1/HIGHRES/image/jpeg/aacfbc5331a2e43f6b99e4491a5388f8/gr1_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr1/HIGHRES/image/jpeg/aacfbc5331a2e43f6b99e4491a5388f8/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 197697 583 1667 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr10/HIGHRES/image/jpeg/42f477cbc1218fe9100e8388b9ab81d6/gr10_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr10/HIGHRES/image/jpeg/42f477cbc1218fe9100e8388b9ab81d6/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 247165 1404 2083 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr11_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr11/HIGHRES/image/jpeg/4aef6c2ce49598d5525626022b33e481/gr11_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr11/HIGHRES/image/jpeg/4aef6c2ce49598d5525626022b33e481/gr11_lrg.jpg gr11 gr11_lrg.jpg jpg 315758 1442 1667 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr2/HIGHRES/image/jpeg/e3101e5f170fd090346c3c0c86a3f70b/gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr2/HIGHRES/image/jpeg/e3101e5f170fd090346c3c0c86a3f70b/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 158723 789 1667 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr3/HIGHRES/image/jpeg/285b2f440a82c822248cf490ebeface8/gr3_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr3/HIGHRES/image/jpeg/285b2f440a82c822248cf490ebeface8/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 399567 2718 1250 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr4/HIGHRES/image/jpeg/cc429e2d4c258b9901271b59e4aea97e/gr4_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr4/HIGHRES/image/jpeg/cc429e2d4c258b9901271b59e4aea97e/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 449758 3236 1250 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr5/HIGHRES/image/jpeg/d9e65de0b8368c29ab21008b887b6c2d/gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr5/HIGHRES/image/jpeg/d9e65de0b8368c29ab21008b887b6c2d/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 621452 2337 2167 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr6/HIGHRES/image/jpeg/17636e2e456e001f52f69b569c8edff8/gr6_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr6/HIGHRES/image/jpeg/17636e2e456e001f52f69b569c8edff8/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 586242 2199 3333 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr7/HIGHRES/image/jpeg/39af6d3b681b2fe49a44427c7e35b5eb/gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr7/HIGHRES/image/jpeg/39af6d3b681b2fe49a44427c7e35b5eb/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 475073 2187 2167 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr8/HIGHRES/image/jpeg/40da981baeba1f87113f80a3f29ced07/gr8_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr8/HIGHRES/image/jpeg/40da981baeba1f87113f80a3f29ced07/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 604122 2351 2167 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/gr9/HIGHRES/image/jpeg/e1e7776a332e10fa1515068b6ff42baa/gr9_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/gr9/HIGHRES/image/jpeg/e1e7776a332e10fa1515068b6ff42baa/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 387641 1381 2083 IMAGE-HIGH-RES 1-s2.0-S0933365716301749-si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/19824a12e17aa974c585f3e11f01d102/si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/19824a12e17aa974c585f3e11f01d102/si1.gif si1 si1.gif gif 131 11 11 ALTIMG 1-s2.0-S0933365716301749-si10.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/de106d5221b4ef699cbca6da905dae28/si10.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/de106d5221b4ef699cbca6da905dae28/si10.gif si10 si10.gif gif 125 12 9 ALTIMG 1-s2.0-S0933365716301749-si11.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/62293abe48e4e9dfebe92e202ee383fd/si11.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/62293abe48e4e9dfebe92e202ee383fd/si11.gif si11 si11.gif gif 950 30 228 ALTIMG 1-s2.0-S0933365716301749-si12.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/bbaebd4d683a3f93817129e04f68b286/si12.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/bbaebd4d683a3f93817129e04f68b286/si12.gif si12 si12.gif gif 223 15 33 ALTIMG 1-s2.0-S0933365716301749-si13.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/bb2a8b23694c6fd5983e5ce1ed201b7f/si13.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/bb2a8b23694c6fd5983e5ce1ed201b7f/si13.gif si13 si13.gif gif 278 15 44 ALTIMG 1-s2.0-S0933365716301749-si14.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/1ce32de7d14072f445da175742d49a4a/si14.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/1ce32de7d14072f445da175742d49a4a/si14.gif si14 si14.gif gif 222 15 35 ALTIMG 1-s2.0-S0933365716301749-si15.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/f33e398137b3f2aaec35d666ac403c9b/si15.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/f33e398137b3f2aaec35d666ac403c9b/si15.gif si15 si15.gif gif 262 15 39 ALTIMG 1-s2.0-S0933365716301749-si16.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/821b2c0e5eeb10c9bc49e9b54a9b2daf/si16.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/821b2c0e5eeb10c9bc49e9b54a9b2daf/si16.gif si16 si16.gif gif 135 11 11 ALTIMG 1-s2.0-S0933365716301749-si17.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/f5ccbda5cf4ea0cc8c3dd0de531e9c62/si17.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/f5ccbda5cf4ea0cc8c3dd0de531e9c62/si17.gif si17 si17.gif gif 766 14 221 ALTIMG 1-s2.0-S0933365716301749-si18.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/c45afdaa2a87b4a133400f34316ba073/si18.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/c45afdaa2a87b4a133400f34316ba073/si18.gif si18 si18.gif gif 332 14 69 ALTIMG 1-s2.0-S0933365716301749-si19.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/f420b7d7d1c928e33972eea61602b81f/si19.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/f420b7d7d1c928e33972eea61602b81f/si19.gif si19 si19.gif gif 255 11 41 ALTIMG 1-s2.0-S0933365716301749-si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/74b11d3af228453936b9b8923c857c53/si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/74b11d3af228453936b9b8923c857c53/si2.gif si2 si2.gif gif 129 12 9 ALTIMG 1-s2.0-S0933365716301749-si20.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/9d9d3877d74483703f1d82cb8a0550e1/si20.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/9d9d3877d74483703f1d82cb8a0550e1/si20.gif si20 si20.gif gif 262 15 48 ALTIMG 1-s2.0-S0933365716301749-si21.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/dea2db951f94227c158fd5cfe3db8aee/si21.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/dea2db951f94227c158fd5cfe3db8aee/si21.gif si21 si21.gif gif 226 14 32 ALTIMG 1-s2.0-S0933365716301749-si22.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/1dbed970d47a3b6bfd431de2be1aba72/si22.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/1dbed970d47a3b6bfd431de2be1aba72/si22.gif si22 si22.gif gif 195 14 24 ALTIMG 1-s2.0-S0933365716301749-si23.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/3416c6bbbc790ca24684c271946063b1/si23.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/3416c6bbbc790ca24684c271946063b1/si23.gif si23 si23.gif gif 225 15 28 ALTIMG 1-s2.0-S0933365716301749-si24.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/a1a055f5714db3ff6bdbe86eda9175cc/si24.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/a1a055f5714db3ff6bdbe86eda9175cc/si24.gif si24 si24.gif gif 236 15 34 ALTIMG 1-s2.0-S0933365716301749-si25.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/c07b7e59701713a94bf0ddd5b1a323f2/si25.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/c07b7e59701713a94bf0ddd5b1a323f2/si25.gif si25 si25.gif gif 359 16 62 ALTIMG 1-s2.0-S0933365716301749-si26.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/687cf1eff17bf33f517c54afeb52f86b/si26.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/687cf1eff17bf33f517c54afeb52f86b/si26.gif si26 si26.gif gif 378 16 68 ALTIMG 1-s2.0-S0933365716301749-si27.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/bf90df5b6d85b2812a15b440f237eeef/si27.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/bf90df5b6d85b2812a15b440f237eeef/si27.gif si27 si27.gif gif 325 15 72 ALTIMG 1-s2.0-S0933365716301749-si28.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/f84f5090a1fb89b6eff8c60998e7a4e3/si28.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/f84f5090a1fb89b6eff8c60998e7a4e3/si28.gif si28 si28.gif gif 315 15 66 ALTIMG 1-s2.0-S0933365716301749-si29.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/c9d4436fcd11d0e4fef577416e36c644/si29.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/c9d4436fcd11d0e4fef577416e36c644/si29.gif si29 si29.gif gif 305 12 68 ALTIMG 1-s2.0-S0933365716301749-si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/cc8a6476d54093ba68f88a9d3968f368/si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/cc8a6476d54093ba68f88a9d3968f368/si3.gif si3 si3.gif gif 191 14 22 ALTIMG 1-s2.0-S0933365716301749-si30.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/01a3ca58ab20739a09c3693f1eaf18dd/si30.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/01a3ca58ab20739a09c3693f1eaf18dd/si30.gif si30 si30.gif gif 299 12 67 ALTIMG 1-s2.0-S0933365716301749-si31.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/cd3685eb9d1b487ec7fae732f896c539/si31.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/cd3685eb9d1b487ec7fae732f896c539/si31.gif si31 si31.gif gif 419 20 79 ALTIMG 1-s2.0-S0933365716301749-si32.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/9a2fc85f61b91c109abd3e21a43100a9/si32.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/9a2fc85f61b91c109abd3e21a43100a9/si32.gif si32 si32.gif gif 324 15 69 ALTIMG 1-s2.0-S0933365716301749-si33.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/d520b76f5b588e08f2d522fe1d73b117/si33.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/d520b76f5b588e08f2d522fe1d73b117/si33.gif si33 si33.gif gif 286 15 63 ALTIMG 1-s2.0-S0933365716301749-si34.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/18a1aee27eef65a2f163ed417b745e56/si34.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/18a1aee27eef65a2f163ed417b745e56/si34.gif si34 si34.gif gif 255 15 48 ALTIMG 1-s2.0-S0933365716301749-si35.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/b97bb3d9c40f168367ec9964bd4031dd/si35.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/b97bb3d9c40f168367ec9964bd4031dd/si35.gif si35 si35.gif gif 124 11 9 ALTIMG 1-s2.0-S0933365716301749-si36.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/cea9a38122553041739a310fed3f0f52/si36.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/cea9a38122553041739a310fed3f0f52/si36.gif si36 si36.gif gif 145 11 14 ALTIMG 1-s2.0-S0933365716301749-si37.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/ded889af55c315618edbcdc5c6cd7e92/si37.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/ded889af55c315618edbcdc5c6cd7e92/si37.gif si37 si37.gif gif 148 11 15 ALTIMG 1-s2.0-S0933365716301749-si38.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/2b754301fb37fc6582769424c6c0db9b/si38.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/2b754301fb37fc6582769424c6c0db9b/si38.gif si38 si38.gif gif 149 11 15 ALTIMG 1-s2.0-S0933365716301749-si39.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/ae44a66e915fa8e4fca0993b625299b4/si39.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/ae44a66e915fa8e4fca0993b625299b4/si39.gif si39 si39.gif gif 145 11 15 ALTIMG 1-s2.0-S0933365716301749-si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/ae9eb0f9ebe394e7f0aed000d8346354/si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/ae9eb0f9ebe394e7f0aed000d8346354/si4.gif si4 si4.gif gif 207 14 28 ALTIMG 1-s2.0-S0933365716301749-si40.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/ab02e03bf9c917213c663986aedece03/si40.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/ab02e03bf9c917213c663986aedece03/si40.gif si40 si40.gif gif 147 11 15 ALTIMG 1-s2.0-S0933365716301749-si41.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/571b66249d5642aa4a7dd805fd134914/si41.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/571b66249d5642aa4a7dd805fd134914/si41.gif si41 si41.gif gif 224 15 40 ALTIMG 1-s2.0-S0933365716301749-si42.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/2b56b1b39c087fc85fb37fa8c9c3a3e3/si42.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/2b56b1b39c087fc85fb37fa8c9c3a3e3/si42.gif si42 si42.gif gif 257 15 43 ALTIMG 1-s2.0-S0933365716301749-si43.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/7fbccdaee74bb4bd21a3c85f8ba82c6f/si43.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/7fbccdaee74bb4bd21a3c85f8ba82c6f/si43.gif si43 si43.gif gif 269 15 48 ALTIMG 1-s2.0-S0933365716301749-si44.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/04006e66d1903e6729058276aee8b54c/si44.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/04006e66d1903e6729058276aee8b54c/si44.gif si44 si44.gif gif 255 15 46 ALTIMG 1-s2.0-S0933365716301749-si45.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/e5322bfdf4ed612c20cae3519253a28b/si45.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/e5322bfdf4ed612c20cae3519253a28b/si45.gif si45 si45.gif gif 169 14 19 ALTIMG 1-s2.0-S0933365716301749-si46.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/9683e6c34a8d8aef323078ea6a0b7556/si46.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/9683e6c34a8d8aef323078ea6a0b7556/si46.gif si46 si46.gif gif 210 14 38 ALTIMG 1-s2.0-S0933365716301749-si47.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/5bcb9a87a180954ba500008e3c052021/si47.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/5bcb9a87a180954ba500008e3c052021/si47.gif si47 si47.gif gif 250 15 43 ALTIMG 1-s2.0-S0933365716301749-si48.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/e3838ba353494bda2b5a5c71eec1e3bb/si48.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/e3838ba353494bda2b5a5c71eec1e3bb/si48.gif si48 si48.gif gif 334 16 73 ALTIMG 1-s2.0-S0933365716301749-si49.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/aea12e38e38a5a6dc643ab2e33ce14e4/si49.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/aea12e38e38a5a6dc643ab2e33ce14e4/si49.gif si49 si49.gif gif 222 14 43 ALTIMG 1-s2.0-S0933365716301749-si5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/16800d0411fbd3e3d31412f7f1a7973b/si5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/16800d0411fbd3e3d31412f7f1a7973b/si5.gif si5 si5.gif gif 184 11 23 ALTIMG 1-s2.0-S0933365716301749-si50.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/8db7c86167238ab15e7c090d1ac673af/si50.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/8db7c86167238ab15e7c090d1ac673af/si50.gif si50 si50.gif gif 326 14 64 ALTIMG 1-s2.0-S0933365716301749-si51.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/a2a1482f2de635a40509e7a05f2ac572/si51.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/a2a1482f2de635a40509e7a05f2ac572/si51.gif si51 si51.gif gif 122 8 10 ALTIMG 1-s2.0-S0933365716301749-si6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/b7df5cdc6df16e8c27507f2452cbbd61/si6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/b7df5cdc6df16e8c27507f2452cbbd61/si6.gif si6 si6.gif gif 181 14 23 ALTIMG 1-s2.0-S0933365716301749-si7.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/1334ee2f943e36c5302e665641d3614e/si7.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/1334ee2f943e36c5302e665641d3614e/si7.gif si7 si7.gif gif 187 14 25 ALTIMG 1-s2.0-S0933365716301749-si8.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/51a8cb1aab6372572b972af540c8ad32/si8.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/51a8cb1aab6372572b972af540c8ad32/si8.gif si8 si8.gif gif 182 13 23 ALTIMG 1-s2.0-S0933365716301749-si9.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0933365716301749/STRIPIN/image/gif/667593d455d8d0c3ebc82203a9adf0db/si9.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0933365716301749/STRIPIN/image/gif/667593d455d8d0c3ebc82203a9adf0db/si9.gif si9 si9.gif gif 130 11 11 ALTIMG ARTMED 1494 S0933-3657(16)30174-9 10.1016/j.artmed.2016.12.003 The Authors Fig. 1 Neural network model topology and layer configuration represented by a p -dimensional input, k -neuron hidden layer and 1 output variable. Fig. 1 Fig. 2 Proposed framework for application of regression neural networks to small datasets. Fig. 2 Fig. 3 Distributions of regression coefficients R a l l and R t e s t across a run of neural networks: (a) large-dataset model (1030 samples), (b) intermediate 100 sample model, and (c) small-dataset model (56 samples). The inset shows the enlarged area highlighted in (a). Fig. 3 Fig. 4 Distributions of regression coefficients R a l l achieved by small-dataset neural networks for surrogates (green) and real concrete data (navy) for (a) large-dataset model (1030 samples), (b) intermediate 100 sample model, and (c) small-dataset model (56 samples). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Fig. 5 Linear regression between target and predicted compressive strength achieved by the specimen large-data (1030 samples) concrete neural network model. Values are reported individually for (a) training (blue), (b) validation (green), (c) testing (red), and (d) the entire dataset (black). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Fig. 6 Linear regression between target and predicted compressive strength achieved by the small-dataset (56 samples) optimised concrete neural network. Values are reported individually for (a) training (blue), (b) validation (green) and (c) testing (red), (d) the entire dataset (black), and (e) for 300 independent test samples (purple). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 Fig. 7 Distributions (a) of regression coefficients achieved by neural networks for surrogates (light blue) and real bone data (navy) and (b) Wilcoxon rank sum test for medians across all samples. Distributions and Wilcoxon test results across test samples are reported in (c) and (d). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 Fig. 8 Linear regression between the target and predicted compressive strength (inMPa) achieved by the bone neural network. Values were reported individually for a) training (blue), b) validation (green) and c) testing (red), and d) the entire dataset (black). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 8 Fig. A1 Number of statistically significant NNs per run for various number of neurons in the hidden layer. Fig. A1 Fig. A2 Distributions of the effective number of parameters in regularised neural networks for various number of neurons in the hidden layer. Fig. A2 Fig. A3 Neural network cost function dynamics during the 30 epoch of training (blue), validation (green) and testing (red). Upon reaching the 9th validation check at 22nd epoch (green circle), the neural network training process was completed. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. A3 Table A1 Real bone data. Table A1 Sample no. SMI tb.th BV/TV Age (years) Gender (F=1) CS (MPa) 1 0.06 243 32.5 41.8 1 20.9 2 1.42 224 21.5 52.0 1 6.91 3 0.48 239 26.6 57.0 1 18.2 4 \u22120.82 212 43.5 63.9 1 9.46 5 1.22 419 17.9 64.0 1 23.1 6 0.64 223 27.6 67.1 1 19.4 7 2.10 197 9.82 68.1 1 2.76 8 0.38 367 26.9 71.5 1 18.9 9 0.80 218 15.4 74.9 1 6.49 10 0.54 314 25.0 76.0 1 17.8 11 0.30 326 32.4 87.0 1 24.2 12 \u22120.17 287 30.4 41.7 0 21.5 13 \u22120.31 284 37.0 47.9 0 16.4 14 0.04 265 38.7 49.8 0 11.1 15 0.82 241 22.7 49.8 0 26.5 16 \u22120.23 303 37.6 65.8 0 28.8 17 1.77 219 25.3 68.0 0 4.91 18 1.33 261 17.4 72.9 0 9.81 19 0.04 307 29.7 73.9 0 23.7 20 0.36 271 31.6 81.8 0 24.4 21 0.31 252 33.8 60.9 1 20.5 22 0.70 283 22.5 62.9 1 12.2 23 1.59 247 13.7 72.6 1 1.93 24 0.45 257 27.4 45.7 0 19.6 25 0.44 266 27.5 62.9 0 18.5 26 0.15 270 32.1 77.8 0 22.2 27 1.08 193 19.4 87.0 0 9.12 28 1.93 154 9.68 49.0 1 8.22 29 0.92 263 25.3 66.0 1 15.4 30 \u22120.43 299 39.7 69.9 1 23.2 31 1.04 239 21.0 73.9 1 8.15 32 \u22120.05 288 35.6 46.8 0 24.3 33 0.39 246 26.6 64.9 0 19.3 34 0.71 178 12.2 68.0 0 14.0 35 0.70 234 21.8 84.9 0 13.3 Bone data were extracted from the original study from [17] using a Plot Digitiser tool. Table A2 Surrogates. Table A2 Sample no. SMI tb.th BV/TV Age (years) Gender (F=1) CS (MPa) 1 1.00 260 32.3 66.8 1 17.43 2 0.58 217 38.0 54.0 0 16.21 3 0.73 260 40.5 82.7 1 6.95 4 0.13 209 19.3 57.4 0 19.89 5 0.53 185 17.6 80.4 1 28.51 6 1.72 314 30.5 55.9 0 13.48 7 0.67 269 16.0 60.9 1 26.99 8 0.63 336 26.8 49.6 1 13.33 9 0.12 287 26.9 68.9 0 23.77 10 0.58 271 35.0 54.2 0 15.24 11 0.80 306 26.0 46.6 1 14.52 12 0.90 320 24.1 71.6 1 10.76 13 0.42 376 29.9 60.1 0 8.40 14 0.37 155 31.5 69.6 1 12.63 15 1.93 317 26.7 61.3 0 20.02 16 \u22120.49 275 23.1 68.5 1 21.19 17 1.38 378 18.0 44.5 1 19.40 18 1.47 264 28.1 79.5 0 2.93 19 1.14 258 21.1 74.4 1 22.59 20 \u22120.23 304 13.5 72.4 1 24.92 21 0.18 224 31.9 74.9 1 20.93 22 0.32 261 20.6 61.2 1 5.17 23 0.90 326 25.1 68.2 1 13.90 24 \u22120.11 270 30.3 66.9 0 19.60 25 0.98 312 23.3 65.6 0 20.09 26 \u22120.20 293 31.4 57.8 0 10.90 27 0.86 272 24.1 56.8 1 11.85 28 0.59 227 30.2 63.3 1 19.02 29 1.10 283 30.6 56.1 1 15.62 30 0.97 194 25.1 74.6 0 18.13 31 1.44 277 11.7 85.3 1 11.17 32 1.39 282 22.7 45.3 0 9.80 33 0.32 292 24.7 65.8 0 22.25 34 0.94 323 21.3 49.6 0 20.85 35 0.04 367 19.4 74.5 1 25.36 Surrogate data were synthesised as a random normal distribution with the mean and standard deviation of the real bone data within the same range. Table A3 Weights and biases. Table A3 I W 0.887 2.382 \u22120.888 \u22123.584 1.301 \u22121.586 0.904 \u22123.841 \u22123.268 0.632 \u22121.342 \u22120.144 \u22121.216 \u22122.153 \u22121.380 \u22123.000 \u22120.620 1.592 \u22120.379 \u22121.169 l w \u2032 \u00af \u22120.698 \u22120.151 2.349 \u22121.501 b ( 1 ) \u00af 0.268 \u22120.006 \u22121.224 \u22124.972 b ( 2 ) 0.623 Handling limited datasets with neural networks in medical applications: A small-data approach Torgyn Shaikhina Natalia A. Khovanova \u204e n.khovanova@warwick.ac.uk School of Engineering, University of Warwick, Coventry CV4 7A L, UK School of Engineering University of Warwick Coventry CV4 7AL UK \u204e Corresponding author. Highlights \u2022 A novel framework enables NN analysis in medical applications involving small datasets. \u2022 An accurate model for trabecular bone strength estimation in severe osteoarthritis is developed. \u2022 Model enables non-invasive patient-specific prediction of hip fracture risk. \u2022 Method of multiple runs mitigates sporadic fluctuations in NN performance due to small data. \u2022 Surrogate data test is used to account for random effects due to small test data. Abstract Motivation Single-centre studies in medical domain are often characterised by limited samples due to the complexity and high costs of patient data collection. Machine learning methods for regression modelling of small datasets (less than 10 observations per predictor variable) remain scarce. Our work bridges this gap by developing a novel framework for application of artificial neural networks (NNs) for regression tasks involving small medical datasets. Methods In order to address the sporadic fluctuations and validation issues that appear in regression NNs trained on small datasets, the method of multiple runs and surrogate data analysis were proposed in this work. The approach was compared to the state-of-the-art ensemble NNs; the effect of dataset size on NN performance was also investigated. Results The proposed framework was applied for the prediction of compressive strength (CS) of femoral trabecular bone in patients suffering from severe osteoarthritis. The NN model was able to estimate the CS of osteoarthritic trabecular bone from its structural and biological properties with a standard error of 0.85MPa. When evaluated on independent test samples, the NN achieved accuracy of 98.3%, outperforming an ensemble NN model by 11%. We reproduce this result on CS data of another porous solid (concrete) and demonstrate that the proposed framework allows for an NN modelled with as few as 56 samples to generalise on 300 independent test samples with 86.5% accuracy, which is comparable to the performance of an NN developed with 18 times larger dataset (1030 samples). Conclusion The significance of this work is two-fold: the practical application allows for non-destructive prediction of bone fracture risk, while the novel methodology extends beyond the task considered in this study and provides a general framework for application of regression NNs to medical problems characterised by limited dataset sizes. Keywords Predictive modelling Small data Regression neural networks Osteoarthritis Compressive strength Trabecular bone 1 Introduction IN recent decades, a surge of interest in Machine learning within the medical research community has resulted in an array of successful data-driven applications ranging from medical image processing and the diagnosis of specific diseases, to the broader tasks of decision support and outcome prediction [1\u20133]. The focus of this work is on predictive modelling for applications characterised by small datasets and real-numbered continuous outputs. Such tasks are normally approached by using conventional multiple linear regression models. These are based on the assumptions of statistical independence of the input variables, linearity between dependent and independent variables, normality of the residuals, and the absence of endogenous variables [4]. However, in many applications, particularly those involving complex physiological parameters, those assumptions are often violated [5]. This necessitates more sophisticated regression models based, for instance, on Machine learning. One such approach \u2013 predictive modelling using feedforward backpropagation artificial neural networks (NNs) \u2013 is considered in this work. NN is a distributed parallel processor which resembles a biological brain in the sense that it learns by responding to the environment and stores the acquired knowledge in interneuron synapses [6]. One striking aspect of NNs is that they are universal approximators. It has been proven that a standard multilayer feedforward NN is capable of approximating any measurable function and that there are no theoretical constraints for the success of these networks [7]. Even when conventional multiple regression models fail to quantify a nonlinear relationship between causal factors and biological responses, NNs retain their capacity to find associations within high-dimensional, nonlinear and multimodal medical data [8,9]. Despite their superior performance, accuracy and versatility, NNs are generally viewed in the context of the necessity for abundant training data. This, however, is rarely feasible in medical research, where the size of datasets is constrained by the complexity and high cost of large-scale experiments. Applications of NNs for regression analysis and outcome prediction based on small datasets remain scarce and thus require further exploration [2,9,10]. For the purposes of this study, we define small data as a dataset with less than ten observations (samples) per predictor variable. NNs trained with small datasets often exhibit unstable behaviour in performance, i.e. sporadic fluctuations due to the sensitivity of NNs to initial parameter values and training order [11\u201313]. NN initialisation and backpropagation training algorithms commonly contain deliberate degrees of randomness in order to improve convergence to the global minimum of the associated cost function [6,9,12,14]. In addition, the order with which the training data is fed to the NN can affect the level of convergence and produce erratic outcomes [12,13]. Such inter-NN volatility limits both the reproducibility of the results and the objective comparison between different NN designs for future optimisation and validation. Previous attempts [15] to resolve the stability problems in NNs demonstrated the success of k-fold cross-validation and ensemble methods for a medical classification problem; the dataset comprised 53 features and 1355 observations, which corresponds to 25 observations per predictor variable. To the best of our knowledge, effective strategies for regression tasks on small biomedical datasets have not been considered, thus necessitating the establishment of a framework for application of NNs to medical data analysis. One important biomedical application of NNs in hard tissue engineering was considered in our previous work [11,16], where a NN was applied for correlation analysis of 35 trabecular bone samples from male and female specimens of various ages suffering from severe osteoarthritis (OA) [17]. OA is common degenerative joint disease associated with damaged cartilage [18]. Unlike in osteoporosis, where decreasing bone mineral density (BMD) decreases bone compressive strength (CS) and increases bone fracture risk, the BMD in OA was seen to increase [19,20]. There is further indication that higher BMD does not protect against bone fracture risk in OA [19,21]. The mathematical relationship between BMD and CS observed in healthy patients does not hold for patients with OA, necessitating development of a CS model for OA. In the current work, we consider the application of NNs to osteoarthritic hip fracture prediction for non-invasive estimation of bone CS from structural and physiological parameters. For this particular application there are two commonly used computational techniques: quantitative computed tomography-based finite element analysis [22,23] and the indirect estimation of local properties of bone tissue through densitometry [24,25]. Yet, subject-specific models for hip fracture prediction from structural parameters of trabecular bone in patients affected by degenerative bone diseases have not been developed. An accurate patient data driven model for CS estimation based on NNs could offer a hip fracture risk stratification tool and provide valuable clinical insights for the diagnosis, prevention and potential treatment of OA [26,27]. The aim of this research is to develop subject-specific models for hip fracture prediction in OA and a general framework for the application of regression NNs to small datasets. In this work we introduce the method of multiple runs to address the inter-NN volatility problem caused by small data conditions. By generating a large set (1000+) of NNs, this method allows for consistent comparison between different NN designs. We also propose surrogate data test in order to account for the random effects due to small datasets. The use of surrogate data was inspired by their successful application in nonlinear physics, neural coding, and time series analysis [28\u201330]. The utility of the proposed framework was explored by considering a larger dataset. Due to the unavailability of a large number of bone samples, a different CS dataset, that of 1030 samples of concrete, was used [31,32]. We designed and trained regression NNs for several smaller subsets of the data and demonstrated that small-dataset (56 samples) NNs developed using our framework can achieve a performance comparable to that of the NNs developed on the entire dataset (1030 samples). The structure of this article is as follows. Section 2 describes the data used for analysis, NN model design, and introduces the new framework. In Section 3, the role of data size on NN performance and generalisation ability is explored to demonstrate the utility of the proposed framework. In Section 4 we apply our framework for prediction of osteoarthritic trabecular bone CS and demonstrate the superiority of the approach over established ensemble NN methods in the context of small data. Section 5 discusses both the methodological significance of the proposed framework and the medical application of the NN model for prediction of hip fracture risk. Additional information on NN outcomes and datasets is provided in the Appendices. 2 Methodology 2.1 Porous solids: data 2.1.1 Compressive strength of trabecular bone Included in this study are 35 patients who suffered from severe OA and underwent total hip arthroplasty (Table A1, Appendix A). The original dataset [17] obtained from trabecular tissue samples taken from the femoral head of the patients contained five predictor features (a 5-D input vector for the NN): patients\u2019 age and gender, tissue porosity (BV/TV), structure model index (SMI), trabecular thickness factor (tb.th), and one output variable, the CS (in MPa). The dataset was divided at random into training (60%), validation (20%) and testing (20%) subsets, i.e. 22, 6 and 7 samples, respectively. 2.1.2 Compressive strength of concrete The dataset [31] of 1030 samples was obtained from a publically available repository [32] and contained the following variables: compressive strength (CS) of concrete samples (in MPa), the amounts of 7 components in the concrete mixture (in kg/m3): cement, blast furnace slag, fly ash, water, superplasticizer, coarse and fine aggregates, and the duration of concrete aging (in days). The CS of concrete is a highly nonlinear function of its components and the duration of aging, yet an appropriately trained NN can effectively capture this complex relationship between the CS and the other 8 variables. A successful application of NNs to CS prediction based on 700 concrete samples has been demonstrated in an original study by Yeh [31]. For the purposes of our NN modelling, the samples were divided at random into training (60%), validation (10%) and testing (30%). Thus, out of 1030 available samples, 630 were used for NN training, 100 for validation and 300 were reserved for testing. 2.2 NN design for CS prediction in porous solids Considering the size and nature of the available data, a feedforward backpropagation NN with one hidden layer, p input features and one output was chosen as the base for the CS model (Fig. 1 ). The k neurons in the hidden layer is characterised by a hyperbolic tangent sigmoid transfer function [33], while the output neuron relates the CS output to the input by using a simple linear transfer function (Fig. 1). The p -by- k input weights matrix I W , k -by-1 layer weights column vector l w \u2032 \u00af , and the corresponding biases b ( 1 ) \u00af and b ( 2 ) for each layer were initialised according to the Nguyen-Widrow method [34] in order to distribute the active region of each neuron in the layer evenly across the layer\u2019s input space. The NNs were trained using the Leverberg-Marquardt backpropagation algorithm [35\u201337]. The cost function was defined by the mean squared error (MSE) between the output and actual CS values. Early stopping on an independent validation cohort was implemented in order to avoid NN overtraining and increase generalisation [38]. The validation subset was sampled at random from the model dataset for each NN, ensuring a diversity among the samples. The resulting NN model mapped the output y (in MPa) to the input vector x \u00af is: (1) y = tanh [ x \u00af \u00b7 I W + b ( 1 ) \u00af ] \u00b7 l w \u2032 \u00af + b ( 2 ) The final values of the weights and bias parameters in (1) for the trained bone data NN are provided in Table A3 in Appendix B. Note, parameter estimation for the optimal network structure, size, training duration, training function, neural transfer function and cost function was conducted at the preliminary stage following an established textbook practice [6,9]. Assessment and comparison of various NN designs were carried out using the multiple runs technique. 2.3 Method of multiple runs In order to address the small dataset problem we introduce the method of multiple runs in which a large number of NNs of the same design are trained simultaneously. In other words, the performance of a given NN design is assessed not on a single NN instance, but repeatedly on a set (multiple run) of a few thousands NNs. Identical in terms of their topology and neuron functions, NNs within each such run differ due to the 3 sources of randomness deliberately embedded in the initialisation and training routines: (a) the initial values of the layer weights and biases, (b) the split between the training and validation datasets (test samples were fixed), and (c) the order with which the training and validation samples are fed into the NN. In every run, several thousand NNs with various initial conditions are generated and trained in parallel, producing a range of successful and unsuccessful NNs evaluated according to criteria set in Section 2.7. Subsequently, their performance indicators are reported as collective statistics across the whole run, thus allowing consistent comparisons of performance among runs despite the limited size of the dataset. This helps to quantify the varying effects of design parameters, such as the NN\u2019s size and the training duration during the iterative parameter estimation process. Finally, the highest performing instance of the optimal NN design is selected as the working model. This strategy principally differs from NN ensemble methods (as discussed below in Section 2.8) in the sense that only the output of a single best performing NN is ultimately selected as the working (optimal) model. In summary, the following terminology applies throughout the paper: \u2022 design parameters are NN size, neuron functions, training functions, etc. \u2022 individual NN parameters are weights and biases \u2022 optimal NN design is based on estimation of appropriate NN size, topology, training functions, etc. \u2022 working (optimal) model is the highest performing instance selected from a run of the optimal NN design. The choice of the number of NNs per run is influenced by the balance between the required precision of the statistical measures and computational efficiency, as larger runs require more memory and time to simulate. It was found that for the bone CS application considered in this study, 2000 NNs maintained most performance statistics, such as mean regression between NN targets and predictions, consistent to 3 decimal places, which was deemed sufficient. For inter-run consistency each 2000 NN run was repeated 10 times, yielding 20,000 NNs in total. The average simulation time for instantiating and training a run of 2000 NNs on a modern PC (Intel\u00ae Core\u2122 i7-3770 CPU @3.40GHz, 32 GB RAM) was 280s. 2.4 Surrogate data test Where a sufficient number of samples is available, the efficiency of learning by NN of the interrelationships in the data is expected to correlate with its test performance. With small datasets, however, the efficiency of learning is decreased and even poorly-designed NNs can achieve a good performance on test samples at random. In order to avoid such situation and to evaluate NN performance in the presence of random effects, a surrogate data test is proposed in this study. Surrogate data mimics the statistical properties of the original dataset independently for each component of the input vector. While resembling the statistical properties of the original data, the surrogates do not retain the intricate interrelationships between the various components of the real dataset. Hence, the NN trained and tested on surrogates is expected to perform poorly. Numerous surrogate data NNs are generated using method of multiple runs described in Section 2.3. The highest performing surrogate NN instance defines the lowest performance threshold for real data models. To pass the surrogate data test, real data NNs must outperform this threshold. The surrogate samples can be generated using a variety of methods [29,39,40]. In this study two approaches were used. For trabecular bone data, all continuous input variables were normally distributed according to the Kolmogorov-Smirnov statistical test [4]. Thus surrogates were generated from random numbers to match the truncated normal distributions, e.g. mean and standard deviation estimated from the original data, as well as the range and size of the original tissue samples (Table A2, Appendix A). For the concrete data, where vector distributions were not normal, random permutations [4] of the original vectors were applied. 2.5 Summary of the proposed framework Combined, the method of multiple runs and surrogate data test comprise a framework for application of regression NNs to small datasets, as summarised in Fig. 2 . Multiple runs enable (i) consistent comparison of various NN designs during design parameter estimation, (ii) comparison between surrogate data and real data NNs during surrogate data test, and (iii) selection of the working model among the models of optimal design. 2.6 Assessing NN generalisation In the context of ML, generalising performance is a measure of how well a model predicts an outcome based on independent test data with which the NN was not previously presented. In recent decades considerable efforts in ML have been dedicated to improving the generalisation of NNs [41,42]. A data-driven predictive model has little practical value if it is not able to form accurate predictions on new data. Yet in small datasets, where such test data are scarce, the simple task of assessing generalisation becomes impractical. Indeed, reserving 20% of the bone data for independent testing leaves us with only 7 samples. The question of whether the NN model would generalise on a larger set of new samples cannot be illustrated with such limited test data. This poses a major obstacle for small medical datasets in general, thus the effect of dataset size on NN performance must be considered. We investigate the effect of the model dataset size on the generalisation ability of the NN models, developed with our framework, on a large dataset of concrete CS samples described in Section 2.1.2. The findings are presented in Section 3.4. 2.7 Performance criteria In order to assess the performance of an individual NN, including the best performing, the linear regression coefficients R between the actual output (target) and predicted output were calculated. In particular, regression coefficients were calculated for the entire dataset ( R a l l ) and separately for training ( R t r a i n ) , validation ( R v a l ) , and testing ( R t e s t ) . R can take values between 0 and 1, where 1 corresponds to the highest model predictive performance (100% accuracy) with equal target and prediction values. R greater than 0.6 defines statistically significant performance, i.e. R a l l \u2265 0.6 , R t r a i n \u2265 0.6 , R v a l \u2265 0.6 , and R t e s t \u2265 0.6 [11]. The root mean squared error ( R M S E ) across the entire dataset was also assessed. R M S E presents the same information regarding model accuracy as the regression coefficient R , but in terms of the absolute difference between NN predictions and targets. RMSE helps to visualise the predictive error since it is expressed in the units of the output variable, i.e. in MPa for CS considered in this work. The c ollective performance of the NNs within a multiple run was evaluated based on the following statistical characteristics: \u2022 mean \u03bc and standard deviation \u03c3 of R t e s t and R a l l averaged across all NNs in the run, \u2022 the number of NNs that are statistically significant, \u2022 the random effect threshold R s u r , m a x set by the highest performing surrogate NN, in terms of R a l l and R t e s t . In order to select the best performing NN in a run, we considered both R t r a i n and R v a l . Commonly the validation subset is used for model selection [9], however under small-data conditions, R v a l is unreliable. On the other hand, although R t r a i n does not indicate the NN performance on new samples, it gives a useful estimation of the highest expected NN performance. It is expected that R t r a i n is higher than R v a l for a trained NN. Subsequently, when selecting the best performing NN, we disregard models with R v a l > R t r a i n and from the remaining models we choose the one with the highest R v a l . Note that R t e s t should not be involved in the model selection as it reflects the generalising performance of NN models on new data. 2.8 Alternative model: NN ensemble methods Ensemble methods refer to powerful ML models based on combining predictions of a series of individual ML models, such as NNs, trained independently [43,44]. The principle behind a good ensemble is that its constituent models are diverse and are able to generalise over different subsets of an input space, effectively offsetting mutual errors. The resulting ensemble is often more robust than any of its constituent models and has superior generalisation accuracy [43,44]. We compared the NN ensemble performance with that of a single NN model developed within the proposed multiple runs framework for both the concrete and bone applications. In an ensemble, the constituent predictor models can be diversified by manipulating the training subset, or by randomising their initial parameters [44]. The former comprises boosting and bagging techniques, which were disregarded as being impractical for the small datasets, as they reduced already scarce training samples. We utilised the latter ensembling strategy, where each constituent NN was initialised with random parameters and trained with the complete training set, similar to the multiple runs strategy described in Section 2.3. Optiz & Maclin showed that this ensemble approach was \u201csurprisingly effective, often producing results as good as Bagging\u201d [43]. The individual predictions of the constituent NNs were combined using a common linear approach of simple averaging [45]. 2.9 Statistical analysis A non-parametric Wilcoxon rank sum test, also known as the Mann\u2013Whitney U test, for medians was utilised for comparing the performances of any two NN runs [46]. The null-hypothesis of no difference between the groups was tested at the 5% significance level and this is presented by p-values. 3 Investigations of the effect of data size on NN performance: concrete CS models In this section, we utilise a large dataset on concrete CS, described in Section 2.1.2, to investigate the role of dataset size on NN performance and generalising ability. It is demonstrated that for a larger number of samples the optimal NN coefficients can be derived without involving the proposed framework, yet the importance of the framework increases as the data size is reduced. 3.1 Collective NN performance (per run) First, a large-dataset NN model was developed on a complete dataset of 1030 samples, out of which 30% (300 samples) were reserved for tests. The NN was designed as in Fig. 1, with p =8 inputs and k =10 neurons in hidden layer. In a multiple run of 1000, all large-data NNs performed with statistically significant regression coefficients (R >0.6). As expected with large data, the collective performance was highly accurate, with \u03bc( R a l l ) =0.95 and \u03bc( R t e s t ) =0.94 when averaged across the multiple run of 1000 NNs. (Fig. 3 a) Secondly, a NN was applied to a smaller subset of the original dataset (Fig. 3b). Out of 1030 concrete samples, 100 samples were sampled at random and without replacement [4]. The proportions for training, validation and testing subsets, as well as the training and initialisation routines, were analogous to those used for the large concrete dataset NN with an exception to the following adjustments: \u2013 2000 and not 1000 NNs were evaluated per run to ensure inter-run repeatability, \u2013 the number of neurons in the hidden layer was reduced from 10 to 5 and the number of maximum fails for early stopping was decreased from 10 to 6 to account for a dataset size reduction. Finally, an extreme case with even smaller subset of the data was considered (Fig. 3c). From the concrete CS dataset with 8 predictors, 56 samples were selected at random to yield the same ratio of the number of observations per predictor variable as in the bone CS dataset (35 samples and 5 predictors). The small-dataset NN based on 56 concrete samples was modelled on 41 samples and initially tested on 15 samples. Fig. 3 illustrates the changes to the regression coefficient distributions as the size of the dataset decreased from (a) 1030 to (b) 100, and to (c) 56 samples. In comparison to the large-dataset NNs (Fig. 3a), the distributions of the regression coefficients along x-axis for smaller dataset NNs (Fig. 3b and c) were within much wider ranges. The standard deviations \u03c3 also increased substantially for NN modes based on smaller datasets compared with the initial large-dataset model (Fig. 3a). Distributions of the regression coefficients achieved by the 2000 NN instances within the same run (Fig. 3c) demonstrate higher intra-run variance when compared to the large-dataset NNs (Fig. 3a). Over half of the NNs did not converge and only 762 NNs produced statistically significant predictions. The mean regression coefficients across the run decreased to \u03bc ( R a l l ) =0.719, and \u03bc ( R t e s t ) =0.542 (Fig. 3c). When considering only statistically significant NNs (R >0.6), the mean performance of all samples was \u03bc ( R a l l , s i g n i f ) =0.839 and individually for tests \u03bc ( R t e s t , s i g n i f ) =0.736. Despite higher volatility, an undesirable distribution spread and lower mean performance, the maximal R values for the small-dataset NNs were comparable with those for the large-dataset NNs. 3.2 Surrogate data test: interpretation for various dataset sizes As expected, NNs trained on the real concrete data consistently outperformed surrogate NNs. Fig. 4 demonstrates how the difference in performance between the real and surrogate NNs increased with the dataset size. For the large-dataset NN developed with 1030 samples (Fig. 4a), the surrogate and real-data NN distributions did not overlap. In fact, the surrogate NNs in this instance achieved approximately zero mean performance, which signifies that random effects would not have an impact on NN learning with a dataset of this size. The 100-sample and 56-sample surrogate NNs had a non-zero mean performance of \u03bc ( R a l l , s u r , 100 ) =0.219 (Fig. 4b) and \u03bc ( R a l l , s u r , 56 ) =0.187 (Fig. 4c), respectively. They were also characterised by a higher standard deviation of \u03b4 = 0.142 and \u03b4 = 0.145 compared to large-dataset NNs ( \u03b4 = 0.048 ) . The non-zero mean performance of NNs suggests that random effects cannot be disregarded with small datasets and require quantification offered by the proposed surrogate data test. For 56-sample datasets (Fig. 4c), the surrogate NNs performed with an average regression of \u03bc ( R a l l , s u r , 56 ) =0.187, as opposed to \u03bc ( R a l l , r e a l , 56 ) =0.715 for real-data NNs. None of the 2000 surrogate small-dataset NNs achieved a statistically significant performance (R \u22650.6) across all regressioin coefficients, i.e. Rtrain , Rval and Rtest . The surrogate threshold for the 56-sample NN was considered: the highest performing surrogate NN achieved R s u r , m a x , 56 =0.791 on training dataset. This was largely due to overtraining, as its corresponding performance on test samples was poor ( R t e s t =0.515). 3.3 Individual NN performance This subsection compares performance of individual NNs: a large-dataset NN (1030 samples) and a small-dataset NN (56 samples) developed using the proposed framework. As shown in Fig. 3a, all large-data NNs performed with high accuracy and small variance, thus one of them could be selected as a working model without the need for multiple runs. The performance of one of 1000 large-data NN from the run in Fig. 3a is demonstrated in Fig. 5 . This NN achieved ( R a l l ) =0.944 and generalised with ( R t e s t ) =0.94 on 300 independent test samples (Fig. 5d). This large-dataset model provides an indication of NN performance achieved with abundant training samples. For small datasets, we are now concerned with NNs that perform above the surrogate data threshold of R s u r , m a x , 56 =0.791 established in Section 3.2. Among the 2000 small-dataset (56-sample) NNs, the best-performing NN was selected using the performance criteria in Section 2.7. This model achieved regression coefficients of ( R a l l ) =0.92 on the entire dataset, and separately: ( R t r a i n ) =0.96, ( R v a l ) =0.92 and ( R t e s t ) =0.90 on 15-sample test (Fig. 6 a\u2013d). In comparison, the large-dataset NN developed with 1030 samples performed only 2.12% higher. The R values were well above the surrogate threshold, indicating that high performance of the small-data NN was not due to luck. This result was confirmed when the small-data NN was subjected to the generalisation assessment on new test samples. 3.4 Generalising performance of the small-dataset NN In order to assess generalisation, 300 new test samples were randomly selected from the available dataset of 1030\u201356=974 samples not previously seen by the NN. Modelled with only 41 samples, the NN was able to predict CS on 300 new test samples with R t e s t , 300 =0.865 (Fig. 6e); the corresponding RMSE was 9.5MPa. This constitutes a 7.5% decrease in generalising performance compared to the specimen large-dataset NN tested with the same number of independent samples (Fig. 5c). In other words, using the proposed framework we were able to develop an 86.5% accurate NN model with an 18 times smaller dataset than the original one, which demonstrates superiority of the suggested methodology and its applicability to the problems characterised by restricted dataset sizes. 3.5 Comparison of the small-dataset NN with the ensemble model for the concrete CS data Firstly, an NN ensemble was designed by combining the outputs of 1000 NNs trained with the complete dataset of concrete samples (analogous to the large-dataset NNs described in Section 3.1 and presented in Fig. 3a). As anticipated, this NN ensemble was able to achieve a superior generalisation accuracy of R t e s t =0.96 when tested on 300 independent samples. The second NN ensemble was designed by combining the 2000 56-sample NNs (analogous to the small-dataset NNs in Section 3.1 and Fig. 3c). This ensemble achieved R t e s t =0.81 on 15 independent test samples. In comparison, our small-dataset concrete NN model developed with the multiple runs technique achieved R t e s t =0.903 on the same test samples. Subsequently the generalising ability of this ensemble was assessed on 300 additional concrete samples. The ensemble was able to retain its generalising ability with the accuracy of R t e s t , 300 =0.81, proving its robustness, irrespective of the test sample size. Despite such striking consistency, the accuracy of the ensemble model was decreased by over 8% when compared with the generalising performance of the single NN model, developed using method of multiple runs ( R t e s t , 300 =0.865, Section 3.4). These results demonstrate that a NN ensemble can achieve a remarkable performance on predictive tasks with sufficient data, but is unable to perform as well as the multiple runs model on small datasets. 4 Results: bone CS model 4.1 NN design configuration The NN design described in Section 2.2 for bone CS data comprised 5 input parameters. The heterogeneous 1\u00d7 5 input vector, x \u00af , was stacked in the following order: x 1 =morphology (SMI), x 2 =level of interconnectivity (tb.th), x 3 =porosity (BV/TV), x 4 =age and x 5 =gender. Following a standard parameter estimation routine, but with the help of multiple runs, the NN design was configured to 4 neurons in the hidden layer (Appendix B). The number of permissible consecutive validation iterations during which the NN performance fails to improve and which directly influences duration of NN training, was set to 9 (Appendix B). 4.2 Surrogate data test Performances of the NNs trained with real and surrogate data were compared by assessing 10 runs of 2000 NNs, i.e. a total of 20,000 NNs. The real dataset NNs consistently outperformed the surrogate NNs with, on average, a 35% performance increase (Fig. 7 a). Wilcoxon rank sum tests for median R a l l and R t e s t across 20,000 NNs revealed significant statistical difference (p =0) between the groups, with median R a l l , s u r =0.38 for surrogates versus median R a l l , r e a l =0.78 for the real dataset (Fig. 7b). Similar differences in the distributions of R t e s t , r e a l and R t e s t , s u r were observed for tests samples (Fig. 7c\u2013d). The surrogate threshold was R s u r , m a x =0.87 which indicated the lower performance threshold for the real dataset NN. Overall, the surrogate test signified that the accurate results yielded by the bone NN model are not due to random effects. 4.3 Optimal bone CS model Among the run of 2000 NNs of optimal design, the best-performing NN was capable of predicting trabecular tissue CS with RMSE =0.85MPa on the test samples. The linear regression coefficients between targets and predictions achieved by the NN were: individually for R t r =0.999, R v a l =0.991, R t e s t =0.983 and R a l l =0.993 (Fig. 8 a\u2013d). This indicates a very high accuracy of predictions despite the limited dataset of 35 samples. The final values of weights and biases of this fully-trained network are provided in Appendix C. 4.4 Comparison with ensemble NN The NN ensemble achieved R t e s t =0.882, which is 11% lower than the accuracy of the proposed multiple run NN model ( R t e s t =0.983) and only marginally higher than the surrogate threshold R s u r , m a x =0.87 established in Section 4.2 for the bone dataset. This result further confirms that the NN ensembles, when tasked with small-dataset applications, were unable to realise their full predictive potential and were inferior to NNs designed within a multiple runs framework. 5 Discussion 5.1 Significance of the proposed methodology A framework for the application of regression NNs to medical datasets has been developed in order to mitigate the small dataset problem. NNs trained with small datasets exhibit sporadic fluctuations in the performance due to degrees of randomness inherent in the NN initialisation and training routines. This raises the problem of consistent comparisons between various NN models. Another problem is the evaluation of NN performance in the presence of random effects when the test data are scarce. The limitations of small datasets have been overcome in this work by using a novel framework comprising: (1) a multiple runs strategy for monitoring the performance measures collectively across a large set of NNs, and (2) surrogate data test for model evaluation. The proposed surrogate data approach provided a mechanism for NN model evaluation where no additional test samples were available. A large-scale study involving 20,000 NNs confirmed that NNs trained on real bone data significantly outperform the NNs trained on surrogate data. The framework has been evaluated via a comparative study that predicted concrete CS using both large (1030 samples) and small (56 samples) datasets. Using the proposed framework it was possible to develop a small-dataset NN with performance R a l l = 0.923 comparable with that of a large-dataset NN ( R a l l = 0.944). This demonstrates that a drastic 18 times reduction in the required dataset size corresponds to only a small decrease in accuracy of 2.12% \u2013 a compromise to be considered in single-center studies where datasets are often limited. When applied to 35 osteoarthritic specimens, our methodology yielded a reliable predictive NN tool for non-destructive estimation of bone compressive strength. The optimised NN achieved a high generalising accuracy of 98.3%. Additionally, by quantifying random effects specific to the dataset, the surrogate data approach allowed us to define a performance threshold of R s u r , m a x =0.87 for successful NNs. The successful application of the proposed methodology confirms that the size of datasets does not necessarily limit the utility of NNs in the medical domain. 5.2 Practical significance of the bone CS model In cellular solids, CS is an exponential function of the apparent density, BV/TV, raised to the power of 3/25 [15,25,47]. Although such an exact relationship has not been established specifically for osteoarthritic trabecular tissue, this power model, with a bivariate regression coefficient R p o w e r m o d e l =0.906 is the best existing fit to the data [17]. The generalising NN performance R t e s t = 0.983 achieved in our study exceeded R p o w e r m o d e l by 8.5%. The proposed NN model yields substantially more accurate predictions by considering variable interrelations within multi-dimensional medical datasets and successfully capturing the complex physiological phenomena in patients suffering from severe OA. The high accuracy of the proposed CS model enables prediction of bone fracture risk based on the structural and physiological parameters that can be derived without invasive tests on the patient. Hence, by predicting how CS correlates with the bone volume fraction, trabecular thickness and structure model index for patients of various age and gender groups, the NN model can provide a decision support tool for hard tissue engineers and clinicians alike [26]. To our best knowledge, the NN presented in this work is the only existing patient-specific model for prediction of CS in trabecular bone affected by OA. The potential practical applications include: the estimation of bone fracture risk in osteoarthritic patients from CT-scans and basic physiological data, load modelling of synthetic bioscaffolds that mimic natural trabecular bone damaged by OA, and the tailoring of bioscaffold designs for an individual patient to match the damaged trabecular tissue at the site of implantation. The predictive NN model can be adapted to larger datasets and to other degenerative bone disorders, such as osteoporosis and metastatic cancer, with marginal increase in design effort and cost [8,9]. Such scalability is inherent in the underlying ML algorithms, which enable NNs to learn and improve their performance with new data [10,14,48,49]. Acknowledgement This work has been partially supported by EPSRC UK (EP/K02504X/1). Data statement: trabecular bone data used in this work were obtained from [17] and presented in Table A1; the data on concrete samples were obtained from the UCI machine learning depository [32]. Appendices Appendix A Trabecular bone data: real vs surrogate samples Data presented in Tables A1 and A2 Appendix B NN design parameter estimation for bone CS data Effects of the number of neurons in hidden layer Limited availability of the training samples necessitates careful selection of the size of the hidden layer in order to achieve well-generalising NNs. The effect of increasing number of neurons in the hidden layer from 1 to 13 was investigated in the series of experiments that involved 10 runs of 2000 NNs for each neuron, i.e. 260,000 NNs in total were analysed for enhanced repeatability. Reported in Fig. A1 is the number of statistically significant NNs, i.e. NNs that exhibited performance of R a l l \u2265 0.6 across the entire dataset, as well as individually for the training, validation and test datasets. Despite the inter-run volatility in the results, on average the highest performing NNs had 2, 3, 4, and 5 neurons in hidden layer with 890, 878, 873 and 851 statistically significant NNs per run, respectively. For statistically significant NNs the distributions of R a l l and R v a l were compared for various neuron configurations. The highest R v a l was achieved in NN designs with 3 and 4 neurons. The Wilkinson rank sum test was used to assess the inter-run volatility for the two candidate designs. Based on comparison of the 50 pairwise p-values at 5% confidence level, NNs with 4 neurons were established to be more stable than those with 3 neurons. Following careful evaluation of the largest number of statistically significant NNs produced, the highest R a l l and R v a l performance, and adequate inter-run stability, NN with 4 neurons in a hidden layer was chosen as the final NN design for the next stage in parameter estimation. Another way to identify optimal NN size is by integrating a parameter regularisation into a training process. A weight decay procedure penalises large weights forcing the NN parameters to shrink. Larger networks have more parameters to start with, but regularisation prevents some of this \u2018excessive capacity\u2019 from being trained unnecessarily. The effective number of parameters in a NN trained with regularisation can serve as an indication of how well the NN utilises its capacity. We investigated the number of effective parameters for NNs of varying hidden layer size (from 1 to 20 neurons) trained by Bayesian regularisation backpropagation (Fig. A2 ). The number of effective parameters rose in NN configurations with 1\u20134 neurons and fell in configurations with 5 neurons and above, indicating that the NN with 4 neurons was most effective. This was further confirmed by considering validation performance R v a l across 20 runs, which was highest for the NNs with 4 neurons. Effects of the training duration Training duration stipulates the balance between the NN training performance and generalisation. Although extended training can lead to exceptional performance on the training dataset, it often results in poor generalisation on the test data that the NNs had not seen before. Early stopping helps to avoid NN over-fitting upon reaching the maximum number of validation checks. The number, n , of consecutive validation iterations during which the NN performance fails to decrease plays key role in controlling the quality of NN training. It also affects computational efficiency of the training algorithm, which deteriorates with the increasing n . When investigated on 20 runs of 2000 NNs, corresponding to n from 1 to 10 in the increments of 1 and 10\u2013100 in the increments of 10, the effect of n on the NN performance was marginal. No statistical difference was established between the distributions of R (neither R v a l nor R a l l ) for various n in any possible pair of Wilkinson rank sum comparisons at 5% significance level. Thus, any configuration that yielded the highest values of R a l l and R v a l was a suitable candidate for the final NN. Based on the above considerations, the n value of 9 allowed for maximum performance across all samples while maintaining adequate simulation efficiency. Appendix C Values of weights and biases of the final NN model for trabecular bone data The small-dataset bone CS NN was trained using the Levenberg Marquardt backpropagation algorithm [37]. During each iteration (epoch), the performance of the NN on training, validation and test samples was monitored in terms of its cost function expressed by MSE. Fig. A3 shows how the NN error on the training set was monotonically decreasing with each epoch. The errors on the validation and test samples were sporadic until the 14th epoch. At the 31st epoch the validation error failed to decrease for 9 consecutive iterations and the early stopping criterion was reached. The weights and biases were then reverted by 9 epochs to the state at which the validation error was least, i.e. the final state of the trained NN weights and biases corresponded to the 22nd epoch. Notably, this is not the state that minimises cost function for the test samples, as these independent test samples were not involved in the model training; their corresponding cost function is provided for illustrative purposes. Table A3 shows the final weight and bias parameters for the trained bone NN: the input weights matrix I W , the layer weights column vector l w \u2032 \u00af , and the corresponding biases b ( 1 ) \u00af and b ( 2 ) . References [1] C. Campbell Machine learning methodology in bioinformatics N. Kasabov Springer handbook of bio-/neuroinformatics 2014 Springer Berlin, Heidelberg 185 206 [2] G. Forman I. Cohen Learning from little: comparison of classifiers given little training Proc. PKDD 19 2004 161 172 [3] B. Inza E. Arma\u00f1anzas P. Larra\u00f1aga J. Lozano Machine learning: an indispensable tool in bioinformatics R. Matthiesen Bioinformatics methods in clinical research vol. 593 2017 Humana Press 2010 25 48 [4] J.L. Johnson Probability and Statistics for Computer Science 2011 Whiley New York [5] R.F. Woolson W.R. Clarke Statistical methods for the analysis of biomedical data 2nd ed. 2002 Wiley-Interscience New York [6] S. Haykin Neural networks: a comprehensive foundation 2nd ed. 1999 Prentice Hall [7] K. Hornik M. Stinchcombe H. White Multilayer feedforward networks are universal approximators Neural Netw 2 1989 359 366 [8] F. Amato A. L\u00f3pez E.M. Pe\u00f1a-M\u00e9ndez P. Va\u0148hara A. Hampl J. Havel Artificial neural networks in medical diagnosis J Appl Biomed 11 2013 47 58 [9] D.L. Hudson M.E. Cohen Neural networks and artificial intelligence for biomedical engineering 2000 IEEE New York [10] E. Grossi K. Suzuki Artificial neural networks and predictive medicine: a revolutionary paradigm shift 2011 InTech 139 150 [11] N.A. Khovanova K.K. Mallick T. Shaikhina Neural networks for analysis of trabecular bone in osteoarthritis Bioinspir Biomim Nanobiomater 4 no. 1 2015 90 100 [12] B. LeBaron A.S. Weigend A bootstrap evaluation of the effect of data splitting on financial time series IEEE Trans Neural Networks 9 1998 213 220 [13] G.J. Bowden Optimal division of data for neural network models in water resources applications Water Resour Res 38 2002 1 11 [14] P.D. Wasserman Neural computing: theory and practice 1989 Van Nostrand-Reinhold New York [15] P. Cunningham J. Carney S. Jacob Stability problems with artificial neural networks and the ensemble solution Artif Intell Med 20 no. 3 2000 217 225 [16] T. Shaikhina N. Khovanova K. Mallick Artificial neural networks in hard tissue engineering: another look at age-dependence of trabecular bone properties in osteoarthritis IEEE EMBS Int. Conf. Biomed. Heal. Informatics Valencia: IEEE 2014 622 625 [17] E. Perilli M. Baleani C. Ohman F. Baruffaldi M. Viceconti Structural parameters and mechanical strength of cancellous bone in the femoral head in osteoarthritis do not depend on age Bone 41 2007 760 768 [18] K. Sinusas Osteoarthritis: diagnosis and treatment Am Fam Physician 1 86 2012 49 56 [19] A. Stewart A.J. Black Bone mineral density in osteoarthritis Curr Opin Rheumatol 12 no. 5 2000 464 467 [20] V. \u017divkovi\u0107 B. Stamenkovi\u0107 J. Nedovi\u0107 Bone mineral density in osteoarthritis Acta Fac Med Naiss 27 3 2010 135 141 [21] M.Y. Chan J.R. Center J.A. Eisman T.V. Nguyen Bone mineral density and association of osteoarthritis with fracture risk Osteoarthritis Cartilage 22 9 2014 1251 1258 [22] M. Bessho I. Ohnishi H. Okazaki W. Sato H. Kominami S. Matsunaga Prediction of the strength and fracture location of the femoral neck by CT-based finite-element method: a preliminary study on patients with hip fracture J Orthop Sci 9 2004 545 550 [23] J.H. Keyak S.A. Rossi K.A. Jones H.B. Skinner Prediction of femoral fracture load using automated finite element modeling J Biomech 31 1997 125 133 [24] D.R. Carter W.C. Hayes Bone compressive strength: the influence of density and strain rate Science 194 1976 1174 1176 [25] B. Helgason E. Perilli E. Schileo F. Taddei S. Brynjolfsson M. Viceconti Mathematical relationships between bone density and mechanical properties: a literature review Clin Biomech 23 2008 135 146 [26] L. Geris Computational modeling in tissue engineering 2013 Springer-Verlag Berlin [27] K. Sinusas Osteoarthritis: diagnosis and treatment Am Fam Phys 85 no. 1 2012 49 56 [28] Y. Hirata Y. Katori H. Shimokawa H. Suzuki T.A. Blenkinsop E.J. Lang Testing a neural coding hypothesis using surrogate data J Neurosci Methods 172 2008 312 322 [29] T. Schreiber A. Schmitz Improved surrogate data for nonlinearity tests Phys Rev Lett 77 no. 4 1996 635 638 [30] J. Theiler S. Eubank A. Longtin B. Galdrikian J. Doyne Farmer Testing for nonlinearity in time series: the method of surrogate data Physica D 58 no. 1\u20134 1992 77 94 [31] I.-C. Yeh Modeling of strength of high-performance concrete using artificial neural networks Cem Concr Res 28 no. 12 1998 1797 1808 [32] I.-C. Yeh UCI machine learning repository: concrete compressive strength data set 2007 Machine Learning Repository, University of California Irvine, Center of Machine Learning and Intelligent Systems Available: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength (Accessed 14 January 2015) [33] H. Yonaba F. Anctil V. Fortin Comparing sigmoid transfer functions for neural network J Hydrol Eng 15 no. 4 2010 275 283 [34] D. Nguyen B. Widrow Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights San Diego: IEEE IEEE Int. Jt. Conf. Neural Networks 3 1990 21 26 [35] K. Levenberg A method for the solution of certain non-linear problems in least-squares Q Appl Math 2 1944 164 168 [36] D.W. Marquardt An algorithm for least-Squares estimation of nonlinear parameters J Soc Ind Appl Math 11 1963 431 441 [37] J.J. More The Levenberg-Marquardt algorithm: implementation and theory Lecture Notes Mat 630 1978 105 116 [38] T. Fushiki Estimation of prediction error by using K-fold cross-validation Stat Comput 21 2009 137 146 [39] J. Timmer Power of surrogate data testing with respect to nonstationarity Phys Rev E 58 October (no. 4) 1998 5153 5156 [40] D.-C. Li C.-S. Wu T.-I. Tsai Y.-S. Lina Using mega-trend-diffusion and artificial samples in small data set learning for early flexible manufacturing system scheduling knowledge Comput Oper Res 34 no. 4 2007 966 982 [41] I. Gomez S.A. Cannas O. Osenda J.M. Jerez L. Franco The generalization complexity measure for continuous input data Sci World J 2014 no. 815156 2014 9 [42] S. Zhang H.-X. Liu D.-T. Gao W. Wang Surveying the methods of improving ANN generalization capability Xian: IEEE Proc. 2003 Int. Conf. Mach. Learn. Cybern. 2 2003 1259 1263 [43] D. Opitz R. Maclin Popular ensemble methods: an empirical study J Artif Intell Res 11 1999 169 198 [44] T.G. Dietterich An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization Mach Learn 40 no. 2 2000 139 157 [45] Z. Ahmad J. Zhang A comparison of different methods for combining multiple neural networks models Honolulu: IEEE Proc. of the 2002 Int. Joint Conf. Neural Networks 1 2002 828 833 [46] M. Hollander D.A. Wolfe 2nd ed. Nonparametric statistical methods vol. 2 1999 Wiley New York [47] L.J. Gibson M.F. Ashby B.A. Harley Cellular materials in nature and medicine 2010 University Press Cambridge [48] C. Eller-Vainicher V.V. Zhukouskaya Y.V. Tolkachev S.S. Koritko E. Cairoli E. Grossi Low bone mineral density and its predictors in type 1 diabetic patients evaluated by the classic statistics and artificial neural network analysis Diabetes Care 34 October (10) 2011 2186 2191 [49] D. Peteiro-Barral V. Bolon-Canedo A. Alonso-Betanzos B. Guijarro-Berdinas N. Sanchez-Marono Toward the scalability of neural networks through feature selection Expert Syst Appl 40 2013 2807 2816", "scopus-id": "85009080898", "pubmed-id": "28363456", "coredata": {"eid": "1-s2.0-S0933365716301749", "dc:description": "Abstract Motivation Single-centre studies in medical domain are often characterised by limited samples due to the complexity and high costs of patient data collection. Machine learning methods for regression modelling of small datasets (less than 10 observations per predictor variable) remain scarce. Our work bridges this gap by developing a novel framework for application of artificial neural networks (NNs) for regression tasks involving small medical datasets. Methods In order to address the sporadic fluctuations and validation issues that appear in regression NNs trained on small datasets, the method of multiple runs and surrogate data analysis were proposed in this work. The approach was compared to the state-of-the-art ensemble NNs; the effect of dataset size on NN performance was also investigated. Results The proposed framework was applied for the prediction of compressive strength (CS) of femoral trabecular bone in patients suffering from severe osteoarthritis. The NN model was able to estimate the CS of osteoarthritic trabecular bone from its structural and biological properties with a standard error of 0.85MPa. When evaluated on independent test samples, the NN achieved accuracy of 98.3%, outperforming an ensemble NN model by 11%. We reproduce this result on CS data of another porous solid (concrete) and demonstrate that the proposed framework allows for an NN modelled with as few as 56 samples to generalise on 300 independent test samples with 86.5% accuracy, which is comparable to the performance of an NN developed with 18 times larger dataset (1030 samples). Conclusion The significance of this work is two-fold: the practical application allows for non-destructive prediction of bone fracture risk, while the novel methodology extends beyond the task considered in this study and provides a general framework for application of regression NNs to medical problems characterised by limited dataset sizes.", "openArchiveArticle": "false", "prism:coverDate": "2017-01-31", "openaccessUserLicense": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0933365716301749", "dc:creator": [{"@_fa": "true", "$": "Shaikhina, Torgyn"}, {"@_fa": "true", "$": "Khovanova, Natalia A."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0933365716301749"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0933365716301749"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0933-3657(16)30174-9", "prism:volume": "75", "prism:publisher": "The Authors. Published by Elsevier B.V.", "dc:title": "Handling limited datasets with neural networks in medical applications: A small-data approach", "prism:copyright": "\u00a9 2017 The Authors. Published by Elsevier B.V.", "openaccess": "1", "prism:issn": "09333657", "dcterms:subject": [{"@_fa": "true", "$": "Predictive modelling"}, {"@_fa": "true", "$": "Small data"}, {"@_fa": "true", "$": "Regression neural networks"}, {"@_fa": "true", "$": "Osteoarthritis"}, {"@_fa": "true", "$": "Compressive strength"}, {"@_fa": "true", "$": "Trabecular bone"}], "openaccessArticle": "true", "prism:publicationName": "Artificial Intelligence in Medicine", "openaccessSponsorType": "FundingBody", "prism:pageRange": "51-63", "prism:endingPage": "63", "prism:coverDisplayDate": "January 2017", "prism:doi": "10.1016/j.artmed.2016.12.003", "prism:startingPage": "51", "dc:identifier": "doi:10.1016/j.artmed.2016.12.003", "openaccessSponsorName": "Engineering and Physical Sciences Research Council"}, "objects": {"object": [{"@category": "thumbnail", "@height": "77", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14963", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "148", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13558", "@ref": "gr10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "189", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr11.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "15293", "@ref": "gr11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "104", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "12979", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "75", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13203", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "63", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11197", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "152", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "20580", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "144", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "16874", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "162", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "20812", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "151", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "17823", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "145", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "18900", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "131", "@width": "376", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "43043", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "317", "@width": "470", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "57535", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "325", "@width": "376", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr11.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "53278", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "178", "@width": "376", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "32310", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "613", "@width": "282", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "71098", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "730", "@width": "282", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "72515", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "527", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "100476", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "497", "@width": "753", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "100325", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "494", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "90982", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "531", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "93135", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "312", "@width": "470", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "65353", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "583", "@width": "1667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr1_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "197697", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1404", "@width": "2083", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr10_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "247165", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1442", "@width": "1667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr11_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "315758", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "789", "@width": "1667", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "158723", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2718", "@width": "1250", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr3_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "399567", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3236", "@width": "1250", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr4_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "449758", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2337", "@width": "2167", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "621452", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2199", "@width": "3333", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr6_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "586242", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2187", "@width": "2167", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "475073", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2351", "@width": "2167", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr8_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "604122", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1381", "@width": "2083", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-gr9_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "387641", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "11", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "131", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si10.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "125", "@ref": "si10", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "30", "@width": "228", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si11.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "950", "@ref": "si11", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "33", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si12.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "223", "@ref": "si12", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "44", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si13.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "278", "@ref": "si13", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "35", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si14.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "222", "@ref": "si14", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "39", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si15.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "262", "@ref": "si15", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si16.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "135", "@ref": "si16", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "221", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si17.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "766", "@ref": "si17", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "69", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si18.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "332", "@ref": "si18", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "41", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si19.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "255", "@ref": "si19", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "129", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "48", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si20.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "262", "@ref": "si20", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "32", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si21.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "226", "@ref": "si21", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "24", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si22.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "195", "@ref": "si22", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "28", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si23.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "225", "@ref": "si23", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "34", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si24.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "236", "@ref": "si24", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "62", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si25.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "359", "@ref": "si25", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "68", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si26.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "378", "@ref": "si26", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "72", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si27.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "325", "@ref": "si27", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "66", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si28.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "315", "@ref": "si28", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "68", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si29.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "305", "@ref": "si29", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "22", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "191", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "12", "@width": "67", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si30.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "299", "@ref": "si30", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "79", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si31.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "419", "@ref": "si31", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "69", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si32.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "324", "@ref": "si32", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "63", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si33.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "286", "@ref": "si33", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "48", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si34.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "255", "@ref": "si34", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "9", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si35.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "124", "@ref": "si35", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "14", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si36.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "145", "@ref": "si36", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si37.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "148", "@ref": "si37", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si38.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "149", "@ref": "si38", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si39.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "145", "@ref": "si39", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "28", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "207", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "15", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si40.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "147", "@ref": "si40", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "40", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si41.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "224", "@ref": "si41", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "43", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si42.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "257", "@ref": "si42", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "48", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si43.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "269", "@ref": "si43", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "46", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si44.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "255", "@ref": "si44", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si45.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "169", "@ref": "si45", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "38", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si46.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "210", "@ref": "si46", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "15", "@width": "43", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si47.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "250", "@ref": "si47", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "16", "@width": "73", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si48.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "334", "@ref": "si48", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "43", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si49.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "222", "@ref": "si49", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "184", "@ref": "si5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "64", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si50.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "326", "@ref": "si50", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "8", "@width": "10", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si51.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "122", "@ref": "si51", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "181", "@ref": "si6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "14", "@width": "25", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si7.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "187", "@ref": "si7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "13", "@width": "23", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si8.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "182", "@ref": "si8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "11", "@width": "11", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0933365716301749-si9.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "130", "@ref": "si9", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85009080898"}}