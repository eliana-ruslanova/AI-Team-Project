{"coredata": {"prism:url": "https://api.elsevier.com/content/article/pii/S0376635711001975", "dc:identifier": "doi:10.1016/j.beproc.2011.10.004", "eid": "1-s2.0-S0376635711001975", "prism:doi": "10.1016/j.beproc.2011.10.004", "pii": "S0376-6357(11)00197-5", "dc:title": "The cognitive mechanisms of optimal sampling ", "prism:publicationName": "Behavioural Processes", "prism:aggregationType": "Journal", "prism:issn": "03766357", "prism:volume": "89", "prism:issueIdentifier": "2", "prism:startingPage": "77", "prism:endingPage": "85", "prism:pageRange": "77-85", "prism:number": "2", "dc:format": "application/json", "prism:coverDate": "2012-02-29", "prism:coverDisplayDate": "February 2012", "prism:copyright": "Copyright \u00a9 2011 Elsevier B.V. All rights reserved.", "prism:publisher": "Elsevier B.V.", "prism:issueName": "Comparative cognition: Function and mechanism in lab and field.- A tribute to the contributions of Alex Kacelnik", "dc:creator": [{"@_fa": "true", "$": "Lea, Stephen E.G."}, {"@_fa": "true", "$": "McLaren, Ian P.L."}, {"@_fa": "true", "$": "Dow, Susan M."}, {"@_fa": "true", "$": "Graft, Donald A."}], "dc:description": "\n               Abstract\n               \n                  How can animals learn the prey densities available in an environment that changes unpredictably from day to day, and how much effort should they devote to doing so, rather than exploiting what they already know? Using a two-armed bandit situation, we simulated several processes that might explain the trade-off between exploring and exploiting. They included an optimising model, dynamic backward sampling; a dynamic version of the matching law; the Rescorla\u2013Wagner model; a neural network model; and \u025b-greedy and rule of thumb models derived from the study of reinforcement learning in artificial intelligence. Under conditions like those used in published studies of birds\u2019 performance under two-armed bandit conditions, all models usually identified the more profitable source of reward, and did so more quickly when the reward probability differential was greater. Only the dynamic programming model switched from exploring to exploiting more quickly when available time in the situation was less. With sessions of equal length presented in blocks, a session-length effect was induced in some of the models by allowing motivational, but not memory, carry-over from one session to the next. The rule of thumb model was the most successful overall, though the neural network model also performed better than the remaining models.\n               \n            ", "openaccess": "0", "openaccessArticle": false, "openaccessType": null, "openArchiveArticle": false, "openaccessSponsorName": null, "openaccessSponsorType": null, "openaccessUserLicense": null, "dcterms:subject": [{"@_fa": "true", "$": "Foraging"}, {"@_fa": "true", "$": "matching law"}, {"@_fa": "true", "$": "Neural networks"}, {"@_fa": "true", "$": "Optimal sampling"}, {"@_fa": "true", "$": "Reinforcement learning"}, {"@_fa": "true", "$": "Rescorla\u2013Wagner model"}, {"@_fa": "true", "$": "Two-armed bandit"}], "link": [{"@href": "https://api.elsevier.com/content/article/pii/S0376635711001975", "@rel": "self", "@_fa": "true"}, {"@href": "https://www.sciencedirect.com/science/article/pii/S0376635711001975", "@rel": "scidir", "@_fa": "true"}]}, "scopus-id": "84856383116", "scopus-eid": "2-s2.0-84856383116", "pubmed-id": "22019373", "link": {"@href": "https://api.elsevier.com/content/abstract/scopus_id/84856383116", "@rel": "abstract"}, "originalText": {"xocs:doc": {"xocs:meta": {"xocs:open-access": {"xocs:oa-article-status": {"@is-open-access": "0", "@is-open-archive": "0"}}, "xocs:available-online-date": {"@yyyymmdd": "20111019", "$": "2011-10-19"}}}}}