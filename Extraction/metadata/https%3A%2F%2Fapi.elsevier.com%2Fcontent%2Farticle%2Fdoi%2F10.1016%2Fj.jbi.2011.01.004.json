{"scopus-eid": "2-s2.0-79952770126", "originalText": "serial JL 272371 291210 291682 291870 291901 31 80 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2011-01-21 2011-01-21 2014-09-30T23:54:04 1-s2.0-S1532046411000062 S1532-0464(11)00006-2 S1532046411000062 10.1016/j.jbi.2011.01.004 S300 S300.2 FULL-TEXT 1-s2.0-S1532046411X00035 2015-05-15T06:30:58.184067-04:00 0 0 20110401 20110430 2011 2011-01-21T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref 1532-0464 15320464 false 44 44 2 2 Volume 44, Issue 2 12 277 288 277 288 201104 April 2011 2011-04-01 2011-04-30 2011 Regular Articles article fla Copyright \u00a9 2011 Elsevier Inc. Published by Elsevier Inc. All rights reserved. ASKHERMESONLINEQUESTIONANSWERINGSYSTEMFORCOMPLEXCLINICALQUESTIONS CAO Y 1 Introduction 2 Background 3 Methods 3.1 Data sources and pre-processing 3.1.1 Data collection 3.1.2 Pre-processing for retaining semantic content 3.2 Question analysis 3.3 Document retrieval 3.4 Passage retrieval 3.5 Summarization and answer presentation 3.5.1 Topical clustering, ranking and hierarchical answer presentation 3.5.2 Redundancy removal based on longest common substring 4 System implementation 5 Results 5.1 Evaluation design 5.2 Performance of AskHERMES in comparison with Google and UpToDate 5.3 Impact of question length on the quality of answers 6 Discussion 7 Conclusions and future work Acknowledgments References TIMPKA 1990 23 29 T BERGUS 2000 541 547 G ELY 1992 265 269 J OSHEROFF 1991 576 581 J COVELL 1985 596 599 D SMITH 1996 1062 1068 R ELY 2005 217 224 J FLETCHER 1997 S5 S14 R HERSH 2002 283 293 W CULLEN 2002 370 379 R STEPHENS 2009 259 264 M KITCHIN 2007 1113 1120 D TANG 2006 1143 1145 H PURCELL 2002 557 558 G JADAD 1998 611 614 A SILBERG 1997 1244 1245 W GLENNIE 2006 25 33 E CHILDS 2005 80 96 S GRIFFITHS 2005 e59 K GRIFFITHS 2000 1511 1515 K WYATT 1997 1879 1881 J BENIGERI 2003 381 386 M CLINE 2001 671 692 R FREEMAN 2009 478 484 M ELY 1999 358 361 J DELEO 2006 902 G MCCORD 2007 298 303 G GOODYEARSMITH 2008 878 882 F PHUA 2008 39 J CIMINO 2008 116 120 J HOOGENDAM 2008 e29 A YU 2006 834 838 H LEE 2006 469 473 M LEE 2006 140 M YU 2007 328 339 H YU 2007 236 251 H YU 2008 96 100 H CAO 2009 Y HERSH 1999 43 45 W PRATT 1997 480 484 W CIMINO 1993 195 206 J DELFIOL 2008 752 759 G COLLINS 2008 S CIMINO 2008 1203 1204 J CIMINO 2006 151 155 J LEI 2003 906 J CIMINO 2003 815 J CIMINO 2002 170 174 J CIMINO 1997 528 532 J CHASE 2009 387 394 H ELHADAD 2005 179 198 N ELHADAD 2005 226 230 N ELY 1997 382 388 J DALESSANDRO 2004 64 69 D ELY 2000 429 432 J SEOL 2004 306 310 Y DEMNERFUSHMAN 2007 63 103 D HUANG 2006 359 363 X FRAKES 1992 W INFORMATIONRETRIEVALDATASTRUCTUREALGORITHMS SALTON 1975 613 620 G MULLER 2004 e309 H DEERWESTER 1990 391 407 S SNEIDERMAN 2007 772 780 C SRINIVASAN 2002 722 726 P HUMPHREYS 1993 170 177 B IDE 2007 253 263 N ARONSON 2001 17 21 A HEARST 1997 33 64 M OSINSKI 2004 359 368 S HIRSCHBERG 1977 664 675 D ABDOU 2008 781 789 S STOKES 2009 17 50 N CAOX2011X277 CAOX2011X277X288 CAOX2011X277XY CAOX2011X277X288XY Full 2013-07-17T11:42:39Z ElsevierBranded http://www.elsevier.com/open-access/userlicense/1.0/ OA-Window item S1532-0464(11)00006-2 S1532046411000062 1-s2.0-S1532046411000062 10.1016/j.jbi.2011.01.004 272371 2014-10-01T02:30:44.708151-04:00 2011-04-01 2011-04-30 1-s2.0-S1532046411000062-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/MAIN/application/pdf/3fecbf811b943c72bc2a129b7c7bdf47/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/MAIN/application/pdf/3fecbf811b943c72bc2a129b7c7bdf47/main.pdf main.pdf pdf true 991622 MAIN 12 1-s2.0-S1532046411000062-main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/PREVIEW/image/png/d741e5e547e2d026614d1c61ebc4de72/main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/PREVIEW/image/png/d741e5e547e2d026614d1c61ebc4de72/main_1.png main_1.png png 81317 849 656 IMAGE-WEB-PDF 1 1-s2.0-S1532046411000062-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/STRIPIN/image/gif/0d803eb298f1bc6860fc49ee14b987ef/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/STRIPIN/image/gif/0d803eb298f1bc6860fc49ee14b987ef/si2.gif si2 si2.gif gif 2378 44 394 ALTIMG 1-s2.0-S1532046411000062-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/STRIPIN/image/gif/fdf84aa0d8b53ccc0016d594caf29a01/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/STRIPIN/image/gif/fdf84aa0d8b53ccc0016d594caf29a01/si1.gif si1 si1.gif gif 1834 72 304 ALTIMG 1-s2.0-S1532046411000062-gr9_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr9/HIGHRES/image/jpeg/41f3c3b48b8f370a71a728c419fb18be/gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr9/HIGHRES/image/jpeg/41f3c3b48b8f370a71a728c419fb18be/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 192851 470 2166 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr8_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr8/HIGHRES/image/jpeg/35a161a4098a814719f59eeaa00f102d/gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr8/HIGHRES/image/jpeg/35a161a4098a814719f59eeaa00f102d/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 679972 1850 2165 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr7/HIGHRES/image/jpeg/5ad497a962fc327744f8c5a847f18f0e/gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr7/HIGHRES/image/jpeg/5ad497a962fc327744f8c5a847f18f0e/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 212620 1333 2183 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr2_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr2/HIGHRES/image/jpeg/538a4fc81157888a5491840de24b261f/gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr2/HIGHRES/image/jpeg/538a4fc81157888a5491840de24b261f/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 146222 534 2166 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr10_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr10/HIGHRES/image/jpeg/59ecc3befb0529e3964766758f886e67/gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr10/HIGHRES/image/jpeg/59ecc3befb0529e3964766758f886e67/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 644851 1359 2165 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr6_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr6/HIGHRES/image/gif/f7371fd3dd48b13bb1609866515baf22/gr6_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr6/HIGHRES/image/gif/f7371fd3dd48b13bb1609866515baf22/gr6_lrg.gif gr6 gr6_lrg.gif gif 254600 3225 4331 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr5_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr5/HIGHRES/image/gif/335ee113fd189ce5d48fa786ab181629/gr5_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr5/HIGHRES/image/gif/335ee113fd189ce5d48fa786ab181629/gr5_lrg.gif gr5 gr5_lrg.gif gif 43821 1649 2756 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr4_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr4/HIGHRES/image/gif/21021159773dc8ddb6e041198bb6dfe5/gr4_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr4/HIGHRES/image/gif/21021159773dc8ddb6e041198bb6dfe5/gr4_lrg.gif gr4 gr4_lrg.gif gif 114431 3183 2953 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr3_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr3/HIGHRES/image/gif/87ce15e744c2521600c62909603f795a/gr3_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr3/HIGHRES/image/gif/87ce15e744c2521600c62909603f795a/gr3_lrg.gif gr3 gr3_lrg.gif gif 35876 1130 3347 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr1_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr1/HIGHRES/image/gif/b8a3c12ead2360b4c1be7e5ed48a7fc9/gr1_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr1/HIGHRES/image/gif/b8a3c12ead2360b4c1be7e5ed48a7fc9/gr1_lrg.gif gr1 gr1_lrg.gif gif 42177 1056 3346 IMAGE-HIGH-RES 1-s2.0-S1532046411000062-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr9/DOWNSAMPLED/image/jpeg/dbe8ce0e770a300e20c1af6468593c2a/gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr9/DOWNSAMPLED/image/jpeg/dbe8ce0e770a300e20c1af6468593c2a/gr9.jpg gr9 gr9.jpg jpg 24885 106 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr8/DOWNSAMPLED/image/jpeg/7f2ac0e3cb286ed251c12c4e5efed0a1/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr8/DOWNSAMPLED/image/jpeg/7f2ac0e3cb286ed251c12c4e5efed0a1/gr8.jpg gr8 gr8.jpg jpg 81142 418 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr7/DOWNSAMPLED/image/jpeg/f8599b301a6e0d9f1122ffd6dc7f7a5c/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr7/DOWNSAMPLED/image/jpeg/f8599b301a6e0d9f1122ffd6dc7f7a5c/gr7.jpg gr7 gr7.jpg jpg 33617 301 493 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr2/DOWNSAMPLED/image/jpeg/dccf55fcb0f2f96bb29556bb582060c5/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr2/DOWNSAMPLED/image/jpeg/dccf55fcb0f2f96bb29556bb582060c5/gr2.jpg gr2 gr2.jpg jpg 23667 121 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr10/DOWNSAMPLED/image/jpeg/398e6a6bc0d85f7915331f0fca6ebda2/gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr10/DOWNSAMPLED/image/jpeg/398e6a6bc0d85f7915331f0fca6ebda2/gr10.jpg gr10 gr10.jpg jpg 74269 307 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr6/DOWNSAMPLED/image/gif/9edb06b3745dc8b9bb79fb9509cc4c0e/gr6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr6/DOWNSAMPLED/image/gif/9edb06b3745dc8b9bb79fb9509cc4c0e/gr6.gif gr6 gr6.gif gif 30954 364 489 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr5.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr5/DOWNSAMPLED/image/gif/9c61291109798ac4de6af5b00492e6c4/gr5.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr5/DOWNSAMPLED/image/gif/9c61291109798ac4de6af5b00492e6c4/gr5.gif gr5 gr5.gif gif 5827 186 311 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr4/DOWNSAMPLED/image/gif/0a4b69613e884d525b631df769cd1c51/gr4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr4/DOWNSAMPLED/image/gif/0a4b69613e884d525b631df769cd1c51/gr4.gif gr4 gr4.gif gif 14195 359 333 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr3/DOWNSAMPLED/image/gif/a6a0f42e6d895239d3f1541407a639c6/gr3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr3/DOWNSAMPLED/image/gif/a6a0f42e6d895239d3f1541407a639c6/gr3.gif gr3 gr3.gif gif 4650 128 378 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr1/DOWNSAMPLED/image/gif/f8e617c8bb9b48bab1f6bc221bd6ad24/gr1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr1/DOWNSAMPLED/image/gif/f8e617c8bb9b48bab1f6bc221bd6ad24/gr1.gif gr1 gr1.gif gif 5153 119 378 IMAGE-DOWNSAMPLED 1-s2.0-S1532046411000062-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr6/THUMBNAIL/image/gif/7302b3766b82369643c3e53e74586796/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr6/THUMBNAIL/image/gif/7302b3766b82369643c3e53e74586796/gr6.sml gr6 gr6.sml sml 7960 163 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr5/THUMBNAIL/image/gif/623e4eb163db10fc70bdbba333671347/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr5/THUMBNAIL/image/gif/623e4eb163db10fc70bdbba333671347/gr5.sml gr5 gr5.sml sml 3644 131 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr4/THUMBNAIL/image/gif/34771bddc06a0dabba0dd156635b1166/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr4/THUMBNAIL/image/gif/34771bddc06a0dabba0dd156635b1166/gr4.sml gr4 gr4.sml sml 3919 164 152 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr3/THUMBNAIL/image/gif/442ad4c6e082b5ba5c9c504b55c4814d/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr3/THUMBNAIL/image/gif/442ad4c6e082b5ba5c9c504b55c4814d/gr3.sml gr3 gr3.sml sml 2026 74 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr1/THUMBNAIL/image/gif/4a12ebebba0c444342893c0320353522/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr1/THUMBNAIL/image/gif/4a12ebebba0c444342893c0320353522/gr1.sml gr1 gr1.sml sml 2335 69 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr9/THUMBNAIL/image/gif/d958abd5b9ea5770e1cd7c567db3b577/gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr9/THUMBNAIL/image/gif/d958abd5b9ea5770e1cd7c567db3b577/gr9.sml gr9 gr9.sml sml 2339 48 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr8/THUMBNAIL/image/gif/98b8eaff048cbc4b3a7cbc616f305c74/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr8/THUMBNAIL/image/gif/98b8eaff048cbc4b3a7cbc616f305c74/gr8.sml gr8 gr8.sml sml 5650 164 192 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr7/THUMBNAIL/image/gif/6265ebabf45d2b7e5817b204fb00c1d9/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr7/THUMBNAIL/image/gif/6265ebabf45d2b7e5817b204fb00c1d9/gr7.sml gr7 gr7.sml sml 3289 134 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr2/THUMBNAIL/image/gif/7794e85951c058b3f3c8db53147892bb/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr2/THUMBNAIL/image/gif/7794e85951c058b3f3c8db53147892bb/gr2.sml gr2 gr2.sml sml 2302 54 219 IMAGE-THUMBNAIL 1-s2.0-S1532046411000062-gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1532046411000062/gr10/THUMBNAIL/image/gif/3e681f0ed9514209574121948c97d0ee/gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1532046411000062/gr10/THUMBNAIL/image/gif/3e681f0ed9514209574121948c97d0ee/gr10.sml gr10 gr10.sml sml 6878 137 219 IMAGE-THUMBNAIL YJBIN 1732 S1532-0464(11)00006-2 10.1016/j.jbi.2011.01.004 Elsevier Inc. Fig. 1 The AskHERMES architecture. Fig. 2 An excerpt of a partial table appearing in an eMedicine article. Fig. 3 An excerpt of a list from a guideline in National Guideline Clearinghouse. Fig. 4 Query term-based clustering algorithm. Fig. 5 Illustration of hierarchical clustering structure based on query terms. Fig. 6 Sample questions that different systems perform best on. Fig. 7 Scatter graph of quality of answer and number of words in question. Fig. 8 AskHERMES\u2019 answers to \u201cWhat is the cause and treatment of this old man\u2019s stomatitis?\u201d (both focuses are covered in a succinct way). Fig. 9 AskHERMES\u2019 answers to the question \u201cWhat is the difference between the Denver II and the regular Denver Developmental Screening Test?\u201d. Fig. 10 Illustration of AskHERMES\u2019 interface for ranked answers and related questions on the query \u201cwhat is the cause and treatment of this old man\u2019s stomatitis?\u201d. Table 1 A typology of question types, with representative examples, collected by Ely and associates in four studies. The left column represents the proportion of generic question types that the 4654 questions could be mapped to; questions beginning with the interrogatives \u201cWhat\u201d, \u201cHow\u201d, \u201cDo\u201d, and \u201cCan\u201d account for 2231 (or 48%), 697 (or 15%), 320 (or 7%), and 187 (or 4%) of the questions, respectively. Representative examples are in the right column. General question type (and percentage) Sample questions \u201cWhat \u2026\u201d (48%) 1. What is the cause and treatment of this old man\u2019s stomatitis? 2. What should you do with someone who is not getting better from epicondylitis after physical therapy and nonsteroidal anti-inflammatory drugs have not worked? \u201cHow \u2026\u201d (15%) 3. How long should you leave a patient on Coumadin and heparin? \u201cDo \u2026\u201d (7%) 4. Do angiotensin II inhibitors work like regular angiotensin converting enzyme inhibitors to preserve kidney function in mild diabetes? \u201cCan \u2026\u201d (4%) 5. Can Lorabid cause headaches? Others (25%) 6. I wonder if this patient could have a rotator cuff thing? Table 2 A list of online non-clinical question answering systems. System Domain Characteristics AnswerBus (http://www.answerbus.com/index.shtml) Open domain Returns relevant documents from WWW in response to an ad hoc question Ask (http://www.ask.com/) Open domain Returns relevant paragraphs in response to a particular question BrainBoost (http://www.answers.com/bb/) Open domain Returns sentences relevant to an ad hoc question EAGLi (http://eagl.unige.ch/EAGLi/) Genomics domain Returns MEDLINE documents in response to an ad hoc genomics question Start (http://start.csail.mit.edu/) Open domain Returns a short phrase in response to a factoid question Why-Question (http://lands.let.ru.nl/cgi-bin/retrieve_wikidoc.pl/) Open domain Returns relevant paragraphs in Wikipedia in response to a why-type question KnowItAll (http://www.cs.washington.edu/research/knowitall/) Open domain Returns a list of extracted relations in response of a predicative query Wolfram Alpha (http://www.wolframalpha.com) Open domain Computational knowledge engine based on internal database Table 3 Median evaluation score (with Interquartile range shown in parentheses) of Google, UpToDate and AskHERMES. Google UpToDate AskHERMES Ease of use 4 (3, 5) 4 (4, 5) 4 (3.75, 5) Quality of answer 3 (1, 4.25) 4 (1, 5) 2.5 (1, 4) Time spent (s) 2.5 (2, 5) 3 (2, 5) 4 (2, 5) Overall performance 3 (1, 4) 4 (1, 5) 3 (1, 4) Table 4 Wilcoxon signed test (based on negative ranks) for overall performance comparison of the three systems (Z is the normal approximation value; p value indicates the significance level). Z p value (2-tailed) Overall: Google \u2013 Overall: AskHERMES \u22120.800 .423 Overall: Uptodate \u2013 Overall: AskHERMES \u22121.604 .109 Overall: UptoDate \u2013 Overall: Google \u22121.175 .240 AskHERMES: An online question answering system for complex clinical questions YongGang Cao a 1 Feifan Liu a Pippa Simpson b Lamont Antieau a Andrew Bennett c d James J. Cimino e John Ely f Hong Yu a g \u204e hongyu@uwm.edu a Department of Health Sciences, University of Wisconsin-Milwaukee, Milwaukee, WI, USA b Department of Pediatrics, Medical College of Wisconsin, Milwaukee, WI, USA c Department of Psychiatry, Medical College of Wisconsin, Milwaukee, WI, USA d The Veterans Affairs Hospital, Milwaukee, WI, USA e Clinical Center, National Institutes of Health, Bethesda, MD, USA f Department of Family Medicine, University of Iowa Hospitals and Clinics, Iowa City, USA g Department of Electrical Engineering and Computer Science, University of Wisconsin-Milwaukee, Milwaukee, WI, USA \u204e Corresponding author. Address: 2400 E Hartford Ave., Room 939, Milwaukee, WI 53211, USA. Fax: +1 414 229 5100. 1 Present address: Amazon.com, Inc., Seattle, WA, USA. Abstract Objective Clinical questions are often long and complex and take many forms. We have built a clinical question answering system named AskHERMES to perform robust semantic analysis on complex clinical questions and output question-focused extractive summaries as answers. Design This paper describes the system architecture and a preliminary evaluation of AskHERMES, which implements innovative approaches in question analysis, summarization, and answer presentation. Five types of resources were indexed in this system: MEDLINE abstracts, PubMed Central full-text articles, eMedicine documents, clinical guidelines and Wikipedia articles. Measurement We compared the AskHERMES system with Google (Google and Google Scholar) and UpToDate and asked physicians to score the three systems by ease of use, quality of answer, time spent, and overall performance. Results AskHERMES allows physicians to enter a question in a natural way with minimal query formulation and allows physicians to efficiently navigate among all the answer sentences to quickly meet their information needs. In contrast, physicians need to formulate queries to search for information in Google and UpToDate. The development of the AskHERMES system is still at an early stage, and the knowledge resource is limited compared with Google or UpToDate. Nevertheless, the evaluation results show that AskHERMES\u2019 performance is comparable to the other systems. In particular, when answering complex clinical questions, it demonstrates the potential to outperform both Google and UpToDate systems. Conclusions AskHERMES, available at http://www.AskHERMES.org, has the potential to help physicians practice evidence-based medicine and improve the quality of patient care. Keywords Clinical question answering Question analysis Passage retrieval Summarization Answer presentation 1 Introduction Physicians generate up to six questions for every patient encounter [1\u20136], and these questions may be of a variety of types. Although it is important for physicians to meet their information needs, studies have shown that many of their questions go unanswered. For example, Ely and colleagues observed that physicians did not pursue answers to 45% of the 1062 questions posed in clinical settings, often because they doubted they could find good answers quickly, and for those they did pursue answers to, they failed to find answers to 41% of them [7]. As a result, more than 67% of the clinical questions posed by physicians remained unanswered. One way to meet information needs is to refer to the published literature for related clinical evidence [8]. Although original research articles that are both scientifically rigorous and clinically relevant appear in high concentrations in only a few select journals (e.g., The New England Journal of Medicine, Annals of Internal Medicine, JAMA, and Archives of Internal Medicine), much clinical evidence appears in a wide range of other biomedical journals [9]. Even with the development of search engines for facilitating relevant biomedical literature searching, the needs of physicians still cannot be met properly, as an evaluation study showed that it took an average of more than 30min for a healthcare provider to search for an answer from MEDLINE, which made \u201cthis type of information seeking is practical only \u2018after hours\u2019 and not in the clinical setting\u201d [10]. Internet search engines (e.g., Google) provide another solution for physicians seeking answers to their questions [11\u201313]. However, the success of Internet searching often depends on skilled physicians [14], and Internet searches inevitably pose challenges in information content relatedness and quality [15\u201324]. Additionally, traditional search engines (e.g., Google) return long lists of articles rather than self-contained answers to specific questions, and many of these articles turn out to be irrelevant to specific questions due to the inevitable query ambiguity of open-domain search engines. In a recent study [25], for instance, PubMed appeared to perform better than Google Scholar at locating relevant and important literature articles to answer specific drug-related questions. The importance of answering physicians\u2019 questions related to patient care has motivated the development of many clinical resources (e.g. UpToDate, Thomson Reuters, eMedicine, National Guideline Clearinghouse) to provide high-quality summaries of clinically relevant information. These summaries, however, are written by domain experts who manually review the literature concerning specific medical topics. As such, these resources may be limited in scope and timeliness. An evaluation study showed that when provided with the 10 most commonly used clinical resources without time constraint physicians were able to answer only 70% of the 105 questions randomly selected from the 1062 questions collected by Ely and his associates [26]. Another study found that despite UptoDate being the top target site used by physicians, only 10.8% physicians use it to do research on rare diseases [27]. In fact, UpToDate was reported to be used infrequently in a number of evaluation studies [13,28\u201331]. In addition, as databases become more complex, it takes a correspondingly greater amount of time to search for an answer even in commercial clinical databases. For example, one evaluation study [32] has shown that it takes over four minutes to search for answers in UpToDate. Studies have found, however, that when a search takes longer than two minutes, it is likely to be abandoned [10,26]. Question answering (QA) systems have the potential to overcome these shortcomings. First, to maximize coverage and improve timeliness, they can automatically mine relevant knowledge from multiple sources and summarize the results to form answers based on important concepts embedded in the question. Secondly, to improve efficiency, they can provide succinct answers rather than entire documents, which can help users pinpoint useful information quickly. However, due to the difficulties that machines have understanding text, current QA approaches are mainly focused on answering factoid questions based on fact extraction or specific question types, such as definitional questions. Unfortunately, because of the potential for inaccurate mining results in general, such common sense factoid QA systems provide inferior usability compared to emerging manually managed fact databases, such as Wikipedia, Answers, Freebase, etc. Our goal is to go beyond such factoid systems and develop a QA system with the ability to handle the kinds of complex questions that are commonly asked in the clinical domain through the use of a structured domain-specific ontology. Domain-specific knowledge can be used to enhance the capabilities of a system to automatically answer questions oriented to sophisticated problem-solving rather than mere fact discovery, which is vital for answering questions asked in the clinical domain. We hypothesize that domain knowledge can greatly enhance the machine learning-based computational model for information retrieval (IR), and text-mining technologies can be coupled with IR to present semantically inherent answer summarization. Our focus is on improving answer quality, especially in response to complex clinical questions, so that instead of providing a single fact or a list of documents, the system will decrease human effort by extracting the most pertinent information to a given question from the large amount of literature in the clinical domain. Our fully automated system AskHERMES \u2013 Help physicians Extract and aRticulate Multimedia information from literature to answer their ad hoc clinical quEstionS [33\u201343] \u2013 automatically retrieves, extracts, analyzes, and integrates information from multiple sources that include the medical literature and other online information resources to formulate answers in response to ad hoc medical questions. Fletcher [9] identified three basic skills necessary for physicians to manage their information needs: (1) find potentially relevant information, (2) judge the best from a much larger volume of less credible information, and (3) judge whether the best information retrieved provides sufficient evidence for making clinical decisions. AskHERMES addresses the first two components by finding and filtering clinical information. We previously found that AskHERMES outperforms several other systems (e.g., PubMed) for answering definitional questions [38,39]. Currently, AskHERMES attempts to answer all types of clinical questions, and this paper reports the development, implementation, and evaluation of the AskHERMES system. 2 Background Question answering can be considered an advanced form of information retrieval. In the 1990s, the Text REtrieval Conference (TREC) supported research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In 1999, TREC introduced a question answering (QA) track, and the earliest instantiations of the QA track focused on answering factoid questions (e.g., \u201cHow many calories are there in a Big Mac?\u201d). Since 2003, TREC has addressed scenario questions (e.g., definitional questions such as \u201cWhat is X?\u201d) that require long and complex answers. TREC Genomics introduced passage retrieval for question answering in the genomics domain [44,45]. Development in question answering has mainly focused on improving the underlying answer extraction performance, especially against a standard set of questions, which has been, for example, the objective of the QA track at TREC [46]. However, improvements by such batch-run experiments may not translate into actual benefits for end users [47]. To date, few question answering systems have focused on designing effective interfaces. Many existing commercial search engines, such as Google, Yahoo and Bing, only return a long ranked list of relevant documents, an interface that is similarly used by PubMed in the biomedical domain. As discussed earlier, such interfaces are insufficient for providing succinct and relevant answers, which is an especially pertinent issue for physicians who have little time for wading through long lists of retrieved documents. Search results clustering, a visualization technique first introduced in the Scatter\u2013Gather system [48], attempts to provide the user with essential information about the structure of topics in the retrieved results, and similar approach has been applied in medical literature search [49] and refined by several search sites, including Vivisimo, iBoogie, and the Carrot system. However, the output of these systems is still based on traditional retrieval results, and users must read through a ranked document list even for a query on a single topic. Addressing clinical question answering has been an active effort of the biomedical community. Cimino et al. [50] tagged clinical questions semantically to make them generic (for example, \u201cDoes aspirin cause ulcers\u201d became \u201cDoes <drug> cause <disease>\u201d). Zweigenbaum [51,52] surveyed the feasibility of question answering in the biomedical domain. Rinaldi and colleagues [106] adapted an open-domain question answering system to answer genomic questions (e.g., \u201cWhere was spontaneous apoptosis observed?\u201d). The EpoCare project (Evidence at Point of Care) proposed a framework to provide physicians the best available medical information from both literature and clinical databases [53,54]. Infobuttons [8,31,55\u201362] served as a medical portal to external information retrieval systems (e.g. PubMed) and databases (e.g., UpToDate). The CIQR (Context Initiated Question Response) project [63] focuses on the analysis of the types of questions asked by clinician when looking up references, which allows speech input in the clinical setting [64]. To incorporate patient specific information in seeking relevant and up-to-date evidence, the PERSIVAL (Personalized Retrieval and Summarization of Image, Video and Language Resource) system [65\u201367] was designed to provide personalized access to a distributed digital library. Automatically analyzing clinical questions is an important step toward answering clinical questions. Physicians often ask complex and verbose questions comprising a wide variety of types. The typology of question types with representative examples collected in four studies [7,26,68,69] was shown in Table 1 . There is a wealth of research proposing ways of categorizing such ad hoc questions. Ely and colleagues manually mapped 1396 clinical questions [70] to a set of 69 question types (e.g. \u201cWhat is the cause of symptom X?\u201d and \u201cWhat is the dose of drug X?\u201d) and 63 medical topics (e.g. drug or cardiology). Cimino and associates [50] predefined a set of generic question types (e.g., \u201cWhat is treatment for disease?\u201d) and then mapped ad hoc clinical questions to those types. Seol and associates [71] identified four major question types: treatment, diagnosis, etiology, and prognosis. Such typologies offer different solutions for automated systems to overcome the wide range of variability in the forms that clinical questions may take. Other researchers have applied the popular Population, Intervention, Comparison, and Outcome (PICO) framework as a way of dealing with the variability in clinical questions [53,54,72\u201375]. Information retrieval component can be integrated with question analysis in the question answering system to retrieval relevant documents. There have been different models developed for information retrieval, including Boolean models [76], vector space models [77], ontology-based approaches [78], latent semantic indexing [79,80], and language models [81]. In addition, many existing systems turn to external knowledge to support deeper semantic analysis in question answering. SemRep [82,83] maps biomedical text to the Unified Medical Language System (UMLS) [84] concepts and represents concept relations with the UMLS semantic relationships (e.g., TREATS, Co-OCCURS_WITH, and OCCURS_IN). The SemRep summarization system condenses the concepts and their semantic relations to generate a short summary [83]. Essie is an information retrieval engine developed and used at the US National Library of Medicine (NLM) that incorporates knowledge-based query expansion and heuristic ranking [85]. CQA-1.0 [72] is designed as a clinical question answering system. Unlike AskHERMES that process ad hoc natural language question and incorporates mostly statistical and machine-learning approaches, CQA-1.0 requires a user to enter a question by the PICO framework (Patient, Intervention, Comparison, and Outcome), and provides semantic analysis at document and text level, identifying PICO elements and documents that are of clinical relevance. Sneiderman et al. [82] integrated three systems (SemRep, Essie, and CQA-1.0) to achieve the best information retrieval system (which outperformed each of the three systems) in response to clinical questions. None of the aforementioned systems, however, are available online for testing. Although question answering is an active research field, most online QA systems as shown in Table 2 are not applicable to the clinical domain. The contributions of the AskHERMES system include: (1) A tailored machine learning model that automatically extracts information needs from complex clinical questions. (2) A dynamic model for hierarchically clustering sentences as answers and a new sentence ranking function. (3) A new answer presentation model in which answers, rather than document lists, are organized by question-orientated keywords. (4) Finally, AskHERMES is the only online system that attempts to automatically answer the full range of complex clinical questions. 3 Methods Fig. 1 shows the system architecture of AskHERMES, which takes as its input a natural language clinical question. Question Analysis automatically extracts information needs from the question and outputs a list of query terms. The UMLS knowledge resource is used for query term expansion. The Related Questions Extraction module returns a list of similar questions. Information Retrieval returns relevant documents that have been locally indexed. Information Extraction identifies relevant passages. Summarization & Answer Presentation aggregates answer passages, removes redundant information, automatically generates structured summaries, and presents the summaries to the user posing the question. In the following sections, we will provide a detailed description of each component. 3.1 Data sources and pre-processing 3.1.1 Data collection At the time of evaluation, AskHERMES had indexed over 17 million MEDLINE abstracts (1966\u20132008); 2732 eMedicine documents (downloaded in 2008); 2254 clinical guidelines (downloaded in 2008); 167,000 full-text articles (downloaded from PubMed Central in 2008); and 735,200 Wikipedia documents. In total, there are 15,046,596 articles containing 3 million unique word tokens. 3.1.2 Pre-processing for retaining semantic content Most NLP approaches focus on mining narrative texts in different kinds of documents or articles, which, however, would lose semantic information embedded in tables and lists. It happens quite often particularly in clinically relevant articles such as those from eMedicine, clinical guidelines, and Wikipedia. For example, Fig. 2 shows a partial table in an article from eMedicine. The cells (e.g., \u201c1994,\u201d \u201cyear,\u201d \u201c2.5%\u201d and \u201cdeath\u201d) in the table alone are of little meaning, but together, the content can help correctly answer questions like \u201cWhat is the death rate for Acute Coronary Syndromes in 1994?\u201d Similarly, grouping items from the list in Fig. 3 allows AskHERMES to answer such questions as \u201cWhat is involved in Survival Skills for diabetes patients?\u201d Therefore, we implemented manually curated rules to retain semantic information contained in each table and list. Specifically, for each table, all the textual information in each row or column (depending on the header location) together with the corresponding header and caption text would be formed as a separate passage to be indexed by the system; for each list, we treated it as a tree and each non-leaf branch node (including the root node) would generate a separate passage to be indexed, by collapsing all the nodes it contains and then combining the corresponding textual information with the caption text. In the meantime, we removed noisy short anchor texts (e.g., \u201cContact Us\u201d) from the raw text before indexing. Another strategy we took in the pre-processing is merging all the section titles with any sentence within the corresponding section, with the assumption that section titles in the articles carry important semantic content which may not necessarily be explicitly described in narrative paragraphs. Despite the simplicity of this approach, it has improved the performance of AskHERMES. For example, by merging the title \u201ccures for type 1 diabetes\u201d with the sentence \u201csimultaneous pancreas-kidney transplant is a promising solution, showing similar or improved survival rates over a kidney transplant alone\u201d, the merged text can be recognized by AskHERMES as an answer to the question \u201cWhat is the cure for type 1 diabetes?\u201d, which would otherwise be missed because the sentence alone does not incorporate the word \u201ccure\u201d, and the title alone does not provide a description of a potential cure. 3.2 Question analysis In the open domain, a common approach for question analysis is to map questions into a predefined question template (e.g., \u201cWhat-type\u201d and \u201cHow-type\u201d) [86]. Such an approach has also been implemented into most of the existing online QA search engines (e.g., AnswerBus). Such template-driven approaches have significant limitations because they cannot handle the variation that is abundant in clinical questions. For example, three questions, shown below, belong to three different templates but require the same answer: (a) How should I treat polymenorrhea in a 14-year-old girl? (b) What is the treatment (or therapy) of polymenorrhea for a 14-year-old girl? (c) Who can tell me the treatment of polymenorrhea for a 14-year-old girl? Furthermore, many clinical questions cannot be mapped to a specific template, as shown in the example below: \u201cThe maximum dose of estradiol valerate is 20 milligrams every 2weeks. We use 25mg every month which seems to control her hot flashes. But is that adequate for osteoporosis and cardiovascular disease prevention?\u201d Therefore, a clinical QA system must have the ability to deal with a wide variety of complex questions, many of which cannot be answered by approaches depending on predefined templates. Accordingly, we have developed novel approaches to automatically extract information needs from complex questions [40]. Firstly, we classify a question into 12 general topics to facilitate information retrieval. Those topics include device, diagnosis, epidemiology, etiology, history, management, pharmacology, physical finding, procedure, prognosis, test and treatment & prevention, which have been used to annotate the 4654 clinical questions [40] by clinicians who recorded the questions. For example, the question above represents a medication, and we can therefore identify a pharmacological database (e.g., Thomson Reuters) as the best resource for potential answer extraction. Secondly, keywords that capture the most important content of the question are identified automatically. In the same question example, the keywords are \u201cestradiol valerate\u201d and \u201costeoporosis and cardiovascular disease prevention\u201d. The keywords can be used as query terms for retrieving relevant documents as well as the anchor terms for answer extraction. We developed supervised machine-learning approaches to automatically classify a question by general topics and to automatically identify keywords. For question classification, we explored several learning algorithms, showing support vector machines (SVMs) [87] achieved the best result. Since a question can be assigned to multiple topics, we developed a binary classifier (Yes or No) for each of the 12 topics. For keyword identification, we formulated it as sequence labeling problem using conditional random fields (CRFs) [88] model. In addition to basic lexical features(e.g. unigram, bigram) and syntactic features(e.g. parts-of-speech), we incorporated the lexical tool MMTx, an implementation of MetaMap [89], to map text to the UMLS concepts and semantic types as learning features for both tasks. Using an annotated collection of the 4654 clinical questions as the training and testing data, our results show an average of 76.5% F1-score for question classification and 58% F1-score for keyword extraction. Details of this work appear in [40]. 3.3 Document retrieval We integrated the latest version of the probabilistic relevance model BM25 [90] with the AskHERMES system for document retrieval, as it proved to be the best performing system [90,91] for tasks such as those at the recent Text REtrieval Conference (TREC) and held the advantage of simplicity, interpretation, and speed of computation. We empirically tuned the retrieval model in our system. 3.4 Passage retrieval Previous work has shown that QA users prefer answers to be passages rather than sentences [92]. This is particularly true in our task, since much important content in terms of discourse relations (e.g., causal and temporal) is missing if groups of isolated sentences are extracted as answers. Much of the work done previously has defined a passage as a naturally occurring paragraph [93] or a fixed window [94]. However, candidate passages determined by such a definition will sometimes be too verbose to be of practical value for question answering. Therefore, we developed an approach that dynamically generates passage boundaries. Specifically, we define a passage in AskHERMES as one or more adjacent sentences, in which every sentence incorporates one or more query terms from the question. Our approach is different from TextTiling [95], a popular method for multi-paragraph segmentation, in that the posed question plays an important role for passage recognition in our system. As part of this work, we defined a novel scoring function for measuring the similarity between a sentence and the question (SS ), which integrates both word-level and word-sequence-level similarity between a question and a sentence in the candidate answer passage, as shown in: (1) S s = S d \u00b7 TF q \u00b7 UT q \u00b7 LCS L q 2 + L p 2 , s \u2208 d Sd denotes question-document similarity based on BM25 similarity, TFq is the total number of query terms that appear in the sentence, UTq is the unique number of query terms in the sentence, and LCS is the similarity between the sentence and the whole question based on the longest common subsequence (LCS) score [96]. LCS is an algorithm that identifies the longest subsequence common to all sequences in a set of sequences (typically just two) and is recognized as being very important for measuring similarity for many text processing applications, e.g. summarization evaluation (ROUGE score [97]). Incorporating the LCS score in the sentence score function can capture more detailed dependency information than mere bag-of-words or even bigrams. For example, given the question \u201cHow do I treat this man\u2019s herpes zoster?\u201d the candidate answers represented by sentences (1) and (2) below, have the same words and frequency that are matched against the extracted query terms (\u201ctreat\u201d, \u201cherpes\u201d, and \u201czoster\u201d), which means that an approach without LCS would rank the two answers the same. However, LCS assigns sentence (1) a value of 3 and sentence (2) a value of 2, giving sentence (1), which is the better answer for meeting the needs of the question, a higher ranking than sentence (2). (1) Corticosteroids have been used to treat herpes zoster for much longer than the antiviral drugs, but the effect of corticosteroids on PHN does not appear to be consistent. (2) A significant proportion of older subjects with herpes zoster develop post-herpetic neuralgia (PHN), a chronic condition that is difficult to treat. Once the relevance score for each sentence is obtained, the score of a passage Sp is determined by the empirical metrics shown in: (2) S p = max ( S s 1 n ) + min ( S s 1 n ) , max ( S s 1 n ) < 2 \u00d7 min ( S s 1 n ) max ( S s 1 n ) , otherwise where n is the number of sentences in this passage, max(Ss ) is the maximum relevance score among all the sentences, and min(Ss ) is the minimum score among all the sentences. 3.5 Summarization and answer presentation We developed a new question-tailored summarization and answer presentation approach based on clustering technique. As stated earlier, clinical questions are typically long and verbose and frequently relate to multiple topics. Our automatic keyword extraction model effectively extracts content-rich keywords from ad hoc questions, and such keywords can then be used to hierarchically structure the summarized answers. For example, in the question \u201cHow should I treat polymenorrhea in a 14-year-old girl?\u201d the terms \u201ctreat\u201d, \u201cpolymenorrhea\u201d, \u201c14-year-old\u201d and \u201cgirl\u201d are four important content terms, and an ideal answer would incorporate all four terms. However, in reality, most answer passages incorporate fewer content terms and sometimes contain only one of the four terms. We speculate that users would be able to identify the answer more efficiently if the answers could be grouped by the content terms. Another benefit of this framework is that if physicians were interested in finding a general treatment for \u201cpolymenorrhea\u201d, they would be able to examine the answer group containing \u201ctreat\u201d and \u201cpolymenorrhea\u201d without any age-related terms. Responding to this motivation, we developed a novel summarization system based on structural clustering using content-bearing terms that provides a more user-friendly answer presentation interface to help physicians quickly and effectively browse answer clusters. 3.5.1 Topical clustering, ranking and hierarchical answer presentation To extend the spirit of search results clustering, in this paper we propose an innovative hierarchical answer presentation interface in which all relevant passages are grouped into different topics based on two-layer clustering. We presented query-related answer passages that were structurally clustered based on content-bearing query terms rather than merely providing a ranked list of documents as output. Topic labels are assigned to each cluster in AskHERMES using query terms and expanded terms from the UMLS. Using a topic-labeled tree structure generated from first-layer clustering, physicians can easily locate information of interest before delving into more detail. In addition, second-layer clustering provides more refined categories for multi-faceted answers. Each leaf node is a small passage rather than a document, which facilitates browsing. For query term-based clustering, we use the original query terms (Q) that appear in the question and the UMLS query expansion terms (QE). We first group together all the synonyms (a query term and its expanded terms are represented using the corresponding query term) and then generate root clusters, each of which contains different combinations of these synonym concepts. More formally, we assume that qi is the ith query term in Q and has Mi synonyms qei1 , qei2 , qei3 , \u2026, qeiMi . By \u201cgroup together\u201d, we mean that we use qi to stand for all the synonyms in the clustering, i.e. the ith concept. Passages containing different combinations of these concepts in the root node are divided into different clusters. Different variants from each synonym combination additionally lead to hierarchical subclusters. Fig. 4 further illustrates how the \u201cbucket-based clustering\u201d algorithm works, leading to the kind of hierarchical clustering structure shown in Fig. 5 . All buckets/clusters can contain multiple attached passages. All these hierarchical buckets are generated dynamically, which prevents a combination explosion, an issue especially relevant to complex questions. For a more compact answer presentation, we ignore root nodes to which no sentences are directly attached and promote their children branches one level up. As in Fig. 5, in cases where there is no sentence containing all three sets of the synonym terms in node \u201cQ1, Q3, Q4\u201d, only its children branches are displayed. We rank clusters based on the query terms appearing in the cluster. We use the same ranking strategy as query weighting in Section 3.2 and sum up all the weights of the query term that occur as the ranking score of this cluster. We also rank the generated root buckets by summing up the IDF values of the query concept it covers. Since there are several synonyms for each query concept in the root buckets, we needed to find a way to choose an IDF value for each collapsed concept. We chose the minimum IDF value among synonyms based on the observation that some common words have rarely used synonyms with very high IDF values. To further facilitate browsing the results, for the top p ranked clusters (not root clusters), if the number of passages in them exceeds q, we conduct a second layer of clustering based on the content of the passages along more refined semantic dimensions. In our current system, both p and q are empirically assigned to 5. For this task, we used Lingo [98], which uses a single vector decomposition approach to find the common labels for clusters and retrieves corresponding content (candidate answer passages in our system) for each cluster. This can generate readable labels for clusters and allow a passage to be put into multiple clusters instead of hard splitting. 3.5.2 Redundancy removal based on longest common substring Because candidate answers are extracted from multiple sources, it is inevitable that they will contain some redundant information. To address this issue, we explored the longest common substring (LCSubstring ) [99], which is an algorithm for identifying the longest string (or strings) that is a substring (or are substrings) of two or more strings. We applied LCSubstring in our system to remove redundancy among sentences as well as passages that were extracted as candidate answers. The difference between the longest common substring and the longest common subsequence (LCS) is that the former is required to be a continuous substring from the original strings, while the latter consists of all the common subsequences that share the same order but include intervals in between the original strings. Thus, the longest common substring has more constraints than the LCS, and we use it for redundancy removal in our system. To ensure that two units are similar enough to be considered duplicates, we set a threshold empirically. Although the longest common substring has been used for automatic summarization evaluation tasks such as ROUGE-L [100], and has been used for paraphrasing [101,102], it has not yet been reported for removing redundancy as a part of summarization. 4 System implementation The AskHERMES system is built on the J2EE framework, in which JBoss is used for the application server and the JBoss Seam for building the user interface. JBoss has built-in EJB (Enterprise JavaBeans) caching and a reuse mechanism that enables heavy load accessing. We also built a round-robin load-balancer in the front web server to distribute the accessing load among six backend servers. The six servers are running linux/solaris operating systems in which AskHERMES is deployed. Currently, AskHERMES system response time averages 20s. 5 Results In this section, we report our pilot evaluation of the AskHERMES system, which is compared with two frequently used state-of-the-art systems: the commercial Google search engine and the UpToDate clinical database system. 5.1 Evaluation design To evaluate our AskHERMES system, we randomly selected 60 questions from the ClinicalQuestions collection [40] and asked three physicians (AB, JJC, and JE) for a manual evaluation of the output. For comparison, we also evaluated the Google search engine (using both Google and Google Scholar) and the UpToDate database system on the same set of questions. Our goal was to examine how well each of the three systems answer the questions and define the four metrics in the evaluation as follows: (1) Ease of Use (scale of 1 to 5). (2) Quality of Answer (scale of 1 to 5). (3) Time Spent (in s). (4) The Overall Performance (scale of 1 to 5). For this pilot evaluation, each physician subject has been presented with a mutually exclusive set of 20 questions randomly selected from our question collection. For each question, each subject has been asked to identify answers from each of the three systems: AskHERMES, Google, and UpToDate, and then assign a score for each system on each evaluation metrics defined above. 5.2 Performance of AskHERMES in comparison with Google and UpToDate Table 3 shows the results of three systems. These results show that AskHERMES\u2019 Ease of Use score was very competitive with the same median evaluation score of 4 as both of the other systems, achieving an average score of 4.079 compared to the best score of 4.132 for UpToDate, which suggests that our clustering-based presentation interface is quite effective and beneficial in terms of user friendliness. This is also reflected in the fact that although AskHERMES has a slower response time than the other systems (a several second delay), the system\u2019s ease of use is compensating for that lost time so that Time Spent scores are comparable. With regard to Quality of Answer, UpToDate attained the best score of 4, which is to be expected since it incorporates a rich domain-specific knowledge resource that is more clinically oriented. Although AskHERMES received the lowest score in this category, its difference with Google in this respect is not statistically significant (Wilcoxon signed test, p >0.1). Similarly, AskHERMES\u2019 Overall Performance evaluation score was slightly lower than the UpToDate system, but as Table 4 shows, the pair-wise performance comparison of the three systems shows no statistically significant differences (p >0.1) based on a two-sided Wilcoxon signed rank test. We also found that AskHERMES yielded the smallest standard deviation in the metrics for Quality of Answer (1.445) and Overall Performance (1.427) compared to Google (1.706/1.603) and UpToDate (1.653/1.636), demonstrating its robustness and ability to adapt. 5.3 Impact of question length on the quality of answers To gain further understanding of the quality of AskHERMES\u2019 performance, we examined the questions that each system performed best on, as shown in Fig. 6 . We found that each system has its own strengths for different kinds of questions; as Fig. 6 shows, AskHERMES performs best at answering complex questions within specific contexts, such as relationships, comparisons, and restrictions; Google performs best at answering short questions that can be answered in the open domain; and UpToDate performs best at answering short clinical questions on specific topics. Fig. 7 shows the relationship between answer quality and the number of words in a given question, demonstrating that Google and UpToDate\u2019s performance fluctuates for different questions while AskHERMES performs consistently across different questions. We observed that when questions contain more than 25\u201335 words, Google and UpToDate produce particularly poor answers compared to our system. Furthermore, we found that only when the word count of a question is less than 25 does UpToDate perform statistically better (p <0.04) than AskHERMES for Quality of Answer. These results demonstrate that AskHERMES has a great potential for answering clinical questions, which are usually long and complex, despite its being a fully automatic system, in contrast to UpToDate, which relies on a great deal of manual effort. 6 Discussion Currently, the development of AskHERMES is in an early stage, and the data resources it has indexed are very limited. Moreover, AskHERMES is an automatic QA system, while UpToDate uses domain experts to manually select only clinically related knowledge. Additionally, AskHERMES currently does not have access to all the full-text articles on a subject. As open-access full-text biomedical articles become increasingly available, we speculate that the performance of AskHERMES will be greatly improved. Our pilot evaluation found that AskHERMES is competitive with other clinical information resources. The overall evaluation score of AskHERMES is not as high as UpToDate, but there were no statistically significant differences, according to Wilcoxon signed tests. Note that this is still a preliminary evaluation of our system, and the statistical test might be underpowered based on the current sample size, which therefore needs more investigation and validation on a larger scale evaluation with more independent samples in future work. Notably, we found that AskHERMES can actually perform better with longer and more complex clinical questions, which suggests that its integration of a question analysis component and domain-specific ontology offers AskHERMES the ability to understand and correctly recognize the information needs of physicians. UpToDate is compiled by experts, while AskHERMES automatically assembles information from natural language texts. Frequently, the texts from which AskHERMES extracted information are not clinically relevant, and therefore, the non-relevancy leads to a lower performance. In addition, AskHERMES is built upon limited text resources, while UpToDate and Google Scholar have a much richer resource including full-text articles and e-books. The aforementioned factors all contribute to AskHERMES\u2019 performance. We emphasize that despite the advantages of UpToDate and Google Scholar in their resources, the differences between AskHERMES and Google Scholar or UpToDate as shown in Table 4 were not statistically significant. Moreover, AskHERMES has achieved the same overall performance score (as shown in Table 3), suggesting that AskHERMES has the potential to outperform UpToDate and Google Scholar if rich resources are available. On the other hand, AskHERMES holds a number of advantages over Google and UpToDate. First, the novel clustering-based summarization and presentation it offers have a clear advantage over the long document lists retrieved from Google, with the potential to save busy physicians\u2019 time in retrieving potentially irrelevant documents. Second, AskHERMES is a fully automatic system, providing an unbeatable advantage over UpToDate in that it does not rely on time-consuming, labor-intensive human effort to maintain and update the system. Third, to use UpToDate, physicians are required to formulate their information needs clearly and succinctly and are required to know the workings of the UpToDate system, while AskHERMES requires little training, as the system has been developed to do this work itself. Furthermore, there are some specific features of our system beyond the evaluation itself that can be summarized as follows: (1) By using structural clustering based on content-bearing query terms, AskHERMES can deal with questions covering multiple focuses or topics. For example, in the question \u201cWhat is the cause and treatment of this old man\u2019s stomatitis?\u201d there are two foci: \u201ccause\u201d and \u201ctreatment\u201d, and it is very difficult to find a single sentence or a succinct passage that can cover both of them. AskHERMES can automatically separate \u201ccause\u201d and \u201ctreatment\u201d based on query term-based clustering, as shown in Fig. 8 . (2) LCS-based ranking and matching enables AskHERMES to immediately identify the best answer when the answer is similar enough to the question no matter how complex the question is. Fig. 9 shows the answer to the question \u201cWhat is the difference between the Denver II and the regular Denver Developmental Screening Test?\u201d where the highest-ranked sentence in our system\u2019s output is the correct answer. (3) In order to help physicians easily obtain information from different points of view, the answer presentation interface in AskHERMES provides both clustered answers (illustrated in Fig. 8) and ranked answers (illustrated in Fig. 10 ). If a user\u2019s question is simple enough or very specific, the user may find answers from ranked answers more quickly. Moreover, related questions (Fig. 10) retrieved from our question collection are also provided by our \u2018interactive\u2019 interface to assist physicians who may want to view answers to related questions. In summary, clinical question answering is a very challenging task, and no current system can always perform well on the myriad questions that can be asked of it. AskHERMES provides a practical and competitive alternative to help physicians find answers. 7 Conclusions and future work We present our online clinical question answering system, AskHERMES, which aims to help physicians quickly meet their information needs. The system relies on the use of supervised and unsupervised learning techniques in different components for exploring various linguistic features. AskHERMES is currently able to analyze and understand complex clinical questions of diverse types that cannot be answered by factoids or single sentences. Our pilot evaluation shows that AskHERMES performs comparably to such state-of-the-art systems as Google and the UpToDate. In particular, our system demonstrates a better ability to answer long and complex clinical questions than other systems, showing robustness across questions of different word counts. In general, according to our preliminary results, there were no statistically significant differences between AskHERMES and the other two systems. Since AskHERMES currently does not integrate the clinical evidence identification component that is manually entered by UpToDate, we plan to develop an automatic system to recognize clinical information to further enhance the answer quality of AskHERMES. Instead of document-based retrieval, we will investigate retrieving clinical information directly based on passage units for which we will also analyze more systematic ways of integrating keyword information. In addition, we are seeking more effective ways for improving the precision of query expansion, as explored in [103\u2013105]. Finally, a future line of research is to conduct more extensive evaluations on the AskHERMES system, including intrinsic evaluation of clustering approach for summarization, extrinsic evaluation of the whole system using a larger data set as well as comparing with existing systems, such as Essie system (http://essie.nlm.nih.gov/), and Semantic Medline (http://skr3.nlm.nih.gov/SemMedDemo/). Acknowledgments The authors acknowledge support from the National Institute of Health (NIH), Grant Number 2R01LM009836. Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect the views of the NIH. References [1] T. Timpka E. Arborelius The GP\u2019s dilemmas: a study of knowledge need and use during health care consultations Methods Inform Med 29 1990 23 29 [2] G.R. Bergus C.S. Randall S.D. Sinift D.M. Rosenthal Does the structure of clinical questions affect the outcome of curbside consultations with specialty colleagues? Arch Family Med 9 2000 541 547 [3] J.W. Ely R.J. Burch D.C. Vinson The information needs of family physicians: case-specific clinical questions J Family Pract 35 1992 265 269 [4] J.A. Osheroff D.E. Forsythe B.G. Buchanan R.A. Bankowitz B.H. Blumenfeld R.A. Miller Physicians\u2019 information needs: analysis of questions posed during clinical teaching Ann Int Med 114 1991 576 581 [5] D.G. Covell G.C. Uman P.R. Manning Information needs in office practice: are they being met? Ann Int Med 103 1985 596 599 [6] R. Smith What clinical information do doctors need? BMJ 313 1996 1062 1068 [7] J.W. Ely J.A. Osheroff M.L. Chambliss M.H. Ebell M.E. Rosenbaum Answering physicians\u2019 clinical questions: obstacles and potential solutions J Am Med Inform Assoc 12 2005 217 224 [8] Cimino JJ, Li J, Graham M, Currie LM, Allen M, Bakken S, et al. Use of online resources while using a clinical information system. In: AMIA ann symp proc; 2003. p. 175\u20139. [9] R.H. Fletcher S.W. Fletcher Evidence-based approach to the medical literature J Gen Int Med 12 Suppl. 2 1997 S5 S14 [10] W.R. Hersh M.K. Crabtree D.H. Hickam L. Sacherek C.P. Friedman P. Tidmarsh Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions J Am Med Inform Assoc 9 2002 283 293 [11] R.J. Cullen In search of evidence: family practitioners\u2019 use of the Internet for clinical information J Med Lib Assoc 90 2002 370 379 [12] M.B. Stephens A.M. Von Thun Military medical informatics: accessing information in the deployed environment Military Med 174 2009 259 264 [13] D.R. Kitchin K.E. Applegate Learning radiology a survey investigating radiology resident use of textbooks, journals, and the internet Acad Radiol 14 2007 1113 1120 [14] H. Tang J.H. Ng Googling for a diagnosis \u2013 use of Google as a diagnostic aid: internet based study BMJ 333 2006 1143 1145 [15] G.P. Purcell P. Wilson T. Delamothe The quality of health information on the internet BMJ 324 2002 557 558 [16] A.R. Jadad A. Gagliardi Rating health information on the Internet: navigating to knowledge or to Babel? JAMA 279 1998 611 614 [17] W.M. Silberg G.D. Lundberg R.A. Musacchio Assessing, controlling, and assuring the quality of medical information on the Internet: Caveant lector et viewor \u2013 let the reader and viewer beware JAMA 277 1997 1244 1245 [18] E. Glennie A. Kirby The career of radiography: information on the web J Diag Radiogr Imaging 6 2006 25 33 [19] S. Childs Judging the quality of internet-based health information Perform Meas Metrics 6 2005 80 96 [20] K.M. Griffiths T.T. Tang D. Hawking H. Christensen Automated assessment of the quality of depression websites J Med Int Res 7 2005 e59 [21] K.M. Griffiths H. Christensen Quality of web based information on treatment of depression: cross sectional survey BMJ 321 2000 1511 1515 [22] J.C. Wyatt Commentary: measuring quality and impact of the World Wide Web BMJ 314 1997 1879 1881 [23] M. Benigeri P. Pluye Shortcomings of health information on the Internet Health Promot Int 18 2003 381 386 [24] R.J. Cline K.M. Haynes Consumer health information seeking on the Internet: the state of the art Health Educ Res 16 2001 671 692 [25] M.K. Freeman S.A. Lauderdale M.G. Kendrach T.W. Woolley Google Scholar versus PubMed in locating primary literature to answer drug-related questions Ann Pharmacother 43 2009 478 484 [26] J.W. Ely J.A. Osheroff M.H. Ebell G.R. Bergus B.T. Levy M.L. Chambliss Evans ER: analysis of questions asked by family doctors regarding patient care BMJ 319 1999 358 361 [27] G. De Leo C. LeRouge C. Ceriani F. Niederman Websites most frequently used by physician for gathering medical information AMIA Annu Symp Proc 2006 902 [28] G. McCord W.D. Smucker B.A. Selius S. Hannan E. Davidson S.L. Schrop Answering questions at the point of care: do residents practice EBM or manage information sources? Acad Med 82 2007 298 303 [29] F. Goodyear-Smith N. Kerse J. Warren B. Arroll Evaluation of e-textbooks. DynaMed, MD Consult and UpToDate Aust Family Phys 37 2008 878 882 [30] J. Phua T.K. Lim How residents and interns utilise and perceive the personal digital assistant and UpToDate BMC Med Educ 8 2008 39 [31] J.J. Cimino D.V. Borotsov Leading a horse to water: using automated reminders to increase use of online decision support AMIA Annu Symp Proc 2008 116 120 [32] A. Hoogendam A.F. Stalenhoef P.F. Robbe A.J. Overbeke Answers to questions posed during daily patient care are more likely to be answered by UpToDate than PubMed J Med Int Res 10 2008 e29 [33] Yu H, Sable C, Zhu HR. Classifying medical questions based on an evidence taxonomy. In: Proceedings of the AAAI 2005 workshop on question answering in restricted domains; 2005. [34] Yu H, Sable C. Being Erlang Shen. Identifying answerable questions. In: Proceedings of the nineteenth international joint conference on artificial intelligence on knowledge and reasoning for answering questions; 2005. [35] H. Yu Towards answering biological questions with experimental evidence. Automatically identifying text that summarize image content in full-text articles AMIA Annu Symp Proc 2006 834 838 [36] M. Lee J. Cimino H.R. Zhu C. Sable V. Shanker J. Ely Beyond information retrieval\u2013medical question answering AMIA Annu Symp Proc 2006 469 473 [37] M. Lee W. Wang H. Yu Exploring supervised and unsupervised methods to detect topics in biomedical text BMC Bioinformatics 7 2006 140 [38] H. Yu K. Kaufman A cognitive evaluation of four online search engines for answering definitional questions posed by physicians Pacific Symp Biocomput 12 2007 328 339 [39] H. Yu M. Lee D. Kaufman J. Ely J. Osheroff G. Hripcsak Development, implementation, and a cognitive evaluation of a definitional question answering system for physicians J Biomed Inform 40 2007 236 251 [40] H. Yu Y.G. Cao Automatically extracting information needs from ad hoc clinical questions AMIA Annu Symp Proc 2008 96 100 [41] Yu H, Cao YG. Using the weighted keyword models to improve biomedical information retrieval. In: AMIA summit on translational bioinformatics, San Francisco, USA; 2009. [42] Y.G. Cao J. Ely L. Antieau H. Yu Evaluation of the clinical question answering presentation BioNLP 2009 [43] Yu H, Cao YG. Using the weighted keyword models to improve clinical question answering. In: IEEE international conference on bioinformatics & biomedicine workshop NLP approaches for unmet information needs in health car; 2009. [44] Hersh W, Cohen A, Ruslen L, Roberts P. TREC 2007 genomics track overview. In: The TREC genomics track conference; 2007. [45] Hersh W, Cohen A, Roberts P, Rekapalli H. TREC 2006 genomics track overview. In: TREC genomics track conference; 2006. [46] Voorhees EM. The TREC-8 question answering track report. In: Proceedings of TREC; 1999. [47] W. Hersh The quality of information on the World Wide Web J Am Coll Dent 66 1999 43 45 [48] Hearst M, Pedersen J. Reexamining the cluster hypothesis: scatter/gather on retrieval results. In: The 19th annual international ACM conference on research and development in information retrieval (SIGIR-96); 1996. [49] W. Pratt Dynamic organization of search results using the UMLS Proc AMIA Annu Fall Symp 1997 480 484 [50] J.J. Cimino A. Aguirre S.B. Johnson P. Peng Generic queries for meeting clinical information needs Bull Med Lib Assoc 81 1993 195 206 [51] Zweigenbaum P. Question answering in biomedicine. In: EACL workshop on natural language processing for question answering, Budapest; 2003. p. 1\u20134. [52] Zweigenbaum P. Question-answering for biomedicine: methods and state of the art. In: MIE 2005 workshop; 2005. [53] Niu Y, Hirst G. Analysis of semantic classes in medical text for question answering. In: ACL 2004 workshop on question answering in restricted domains; 2004. [54] Niu Y, Hirst G, McArthur G, Rodriguez-Gianolli P. Answering clinical questions with role identification. In: ACL workshop on natural language processing in biomedicine; 2003. [55] G. Del Fiol P.J. Haug J.J. Cimino S.P. Narus C. Norlin J.A. Mitchell Effectiveness of topic-specific infobuttons: a randomized controlled trial J Am Med Inform Assoc 15 2008 752 759 [56] S.A. Collins L.M. Currie S. Bakken J.J. Cimino Information needs, infobutton manager use, and satisfaction by clinician type: a case study J Am Med Inform Assoc 2008 [57] J. Cimino Infobuttons: anticipatory passive decision support AMIA Annu Symp Proc 2008 1203 1204 [58] J.J. Cimino Use, usability, usefulness, and impact of an infobutton manager AMIA Annu Symp Proc 2006 151 155 [59] J. Lei E.S. Chen P.D. Stetson L.K. McKnight E.A. Mendonca J.J. Cimino Development of infobuttons in a wireless environment AMIA Annu Symp Proc 2003 906 [60] J.J. Cimino J. Li Sharing infobuttons to resolve clinicians\u2019 information needs AMIA Annu Symp Proc 2003 815 [61] J.J. Cimino J. Li S. Bakken V.L. Patel Theoretical, empirical and practical approaches to resolving the unmet information needs of clinical information system users Proc AMIA Symp 2002 170 174 [62] J.J. Cimino G. Elhanan Q. Zeng Supporting infobuttons with terminological knowledge Proc AMIA Annu Fall Symp 1997 528 532 [63] Mendon\u00e7a EA, Kaufman D, Johnson SB. Answering information needs in workflow. In: Proceedings of the 9th world congress on health information and libraries; 2005. [64] H.S. Chase D.R. Kaufman S.B. Johnson E.A. Mendonca Voice capture of medical residents\u2019 clinical information needs during an inpatient rotation J Am Med Inform Assoc 16 2009 387 394 [65] N. Elhadad M. Kan J.L. Klavans K.R. McKeown Customization in a unified framework for summarizing medical literature Artif Intell Med 33 2005 179 198 [66] N. Elhadad K. McKeown D. Kaufman D. Jordan Facilitating physicians\u2019 access to information via tailored text summarization AMIA Annu Symp Proc 2005 226 230 [67] McKeown K, Chang SF, Cimino JJ, Feiner SK, Friedman C, Gravano L, et al. PERSIVAL, a system for personalized search and summarization over multimedia healthcare information. In: Proceedings of the 1st ACM/IEEE-CS joint conference on digital libraries; 2001. [68] J.W. Ely J.A. Osheroff K.J. Ferguson M.L. Chambliss D.C. Vinson J.L. Moore Lifelong self-directed learning using a computer database of clinical questions J Family Pract 45 1997 382 388 [69] D.M. D\u2019Alessandro C.D. Kreiter M.W. Peterson An evaluation of information-seeking behaviors of general pediatricians Pediatrics 113 2004 64 69 [70] J.W. Ely J.A. Osheroff P.N. Gorman M.H. Ebell M.L. Chambliss E.A. Pifer A taxonomy of generic clinical questions: classification study BMJ 321 2000 429 432 [71] Y.H. Seol D.R. Kaufman E.A. Mendonca J.J. Cimino S.B. Johnson Scenario-based assessment of physicians\u2019 information needs Medinfo 11 2004 306 310 [72] D. Demner-Fushman J. Lin Answering clinical questions with knowledge-based and statistical techniques Comput Linguist 33 2007 63 103 [73] X. Huang J. Lin D. Demner-Fushman Evaluation of PICO as a knowledge representation for clinical questions AMIA Annu Symp Proc 2006 359 363 [74] Demner-Fushman D, Lin J. Answer extraction, semantic clustering, and extractive summarization for clinical question answering. In: Proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics (COLING/ACL); 2006. p. 945\u201352. [75] Lin J, Demner-Fushman D. The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine. In: Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR), Seattle, Washington; 2006. p. 99\u2013106. [76] W.B. Frakes R. Baeza-Yates Information retrieval: data structure and algorithms 1992 Prentice-Hall [77] G. Salton A vector space model for information retrieval CACM 18 1975 613 620 [78] H.M. Muller E.E. Kenny P.W. Sternberg Textpresso: an ontology-based information retrieval and extraction system for biological literature PLoS Biol 2 2004 e309 [79] S.C. Deerwester S.T. Dumais T.K. Landauer G.W. Furnas R.A. Harshman Indexing by latent semantic analysis J Am Soc Inform Sci 41 6 1990 391 407 [80] Hofmann T. Probabilistic latent semantic indexing. In: Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, California, USA; 1999. p. 50\u20137. [81] Ponte J, Croft W. A language modeling approach to information retrieval. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, Melbourne, Australia; 1998. p. 275\u201381. [82] C.A. Sneiderman D. Demner-Fushman M. Fiszman N.C. Ide T.C. Rindflesch Knowledge-based methods to help clinicians find answers in MEDLINE J Am Med Inform Assoc 14 2007 772 780 [83] P. Srinivasan T. Rindflesch Exploring text mining from MEDLINE Proc AMIA Symp 2002 722 726 [84] B.L. Humphreys D.A. Lindberg The UMLS project: making the conceptual connection between users and the information they need Bull Med Lib Assoc 81 1993 170 177 [85] N.C. Ide R.F. Loane D. Demner-Fushman Essie: a concept-based search engine for structured biomedical text J Am Med Inform Assoc 14 2007 253 263 [86] Hovy E, Hermjakob U, Lin CY. The use of external knowledge in factoid QA. In: TREC 2001; 2001. p. 644\u201352. [87] Joachims T. Text categorization with support vector machines: learning with many relevant features. Lecture notes in computer science. Berlin/Heidelberg: Springer; 1997. p. 137\u201342. [88] Lafferty J, McCallum A, Pereira F. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In: Proceedings of the eighteenth international conference on machine learning (ICML); 2001. p. 282\u20139. [89] A.R. Aronson Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program Proc AMIA Symp 2001 17 21 [90] Robertson S, Zaragoza H, Taylor M. Simple BM25 extension to multiple weighted fields. In: ACM CIKM; 2004. [91] Zaragoza H, Craswell N, Taylor M, Saria S, Robertson S. Microsoft Cambridge at TREC-13: web and HARD tracks. In: Proc of TREC 2004; 2004. [92] Tellex S, Katz B, Lin J. Quantitative evaluation of passage retrieval algorithms for question answering. In: Proceedings of the 26th annual international ACM SIGIR conference on research and development in information retrieval; 2003. p. 41\u20137. [93] Gobeill J, Ehrler F, Tbahriti I, Ruch P. Vocabulary-driven passage retrieval for question-answering in genomics. In: The sixteenth text retrieval conference, TREC, Gaithersburg, MD; 2007. [94] Liu X, Croft WB. Passage retrieval based on language models. In: Proc of CIKM; 2002. p. 375\u201382. [95] M.A. Hearst TextTiling: segmenting text into multi-paragraph subtopic passages Comput Linguist 23 1997 33 64 [96] Paterson M, Dancik V. Longest common subsequences. In: Proc of 19th MFCS, No. 841 in LNCS, vol. 841; 1994. p. 127\u201342. [97] Liu Feifan, Liu Yang. Exploring correlation between ROUGE and human evaluation on meeting summaries. Audio, speech, and language processing. IEEE Trans 2010;18:187\u201396. [98] S. Osinski J. Stefanowski D. Weiss Lingo: search results clustering algorithm based on singular value decomposition Intell Inform Syst 2004 359 368 [99] D.S. Hirschberg Algorithms for the longest common subsequence problem J ACM 24 1977 664 675 [100] Lin C. ROUGE: a package for automatic evaluation of summaries. In: Proceedings of the ACL workshop: text summarization braches out 2004; 2004. p. 74\u201381. [101] Ng RT, Zhou X. Scalable discovery of hidden emails from large folders. In: ACM SIGKDD\u201905; 2005. p. 544\u20139. [102] Zhang Y, Patrick J. Paraphrase identification by text canonicalization. In: Proceedings of the Australasian language technology workshop 2005; 2005. [103] S. Abdou J. Savoy Searching in Medline: query expansion and manual indexing evaluation Inform Process Manage 44 2008 781 789 [104] Zighelnic L, Kurland O. Query-drift prevention for robust query expansion. In: Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval; 2008. p. 825\u20136. [105] N. Stokes Y. Li L. Cavedon J. Zobel Exploring criteria for successful query expansion in the genomic domain Inform Retrieval 12 2009 17 50 [106] Rinaldi F, Dowdall J, Schneider G, Persidis A. Answering questions in the genomics domain. ACL-2004 workshop on question answering in restricted domains, Barcelona, Spain; 2007.", "scopus-id": "79952770126", "pubmed-id": "21256977", "coredata": {"eid": "1-s2.0-S1532046411000062", "dc:description": "Abstract Objective Clinical questions are often long and complex and take many forms. We have built a clinical question answering system named AskHERMES to perform robust semantic analysis on complex clinical questions and output question-focused extractive summaries as answers. Design This paper describes the system architecture and a preliminary evaluation of AskHERMES, which implements innovative approaches in question analysis, summarization, and answer presentation. Five types of resources were indexed in this system: MEDLINE abstracts, PubMed Central full-text articles, eMedicine documents, clinical guidelines and Wikipedia articles. Measurement We compared the AskHERMES system with Google (Google and Google Scholar) and UpToDate and asked physicians to score the three systems by ease of use, quality of answer, time spent, and overall performance. Results AskHERMES allows physicians to enter a question in a natural way with minimal query formulation and allows physicians to efficiently navigate among all the answer sentences to quickly meet their information needs. In contrast, physicians need to formulate queries to search for information in Google and UpToDate. The development of the AskHERMES system is still at an early stage, and the knowledge resource is limited compared with Google or UpToDate. Nevertheless, the evaluation results show that AskHERMES\u2019 performance is comparable to the other systems. In particular, when answering complex clinical questions, it demonstrates the potential to outperform both Google and UpToDate systems. Conclusions AskHERMES, available at http://www.AskHERMES.org, has the potential to help physicians practice evidence-based medicine and improve the quality of patient care.", "openArchiveArticle": "true", "prism:coverDate": "2011-04-30", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S1532046411000062", "dc:creator": [{"@_fa": "true", "$": "Cao, YongGang"}, {"@_fa": "true", "$": "Liu, Feifan"}, {"@_fa": "true", "$": "Simpson, Pippa"}, {"@_fa": "true", "$": "Antieau, Lamont"}, {"@_fa": "true", "$": "Bennett, Andrew"}, {"@_fa": "true", "$": "Cimino, James J."}, {"@_fa": "true", "$": "Ely, John"}, {"@_fa": "true", "$": "Yu, Hong"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S1532046411000062"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S1532046411000062"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S1532-0464(11)00006-2", "prism:volume": "44", "prism:publisher": "Elsevier Inc. Published by Elsevier Inc.", "dc:title": "AskHERMES: An online question answering system for complex clinical questions", "prism:copyright": "Copyright \u00a9 2011 Elsevier Inc. Published by Elsevier Inc. All rights reserved.", "openaccess": "1", "prism:issn": "15320464", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Clinical question answering"}, {"@_fa": "true", "$": "Question analysis"}, {"@_fa": "true", "$": "Passage retrieval"}, {"@_fa": "true", "$": "Summarization"}, {"@_fa": "true", "$": "Answer presentation"}], "openaccessArticle": "true", "prism:publicationName": "Journal of Biomedical Informatics", "prism:number": "2", "openaccessSponsorType": "ElsevierBranded", "prism:pageRange": "277-288", "prism:endingPage": "288", "prism:coverDisplayDate": "April 2011", "prism:doi": "10.1016/j.jbi.2011.01.004", "prism:startingPage": "277", "dc:identifier": "doi:10.1016/j.jbi.2011.01.004", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "44", "@width": "394", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "2378", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "72", "@width": "304", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1834", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "470", "@width": "2166", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr9_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "192851", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1850", "@width": "2165", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr8_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "679972", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1333", "@width": "2183", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "212620", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "534", "@width": "2166", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr2_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "146222", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1359", "@width": "2165", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr10_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "644851", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "3225", "@width": "4331", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr6_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "254600", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1649", "@width": "2756", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr5_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "43821", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "high", "@height": "3183", "@width": "2953", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr4_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "114431", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1130", "@width": "3347", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr3_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "35876", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1056", "@width": "3346", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr1_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "42177", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "106", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24885", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "418", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "81142", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "301", "@width": "493", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33617", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "121", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "23667", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "307", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "74269", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "364", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "30954", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "186", "@width": "311", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr5.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "5827", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "359", "@width": "333", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "14195", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "128", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "4650", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "119", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "5153", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "163", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "7960", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "131", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3644", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "152", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3919", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "74", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2026", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "69", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2335", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "48", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2339", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "192", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5650", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "134", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3289", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "54", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2302", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "137", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S1532046411000062-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6878", "@ref": "gr10", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/79952770126"}}