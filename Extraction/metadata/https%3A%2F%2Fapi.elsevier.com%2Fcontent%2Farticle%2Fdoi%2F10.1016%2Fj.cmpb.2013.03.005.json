{"scopus-eid": "2-s2.0-84878495505", "originalText": "serial JL 271322 291210 291791 291871 291901 31 90 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2013-04-06 2013-04-06 2014-08-29T04:37:22 1-s2.0-S0169260713000813 S0169-2607(13)00081-3 S0169260713000813 10.1016/j.cmpb.2013.03.005 S300 S300.2 FULL-TEXT 1-s2.0-S0169260713X00070 2015-05-14T05:25:39.534626-04:00 0 0 20130701 20130731 2013 2014-08-23T06:35:48.305775Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccessdate sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast authsuff grantsponsor primabst ref 0169-2607 01692607 DELAY 2014-05-22 BZG false 111 111 1 1 Volume 111, Issue 1 23 228 248 228 248 201307 July 2013 2013-07-01 2013-07-31 2013 Section III: Experiences with Methods, Systems and Programs article fla Copyright \u00a9 2013 Elsevier Ireland Ltd. ONLINEPREDICTIONFEEDINGPHASEINHIGHCELLDENSITYCULTIVATIONRECOLIUSINGCONSTRUCTIVENEURALNETWORKS NICOLETTI M 1 Introduction 2 S. pneumoniae \u2013 main characteristics, resistance and virulence factors 3 The state-of-art of available pneumococcal vaccines 3.1 Vaccines based exclusively on PS (pneumococcal polysaccharide vaccines) (PPSV or PV) 3.1.1 The 14-valent polysaccharide vaccine 3.1.2 The 23-valent polysaccharide vaccine 3.2 Vaccines based on PS conjugated to a protein (pneumococcal conjugate vaccines) (PCV) 4 Growing recombinant E. coli 4.1 High cell density cultures (HCDC) \u2013 main characteristics 4.2 Material, methods and operational conditions 4.2.1 Strains and plasmid 4.2.2 Medium preparation 4.2.3 Bioreactor operation 4.2.4 Analytical methods 5 Constructive neural networks (CoNN) and five CoNN algorithms 5.1 A brief introduction to CoNNs 5.2 The Pyramid algorithm 5.3 The Baricentric-based constructive neural network algorithm (BabCoNN) 5.4 The Shift algorithm 5.5 The Perceptron Cascade algorithm 5.6 The Cascade Correlation algorithm (CasCor) 6 Neural networks as monitoring tools for high cell density cultivations (HCDC) \u2013 inferring the start-up of the feed process in HCDC 6.1 Methodology for training and evaluating the NNs 6.2 Results and discussion 7 Conclusions Acknowledgments References ALBERTS 2002 B MOLECULARBIOLOGYCELL AMALDI 1997 629 645 E AUSTRIAN 1976 184 194 R AUSTRIAN 1999 S338 S341 R BAROCCHI 2007 2963 2973 M BATTIG 2006 2612 2617 P BERGMANN 2006 295 303 S BERTINI 2006 J PROCEEDINGSIEEESMC ACOMPARATIVEEVALUATIONCONSTRUCTIVENEURALNETWORKSMETHODSUSINGPRMBCPTLUTRAININGALGORITHMS BERTINI 2008 1 12 J COMPUTATIONALINTELLIGENCEMETHODSAPPLICATIONS ACONSTRUCTIVENEURALNETWORKALGORITHMBASEDGEOMETRICCONCEPTBARYCENTERCONVEXHULL BLACK 2000 187 195 S BOGAERT 2004 144 154 D BOGAERT 2004 2209 2220 D BRANDILEONE 1995 2788 2791 M BRANDILEONE 2003 1206 1212 M BRANDILEONE 2004 3890 3896 M BREIMAN 1993 L CLASSIFICATIONREGRESSIONTREES BRILES 1998 645 657 D BRUYN 1991 897 910 G BURGESS 1994 59 66 N BUTLER 1999 69S 76S J CHOI 2006 876 885 J CHOU 2007 521 532 C COSTERTON 1981 303 338 J CRAIN 1990 3293 3299 M CSORDAS 2008 2925 2929 F CUTTS 2005 1139 1146 F DAGAN 2004 283 313 L PNEUMOCOCCUS CHANGINGECOLOGYPNEUMOCOCCIANTIBIOTICSVACCINES DARRIEUX 2007 5930 5938 M FAHLMAN 1988 S PROCEEDINGS1988CONNECTIONISTMODELSSUMMERSCHOOLMORGANKAUFMANN FASTERLEARNINGVARIATIONSBACKPROPAGATIONEMPIRICALSTUDY FAHLMAN 1990 524 532 S ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS2 CASCADECORRELATIONARCHITECTURE 2009 CONSTRUCTIVENEURALNETWORKSSTUDIESINCOMPUTATIONALINTELLIGENCE FREAN 1990 198 209 M GALLANT 1986 S PROCEEDINGSEIGHTHANNUALCONFERENCECOGNITIVESCIENCESOCIETYAMHERSTMA THREECONSTRUCTIVEALGORITHMSFORNETWORKLEARNING GALLANT 1994 S NEURALNETWORKLEARNINGEXPERTSYSTEMS GIORDANO 2008 101 109 R GILLESPIE 1989 237 248 S GNOTH 2008 21 39 S GONCALVES 2003 283 287 V GONCALVES 2006 1009 1014 V HORTA 2011 891 901 A ISAACMAN 2010 e197 e209 D JACKSON 2002 431 436 C JEDRZEJAS 2001 187 207 M JEDRZEJAS 2000 116 125 M JEDRZEJAS 2001 33121 33128 M JEFFERSON 2006 405 410 T KADIOGLU 2008 288 301 A KARP 1999 43 50 P KESELER 2009 D464 D470 I KIM 2004 147 150 B KLUGMAN 2003 1341 1348 K KORZ 1995 59 65 D KWOK 1999 630 645 T LEE 1996 98 105 S LIBERMAN 2008 1441 1445 C LIU 2000 156 160 Y LULI 1990 1004 1011 G MADHI 2007 2451 2457 S MANGTANI 2003 71 78 P MELTON 2001 44 47 K MEZARD 1989 2191 2203 M MIYAJI 2002 805 812 E MURATA 1994 865 872 N PATON 1993 89 115 J PIMENTA 2006 2838 2843 F POLAND 1999 1674 1679 G POULARD 1995 710 713 H RIDGEN 2003 143 168 D RIEDMILLER 1993 M PROCEEDINGSIEEEINTERNATIONALCONFERENCENEURALNETWORKSICNN ADIRECTADAPTIVEMETHODFORFASTERBACKPROPAGATIONLEARNINGRPROPALGORITHM RIESENBERG 1999 422 430 D RIPLEY 1996 B PATTERNRECOGNITIONNEURALNETWORKS ROBERTS 1996 285 315 I RUMELHART 1986 318 362 D PARALLELDISTRIBUTEDPROCESSINGEXPLORATIONSINMICROSTRUCTURECOGNITIONVOL1FOUNDATIONS LEARNINGINTERNALREPRESENTATIONSBYERRORPROPAGATION SCHOLKOPF 2002 B LEARNINGKERNELS SEEGER 1995 947 953 A SHILOACH 2005 345 357 J SILVA 2007 146 154 M SOINI 2008 1 11 J STARR 2008 363 370 J SUYKENS 2002 J LEASTSQUARESSUPPORTVECTORMACHINES TERPE 2006 211 222 K TOMASZ 1970 138 140 A VANDERPOLL 2009 1543 1556 T VAKEVAINEN 2001 789 793 M WATSON 1993 913 924 D WEINBERGER 2008 1511 1518 D WHITFIELD 1993 135 146 C WRIGHT 1914 87 95 A NICOLETTIX2013X228 NICOLETTIX2013X228X248 NICOLETTIX2013X228XM NICOLETTIX2013X228X248XM Full 2014-05-22T10:33:07Z FundingPartnerOpenArchive Brazilian Government http://www.elsevier.com/open-access/userlicense/1.0/ item S0169-2607(13)00081-3 S0169260713000813 1-s2.0-S0169260713000813 10.1016/j.cmpb.2013.03.005 271322 2014-08-29T02:29:25.3269-04:00 2013-07-01 2013-07-31 DELAY 2014-05-22 BZG 1-s2.0-S0169260713000813-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/MAIN/application/pdf/7f29c64021d2efed0b1b72f26b5c777c/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/MAIN/application/pdf/7f29c64021d2efed0b1b72f26b5c777c/main.pdf main.pdf pdf true 2759740 MAIN 21 1-s2.0-S0169260713000813-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/PREVIEW/image/png/23be83bde0088ebaf3a8956ba9de2e58/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/PREVIEW/image/png/23be83bde0088ebaf3a8956ba9de2e58/main_1.png main_1.png png 46482 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0169260713000813-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/STRIPIN/image/gif/6de5715423690fe28ecb5e334d6c2dda/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/STRIPIN/image/gif/6de5715423690fe28ecb5e334d6c2dda/si1.gif si1 si1.gif gif 1618 34 424 ALTIMG 1-s2.0-S0169260713000813-gr9_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr9/HIGHRES/image/jpeg/fc310728f78cbdc74c5150b012967755/gr9_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr9/HIGHRES/image/jpeg/fc310728f78cbdc74c5150b012967755/gr9_lrg.jpg gr9 gr9_lrg.jpg jpg 316943 1216 2947 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr8_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr8/HIGHRES/image/jpeg/cc4d2bf9bf935fd7744c8df41eb66083/gr8_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr8/HIGHRES/image/jpeg/cc4d2bf9bf935fd7744c8df41eb66083/gr8_lrg.jpg gr8 gr8_lrg.jpg jpg 442409 1234 2905 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr7_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr7/HIGHRES/image/jpeg/1f15558cf92ba7f583227e47669477c5/gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr7/HIGHRES/image/jpeg/1f15558cf92ba7f583227e47669477c5/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 456450 1275 2903 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr5_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr5/HIGHRES/image/jpeg/2fd022fcdd3885830236c57798781048/gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr5/HIGHRES/image/jpeg/2fd022fcdd3885830236c57798781048/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 150013 1429 1675 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr10_lrg.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr10/HIGHRES/image/jpeg/eb714502ef1d4bef8162bd27f4fff804/gr10_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr10/HIGHRES/image/jpeg/eb714502ef1d4bef8162bd27f4fff804/gr10_lrg.jpg gr10 gr10_lrg.jpg jpg 354578 1372 3123 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr6_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr6/HIGHRES/image/gif/b828b5c78d8bb4f43b3bacafd70b097e/gr6_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr6/HIGHRES/image/gif/b828b5c78d8bb4f43b3bacafd70b097e/gr6_lrg.gif gr6 gr6_lrg.gif gif 92672 2296 5416 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr4_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr4/HIGHRES/image/gif/91da60a820f4cfc160bf7158080553b8/gr4_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr4/HIGHRES/image/gif/91da60a820f4cfc160bf7158080553b8/gr4_lrg.gif gr4 gr4_lrg.gif gif 14647 466 3166 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr3_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr3/HIGHRES/image/gif/fbef51ab03a0079b186d9054a943fcb7/gr3_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr3/HIGHRES/image/gif/fbef51ab03a0079b186d9054a943fcb7/gr3_lrg.gif gr3 gr3_lrg.gif gif 128897 3306 3430 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr2_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr2/HIGHRES/image/gif/82ca38f1931b2996ed3cfc0540597e19/gr2_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr2/HIGHRES/image/gif/82ca38f1931b2996ed3cfc0540597e19/gr2_lrg.gif gr2 gr2_lrg.gif gif 44418 2332 3234 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr1_lrg.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr1/HIGHRES/image/gif/a1a368ddf43a45c88a067c31b5906984/gr1_lrg.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr1/HIGHRES/image/gif/a1a368ddf43a45c88a067c31b5906984/gr1_lrg.gif gr1 gr1_lrg.gif gif 64606 2274 2696 IMAGE-HIGH-RES 1-s2.0-S0169260713000813-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr9/DOWNSAMPLED/image/jpeg/cd9686cc06f6a4a93c238c0b6b393047/gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr9/DOWNSAMPLED/image/jpeg/cd9686cc06f6a4a93c238c0b6b393047/gr9.jpg gr9 gr9.jpg jpg 47664 275 666 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr8/DOWNSAMPLED/image/jpeg/34c351390a68e43335057a2255571d43/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr8/DOWNSAMPLED/image/jpeg/34c351390a68e43335057a2255571d43/gr8.jpg gr8 gr8.jpg jpg 56187 279 656 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr7/DOWNSAMPLED/image/jpeg/583e50d515cc0739ead76b6cf93b38f1/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr7/DOWNSAMPLED/image/jpeg/583e50d515cc0739ead76b6cf93b38f1/gr7.jpg gr7 gr7.jpg jpg 60090 288 656 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr5/DOWNSAMPLED/image/jpeg/865fa9b3e322635066f69f33252aa626/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr5/DOWNSAMPLED/image/jpeg/865fa9b3e322635066f69f33252aa626/gr5.jpg gr5 gr5.jpg jpg 28635 322 378 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr10/DOWNSAMPLED/image/jpeg/a5346c9fbdc3996aef3d0de65eccf91b/gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr10/DOWNSAMPLED/image/jpeg/a5346c9fbdc3996aef3d0de65eccf91b/gr10.jpg gr10 gr10.jpg jpg 52612 310 705 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr6.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr6/DOWNSAMPLED/image/gif/a8df5f6a6f578f946b2c40e9c32c8345/gr6.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr6/DOWNSAMPLED/image/gif/a8df5f6a6f578f946b2c40e9c32c8345/gr6.gif gr6 gr6.gif gif 13150 259 612 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr4/DOWNSAMPLED/image/gif/177df90020583898e9ca5a1cd8058fad/gr4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr4/DOWNSAMPLED/image/gif/177df90020583898e9ca5a1cd8058fad/gr4.gif gr4 gr4.gif gif 1983 53 357 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr3/DOWNSAMPLED/image/gif/72a5dc2a50df5e57006374d661c48eb9/gr3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr3/DOWNSAMPLED/image/gif/72a5dc2a50df5e57006374d661c48eb9/gr3.gif gr3 gr3.gif gif 16122 373 387 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr2/DOWNSAMPLED/image/gif/65846d55980272de525716e95daecfe7/gr2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr2/DOWNSAMPLED/image/gif/65846d55980272de525716e95daecfe7/gr2.gif gr2 gr2.gif gif 5488 263 365 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr1/DOWNSAMPLED/image/gif/16976a51ebcee65f6a277055bbb6d614/gr1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr1/DOWNSAMPLED/image/gif/16976a51ebcee65f6a277055bbb6d614/gr1.gif gr1 gr1.gif gif 8216 256 304 IMAGE-DOWNSAMPLED 1-s2.0-S0169260713000813-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr6/THUMBNAIL/image/gif/3fd9c66bddd1f2fedbee838893af663d/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr6/THUMBNAIL/image/gif/3fd9c66bddd1f2fedbee838893af663d/gr6.sml gr6 gr6.sml sml 2901 93 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr4/THUMBNAIL/image/gif/3bd8a8a093f9ce618792646b1b22b711/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr4/THUMBNAIL/image/gif/3bd8a8a093f9ce618792646b1b22b711/gr4.sml gr4 gr4.sml sml 1062 32 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr3/THUMBNAIL/image/gif/3dc3710bbc3b0c13d15ed0e6e9053a78/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr3/THUMBNAIL/image/gif/3dc3710bbc3b0c13d15ed0e6e9053a78/gr3.sml gr3 gr3.sml sml 4458 164 170 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr2/THUMBNAIL/image/gif/d1e87f278693330882877e160fff426a/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr2/THUMBNAIL/image/gif/d1e87f278693330882877e160fff426a/gr2.sml gr2 gr2.sml sml 2777 158 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr1/THUMBNAIL/image/gif/8b8719c29d955c42f7cb953d29c8b240/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr1/THUMBNAIL/image/gif/8b8719c29d955c42f7cb953d29c8b240/gr1.sml gr1 gr1.sml sml 4258 164 194 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr9/THUMBNAIL/image/gif/b97cf79b2d5be4fcade92c1a82ddd4f3/gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr9/THUMBNAIL/image/gif/b97cf79b2d5be4fcade92c1a82ddd4f3/gr9.sml gr9 gr9.sml sml 3495 90 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr8/THUMBNAIL/image/gif/1e2e394c1e9ca421918cc80f5fb806b6/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr8/THUMBNAIL/image/gif/1e2e394c1e9ca421918cc80f5fb806b6/gr8.sml gr8 gr8.sml sml 4286 93 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr7/THUMBNAIL/image/gif/22721d34abef3943b46126d794b8c5bc/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr7/THUMBNAIL/image/gif/22721d34abef3943b46126d794b8c5bc/gr7.sml gr7 gr7.sml sml 4265 96 219 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr5/THUMBNAIL/image/gif/bc1ceba6caf8421f0409dccbe591def5/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr5/THUMBNAIL/image/gif/bc1ceba6caf8421f0409dccbe591def5/gr5.sml gr5 gr5.sml sml 4666 164 192 IMAGE-THUMBNAIL 1-s2.0-S0169260713000813-gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0169260713000813/gr10/THUMBNAIL/image/gif/ca60badc039bdadc209f7a029ff4bf44/gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0169260713000813/gr10/THUMBNAIL/image/gif/ca60badc039bdadc209f7a029ff4bf44/gr10.sml gr10 gr10.sml sml 3657 96 219 IMAGE-THUMBNAIL COMM 3520 S0169-2607(13)00081-3 10.1016/j.cmpb.2013.03.005 Elsevier Ireland Ltd Fig. 1 BabCoNN algorithm for constructing a NN. Fig. 2 Hidden neuron classification process. Fig. 3 BabCoNN classification process. Fig. 4 The softening process applied to variable V transforms datasets C1, \u2026, C4 into datasets CC1, \u2026, CC4 respectively. Fig. 5 Softening the human expert decision (variable V \u2013 in the figure referred to as Output label) associated to each data register in dataset C1, using the Bell function. Fig. 6 Inference of four CasCor NNs and determination of the threshold (\u03b8) of the output neuron of each NN, according to formula (1). Fig. 7 Scatter plots for agitation, permittivity and CO2 for datasets (a) C1 and (b) C2. Fig. 8 Scatter plots for agitation, permittivity and CO2 for datasets (a) C3 and (b) C4. Fig. 9 Scatter plots for CER, permittivity and conductivity for (a) C1 and (b) C2. Fig. 10 Scatter plots for CER, permittivity and conductivity for (a) C3 and (b) C4. Table 1 Pneumococcal proteins of interest (for a vaccine) and their level of evidence (LE) according to van der Poll and Opal's criteria [91]. Protein LE Protein LE Choline binding protein A (CbpA) 2+ Neurominidase (NanB) 1+ Hyaluronate lyase (Hyl) 1+ Pneumococcal surface protein A (PspA) 2+ Major autolysin (LytA) 2+ Pneumococcal surface antigen A (PsaA) 1+\u20132+ Neuraminidase (NanA) 1+ Pneumolysin (Ply) 3+ Table 2 Batch media composition for the four rE. coli cultivations (Cultivations 1\u20133: PspA3 and Cultivation 4: PspA245). Component Cultivation 1 Cultivation 2 Cultivation 3 Cultivation 4 Glycerol 40.00g/L 40.00g/L 40.00g/L 40.00g/L KH2PO4 13.30g/L 13.30g/L 13.30g/L 13.30g/L (NH4)2HPO4 4.00g/L 4.00g/L 4.00g/L 4.00g/L Citric acid\u00b7H2O 1.70g/L 1.70g/L 1.70g/L 1.70g/L Fe(III) citrate 100.80mg/L 100.80mg/L 100.80mg/L 100.80mg/L CoCl2\u00b76H2O 2.50mg/L 2.50mg/L 2.50mg/L 2.50mg/L MnCl2\u00b74H2O 15.00mg/L 15.00mg/L 15.00mg/L 15.00mg/L CuCl2\u00b72H2O 1.50mg/L 1.50mg/L 1.50mg/L 1.50mg/L Zn(CH3COO)2\u00b72H2O 33.80mg/L 33.80mg/L 33.80mg/L 33.80mg/L Na2MoO4\u00b72H2O 2.10mg/L 2.10mg/L 2.10mg/L 2.10mg/L H3BO3 3.00mg/L 3.00mg/L 3.00mg/L 3.00mg/L EDTA 14.10mg/L 14.10mg/L 14.10mg/L 14.10mg/L MgSO4\u00b77H2O 1.20g/L 1.20g/L 1.20g/L 1.20g/L Thiamine 4.50mg/L 45.00mg/L 9.00mg/L 45.00mg/L Kanamycin 25.00mg/L 25.00mg/L 25.00mg/L 25.00mg/L Antifoam PPG 30% 1ml/L 1ml/L 1ml/L 1ml/L Table 3 Feed media composition for the four rE. coli cultivations. Component Cultivation 1 Cultivation 2 Cultivations 3\u20134 Glycerol 696.00g/L 800.00g/L 800.00g/L KH2PO4 0 0 21.80g/L (NH4)2HPO4 0 0 6.40g/L Citric acid\u00b7H2O 0 0 2.70g/L Fe (III) citrate 40.00mg/L 40.00mg/L 40.00mg/L CoCl2\u00b76H2O 4.00mg/L 4.00mg/L 4.00mg/L MnCl2\u00b74H2O 15.00mg/L 15.00mg/L 23.50mg/L CuCl2\u00b72H2O 2.30mg/L 2.30mg/L 2.30mg/L Zn(CH3COO)2\u00b72H2O 16.00mg/L 16.00mg/L 16.00mg/L Na2MoO4\u00b72H2O 4.00mg/L 4.00mg/L 4.00mg/L H3BO3 4.70mg/L 4.70mg/L 4.70mg/L EDTA 13.00mg/L 13.00mg/L 22.60mg/L MgSO4\u00b77H2O 20.00g/L 20.00g/L 20.00g/L Thiamine 4.50/L 4.50/L 4.50/L Kanamycin 25.00mg/L 25.00mg/L 25.00mg/L Antifoam PPG 30% 1ml/L 1ml/L 1ml/L Table 4 Variables describing each cultivation data register. Variable Acronym (unit) Variable Acronym (unit) (1) Stirring speed Agit (rpm) (6) Carbon dioxide evolution rate CER (mol/min) (2) Air flow rate F_air (L/min) (7) Oxygen uptake rate OUR (mol/min) (3) Dissolved oxygen concentration DOC (%saturation) (8) Oxygen flow rate F_O2 (L/min) (4) Permittivity Perm (pF/cm) (9) CO2 molar fraction in the exhaustion gas CO2 (%) (5) Conductivity Cond (mS/cm) Table 5 Data characteristics of the four HCDCs (NNSF: batch phase; INSF: fed-batch phase). Dataset Total number of data register/~time (hs) NNSF register INSF register C1 5281/14.67 5085 196 C2 4274/11.87 4249 25 C3 5154/14.32 5110 44 C4 5729/15.92 5657 72 Table 6 Main characteristics of the five CoNN algorithms implemented. HN: hidden neuron. CoNN algorithm Algorithm used for training TLUs Stopping criteria used when training TLUs Stopping criteria when using the CoNN Pyramid PRM 1000 iterations 15 HN BabCoNN BCP 1000 iterations 15 HN Shift PRM 1000 iterations 15 HN PC PRM 1000 iterations 15 HN CasCor QuickProp 1000 iterations 15 HN Table 7 Defining the four training and four testing datasets used for inducing the NNs. Training dataset Testing dataset TD1: C2\u222aC3\u222aC4 C1 TD2: C1\u222aC3\u222aC4 C2 TD3: C1\u222aC2\u222aC4 C3 TD4: C1\u222aC2\u222aC3 C4 Table 8 Time difference between the feed start up estimated by the human expert and by each of the four induced neural networks, taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by all variables listed in Table 4. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons created by the corresponding CoNN. Testing dataset Pyramid BabCoNN Shift PC Training and testing datasets described by 9 variables ( Table 4 ) (a) Training set: 1 data register at every 1.00min C1 49.2min (295/3) 59.2min (355/2) 19.5min (117/8) 11.5min (69/5) C2 0 (0/9) 45.8min (275/2) \u221220s (\u22122/2) 35.8min (215/14) C3 18.5min (111/12) 1h 45min (635/2) 2.8min (17/8) 15min (90/11) C4 2h 27min (887/9) 2h 27min (887/2) 2h 27min (887/12) 1h 04min (384/12) (b) Training set: 1 data register at every 1.40min C1 1h 13min (438/8) 38min (228/3) 1h 02min (380/10) 1h 48min (648/8) C2 19.5min (117/9) 24min (144/2) 2.8min (17/10) 15.6min (94/9) C3 14.7min (88/9) 28.3min (170/2) 1h (60/11) 23.5min (141/5) C4 2h 27min (887/8) 2h 03min (745/2) 2h 28min (887/5) 2h 27min (886/10) (c) Training set: 1 data register at every 3.33min C1 \u22121.2min (\u22127/14) 10.1min (61/2) \u22121min (\u22126/5) \u22121min (\u22126/14) C2 \u221210s (\u22121/3) 11.1min (67/2) 2.2min (13/4) 10.6min (64/13) C3 25.5min (153/11) 14.7min (88/2) 25.5min (153/2) 23min (138/9) C4 2h 28min (895/9) 2h 01min (704/3) 2h 49min (1021/7) 1h 46min (637/3) Table 9 Time difference between the feed start up estimated by the human expert and by each of the four induced neural networks, taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by variables Agit, Perm and CO2. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons created by the corresponding CoNN. Testing dataset Pyramid BABCoNN Shift PC Training and testing datasets described by variables Agit, Perm & CO 2 (a) Training set: 1 data register at every 1.00min C1 50.1min (301/14) 12.8min (77/6) 38.0min (228/12) 3.16min (19/6) C2 69.1min (415/9) 3h 07min (1128/2) 1h 10min (424/7) 0 (0/5) C3 \u22121.67min (\u221210/2) 2h 33min (925/2) \u22121.67min (\u221210/2) \u22121.16min (\u221210/14) C4 2h 28min (886/9) 4h 27min (1608/2) 2h 36min (943/1) 1h 59min (716/5) (b) Training set: 1 data register at every 1.40min C1 37.8min (227/5) 19.8min (119/2) 1h 18min (469/4) 49.5min (297/11) C2 1h 44min (629/11) 36.0min (216/2) 1h 34min (565/15) \u22121.16min (7/13) C3 \u22122.67min (\u221216/2) 40.6min (244/2) \u22121.67min (\u221210/2) \u22121.67min (\u221210/14) C4 2h 29min (895/5) 28.3min (170/2) 2h 36min (943/1) 1h 59min (716/4) (c) Training set: 1 data register at every 3.33min C1 59.0min (355/2) 20.3min (122/2) 52.5min (315/10) 1h 17min (462/4) C2 1h 45min (629/1) 26.7min (160/2) 1h 44min (624/1) 1h 36min (576/11) C3 14.8min (89/8) 39.6min (238/3) 9.2min (55/3) 9.5min (57/12) C4 2h 04min (745/1) 28.5min(171/2) 1h 59min (716/1) 1h 59min (716/5) Table 10 Time difference between the feed start up estimated by the human expert and by each of the four induced neural networks, taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by variables CER, Perm and Cond. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons created by the corresponding CoNN. Testing dataset Pyramid BABCoNN Shift PC Training and testing datasets described by variables Cer, Perm & Cond (a) Training set: 1 data register at every 1.00min C1 1h 25min (514/6) 50.5min (303/3) 1h 08min (409/12) 48.8min (293/11) C2 1h 30min (544/3) 1h 30min (544/2) 1h 07min (412/3) 56.5min (339/10) C3 2h 14min (797/1) 2h 10min (789/2) 2h 12min (794/1) 2h 10min (784/14) C4 2h 27min (887/13) 1h 29min (545/2) 2h 27min (887/5) 1h 33min (559/12) (b) Training set: 1 data register at every 1.40min C1 1h 26min (513/1) 49min (294/2) 1h 14min (448/12) 50.5min (303/10) C2 1h 48min (648/3) 1h 08min (410/2) 1h 09min (417/14) 1h 09min (416/5) C3 2h 12min (795/12) 2h 22min (857/2) 2h 16min (820/14) \u22124.6min (\u221228/5) C4 2h 27min (887/5) 2h 05min (727/2) 2h 27min (887/6) 2h 2min (887/11) (c) Training set: 1 data register at every 3.33min C1 1h 26min (514/1) 51.2min (307/2) 58.3min (350/7) 1h 08min (410/14) C2 1h 09min (416/1) 1h 09min (416/3) 35.8min (215/2) 1h 36min (576/12) C3 2h 27min (881/1) 2h 18min (832/2) 2h 26min (879/4) 1h 45min (631/4) C4 2h 28min (887/8) 1h 33min (562/2) 1h 31min (551/9) 1h 14min (446/15) Table 11 Time difference between the feed start up estimated by a human expert (and softened by procedure described in Fig. 4) and CasCor neural networks, taking into account three strategies (second to fourth columns respectively) for composing the training sets: 1 register each 1.00min; one register each 1.40min and one register each 3.33min. Training and testing sets described by three groups of variables: (a) all nine variables; (b) Agit, Perm and CO2 and (c) CER, Perm and Cond. Testing dataset Training set: 1 data register at every 1.00min Training set: 1 data register at every 1.40min Training set: 1 data register at every 3.33min Cascade correlation (CasCor) (a) All 9 variables C1 17.5min (105/1) 10.1min (61/3) 12.6min (76/2) C2 5.2min (31/2) 8.5min (51/1) 1.8min (11/3) C3 \u22123.8min (\u221223/3) \u22123.6min (\u221222/3) \u22123.6min (\u221222/2) C4 16.2min (97/4) 15min (90/3) 18.7min (112/4) (b) Agit, Perm and CO2 C1 18min (108/2) 15min (90/2) 10.1min (61/2) C2 26.7min (160/2) 26.5min (159/2) 27.1min (163/1) C3 \u22123.6min (\u221222/3) \u22123.8min (\u221223/3) \u22123.8min (\u221223/4) C4 18min (108/2) 24min (144/4) 23.5min (141/2) (c) CER, Perm and Cond C1 13.3min (80/2) 25.5min (153/5) 0.0min (0/2) C2 34.1min (205/3) 5.5min (33/1) 12.7min (76/1) C3 \u22123.5min (\u221221/2) \u221222.0min (\u221222/4) \u22123.6min (\u221222/3) C4 24.1min (887/3) 20.5min (123/2) 17.3min (104/3) Table 12 Time difference between the feed start up estimated by a human expert and three classifiers: DT (decision tree), SVM (support vector machine) and MLP (feed-forward neural network \u2013 multi-layer perceptron), taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by variables CER, Perm and Cond. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons (applies only to MLP). Testing dataset DT SVM MLP Training and testing datasets described by variables Cer, Perm & Cond (a) Training set: 1 data register at every 1.00min C1 35.7min (214) 23.8min (143) \u20133.0min (\u201318/6) C2 48.2min (289) 43.5min (261) 1.8min (11/8) C3 18.5min (111) 2h 11min (789) 18.5min (111/15) C4 35.2min (211) 13.3min (80) 9.8min (59/17) (b) Training set: 1 data register at every 1.40min C1 36.0min (216) 35.3min (212) 48.8min (293/8) C2 45.8min (275) 44.3min (266) 2.8min (17/11) C3 18.5min (111) 2h 11min (789) 25.5min (153/18) C4 36.7min (220) 14.5min (87) 14.5min (87/16) (c) Training set: 1 data register at every 3.33min C1 46.5min (279) 47.3min (284) 49.0min (294/12) C2 48.3min (290) 44.3min (266) \u20131.3min (\u20138/12) C3 18.5min (111) 2h 11min (791) 2h 11min (789/18) C4 13.3min (80) 16.0min (96) 16.0min (96/15) Table 13 Time difference between the feed start up estimated by a human expert and three classifiers: DT (decision tree), SVM (support vector machine) and MLP (feed-forward neural network \u2013 multi-layer perceptron), taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by variables Agit, Perm and CO2. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons (applies only to MLP). Testing dataset DT SVM MLP Training and testing datasets described by variables Agit, Perm & CO 2 (a) Training set: 1 data register at every 1.00min C1 1h 25min (514) \u22122.2min (\u221213) 6.8min (41/15) C2 48.2min (289) 47.8min (287) 0s (0/10) C3 25.5min (153) 28.3min (170) 20.0min (120/12) C4 50s (5) 50s (5) 50s (5/4) (b) Training set: 1 data register at every 1.40min C1 1h 17min (463) 48.7min (292) 37.7min (226/7) C2 48.0min (288) 47.8min (287) 24.0min (144/5) C3 18.3min (110) 2h 11min (789) \u22121.7min (\u221210/5) C4 50s (5) 50s (5) 40s (4/15) (c) Training set: 1 data register at every 3.33min C1 1h 17min (463) 1h 25min (513) 51.3min (308/9) C2 48.2min (289) 48.2min (289) 46.3min (278/9) C3 18.3min (110) 2h 12min (794) \u22121.7min (\u221210/8) C4 1.0min (6) 1h 59min (714) 50s (5/8) Table 14 Time difference between the feed start up estimated by a human expert and three classifiers: DT (decision tree), SVM (support vector machine) and MLP (feed-forward neural network \u2013 multi-layer perceptron), taking into account three strategies for composing the training sets: (a) 1 register each 1.00min; (b) one register each 1.40min and (c) one register each 3.33min. Training and testing sets described by all nine variables. Notation (N/M): N =approximate number of data registers before or after the beginning of the fed-phase, considering (a), (b) or (c), and M =number of hidden neurons (applies only to MLP). Testing dataset DT SVM MLP Training and testing datasets described by variables all 9 attributes (a) Training set: 1 data register at every 1.00min C1 1h 17min (463) 37.7min (226) 35.7min (214/12) C2 6h 36min (2379) 6h 36min (2377) 0s (0/9) C3 25.5min (153) 18.5min (111) \u22121.3min (\u22128/3) C4 50s (5) 2h (720) 10.0min (60/10) (b) Training set: 1 data register at every 1.40min C1 1h 17min (463) 37.7min (226) 36.7min (220/8) C2 6h 36min (2379) 11h 30min (4145) 0s (0/11) C3 25.5min (153) 18.5min (111) 4.3min (26/9) C4 50s (5) 2h (720) \u22121.3min (\u22128/10) (c) Training set: 1 data register at every 3.33min C1 1h 17min (463) 36.7min (220) 36.7min (220/12) C2 6h 36min (2379) 11h 34min (4167) 0s (0/17) C3 25.5min (153) 1h 36min (581) 16.8min (101/22) C4 2h 7min (764) 2h 0.5min (723) 17.3min (104/16) On-line prediction of the feeding phase in high-cell density cultivation of rE. coli using constructive neural networks M.C. Nicoletti a b * carmo@dc.ufscar.br carmo.nicoletti@pq.cnpq.br J.R. Bertini Jr. c M.M. Tanizaki d T.C. Zangirolami e V.M. Gon\u00e7alves d A.C.L. Horta e R.C. Giordano e a Depto. de Computa\u00e7\u00e3o, UFSCar, S. Carlos, SP, Brazil Depto. de Computa\u00e7\u00e3o, UFSCar S. Carlos SP Brazil b FACCAMP, C.L. Paulista, SP, Brazil FACCAMP C.L. Paulista SP Brazil c Depto. de Computa\u00e7\u00e3o, ICMC, USP, S. Carlos, SP, Brazil Depto. de Computa\u00e7\u00e3o, ICMC, USP S. Carlos SP Brazil d Centro de Biotecnologia, Instituto Butantan, S. Paulo, SP, Brazil Centro de Biotecnologia, Instituto Butantan S. Paulo SP Brazil e Depto. de Engenharia Qu\u00edmica, UFSCar, S. Carlos, SP, Brazil Depto. de Engenharia Qu\u00edmica, UFSCar S. Carlos SP Brazil * Corresponding author at: Depto. de Computa\u00e7\u00e3o, UFSCar, S. Carlos, SP, Brazil. Tel.: +55 16 33518232; fax: +55 16 3351 8233. Abstract Streptococcus pneumoniae (pneumococcus) is a bacterium responsible for a wide spectrum of illnesses. The surface of the bacterium consists of three distinctive membranes: plasmatic, cellular and the polysaccharide (PS) capsule. PS capsules may mediate several biological processes, particularly invasive infections of human beings. Prevention against pneumococcal related illnesses can be provided by vaccines. There is a sound investment worldwide in the investigation of a proteic antigen as a possible alternative to pneumococcal vaccines based exclusively on PS. A few proteins which are part of the membrane of the pneumococcus seem to have antigen potential to be part of a vaccine, particularly the PspA. A vital aspect in the production of the intended conjugate pneumococcal vaccine is the efficient production (in industrial scale) of both, the chosen PS serotypes as well as the PspA protein. Growing recombinant Escherichia coli (rE. coli) in high-cell density cultures (HCDC) under a fed-batch regime requires a refined continuous control over various process variables where the on-line prediction of the feeding phase is of particular relevance and one of the focuses of this paper. The viability of an on-line monitoring software system, based on constructive neural networks (CoNN), for automatically detecting the time to start the fed-phase of a HCDC of rE. coli that contains a plasmid used for PspA expression is investigated. The paper describes the data and methodology used for training five different types of CoNNs, four of them suitable for classification tasks and one suitable for regression tasks, aiming at comparatively investigate both approaches. Results of software simulations implementing five CoNN algorithms as well as conventional neural networks (FFNN), decision trees (DT) and support vector machines (SVM) are also presented and discussed. A modified CasCor algorithm, implementing a data softening process, has shown to be an efficient candidate to be part of an on-line HCDC monitoring system for detecting the feeding phase of the HCDC process. Keywords Immunology Vaccine production High-cell density cultivations Bioprocess control Constructive neural networks 1 Introduction Streptococcus pneumoniae (pneumococcus) is a Gram-positive bacterium responsible for a wide spectrum of illnesses varying from non-invasive, such as otitis and sinusitis, to severe invasive diseases, such as meningitis [11,48,52,71]. The surface of the bacterium consists of three distinctive membranes: plasmatic, cellular and the polysaccharide (PS) capsule. PS capsules may mediate several biological processes, particularly invasive infections of human beings [80]. Based on the structure and antigenic properties of the polysaccharide, S. pneumoniae strains can be divided into more than 90 structurally and antigenically distinct types, which have been classified into serogroups (designated by numbers) and further subdivided into serotypes (designated by letters) according to the Danish designation system \u2013 serotypes that are structurally and immunologically closely related belong to the same serogroup [39]. Serotypes differ from each other in relation to virulence, predominance and drug resistance [6]. Differences in the virulence characteristics between pneumococcal serotypes are important since available vaccines target only a few of the known serotypes. The epidemiological profile of a pneumococcal disease is strongly related to the prevalence of the corresponding serotype and varies with time, geographic region and individual's age [46]. Invasive diseases, such as meningitis, happen mainly during the first months after birth; a large number of pneumococcal diseases in children are associated with a restrict number of serotypes, which varies according to the geographical region. In spite of the existing over 90 different serotypes, recent data suggests that 11 serotypes are responsible for approximately 75% of worldwide S. pneumoniae infections. In USA and Europe the most prevalent serotypes in children are 6, 14, 19 and 23 [51] while in adults the most prevalent are 3, 4, 7, 8 and 14 [29]. In developing countries it is common to find serotypes 1 and 5 in adults [64]. In Brazil, 45% of pneumococcal diseases are caused by serotypes 1, 6B and 14. Serotypes 1 and 6B are prevalent in all ages; serotype 14 is responsible for a greater number of infections in children while serotypes 3 and 4 are more common in adults [13\u201315]. Prevention against pneumococcal related illnesses can be provided by vaccines. Currently all the existing pneumococcal-related vaccines are based on the free occurrence of the capsular polysaccharide or then have the PS conjugated to a protein aiming at enhancing protection. One of the major difficulties in developing and commercializing a pneumococcal conjugate vaccine in Brazil is the large number of existing pathogenic serotypes in the country, namely 1, 3, 4, 5, 6A, 6B, 9V, 14, 18C, 19A, 19F, 23F, corresponding to approximately 80% of the clinically isolated strains [14]. It is well known that the process of producing a conjugated vaccine is highly elaborated and complex, involving many variables and whose final product yield is around 30%. The culture of the microorganism is a difficult process since not all serotypes grow in a similar way in the same medium. These difficulties, among many others, reflect on the final cost of the vaccine which makes almost impossible its broad use in developing countries. There is a sound investment worldwide in the investigation of a proteic antigen as a possible alternative to vaccines based exclusively on PS [5,7,11,71,76]. A few proteins which are part of the membrane of the pneumococcus seem to have antigen potential to be part of a vaccine. Particularly the PspA has presented itself as one of the most promising proteins, mainly because it is externally exposed to the polysaccharide capsule of the S. pneumoniae [17,49,50]. The work described in this paper is part of a broader project that investigates the viability of the design of a new pneumococcal conjugate vaccine customized to Brazilian most prevalent PS serotypes. Three laboratories from the Biotechnology Center of Butantan Institute (S. Paulo \u2013 SP) have already been working for a few years on subjacent areas to the project, such as the design of efficient methods of bacterial polysaccharide production and purification [41\u201343,60] and the implications of considering as the carrier protein a surface protein from the S. pneumoniae capable of inducing protection [27,30,67,72,85]. One of the proteins of choice was the PspA. A vital aspect in the production of the intended conjugate pneumococcal vaccine is the efficient production (in industrial scale) of both, the chosen PS serotypes as well as the PspA protein. This paper particularly investigates the viability of an on-line monitoring system, based on constructive neural networks, for automatically detecting the time to start the fed-phase of a high-cell density culture (HCDC) of the recombinant Escherichia coli (rE. coli) that contains a plasmid used for PspA expression. The layout of the paper is as follows. Section 2 presents the main aspects related to S. pneumoniae focusing on pneumococcal virulence factors and their role in colonization and disease. Section 3 describes a recent history of the various vaccines against the pneumococcus, their main characteristics and effectiveness, particularly taking into account the Brazilian population. Section 4 focuses on the fermentation process for growing rE. coli aiming at the production of the PspA protein for vaccine purposes. Section 4.1 approaches some of the technicalities involved in high cell density cultures (HCDC) of recombinant organisms, particularly the control of the feeding process and Section 4.2 describes the composition and operational conditions of four rE. coli cultivations, whose data are used in the monitoring experiments described in Section 6. Section 5 first presents a brief motivation for using constructive neural networks (CoNNs) and then describes the general characteristics of five CoNN algorithms used in the experiments described in Section 6. Section 6 presents the data and the methodology used for training the CoNNs as the online monitoring software for identifying the start of the feeding phase in HCDC cultures of rE. coli. Results of software simulations implementing the five CoNN algorithms as well as conventional neural networks (FFNN), decision trees (DT) and support vector machines (SVM) are presented and discussed. Finally Section 7 summarizes the work done highlighting its main conclusions and contributions. 2 S. pneumoniae \u2013 main characteristics, resistance and virulence factors S. pneumoniae is commonly found in the respiratory tract colonizing the nasopharyngeal cavity of the human host with other co-resident microorganisms. Colonization begins shortly after birth and by the age of 12 weeks has increased to rates to those found in mothers (Wiley and Douglas, 1981, as cited in [39]). After colonization it persists for weeks (in adults) or month (in children) usually without any adverse sequelae. This carrier state maintains the microorganism within human populations, and induces some acquired B-cell-mediated immunity to reinfection [11,94]. Although colonization is usually asymptomatic, in specific situations it can progress to disease when the colonizing S. pneumoniae spread to the paranasal tissues and lungs causing mucosal infections, such as pneumonia, or then invade the bloodstream and cause meningitis. Persistent carriage of pneumococci allows for low-level and longlasting transmission, thereby maintaining the non-invasive strains in human populations [91]. The pneumococcus is generally transmitted by direct contact with contaminated respiratory secretions among people or through fomites (substances capable of carrying microorganisms) [87]. Those at greatest risk for infections caused by S. pneumoniae are the elderly (\u226565 years), infants and children younger than 2 years [21,22]. As pointed out in [47], although the incidence of invasive pneumococcal disease peaks for children aged 6\u201318 months, children aged up to 5 years remain at risk. Young children have poor responses to carbohydrate antigens [39]. This age group is particularly vulnerable to infection for several reasons, including an incompletely developed anatomy, an immature immune system response, and frequent exposure and colonization by S. pneumoniae. Airway colonization by pneumococci is readily detectable in about 10% of healthy adults. Between 20 and 40% of healthy children are carriers, and more than 60% of infants and children in day-care settings can be carrier [91]. The ability of bacteria to survive but not grow in the presence of antibiotics (known as antibiotic tolerance) is a precursor phenotype to resistance [90]. In addition to that, antibiotic misuse (e.g., antibiotics prescribed for viral infections, dosages/duration of the treatment incorrectly prescribed, etc.), its excessive use as well as its use in veterinary medicine have been stressing the drug-resistance tendency. In the past the polysaccharide capsule was considered the primary virulence factor of S. pneumoniae because nonencapsulated bacteria are almost completely harmless compared with the same encapsulated strain [48]. A table gathering 21 known virulence factors in S. pneumoniae infection (including several proteins) is presented in [91]. In the table each virulence factor is described by its mechanism of action and an associated level of evidence (LE) varying from 1+ up to 4+, meaning 1+: one or few animal studies; 2+: some evidence from several animal studies; 3+: confirmed in several animal studies and 4+: confirmed in people. Among the 21, only the PS capsule has LE=4+ and the pneumolysin has LE=3+. The remainder 19 virulence factors have their LE values distributed as: eight have LE=1+, eight have LE=2+ and three have LE between 1+ and 2+. As commented by the authors, \u201cMany virulence factors probably contribute to invasive pneumococcal disease in people, but unequivocal evidence exists only for pneumococcal capsules \u2013 the target of present vaccine formulations\u201d. Capsular polysaccharides are highly immunogenic (although type-specific) and highly hydrated molecules that are over 95% water; they are linked to the cell surface of the bacterium via covalent attachments to either phospholipids or lipid-A molecules [11,25,95]. They are composed of repeating single units (monosaccharides) joined by glycosidic linkages. The layer underneath the capsule, the cell wall, consists of polysaccharides and teichoic acid and serves as an anchor for cell-wall-associated surface proteins [11]. Besides typical Gram-positive surface proteins, the cell surface of S. pneumoniae has several proteins that contribute as virulence factors for the microorganism. A number of reviews in the literature [7,48,76,80,91] describe the most well-known S. pneumoniae surface-exposed proteins and the virulence role they play. Studies have suggested that certain pneumococcal proteins could be used in vaccines [5,17,11,12]. Table 1 presents several candidate proteins with their associated LE value according to [91]. As suggested in [48], if antibodies to these proteins could offer better protection, they could provide the source of a pneumococcal vaccine to be used in conjunction with or in place of the more traditional capsular polysaccharide vaccine\u2026 The author goes on by saying that some of these proteins, such as PspA or Ply have already shown potentials for use in an alternative vaccine approach. PspA, particularly, is expressed by all clinically relevant capsular serotypes of S. pneumoniae [26]. 3 The state-of-art of available pneumococcal vaccines Vaccines against S. pneumoniae are known as pneumococcal vaccines. The history of vaccines against S. pneumoniae goes back to 1911 with the development of a whole-cell pneumococcal vaccine [96]. In spite of a few attempts to develop a pneumococcal vaccine over the few following decades, the growing interest in pneumococcal vaccines declined with the discovery and use of penicillin. Later on, however, the interest in pneumococcal vaccines was revived as a consequence of the increasing antibiotic resistance among microorganisms. A detailed history of developments of pneumococcal vaccines, divided into two time-periods, 1911\u20131950 and 1960\u20131991, is presented in [18]. Several other relevant articles cover historical aspects of research work in pneumococcus and vaccines such as [4,20,93], to name a few. The pneumococcal vaccines developed in the few last decades have been approached divided into two groups: identified in this paper as Sections 3.1 and 3.2. 3.1 Vaccines based exclusively on PS (pneumococcal polysaccharide vaccines) (PPSV or PV) 3.1.1 The 14-valent polysaccharide vaccine It was licensed in the US in 1978 and designed based on studies reporting that more than 80% of S. pneumoniae infections were caused by the following 14 serotypes: 1, 2, 3, 4, 6A, 7F, 8, 9N, 12F, 14, 18C, 19F, 23F and 25 ([3], as cited in [64,18]). 3.1.2 The 23-valent polysaccharide vaccine Both currently licensed 23-valent polysaccharide pneumococcal vaccines (PV23), the Pneumovax 23 (manufactured by Merck & Company, Pennsylvania) and the Pnu-Imune 23 (manufactured by Lederle Laboratories, New Jersey) contain 25\u03bcg of each of 23 purified capsular polysaccharide antigens of S. pneumoniae (serotypes 1, 2, 3, 4, 5, 6B, 7F, 8, 9N, 10A, 11A, 12F, 14, 15B, 17F, 18C, 19A, 19F, 20, 22F, 23F, and 33F). The 23 serotypes represent at least 85\u201390% of the serotypes that cause invasive pneumococcal infection among children and adults in the US [65]. The vaccines are only moderately effective and are not prescribed for children younger than 2 years due to the poor antibody responses to the polysaccharides. Also, the PV23 have limited efficacy for reducing the incidence of otitis media, have no effect on nasopharyngeal carriage of the bacteria and do not elicit a booster response with repeated exposure to the bacteria [47]. 3.2 Vaccines based on PS conjugated to a protein (pneumococcal conjugate vaccines) (PCV) In 1929 it was shown by Avery and co-workers that the covalent binding of capsular PS to proteins (carriers) increased the immunogenicity of the PS [73]. Conjugate vaccines using PS from different serotypes bounded to different proteins have been developed for S. pneumoniae. As pointed out in [12] the linkage of PSs to proteins is restricted: too much carrier antigen may impair the antibody response to the PSs by antigen competition or carrier-mediated epitope suppression; thus, protection provided by conjugate vaccines is restricted to a certain number of serotypes. Both tetravalent conjugate vaccines, PncD and PncT, produced by Aventis Pasteur, contain the 6B, 14, 19F and 23F PSs conjugated to diphtheria and tetanus toxoid, respectively [92]. A conjugate vaccine against the pneumococcus (PCV7) was licensed in 2000 under the commercial name of Prevenar (Wyeth, Europe) or Prevnar (Wyeth, USA) and has PS from 7 serotypes: 4, 6B, 9V, 14, 18C, 19F and 23F conjugated to the mutated diphtheria toxin CRM197. Within the US the seven serotypes are responsible for 83% of invasive disease in children younger than 4 years of age, as reported in the study about the efficacy, safety and immunogenicity of the PCV7, described in [10]. There are a few reports on clinical trials of a 9-valent PCV which, besides the seven previously mentioned, also includes serotypes 1 and 5 [14,28,56,63]. In Brazil, as well as in many developing countries, due to the high price of PCV vaccines, they are only accessible to a small fraction of the population. In the study described in [14], for children up to age 5 years, 9 serotypes: 1, 5, A, 6B, 9V, 14, 18C, 19F and 23F remained the most prevalent during the research period. As authors point out, in Brazil the estimated coverage against invasive pneumococcal diseases by PCV7 and PCV9 during childhood is ~58.2% and ~73.0%, respectively. The PCV Synflorix, produced by Glaxo-Smith-Kline is a 10-valent PCV and has PSs from the serotypes: 1, 4, 5, 6B, 7F, 9V, 14, 18C, 19F and 23F, which are conjugated to a conserved homophillus carrier protein. In 2010 Pfizer introduced the Prevnar 13, which contains six new serotypes (to the previously 7 of the 7-valent Prevnar) namely: 1, 3, 5, 6A, 7F and 19A conjugated to the same carrier protein CRM197. 4 Growing recombinant E. coli This section is organized into two sections. The first focuses on some technicalities involved in growing rE. coli in high cell density cultures (HCDC) stressing the importance a refined control over a few relevant variables plays in a successful result \u2013 particularly the on-line automatic detection of the beginning of the feeding phase. The second describes the material, methods and laboratorial setup used in the four high-cell density cultivations of rE. coli whose data were used in the control simulation experiments described in Section 6. 4.1 High cell density cultures (HCDC) \u2013 main characteristics The so called recombinant proteins are proteins produced by microorganisms or animal cells that have been genetically modified for producing them. The modifications are introduced into the microorganisms (or cells) using expressions vectors. Generally an expression vector is a plasmid especially designed (i.e., created in laboratory) to produce a large amount of mRNA, which can be translated into protein, in the microorganisms. Since the desired protein is produced inside the microorganism (or animal cell), it must be purified away from the host cell proteins by chromatography, following cell lysis [1]. In principle all bacteria can be used for heterologous protein production [89]. Many of them, however, never come close to being used mainly due to the lack of information about their regulation and mechanism, combined with the fact that usually there are no vector and promoter systems commercially available. In general overexpressed recombinant proteins accumulate in one of the three compartments of the bacteria: the cytoplasm, the periplasm, and the extracellular space. As suggested in [23] the decision to target recombinant proteins to the cytoplasmatic space, periplasmatic space or culture medium rests on balancing the advantages and disadvantages of each compartment. The cytoplasm is usually the first choice for heterologous protein production because the higher yield seems to be more attractive [89]. E. coli is a Gram-negative bacterium which usually is the microorganism of choice for the expression of a variety of recombinant proteins. E. coli systems are largely used for industrial and pharmaceutical protein production. Its genetics are well understood and comparatively simple as well as easily manipulated. Also, it can grow to high densities on inexpensive media [40,53,54,61]. However cultivations for industrial purposes often presents E. coli cells with a growth condition highly different from their natural environment, resulting in deterioration in cell physiology and limitation in cell's productivity [24]. Fed-batch is often the cultivation method of choice to obtain high cell densities and, as a consequence, high productivity [62,86]. Fed-batch cultivation is a very common type of biotechnological process which corresponds to a bioreactor to which substrate is continuously fed while the microorganisms and products remain in the bioreactor until the end of the process. The control of such a bioprocess can be approached in a few different ways and, depending on the approach, computational intelligence techniques can be used with different goals [70]. In order to achieve high cell density, the stresses that usually limit cell growth and recombinant protein expression must be removed, thus allowing continued growth and maximum expression of recombinant products [59,78,84]. High cell density cultures of E. coli can be approached as a three-phase process: (1) batch phase, where microorganism growth is supported by the nutrients present in the initial medium; (2) fed-batch phase, where additional medium is supplied into the bioreactor, usually in an exponential feed rate and (3) the induction phase, where the heterologous protein production takes place after the addition of the adequate inductor [57,83]. As pointed out in [45], a HCD cultivation can exceed 50h of operation, requiring constant surveillance by skillful operators, to adjust operational conditions in response to changes in the cell metabolism occurring in each phase. The detection of the right moment to shift from the batch to the fed-batch phase is a critical issue. On the one hand if the feed supply starts too early, growth inhibition by the substrate and/or production of organic acids due to the overflow metabolism may occur. On the other hand, if the feed supply begins too late, the duration of the time period under a shortage of nutrients may lead to the loss of viability. In either situation, the overall process productivity regarding biomass formation (and consequently recombinant protein expression) will be greatly affected [55]. Growing rE. coli in high cell density cultures under a fed-batch regime requires a refined continuous control over various process variables; as mentioned before, the on-line prediction of the feeding phase is of particular relevance and interest and one of the focuses of this paper, detailed in Section 6. 4.2 Material, methods and operational conditions 4.2.1 Strains and plasmid (a) The N-terminal recombinant fragment of the PspA gene from clade 3 of family 2 (PspA3) of S. pneumoniae was cloned into the pET37b+ plasmid vector (Department of Microbial Genetics, National Institute of Genetics Shizuoka-ken, Japan) with a polyhistidine tag in the amino terminal, and inserted into E. coli BL21 (DE3). This construct was kindly provided by Dr. Eliane N. Miyaji (Centro de Biotecnologia, Instituto Butantan, S. Paulo, Brazil). (b) The N-terminal recombinant fragment of the PspA gene from clade 1 of family 1 (PspA245) of S. pneumoniae was cloned into the pET37b+ plasmid vector (Department of Microbial Genetics, National Institute of Genetics Shizuoka-ken, Japan) with a polyhistidine tag in the amino terminal, and inserted into E. coli BL21(DE3). This construct was kindly provided by Dr. Luciana C. C. Leite (Centro de Biotecnologia, Instituto Butantan, S. Paulo, Brazil). 4.2.2 Medium preparation The medium was prepared as described in [57,83]. For preparation of all media (i.e., pre-culture, batch and feeding media), (NH4)2HPO4, KH2PO4, citric acid, EDTA and trace elements were dissolved in distilled water, the pH was adjusted to 6.3 with NH4OH, and the solutions were sterilized for 30min at 121\u00b0C. Stock solutions of MgSO4 and glycerol were sterilized separately for 30min at 121\u00b0C. Thiamine and kanamycin were sterilized separately by filtration. After cooling, all solutions were mixed, including polypropylene glycol 30% v/v as antifoam agent. Preliminary studies employing different temperatures were conducted in shake flasks, with agitation at 280rpm for 14\u201322h. Erlenmeyer flaks (500mL volume) containing 50mL of batch medium Cultivation 1 (Table 2 ), with 20g/L of glycerol, were used. After identification of the most suitable conditions for growth and protein production, bioreactor runs were carried out to study the conditions selected for the high density cultures. 4.2.3 Bioreactor operation The four HCDC (referred to as Cultivations 1\u20134) were carried out in a 5L in-house bioreactor monitored using the LabView\u00ae-based software SuperSys_HCDC\u00ae [44]. Table 2 details the batch media composition of the four cultivations and Table 3 their feed media composition. The pH was controlled (on/off) (GLI PRO pH meter) at 6.7 during the batch phase and at 6.9 during the fed-batch phase, by addition of NH4OH (30%). The temperature was set at 30\u00b0C and dissolved oxygen concentration (DOC) was monitored by a dissolved oxygen probe (Mettler Toledo probe Inpro 6800, connected to a CE O2 transmitter), and kept at 30% of saturation by a hybrid controller which automatically altered both, the agitation speed (between 200 and 900rpm) and the composition of the gas stream supplied to the bioreactor (by mixing pure oxygen with air). The inlet gas flow rate was maintained at ~4L/min by two mass flow controllers (GFC Aalborg). The broth permittivity and conductivity were monitored using a biomass sensor (Fogale Nanotech) with on-line data acquisition via CFP. The exhaust gas composition was analyzed using a Sick/Maihak S.710 analyzer. Supplementary feed supply was controlled by SuperSys_HCDC\u00ae to match the exponential growth requirements, as described in [44]. 4.2.4 Analytical methods Culture samples were collected at 2 or 3h for off-line measurements of key variables (optical density, dry weight, organic acids and glycerol concentration). Biomass formation was estimated by the optical density and dry cell weight method. Metabolites concentration and carbon sources consumption were measured by HPLC. A more detailed description of the laboratorial experiments can be found in [45], where data collected from the four cultivations have also been used aiming at controlling the feeding phase of rE. coli cultivations. 5 Constructive neural networks (CoNN) and five CoNN algorithms This section presents and discusses the general ideas and relevant aspects of five constructive neural networks (CoNN) algorithms used in the simulated experiments for controlling the start of the feeding phase of rE. coli cultures, described in Section 6. The section is organized in six short sections: Section 5.1 introduces neural networks (NNs) and gives the motivations for using CoNN algorithms. Each of the five following sections presents a brief description of one of the five CoNN algorithms used in the control experiments, namely: (Section 5.2) Pyramid, (Section 5.3) BabCoNN, (Section 5.4) Shift, (Section 5.5) Perceptron Cascade and finally (Section 5.6) Cascade Correlation. Except for BabCoNN all the others are well-known algorithms with detailed descriptions easily found in the literature (see e.g. [34]) and, for this reason, only BaBCoNN is described in more detail. The first four are two-class classification algorithms and the fifth is suitable for regression tasks. 5.1 A brief introduction to CoNNs Similarly to other machine learning techniques, neural network (NN) algorithms can be characterized as supervised i.e., when target values are known and the algorithm uses this information, or as unsupervised, when such information is not used by the algorithm (typically for dealing with situations where the information is not available). NN architectures can be organized into two main groups: (a) feedforward, where the connections between neurons do not form cycles and (b) feedback (or recurrent), where the connections may form cycles. NNs may also differ in relation to several other characteristics, particularly in relation to the type of data they deal with \u2013 the two more popular being categorical and quantitative. Learning with categorical targets is known as classification while learning with quantitative target values is referred to as regression; classification tasks can be approached as a particular type of regression tasks [69]. Traditional neural network training algorithms such as Backpropagation [81] require the definition of the network architecture, prior to training. Generally speaking, these methods work well only when the network architecture is appropriately chosen. However, there is no general solution to the problem of defining a NN architecture for a given application. On the one hand, if the chosen architecture is too small the NN may not be capable to represent the association between inputs and outputs; on the other hand, if it is too large, it may not be capable to generalize appropriately. Usually the task of defining a suitable architecture is approached by systematically defining several different architectures (particularly varying the number of hidden layers and the number of hidden neurons per layer) and, as a subsequent task, training and evaluating of each of them. At the end of the process, the one with the best performance is chosen. An alternative to this time-consuming procedure is the use of constructive neural network (CoNN) methods, which construct the neural network architecture along (and as a result) of its training. Several proposals of CoNN algorithms can be found in the literature, all of them trying to solve the problem of defining the best NN architecture for a given application, by dealing with both problems i.e., architecture and application, at the same time [38]. Typically CoNN algorithms start with a neural network with no hidden neurons. The definition of the NN architecture as well as its training are an integrated process, where hidden nodes (generally Threshold Logic Units (TLU)) are added to the network and subsequently trained, one at a time, until a stopping criterion (generally characterized by a satisfactory solution) is reached. In so doing, the process of determining the size and the architecture of the network is intertwined with the learning process. Among the most well known classifiers are: Tower and Pyramid [36], Tiling [66], Upstart [35], Pti and Shift [2] and Perceptron-Cascade [19]; each of them can be found in two different versions, suitable for two-class and multi-class problems respectively. There are not many constructive NN proposals to deal with regression problems. The Cascade Correlation (CasCor) [33] is generally the algorithm of choice when dealing with such problems. Generally a CoNN algorithm uses the Perceptron or any of its variants, such as Pocket or Pocket with Ratchet Modification (PRM) [37] or then the geometry-based Barycentric Correction Procedure (BCP) [74] to train each neuron (TLU) added to the NN. On the one hand the Perceptron is certainly the most well known algorithm for training a TLU and its wide use has confirmed its importance, especially in Backpropagation trained algorithms. On the other hand the BCP has established itself as a good option compared to the PRM when used by CoNN algorithms [8]. 5.2 The Pyramid algorithm The Pyramid algorithm and its counterpart the Tower algorithm, both proposed in [36], are CoNN algorithms that iteratively grow the architecture of a NN by adding hidden layers to it; each new hidden layer has only one TLU. Particularly each new hidden node (defining a new hidden layer) created by Pyramid is connected to every hidden node previously added to the network, as well as to the input nodes. Each step of the learning phase expands the training patterns in one dimension (i.e., the dimension that represents the last hidden neuron added). Considering that the first step adds the first hidden node to the network, the kth step adds the kth hidden node; input to this neuron are the p +1 input values from the input layer plus bias, as well as the output of the (k \u22121) hidden nodes previously added to the network. As it is well known, a single neuron can learn with 100% precision only from linearly separable training sets. If that is not the case, during the training phase, the Pyramid algorithm continues to add neurons to the network (one per hidden layer) until a stopping criterion is satisfied. Generally three stopping criteria can be implemented: (1) the NN correctly classifies the training set; (2) adding a new hidden neuron to the NN does not contribute to increasing its accuracy and (3) a predefined maximum number of hidden neurons has been reached. Pyramid and Tower differ only in the way neurons are connected. Both algorithms can be approached as incremental versions of the Perceptron algorithm. 5.3 The Baricentric-based constructive neural network algorithm (BabCoNN) A neuron u produces what is called wrongly_off errors when positive training patterns are misclassified by u as negative. Similarly, it produces wrongly_on errors when negative training patterns are misclassified as positive. CoNN algorithms have different strategies for dealing with these types of errors. The BabCoNN, proposed in [9], is a two-class CoNN algorithm based on some of the concepts used by the geometric-based algorithm (BCP) algorithm. Similarly to the Upstart, Perceptron Cascade (PC) and Shift algorithms, BabCoNN also constructs the network beginning with the output neuron. However, it creates only one hidden layer; each hidden neuron added to it is connected to the input layer as well as to output neuron, similarly to Shift, although the connections BabCoNN creates do not have an associated weight. While Upstart, PC and Shift construct the network by adding new hidden neurons specialized in correcting wrongly-on or wrongly-off errors, BabCoNN employs a different strategy to add new hidden neurons to the network. The network construction starts by training the output neuron, using the BCP. Next, the algorithm identifies all the misclassified training patterns; if there are none, the algorithm ends otherwise, it starts adding neurons (one at a time) to the unique hidden layer of the network, aiming at not having misclassified patterns. A hidden neuron added to the hidden layer will be trained with the training patterns that were misclassified by the last added neuron. The first hidden neuron will be trained with the patterns that were misclassified by the output neuron; the second hidden neuron will be trained with the set containing the patterns that the first hidden neuron was unable to classify correctly, and so on. The process continues up to the point that no training patterns remain or all the remaining patterns belong to the same class. The process of building the network architecture is described by the pseudocode in Fig. 1 , where E ={E 1, E 2, \u2026 E n } represents the training set containing n training patterns and each training pattern is described as E i =\u3008x 1, x 2, \u2026, x k , C\u3009, i.e., by k attributes and an associated class C \u2208{\u20131, 1}. In Fig. 1 variable output represents a single hidden neuron and variable hiddenLayer is a vector of hidden neurons; both variables define the NN. Function bcp() stands for the BCP algorithm, used for training individual neurons. Function removeClassifiedPatterns() removes the patterns that were correctly classified by the last added neuron and bothClasses() is a Boolean function that returns \u2018true\u2019 if the current training set still has patterns belonging to both classes and \u2018false\u2019 otherwise. Due to the way the learning phase is conducted by BabCoNN, each hidden neuron of the network is trained using patterns belonging to a region of the training space (i.e., the one defined by the patterns that were misclassified by the previous hidden neuron added to the network). This particular aspect of the algorithm has the effect of introducing an undesirable \u2018redundancy\u2019, in the sense of a pattern being correctly classified by more than one hidden neuron. This has been sorted out by implementing a classification process, where the neurons of the hidden layer have a particular way of firing their output. Fig. 2 gives an example of the classification process in a 2D space and Fig. 3 describes the pseudocode of the classification procedure. In Fig. 2 the 2D patterns used for training the hidden neuron are represented by \u2018+\u2019 (positive) and \u2018\u2212\u2019 (negative); b 1 and b 2 are the barycenters of the regions defined by the \u2018+\u2019 and the \u2018\u2212\u2019 patterns respectively; W is the weight vector after the training and H is the hyperplane defined by W and the bias. For each class, its associate region is defined as the hypersphere whose radius is given by the largest distance between all correctly classified patterns and the barycenter of such region. To exemplify how a hidden neuron behaves during the classification phase, let Y i =\u3008y 1, y 2\u3009 (i =1, 2, 3, 4) be a given set of patterns to be classified. As can be visualized in Fig. 2, four situations can happen: (1) The new pattern (Y 1) lies on the positive region of classification of the hidden neuron. The pattern Y 1 is classified positive by the neuron, that fires +1. (2) The new pattern (Y 2) lies on the positive region, but now laying on the other side of the hyperplane; this would make the neuron classify Y 2 as negative. However, the neuron will fire the value 0 since there is no guarantee that the pattern is negative. (3) The new pattern (Y 3) lies on the outside of both regions; in this case the neuron fires the value 0 independently of the classification given by the hyperplane it represents. (4) The new pattern (Y 4) lies on the negative region of classification of the hidden neuron. The pattern Y4 is classified negative and the neuron fires the value \u22121. Note that the regions may overlap each other and, eventually, a pattern may lie on both regions. When that happens, the hidden neuron (version used in the experiments described in Section 6) assigns the pattern the class defined by the hyperplane classification. The pseudocode of the classification procedure is described in Fig. 3. In the classification procedure described in Fig. 3, after each hidden neuron fires its output, the output neuron decides what class the given pattern belongs to. The decision process is based on the sum of all the responses; if the resulting value is positive the pattern is classified as positive, otherwise, as negative. If the sum result is 0, the output node does the classification of the pattern. The function classification() returns the neuron's classification (1 or \u22121), this is the usual classification that uses weight vector and bias. Both functions belongsToPositive() and belongsToNegative() are Boolean functions. The first returns \u2018true\u2019 if the pattern lies in the positive region and \u2018false\u2019 otherwise. The second returns \u2018true\u2019 if the pattern lies in the negative region and \u2018false\u2019 otherwise. The Hlc vector stores the classifications given by all hidden neurons, for a given pattern and the last conditional command in the classification algorithm defines the class associated with the input pattern X. 5.4 The Shift algorithm Like the Upstart and Perceptron Cascade, the Shift algorithm [2] also constructs the network beginning with the output neuron. Shift, however, creates only one hidden layer, iteratively adding neurons to it, each of them connected to the input neurons and to the output neuron. It also identifies wrongly-on and wrongly-off errors and trains a hidden neuron to correct the most frequent between the two types of errors. Consider a learning situation with M training patterns and let v k be the sum of all inputs of the output neuron when the kth pattern is fed to the network. The training set of a wrongly-off neuron has all patterns i such that v i \u22640, 1\u2264 i \u2264 M and of a wrongly-on neuron, those such that v i >0, 1\u2264 i \u2264 M. In spite of training a new neuron to correct wrongly-on (or wrongly-off) errors, Shift calculates a weight associated with the connection between the new neuron and the output neuron based on the values of v i , 1\u2264 i \u2264 M. 5.5 The Perceptron Cascade algorithm The Perceptron Cascade (PC) algorithm [19] is a constructive neural network algorithm suitable for two-class classification tasks with an architecture similar to that created by the Cascade Correlation algorithm [33] (Section 5.6) and employs similar error-correction strategy to the one used by the Upstart algorithm [35]. PC grows the NN from the output node toward the input layer. It begins the construction of the network by training the output neuron. If this neuron does not classify all training patterns correctly, the algorithm begins to add hidden neurons to the network. Each new added hidden neuron is connected to all previous hidden neurons as well as to the input neurons. The new hidden neuron is then connected to the output neuron; each time a hidden neuron is added, the output neuron needs to be retrained. The addition of a new hidden neuron enlarges the space in one dimension. The algorithm has three stopping criteria: (1) the network converges i.e. correctly classifies all training patterns; (2) a pre-defined maximum number of hidden neurons has been achieved and (3) the most common, the addiction of a new hidden neuron degrades the network's performance. Similarly to Upstart algorithm, hidden neurons are added to the network in order to correct wrongly-on and wrongly-off errors. Following the same strategy employed by Upstart, what distinguishes a neuron created for correcting wrongly-on or for correcting wrongly-off errors caused by the output neuron is the training set used for training the neuron. To correct wrongly-on errors the training set used should have all negative patterns plus the patterns causing the wrongly-on errors. For correcting wrongly-off errors, the training set should have all positive patterns plus the negative patterns that cause wrongly-off errors. Unlike Upstart, however, the PC algorithm only adds and trains one neuron at a time in order to correct the most frequent between wrongly-on and wrongly-off errors produced by the output neuron. 5.6 The Cascade Correlation algorithm (CasCor) The Cascade Correlation (CasCor) algorithm is one of the very few CoNN algorithms suitable for modeling regression problems and probably the most popular. As described in [33], the CasCor algorithm combines two approaches: (1) a network with an architecture that resembles a cascade and (2) a learning algorithm for training and creating new hidden neurons. The main motivations that subsidized the CasCor proposal were the existing limitations concerning the training time of Backpropagation. In the literature, the two main problems related to the Backpropagation training procedure are the definition of the network architecture and the excessive time for learning (see [58,68] for details). Also, there is the possibility that networks trained with Backpropagation lack good generalization capabilities and get trapped into a local minimum (see [31\u201333,79]). Additionally, as mentioned in Renner [75], the search for a set of modifiable training parameters aiming at a good performance can become a time-consuming process. Quoting Fahlman and Lebiere, \u201cthe learning algorithm begins with no hidden units. The direct input-output connections are trained as well as possible over the entire training set. With no need to back-propagate through hidden units, we can use the Widrow-Hoff or \u2018delta\u2019 rule, the Perceptron learning algorithm, or any of the other well-known learning algorithms for single-layer networks\u201d [33]. In the experiments described in this paper, the Quickprop algorithm, described in [31,32], which shares some similarities with the Backpropagation algorithm, was used. The training process using CasCor has two steps: the first trains the output weights and the second trains the new hidden neuron added to the network. The two steps are repeated until a termination condition is satisfied. Basically, there are three termination criteria: (1) the addition of a hidden neuron do not minimizes network's mean error \u2013 the most common; (2) the number of pre-estimated hidden neurons (here, 100) was reached; or (3) the network perfectly fits the input data \u2013 the most improbable, at least for real-world applications. For the experiments outlined in Section 6, the Quickprop used the hyperbolic tangent activation function; the algorithm was executed up to 100 iterations, the learning rate was initially set to 1, and had its value multiplied by 0.95 each time the condition (100mod (100 \u2013 itr)=0) was satisfied, where itr represents the number of the current iteration. The initial weight vector, for each neuron, was randomly generated within the interval (1, 2). 6 Neural networks as monitoring tools for high cell density cultivations (HCDC) \u2013 inferring the start-up of the feed process in HCDC In the literature there have been many different approaches for solving the problem of estimating the optimal feeding strategies aiming a improving the outcome of a fed-batch bioreactor. Among the many proposals there are those based on mathematical methods for representing the kinetics and mass balances of the fermentation process, such as dynamical programming as well as the data-driven methods, such as neural networks [70]. Of particular interest in this paper is the on-line detection of the starting point of the fed-batch phase of cultivations of recombinant E. coli, through data-driven algorithms, here implemented as CoNNs. This section is organized into two sections: Section 6.1 describes the available experimental data, their pre-processing and the methodology employed for training the NNs. Section 6.2 presents and discusses results using the five CoNN algorithms as well as FFNN, DT and SVM. 6.1 Methodology for training and evaluating the NNs The data used for training and evaluating the neural networks constructed-trained by each of the five CoNN algorithms were obtained in four laboratorial cultivations of HCDC of rE. coli, as described in Section 4. For the sake of simplifying notation the corresponding data will be referred to as: dataset C1 (Cultivation 1), dataset C2 (Cultivation 2), dataset C3 (Cultivation 3) and dataset C4 (Cultivation 4). A detailed description of the four cultivations components, at the beginning of their corresponding batch phase, is given in Table 2. It is worth reminding that the first three cultivations were of rE. coli expressing PspA3 and the fourth was of rE. coli expressing PspA245. In each cultivation one data register was collected at approximately every 10s. Each data register (in the four datasets C1, C2, C3 and C4) is described by the values of the nine variables shown in Table 4 . It is very important to mention that each dataset C1, C2, C3 and C4 stores data registers collected from the corresponding laboratorial cultivation until shortly after the feeding time has been reached. This contributes to a high unbalance between the numbers of data registers representing the two scenarios: (1) no-need of supplementary feed (NNSF) and (2) in-need of supplementary feed (INSF), stored in each dataset. Table 5 shows the unbalanced numbers. Previously to its use, each dataset was inspected and an extra variable V, reflecting the need (V =1) or not (V =0) of supplementary feed, given the values of the variables describing each data register, was added to the corresponding register. The addition of the extra variable made it possible to deal with the problem of identifying the starting point of the feeding phase as a classification task, suitable to be solved by classification algorithms. As described in Section 5, the four constructive neural network algorithms Pyramid, BabCoNN, Shift and Perceptron Cascade are suitable for classification problems. Their main customized characteristics adopted for the simulated experiments are summarized in Table 6 . CasCor is included in Table 6 for convenience since it has been used for a regression task, as it will be described next. The choice of CasCor was motivated by authors\u2019 intent to investigate its use in classification tasks. The adopted stopping criteria (i.e., the addition of hidden neurons up to a maximum of 15 as informed in the last column of Table 6) was used having in mind that, throughout the whole process of growing the neural network, the best architecture so far was kept. For the CoNN control experiments approached as a classification task (results presented in Tables 8\u201310), the training and testing datasets are defined as in Table 7 . As mentioned before, CasCor is suitable for regression tasks. To approach the classification problem as a regression problem the new variable (V) that identifies a data register as belonging to the batch phase or to the fed-batch phase, was \u2018softened\u2019. The so called softening process aimed at modifying the crisp transition between both phases by replacing their crisp boundary with a \u2018soft\u2019 boundary promoting, as much as possible, a smooth transition between the two phases. As the cultivation process progresses, data patterns gradually become less representative of the batch phase and more representative of the fed-batch phase. To implementing the softening process four new datasets were created (CC1, CC2, CC3 and CC4) by altering the value of variable V in their registers, according to the Bell function (parameters: a =500, b =2 and c set as the value of the first data register with V =1, corresponding to the center of the curve). The diagram in Fig. 4 shows the corresponding process and Fig. 5 graphically shows the softening process, a strategy proposed in this work, applied to data registers in dataset C1. The experiments using CasCor were conducted according to the scheme shown in Fig. 6 . For each training set Tr i (i =1, \u2026, 4) a neural network referred to as CasCor i was induced. After that, the corresponding TD i was input to CasCor i and a threshold value \u03b8 i (associated to CasCor i output neuron) was determined, according to formula given in Eq. (1). Formula (1) is used for determining the corresponding threshold value associated to the output node of each induced CasCor k network (k =1, \u2026,4). The empirically established formula promotes a \u2018balance\u2019 between the three fed-batch crisp starts reflected in the original datasets that gave rise to the corresponding modified training datasets used to induce CasCor k . A few different strategies have been employed in an attempt to find a suitable value for \u03b8. The best results were obtained by formula (1) which chooses the maximum value among the outputs produced by CasCor in the neighborhood of the data region where the supplementary feed should be provided. A possible explanation for the success of formula (1) can be credited to CasCor tendency to learn curves representing the data by approximating them from below. The user-defined parameter j represents a window of registers. In the experiments j was set to 1. (1) \u03b8 k = max i = x \u2212 j x + j { CasCor ( i ) , x \u2208 T D k | V ( x \u2212 1 ) = 0 & V ( x ) = 1 } 6.2 Results and discussion To have a better understanding of the available experimental data, Figs. 7 and 8 show the four datasets plotted in a 3D space, represented by their values of agitation, permittivity and CO2 and Figs. 9 and 10 show them plotted in a 3D space, represented by their values of CER, permittivity and conductivity. Tables 8\u201310 show results from the simulation experiments for the automatic start-up of supplementary medium feeding in HCDC of recombinant E. coli. Each table groups results from neural networks induced by Pyramid, BabCoNN, Shift and PC algorithms, having as input training sets described by: (a) all the nine variables listed in Table 4 (Table 8); (b) variables agitation, permittivity and CO2 molar fraction in the exhaustion gas (Table 9) and (c) Carbon dioxide evolution rate, permittivity and conductivity (Table 10). In the tables positive values stand for earlier predictions and negative values stand for later predictions. As described in Section 6.1, each algorithm used as training dataset data from three cultivations and as testing dataset data from the fourth (e.g. training dataset=C2\u222aC3\u222aC4, testing dataset=C1 \u2013 see Table 7). The CoNN algorithms were implemented in Java and run on Windows. Due to the excessive number of data registers (see Table 5), Tables 8\u201310 also present results from three different strategies adopted for reducing the number of training data registers, namely: (a) selection (for composing the training dataset) of 1 data register at every 1.00min; (b) selection of 1 data register at every 1.40min; (c) selection of 1 data register at every 3.33min. All three tables also provide the number of hidden neurons in the induced NN as well as the number of classification errors of the NN when classifying the corresponding testing dataset. Table 11 shows the results from the simulation experiments for the automatic start-up of supplementary medium feeding in HCDC of rE. coli using NNs induced by the CasCor algorithm. Similarly to the previous tables, Table 11 also shows results induced by the algorithm having as input training sets described by: (a) all the nine variables in Table 4; (b) variables Agit, Perm and CO2 molar fraction in the exhaustion gas and (c) variables CER, Perm and Cond. The second, third and fourth column show results obtained considering the same three strategies for selecting input registers as before, respectively. Notation (N/M) employed in the table represents the same as before. As far as the constructive neural networks are concerned, taking into account values shown in the four previous tables, it can be noticed that they estimate the feeding phase earlier than they should (considering the information provided by the human expert represented by variable V). One reason for such bias could be the unbalanced between the number of registers representing the batch phase (NNSF) versus the number of registers representing the fed-batch phase (INSF), as shown in Table 5 (INSF\u226aNNSF). Focusing on results related to the induced classifying neural networks, presented in Tables 8\u201310, it can also be seen that the sampling size of the training dataset has a strong influence on the performance of the induced NNs. On average, the classifying NNs have better classification results when their corresponding training sets are constructed by selecting 1 training register per 3.33min. This result, however, is not a general rule and consequently, it should be taken with caution. On the one hand, an excessive number of training data registers may overload the learning capability of some algorithms and, on the other hand, a reduced training dataset may not be enough to produce a robust generalization. Considering the different simulation experiments related to classifying NNs, the ones induced by BaBCoNN were the most stable as far as accuracy is concerned. An attractive characteristic of the BabCoNN algorithm is the size of the induced NNs (generally smaller than those induced by other algorithms). In relation to the other classification algorithms, the PC was the least stable algorithm since it had very good results in some experiments and had a very poor performance in others. Shift and Pyramid had similar performances, with best results when trained with nine variables. Based on results shown in the previous four tables, there is no doubt that the best performances were obtained with CasCor networks (Table 11), with no significant differences when the training sets were described by the set of 9 variables or by either of its two chosen subsets (i.e., {Agit, Perm, CO2} and {CER, Perm, Cond}). It is worth mentioning that the two variable subsets were chosen based on human expertise in HCDC cultivations. The comparatively best results obtained by CasCor corroborates the idea that the task at hand can be much better characterized as a regression task than a classification task, in spite of needing an adaptation step such as the softening process, suggested and implemented in this work. Most of the 36 performance values shown in Table 11 can be considered very good and are distributed as: (a) 10 of them represent the start of the supplementary feed within a time interval of 5min the most (earlier or later), of the time predicted by the human expert; (b) 3 are within a 10min time interval; (c) 13 within a 20min time interval and finally (d) 10 are within approximately \u00bdh time interval. As far as the strategies for composing the training sets are concerned, the best results show a draw between choosing one data register per 1.40min and 1 per 3.33, with 5 wins each. The 36 induced CasCor networks were quite small with their hidden neurons numbers (HN) distributed as: (a) 1 HN (5); (b) 2 HNs (14); (c) 3 HNs (11); (d) 4 HNs (5) and (e) 5 HNs (1). So, for generalizing the four datasets a CasCor network with three input variables: Agit, Perm and CO2 would be a good choice. The main goal of the paper is to show that the human-based monitoring and inspection process to detect the suitable moment to start the feed supply, can be successfully implemented using computational methods and, among them, particularly, CoNN algorithms. CoNN algorithms are a convenient type of neural network because they do not require the predefinition of the NN architecture, prior to learning and, consequently, the whole process can be simplified. Also, as CoNN algorithms define the NN architecture along with its training, their use does not require a parameter adjustment phase (i.e., model selection) where several models should be induced and the best one (on a validation set) is chosen. This process is computationally costly, requires more available empirical data and can be unproductive in real world applications since any modification in either, training or validation data, can have a deep impact on the classifier results. As consequence, a method that does not require model selection and achieves just about the same results, can be a better choice over a parametric method that requires a high cost phase for model selection. Tables 12\u201314 present results related to experiments using decision trees (DT), support vector machines (SVM) and FFNN (MLP), for comparison purposes. The same approach and conventions established before, for presenting CoNN related tables, have been adopted. The simulation experiments using DT, SVM and MLP were done using MatLab on a Windows Vista environment (toolboxes Bioinformatics, Statistics and Neural Network). The implementation of the decision tree (DT) algorithm used the Gini's diversity index as splitting criterion. During the construction of the decision tree the following heuristic was adopted: nodes in the tree needed to group more than 10 patterns to be further split. Also, pruning was carried out on the final tree [16]. The kernel used for implementing SVM was a Gaussian radial basis function with scaling factor of \u03c3 =1 and the least square as optimization method [88,82]. The feedforward neural network (MLP) implements the Resilient Backpropagation algorithm [77] which is considered to be a fast variation of the popular Backpropagation algorithm [81]. For each experiment, to define the MLP architecture to work with, several network architectures were considered (varying the number of neurons in the hidden layer from 2 to 25); the one with the best performance was chosen. As can be seen in the third column of Tables 12\u201314, the number of hidden neurons in the hidden layer varied, depending on the data. DT showed to be the most data dependent method among all; its data sensitivity can be detected by inspecting its unstable results. On the one hand it had a stable performance when attributes CER, Perm and Cond were used to represent data (second column of Table 12) and on the other, when using attributes Agit, Perm and CO2, its performance improved in C4 and degraded in C1 (second column of Table 13). When data was described by all the nine attributes, its performance degraded in C2 (second column of Table 14). Results obtained with SVM followed a similar pattern to those of DT when using CER, Perm and Cond, except in C3, where its performance degraded. The best performances regarding SVM were obtained using data described by Agit, Perm and CO2 and taking data registers at every 1.00min. It is important to mention that SVM could also have its parameter tuned for model selection, prior to learning, which would certainly reflect positively on its global performance. Also, a wide variation of SVM and kernel methods could be considered to handle this particular problem; to pursue this path, however, would take us away from the current line of investigation and its main goal. Taking into account the results obtained with the five CoNN as well as with the DT, SVM and MLP methods, described in the previous seven tables as well as effort (time), it can be said that the proposed softened CasCor had the best performance. As a CoNN method the softened CasCor does not require the time consuming task of going through a model selection phase and, as far as results are concerned, it had a stable performance in al simulation experiments. Excluding the softened CasCor, the best results were obtained with the MLP implementing the Resilient Backpropagation; its good results, however, were partially due to the fact that it was the only method that went through a model selection phase. MLP results, however, corroborate the fact that the on-line prediction of the feeding phase in HCDC can be carried out by a controlling system implementing neural networks. It is worth mentioning that during the experiments the MLP was also implemented with Backpropagation; the approach, however, was unsuccessful in defining a suitable threshold, probably due to overfitting. The process of tuning parameters of a model can be highly unproductive in real world applications, such as HCDC. Small data modifications, either in training or validation data, can strongly impact the results of a classifier. Therefore, to get a near best performance, model selection must be carried out whenever a new situation is presented or, then, periodically scheduled, since incoming data may change and follow a different pattern over time. In the light of this scenario a method that does not require model selection and achieves just about the same results, is preferred over a parametric method that requires a costly model selection phase. 7 Conclusions The main goal of research work described in this paper was to propose, implement and analyze results related to the use of computational intelligent techniques, particularly constructive neural networks, for the on-line monitoring and control of sub-processes in high cell density cultivations of recombinant E. coli (BL21(DE3)). As mentioned in the introduction to this paper, this research is part of a broader project aiming at investigating the feasibility of producing a pneumococcal vaccine conforming to Brazilian population PS serotype profile. The paper also intended to contextualize the vaccine research area particularly related to the design and production of pneumococcal vaccines, stressing some of its many problems and the high complexity involved in the whole process. The proposal described in Section 6 is part of the hybrid research area that gathers knowledge from Computational Intelligence, Chemical Engineering, Microbiology and Molecular Microbiology, to name a few, in an attempt to contribute to the processes of vaccine design and production, by investigation the proposal of CoNN as part of a computational system for detecting the start of the feed batch phase in rE. coli HCDC. Results obtained using the proposed softened CasCor networks are promising and can, in fact, support the expansion of CasCor networks into an on-line monitoring system. As reminded in [43], in spite of the incredible number of studies about the immunogenic characteristics of PS vaccines, the know-how of large industrial scale production and purification processes is not for free online access, the literature on the issue is scarce and most of it is related to patents. It is important to remind that the computational intelligent techniques implemented in this work deal with sensor-produced data from HCDC environments; such types of environments are dynamic, nonstationary and evolving, characteristics that added an extra degree of challenge to achieve the main goal of the work. Nevertheless we believe that this paper, by investigating and showing the feasibility of the on-line prediction of the feeding phase in HCDC environments, has contributed to both: enlarging the scope of applications of computational techniques in nonstationary and evolving environments, particularly of constructive neural networks and, also, helping the automation of a vital process to the vaccine production in large scale. Acknowledgments The first author thanks FAPESP for the two-month visiting researcher grant received and the second author thanks FAPESP for the researcher grant received and all authors thank the reviewers for suggestions on an earlier version of this paper. References [1] B. Alberts A. Johnson J. Lewis M. Raff K. Roberts P. Walter Molecular Biology of the Cell 2002 Garland Science-Taylor & Francis Group NY [2] E. Amaldi B. Guenin Two constructive methods for designing compact feedforward networks of threshold units International Journal of Neural System 8 5 1997 629 645 [3] R. Austrian R.M. Douglas G. Schiffman A.M. Coetzee H.J. Koornhof S. Hayden-Smith R.D. Reid Prevention of pneumococcal pneumonia by vaccination Transactions of the Association of American Physicians 89 1976 184 194 [4] R. Austrian The pneumococcus at the millennium: not down, not out The Journal of Infectious Diseases 179 Suppl. 2 1999 S338 S341 [5] M.A. Barocchi S. Censini R. Rappuoli Vaccines in the era of genomics: the pneumococcal challenge Vaccine 25 2007 2963 2973 [6] P. B\u00e4ttig L.J. Hathaway S. Hofer K. M\u00fchlemann Serotype-specific invasiveness and colonization prevalence in Streptococcus pneumoniae correlate with the lag phase during in vitro growth Microbes and Infection 8 2006 2612 2617 [7] S. Bergmann S. Hammerschmidt Versatility of pneumococcal surface proteins Microbiology 152 2006 295 303 [8] J.R. Bertini Jr. M.C. Nicoletti E.R. Hruschka Jr. A comparative evaluation of constructive neural networks methods using PRM and BCP as TLU training algorithms Proceedings of the IEEE SMC Taipei, Taiwan 2006 pp. 3497\u20133502 [9] J.R. Bertini Jr. M.C. Nicoletti A constructive neural network algorithm based on the geometric concept of barycenter of convex hull L. Rutkowski R. Tadeusiewiza L.A. Zadeh J. Zurda Computational Intelligence: Methods and Applications vol. 1 2008 Academic Publishing House EXIT Warsaw 1 12 [10] S. Black H. Shinefield B. Fireman E. Lewis P. Ray J.R. Hansen L. Elvin K.M. Ensor J. Hackell G. Siber F. Malinoski D. Madore I. Chang R. Kohberger W. Watson R. Austrian K. Edwards Efficacy, safety and immunogenicity of heptavalent pneumococcal conjugate vaccine in children The Pediatric Infectious Diseases Journal 19 2000 187 195 [11] D. Bogaert R. de Groot P.W.M. Hermans Streptococcus pneumoniae colonization: the key to pneumococcal disease The Lancet Infectious Diseases 4 2004 144 154 [12] D. Bogaert P.W.M. Hermans P.V. Adrian H.C. R\u00fcmke R. de Groot Pneumococcal vaccines: an update on current strategies Vaccine 22 2004 2209 2220 [13] M.C. Brandileone V.S. Vieira R.C. Zanella I.M. Landgraf C.E. Melles A. Taunay J.C. de Morais R. Austrian Distribution of serotypes of Streptococcus pneumoniae isolated from invasive infections over a 16 year period in the greater S Paulo, Brazil Journal of Clinical Microbiology 33 1995 2788 2791 [14] M.C. Brandileone A.L. de Andrade J.L. Di Fabio M.L. Guerra R. Austrian Appropriateness of a pneumococcal conjugate vaccine in Brazil: potential impact of age and clinical diagnosis, with emphasis on meningitis The Journal of Infections Diseases 187 2003 1206 1212 [15] M.C. Brandileone A.L. Andrade E.M. Teles R.C. Zanella T.I. Yara J.L. Di Fabio S.K. Hollingshead Typing of pneumococcal surface protein A (PspA) in Streptococcus pneumoniae isolated during epidemiological surveillance in Brazil: towards novel pneumococcal protein vaccines Vaccine 22 2004 3890 3896 [16] L. Breiman J. Friedman C.J. Stone Classification and Regression Trees 1993 Chapman & Hall Boca Raton, USA [17] D.E. Briles R.C. Tart E. Swiatlo J.P. Dillard P. Smith K.A. Benton B.A. Ralph A. Brooks-Walter M.J. Crain S.K. Hollingshead L.S. McDaniel Pneumococcal diversity: considerations for new vaccine strategies with emphasis on pneumococcal surface protein A (PspA) Clinical Microbiology Reviews 11 4 1998 645 657 [18] G.A.W. Bruyn R. van Furth Pneumococcal polysaccharide vaccines: indications, efficacy and recommendations European Journal of Clinical Microbiology Infectious Diseases 10 11 1991 897 910 [19] N. Burgess A constructive algorithm that converges for real-valued input patterns International Journal of Neural Systems 5 1 1994 59 66 [20] J.C. Butler E.D. Shapiro G.M. Carlone Pneumococcal vaccines: history, current status and future directions The American Journal of Medicine 107 1A 1999 69S 76S [21] Centers for Disease Control and Prevention. Division of Bacterial and Mycotic Diseases. Surveillance Reports: Streptococcus pneumoniae [Active bacterial core surveillance Website], 1998, Available at: http://www.cdc.gov/ncidod/dbmd/abcs/survreports.htm (accessed August 25, 2011). [22] Centers for Disease Control and Prevention. Preventing pneumococcal disease among infants and young children. Recommendations of the Advisory Committee on Immunization Practices (ACIP). MMWR Recomm. Rep. 2000 October 6, 49(rr-9): 1\u201338. Available at: http://www.cdc.gov/mmwr/preview/mmwrhtml/rr4909a1.htm (accessed August 24, 2011). [23] J.H. Choi K.C. Keum S.Y. Lee Production of recombinant proteins by high cell density culture of Escherichia coli Chemical Engineering Science 61 2006 876 885 [24] C.P. Chou Engineering cell physiology to enhance recombinant protein production in Escherichia coli Applied Microbiology and Biotechnology 76 2007 521 532 [25] J.W. Costerton R.T. Irvin K.-J. Cheng I.W. Sutherland The role of bacterial surface structures in pathogenesis Critical Reviews in Microbiology 8 4 1981 303 338 [26] M.J. Crain W.D.H. Waltman J.S. Turner J. Yother D.F. Talkingon L.S. McDaniel B.M. Gray D.D. Briles Pneumocococcal surface protein A (PspA) is serologically highly variable and is expressed by all clinically important capsular serotypes of Streptcoccus pneumoniae Infection and Immunity 58 1990 3293 3299 [27] F.C. Csordas C.T. Perciani M. Darrieux V.M. Gon\u00e7alves J. Cabrera-Crespo M. Takagi M.E. Sbrogio-Almeida L.C.C. Leite M.M. Tanizaki Protection induced by pneumococcal surface protein A (PspA) is enhanced by conjugation to a Streptococcus pneumoniae capsular polysaccharide Vaccine 26 2008 2925 2929 [28] F.T. Cutts S.M. Zaman G. Enwere S. Jaffar O.S. Levine J.B. Okoko C. Oluwalana A. Vaughan S.K. Obaro A. Leach K.P. McAdam E. Biney M. Saaka U. Onwuchekwa F. Yallop N.F. Pierce B.M. Greenwood R.A. Adegbola Efficacy of nine-valent pneumococcal conjugate vaccine against pneumonia and invasive pneumococcal disease in The Gambia: randomized, double-blind, placebo-controlled trial The Lancet 365 2005 1139 1146 [29] L.M. Dagan Changing the ecology of pneumococci with antibiotics and vaccines E. Tuomanen D.A. Morrison B.G. Spratt The Pneumococcus 2004 ASM Washington, DC 283 313 [30] M. Darrieux E.N. Miyaji D.M. Ferreira L.M. Lopes A.P.Y. Lopes B. Ren D.E. Briles S.K. Hollingshead L.C.C. Leite Fusion proteins containing family 1 and family 2 PspAs elicit protection against Streptococcus pneumoniae that correlates with antibody-mediated enhancement of complement deposition Infection and Immunity 75 2007 5930 5938 [31] S. Fahlman Faster learning variations on backpropagation: an empirical study Proceedings of the 1988 Connectionist Models Summer School, Morgan Kaufmann 1988 [32] S. Fahlman, An empirical study of learning speed in backpropagation networks. School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, Technical Report CMU-CS-88-162 (1988b). [33] S. Fahlman C. Lebiere The cascade correlation architecture D.S. Touretzky Advances in Neural Information Processing Systems 2 1990 Morgan Kaufman San Francisco, CA 524 532 [34] L. Franco D.A. Elizondo J.M. Jerez Constructive Neural Networks, Studies in Computational Intelligence vol. 258 2009 Springer-Verlag Heidelberg [35] M. Frean The upstart algorithm: a method for constructing and training feedforward neural networks Neural Computation 2 1990 198 209 [36] S.I. Gallant Three constructive algorithms for network learning Proceedings of the Eighth Annual Conference of the Cognitive Science Society, Amherst, MA 1986 pp. 652\u2013660 [37] S.I. Gallant Neural Network Learning & Expert Systems 1994 The MIT Press Cambridge, MA [38] R.C. Giordano J.R. Bertini Jr. M.C. Nicoletti R.L.C. Giordano Online filtering of CO2 signals from a bioreactor gas outflow using a committee of constructive neural networks Bioprocess and Biosystems Engineering 31 2 2008 101 109 [39] S.H. Gillespie Aspects of pneumococcal infection including bacterial virulence, host response and vaccination International Journal of Medical Microbiology 28 1989 237 248 [40] S. Gnoth M. Jenzsch R. Simutis A. L\u00fcbbert Control of cultivation processes for recombinant protein production: a review Bioprocess and Biosystems Engineering 31 1 2008 21 39 [41] V.M. Gon\u00e7alves M. Takagi R.B. Lima H. Massaldi R.C. Giordano M.M. Tanizaki Purification of capsular polysaccharide from Streptococcus pneumoniae serotype 23F by a procedure suitable for scale-up Biotechnology and Applied Biochemistry 37 2003 283 287 [42] V.M. Gon\u00e7alves M. Takagi S.M. Carneiro R.C. Giordano M.M. Tanizaki Introduction of air in the anaerobic culture of Streptococcus pneumoniae serotype 23F induces the release of capsular polysaccharide from bacterial surface into the cultivation medium Journal of Applied Microbiology 101 2006 1009 1014 [43] V. Maimoni Gon\u00e7alves, M. Takagi, T. Souza Carmo, R. Barbosa Lima, S. Maria Ferreira Albani, J. Ventura Pinto, T. Cristina Zangirolami, R. de Campos Giordano, M. Massako Tanizaki, J. Cabrera-Crespo, Simple and efficient method of bacterial polysaccharides purification for vaccines production using hydrolytic enzymes and tangential flow ultrafiltration, in: A. M\u00e9ndez-Vilas (Ed.), Communicating Current Research and Educational Topics and Trends in Applied Microbiology, Microbiology Book Series, vol. 1, 2007, pp. 450\u2013457, http://www.formatex.org/microbio/pdf/Pages450-457.pdf. [44] A.C.L. Horta, T.C. Zangirolami, R.C. Giordano, A.J.G. Cruz, G.B. Reis, C.D.F. Jesus (2010). Supervisory system for bioreactor high scell density cultivations. Application for software registration, No. 0000271004222441 INPI, Brazil. [45] A.C.L. Horta A.J. Silva C.R. Sargo V.M. Gon\u00e7alves T.C. Zangirolami R.C. Giordano Robust artificial intelligence tool for automatic start-up of the supplementary medium feeding in recombinant E. coli cultivations Bioprocess and Biosystems Engineering 34 7 2011 891 901 [46] D.J. Isaacman E.D. McIntosh R.R. Reinert Burden of invasive pneumococcal disease and serotype distribution among Streptococcus pneumoniae isolates in young children in Europe: impact of the 7-valent pneumococcal conjugate vaccine and considerations for future conjugate vaccines International Journal of Infectious Diseases 14 2010 e197 e209 [47] C.R. Jackson Clinical experience with pneumococcal conjugate vaccines in infants and children Journal of the American Osteopathic Association 102 8 2002 431 436 [48] M.J. Jedrzejas Pneumococcal virulence factors: structure and function Microbiology and Molecular Biology Reviews 2001 187 207 [49] M.J. Jedrzejas S.K. Hollingshead J. Lebowitz L. Chantalat D.E. Briles E. Lamani Production and characterization of the functional fragment of pneumococcal surface protein A Archives of Biochemistry and Biophysics 373 1 2000 116 125 [50] M.J. Jedrzejas E. Lamani R.S. Becker Characterization of selected strains of pneumococcal surface protein A The Journal of Biological Chemistry 276 35 2001 33121 33128 [51] T. Jefferson E. Ferroni F. Curtale P.G. Rossi P. Borgia Streptococcus pneumoniae in Western Europe: serotype distribution and incidence in children less than 2 year old The Lancet Infectious Diseases 6 2006 405 410 [52] A. Kadioglu F.N. Weiser J.C. Paton P.W. Andrew The role of Streptococcus pneumoniae virulence factors in host respiratory colonization and disease Nature 6 2008 288 301 [53] P.D. Karp M. Riley S.M. Paley A. Pellegrini-Toole M. Krummenacker EcoCyc: encyclopedia of E. coli genes and metabolism Nucleic Acids Research 25 1 1999 43 50 [54] I.M. Keseler C. Bonavides-Mart\u00ednez J. Collado-Vides S. Gama-Castro R.P. Gunsalus D.A. Johnson M. Krummenacker L.M. Nolan S. Paley I.T. Paulsen M. Peralta-Gil A. Santos-Zavaleta A.G. Shearer P.D. Karp EcoCyc: a comprehensive view of Escherichia coli biology Nucleic Acids Research 37 Suppl. 1 2009 D464 D470 [55] B.S. Kim S.C. Lee S.Y. Lee Y.K. Chang H.N. Chang High cell density fed-batch cultivation of Escherichia coli using exponential feeding combined with pH-stat Bioprocess and Biosystems Engineering 26 2004 147 150 [56] K.P. Klugman S.A. Madhi R.E. Huebner R. Kohberger N. Mbelle N. Pierce A trial of a 9-valent pneumococcal conjugate vaccine in children with and those without HIV infection The New England Journal of Medicine 349 14 2003 1341 1348 [57] D.J. Korz U. Rinas K. Hellmuth E.A. Snaders E.A. Deckwer Simple fed-batch technique for high cell density cultivation of Escherichia coli Journal of Biotechnology 39 1995 59 65 [58] T.-Y. Kwok D.-Y. Yeung Constructive algorithms for structure learning in feedforward neural networks for regression problems IEEE Transactions on Neural Networks 8 1999 630 645 [59] S.Y. Lee High cell-density culture of Escherichia coli Trends in Biotechnology 14 3 1996 98 105 [60] C. Liberman M. Takagi J. Cabrera-Crespo M.E. Sbrogio-Almeida W.O. Dias L.C.C. Leite V.M. Gon\u00e7alves Pneumococcal whole-cell vaccine: optimization of cell growth of unencapsulated Streptococcus pneumoniae in bioreactor using animal-free medium Journal of Industrial Microbiology Biotechnology 35 2008 1441 1445 [61] Y.-C. Liu L.-C. Liao W.-T. Wu Cultivation of recombinant Escherichia coli to achieve high cell density with a high level of penicillin G acylase activity Proceedings of the National Science Council, Republic of China (B) 24 4 2000 156 160 [62] G.W. Luli W.W. Strohl Comparison of growth, acetate production and acetate inhibition of Escherichia coli strains in batch and fed-batch fermentations Applied and Environmental Microbiology 56 4 1990 1004 1011 [63] S.A. Madhi P. Adrian L. Kuwanda W. Jassat S. Jones T. Little A. Soininen C. Cutland K.P. Klugman Long-term immunogenicity and efficacy of a 9-valent conjugate pneumococcal vaccine in human immunodeficient virus infected and non-infected children in the absence of a booster dose of vaccine Vaccine 25 2007 2451 2457 [64] P. Mangtani F. Cutts A.J. Hall Efficacy of polysaccharide pneumococcal vaccine in adults in more developed countries: the state of the evidence The Lancet Infectious Diseases 3 2003 71 78 [65] K.A. Melton Pneumococcal vaccine Primary Care Update OB/GYNS 8 1 2001 44 47 [66] M. M\u00e9zard J. Nadal Learning feedforward networks: the tiling algorithm Journal of Physics A: Mathematical and General 22 1989 2191 2203 [67] E.N. Miyaji W.O. Dias M. Gamberini V.C. Gebara R.P. Schenkman J. Wild P. Riedl J. Reimann R. Schirmbeck L.C. Leite PsaA (pneumococcal surface adhesin A) and PspA (pneumococcal surface protein A) DNA vaccines induce humoral and cellular immune responses against Streptococcus pneumoniae Vaccine 20 2002 805 812 [68] N. Murata S. Yoshizawa S.I. Amari Network information criterion \u2013 determining the number of hidden units for an artificial neural network model IEEE Transactions on Neural Networks 5 1994 865 872 [69] M.C. Nicoletti, J.R. Bertini Jr., D.A. Elizondo, L. Franco, J.M. Jerez, Constructive neural network algorithms for feedforward architectures suitable for classification tasks, Chapter 1, in: L. Franco, D.A. Elizondo, J.M. Jerez (Eds.), Constructive Neural Networks, Studies in Computational Intelligence, Vol. 258, Springer-Verlag, Berlin, 2009a, pp. 1\u201323. [70] M.C. Nicoletti, L.C. Jain, R.C. Giordano, Computational intelligence techniques as tools for bioprocess modeling, optimization, supervision and control, Chapter 1, in: M.C. Nicoletti, L.C. Jain (Eds.), Computational Intelligence Techniques for Bioprocess Modelling, Supervision and Control, Studies in Computational Intelligence, Vol. 218, Springer-Verlag, Berlin, 2009b, pp. 1\u201323. [71] J.C. Paton P.W. Andrew G.J. Boulnois T.J. Mitchell Molecular analysis of the pathogenicity of Streptococcus pneumoniae: the role of pneumococcal proteins Annual Review of Microbiology 47 1993 89 115 [72] F.C. Pimenta F. Ribeiro-Dias M.C. Brandileonie E.N. Miyaji L.C.C. Leite A.L. Sgambatti de Andrade Genetic diversity of PspA types among nasopharyngeal isolates collected during an ongoing surveillance study of children in Brazil Journal of Clinical Microbiology 44 2006 2838 2843 [73] G.A. Poland The burden of pneumococcal disease: the role of conjugate vaccines Vaccine 17 13/14 1999 1674 1679 [74] H. Poulard Baricentric correction procedure: a fast method for learning threshold unit Proceedings of the WCNN 95 1 1995 710 713 [75] R.S. Renner, Improving generalization of constructive neural networks using ensembles. Ph.D. Dissertation, Florida State University, Dept. of Computer Science, Tallahassee, Fl., USA (1990). [76] D.J. Ridgen M.Y. Galperin M.J. Jedrzejas Analysis of structure and function of putative surface-exposed proteins encoded in the Streptococcus pneumoniae genome: a bioinformatics-based approach to vaccine and drug design Critical Reviews in Biochemistry and Molecular Biology 38 2 2003 143 168 [77] M. Riedmiller H. Braun A direct adaptive method for faster backpropagation learning: the RPROP algorithm Proceedings of the IEEE International Conference on Neural Networks (ICNN) 1993 pp. 586\u2013591 [78] D. Riesenberg R. Guthke High cell density cultivation of microorganisms Applied Microbiology Biotechnology 51 1999 422 430 [79] B.D. Ripley Pattern Recognition and Neural Networks 1996 Cambridge University Press Cambridge [80] I.S. Roberts The biochemistry and genetics of capsular polysaccharide production in bacteria Annual Review of Microbiology 59 1996 285 315 [81] D.E. Rumelhart G.E. Hinton R.J. Williams Learning internal representations by error propagation D.E. Rumelhard J.L. McClelland P.D.P. the Research Group Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations 1986 MIT Press Cambridge, MA 318 362 [82] B. Scholkopf A.J. Smola Learning with Kernels 2002 MIT Press Cambridge, MA [83] A. Seeger B. Schneppe J.E.G. McCarthy W.D. Deckwer U. Rinas Comparion of temperature and isopropyl-P-d-thiogalacto-pyranoside-induced systhesys of basic fibroblast growth factor in high-cell-density cultures of recombinant Escherichia coli Enzyme and Microbial Technology 17 1995 947 953 [84] J. Shiloach R. Fass Growing E. coli to high cell density \u2013 a historical perspective on method development Biotechnology Advances 23 2005 345 357 [85] M. Silva J. Cabrera-Crespo M.E. Sbrogio-Almeida E.N. Miyaji P.L. Ho L.C.C. Leite A.P.Y. Lopes Optimizing expression of Streptococcus pneumoniae surface protein A, PspA: serocross-reactivity within families of antisera induced against clades 1 and 3 Molecular Biotechmology 37 2007 146 154 [86] J. Soini K. Ukkonen P. Neubauer High cell density media for Escherichia coli are generally designed for aerobic cultivations \u2013 consequences for large-scale bioprocesses and shake flash cultures Microbial Cell Factories 7 26 2008 1 11 [87] J.A. Starr G.W. Fox J.K. Clayton Streptococcus pneumoniae: an update on resistance patterns in the United States Journal of Pharmacy Practice 21 5 2008 363 370 [88] J.A.K. Suykens T. Van Gestel J. De Brabanter B. De Moor J. Vandewalle Least Squares Support Vector Machines 2002 World Scientific Singapore [89] K. Terpe Overview of bacterial expression systems for heterologous protein production: from molecular and biochemical fundamentals to commercial systems Applied Microbiology Biotechnology 72 2006 211 222 [90] A. Tomasz A. Albino E. Zanati Multiple antibiotic resistance in a bacterium with suppressed autolytic system Nature 227 1970 138 140 [91] T. van der Poll S.M. Opal Patogenesis, treatment, and prevention of pneumococcal pneumonia The Lancet 374 2009 1543 1556 [92] M. V\u00e4kev\u00e4inen C. Eklund J. Eskola H. K\u00e4yhty Cross-reactivity of antibodies to type 6B and 6A polysaccharides of Streptococcus pneumoniae, evoked by pneumococcal conjugate vaccines in infants The Journal of Infectious Diseases 184 2001 789 793 [93] D.A. Watson D.M. Musher J.W. Jacobson J. Verhoef A brief history of the pneumococcus in biomedical research: a panoply of scientific discovery Clinical Infectious Diseases 17 1993 913 924 [94] D.M. Weinberger R. Dagan N. Givon-Levi G. Regev-Yochay R. Malley M. Lipsitch Epidemiologic evidence for serotype-specific acquired immunity to pneucoccal carriage The Journal of Infectious Diseases 197 2008 1511 1518 [95] C. Whitfield M.A. Valvano Synthesis and expression of cell surface polysaccharides in Gram-negative bacteria Advances in Microbial Physiology 35 1993 135 146 [96] A.E. Wright W. Morgan L. Colbrook R.W. Dodgson Observations on prophylactic inoculations against pneumococcus infection, and on the results with have been achieved by it The Lancet 1 1914 87 95", "scopus-id": "84878495505", "pubmed-id": "23566708", "coredata": {"eid": "1-s2.0-S0169260713000813", "dc:description": "Abstract Streptococcus pneumoniae (pneumococcus) is a bacterium responsible for a wide spectrum of illnesses. The surface of the bacterium consists of three distinctive membranes: plasmatic, cellular and the polysaccharide (PS) capsule. PS capsules may mediate several biological processes, particularly invasive infections of human beings. Prevention against pneumococcal related illnesses can be provided by vaccines. There is a sound investment worldwide in the investigation of a proteic antigen as a possible alternative to pneumococcal vaccines based exclusively on PS. A few proteins which are part of the membrane of the pneumococcus seem to have antigen potential to be part of a vaccine, particularly the PspA. A vital aspect in the production of the intended conjugate pneumococcal vaccine is the efficient production (in industrial scale) of both, the chosen PS serotypes as well as the PspA protein. Growing recombinant Escherichia coli (rE. coli) in high-cell density cultures (HCDC) under a fed-batch regime requires a refined continuous control over various process variables where the on-line prediction of the feeding phase is of particular relevance and one of the focuses of this paper. The viability of an on-line monitoring software system, based on constructive neural networks (CoNN), for automatically detecting the time to start the fed-phase of a HCDC of rE. coli that contains a plasmid used for PspA expression is investigated. The paper describes the data and methodology used for training five different types of CoNNs, four of them suitable for classification tasks and one suitable for regression tasks, aiming at comparatively investigate both approaches. Results of software simulations implementing five CoNN algorithms as well as conventional neural networks (FFNN), decision trees (DT) and support vector machines (SVM) are also presented and discussed. A modified CasCor algorithm, implementing a data softening process, has shown to be an efficient candidate to be part of an on-line HCDC monitoring system for detecting the feeding phase of the HCDC process.", "openArchiveArticle": "false", "prism:coverDate": "2013-07-31", "openaccessUserLicense": "http://www.elsevier.com/open-access/userlicense/1.0/", "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0169260713000813", "dc:creator": [{"@_fa": "true", "$": "Nicoletti, M.C."}, {"@_fa": "true", "$": "Bertini, J.R."}, {"@_fa": "true", "$": "Tanizaki, M.M."}, {"@_fa": "true", "$": "Zangirolami, T.C."}, {"@_fa": "true", "$": "Gon\u00e7alves, V.M."}, {"@_fa": "true", "$": "Horta, A.C.L."}, {"@_fa": "true", "$": "Giordano, R.C."}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0169260713000813"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0169260713000813"}], "dc:format": "application/json", "openaccessType": "Full", "pii": "S0169-2607(13)00081-3", "prism:volume": "111", "prism:publisher": "Elsevier Ireland Ltd.", "dc:title": "On-line prediction of the feeding phase in high-cell density cultivation of rE. coli using constructive neural networks", "prism:copyright": "Copyright \u00a9 2013 Elsevier Ireland Ltd.", "openaccess": "1", "prism:issn": "01692607", "prism:issueIdentifier": "1", "dcterms:subject": [{"@_fa": "true", "$": "Immunology"}, {"@_fa": "true", "$": "Vaccine production"}, {"@_fa": "true", "$": "High-cell density cultivations"}, {"@_fa": "true", "$": "Bioprocess control"}, {"@_fa": "true", "$": "Constructive neural networks"}], "openaccessArticle": "true", "prism:publicationName": "Computer Methods and Programs in Biomedicine", "prism:number": "1", "openaccessSponsorType": "FundingPartnerOpenArchive", "prism:pageRange": "228-248", "prism:endingPage": "248", "prism:coverDisplayDate": "July 2013", "prism:doi": "10.1016/j.cmpb.2013.03.005", "prism:startingPage": "228", "dc:identifier": "doi:10.1016/j.cmpb.2013.03.005", "openaccessSponsorName": "Brazilian Government"}, "objects": {"object": [{"@category": "thumbnail", "@height": "34", "@width": "424", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1618", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "high", "@height": "1216", "@width": "2947", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr9_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "316943", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1234", "@width": "2905", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr8_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "442409", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1275", "@width": "2903", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr7_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "456450", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1429", "@width": "1675", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr5_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "150013", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "1372", "@width": "3123", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr10_lrg.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-HIGH-RES", "@size": "354578", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "high", "@height": "2296", "@width": "5416", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr6_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "92672", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "high", "@height": "466", "@width": "3166", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr4_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "14647", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "high", "@height": "3306", "@width": "3430", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr3_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "128897", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "high", "@height": "2332", "@width": "3234", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr2_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "44418", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "high", "@height": "2274", "@width": "2696", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr1_lrg.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-HIGH-RES", "@size": "64606", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "275", "@width": "666", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "47664", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "279", "@width": "656", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "56187", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "288", "@width": "656", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "60090", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "322", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28635", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "310", "@width": "705", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "52612", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "standard", "@height": "259", "@width": "612", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr6.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "13150", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "53", "@width": "357", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "1983", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "373", "@width": "387", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "16122", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "263", "@width": "365", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "5488", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "256", "@width": "304", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "8216", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "93", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2901", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "32", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1062", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "170", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4458", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "158", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2777", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "194", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4258", "@ref": "gr1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "90", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3495", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "93", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4286", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "96", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4265", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "164", "@width": "192", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4666", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "96", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0169260713000813-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3657", "@ref": "gr10", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84878495505"}}